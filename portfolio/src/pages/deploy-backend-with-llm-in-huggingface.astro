---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Desplegar backend con LLM en HuggingFace';
const end_url = 'deploy-backend-with-llm-in-huggingface';
const description = '¬øQuieres desplegar un backend con tu propio LLM? En este post te explico c√≥mo hacerlo con HuggingFace Spaces, FastAPI, Langchain y Docker.';
const keywords = 'hugging face, fastapi, langchain, docker, backend, llm';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
const opening_tag = '<';
const closing_tag = '>';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=960
    image_height=720
    image_extension=webp
    article_date=2025-03-02+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Desplegar-backend-con-Gradio"><h2>Desplegar backend con Gradio</h2></a>
      <a class="anchor-link" href="#Crear-space"><h3>Crear space</h3></a>
      <a class="anchor-link" href="#C%C3%B3digo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#README.md"><h4>README.md</h4></a>
      <a class="anchor-link" href="#Despliegue"><h3>Despliegue</h3></a>
      <a class="anchor-link" href="#Backend"><h3>Backend</h3></a>
      <a class="anchor-link" href="#Prueba-de-la-API"><h3>Prueba de la API</h3></a>
      <a class="anchor-link" href="#Desplegar-backend-con-FastAPI,-Langchain-y-Docker"><h2>Desplegar backend con FastAPI, Langchain y Docker</h2></a>
      <a class="anchor-link" href="#Crear-space"><h3>Crear space</h3></a>
      <a class="anchor-link" href="#C%C3%B3digo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#Dockerfile"><h4>Dockerfile</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#README.md"><h4>README.md</h4></a>
      <a class="anchor-link" href="#Token-de-HuggingFace"><h3>Token de HuggingFace</h3></a>
      <a class="anchor-link" href="#A%C3%B1adir-el-token-a-los-secrets-del-espacio"><h3>A√±adir el token a los secrets del espacio</h3></a>
      <a class="anchor-link" href="#Despliegue"><h3>Despliegue</h3></a>
      <a class="anchor-link" href="#URL-del-backend"><h3>URL del backend</h3></a>
      <a class="anchor-link" href="#Documentaci%C3%B3n"><h3>Documentaci√≥n</h3></a>
      <a class="anchor-link" href="#Prueba-de-la-API"><h3>Prueba de la API</h3></a>
      <a class="anchor-link" href="#Desplegar-backend-con-Gradio-y-modelo-corriendo-en-el-servidor"><h2>Desplegar backend con Gradio y modelo corriendo en el servidor</h2></a>
      <a class="anchor-link" href="#Crear-Space"><h3>Crear Space</h3></a>
      <a class="anchor-link" href="#C%C3%B3digo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#Prueba-de-la-API"><h4>Prueba de la API</h4></a>
      <a class="anchor-link" href="#Desplegar-backend-con-FastAPI,-Langchain-y-Docker-y-modelo-corriendo-en-el-servidor"><h2>Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor</h2></a>
      <a class="anchor-link" href="#Crear-Space"><h3>Crear Space</h3></a>
      <a class="anchor-link" href="#C%C3%B3digo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#Dockerfile"><h4>Dockerfile</h4></a>
      <a class="anchor-link" href="#Prueba-de-la-API"><h3>Prueba de la API</h3></a>
      <a class="anchor-link" href="#Conclusiones"><h2>Conclusiones</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Desplegar-backend-en-HuggingFace">Desplegar backend en HuggingFace<a class="anchor-link" href="#Desplegar-backend-en-HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este post vamos a ver c√≥mo desplegar un backend en HuggingFace. Vamos a ver c√≥mo hacerlo de dos maneras, mediante la forma com√∫n, creando una aplicaci√≥n con Gradio, y mediante una opci√≥n diferente usando FastAPI, Langchain y Docker</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para ambos casos va a ser necesario tener una cuenta en HuggingFace, ya que vamos a desplegar el backend en un space de HuggingFace.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar-backend-con-Gradio">Desplegar backend con Gradio<a class="anchor-link" href="#Desplegar-backend-con-Gradio"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear-space">Crear space<a class="anchor-link" href="#Crear-space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero de todo, creamos un nuevo espacio en Hugging Face.</p>
      <ul>
      <li>Ponemos un nombre, una descripci√≥n y elegimos la licencia.</li>
      <li>Elegimos Gradio como el tipo de SDK. Al elegir Gradio, nos aparecer√°n plantillas, as√≠ que elegimos la plantilla de chatbot.</li>
      <li>Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t√∫ elige lo que mejor consideres.</li>
      <li>Y por √∫ltimo hay que elegir si queremos crear el espacio p√∫blico o privado.</li>
      </ul>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend gradio - create space" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp" width="880" height="773"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="C%C3%B3digo">C√≥digo<a class="anchor-link" href="#C%C3%B3digo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al crear el space, podemos clonarlo o podemos ver los archivos en la propia p√°gina de HuggingFace. Podemos ver que se han creado 3 archivos, <code>app.py</code>, <code>requirements.txt</code> y <code>README.md</code>. As√≠ que vamos a ver qu√© poner en cada uno</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aqu√≠ tenemos el c√≥digo de la aplicaci√≥n. Como hemos elegido la plantilla de chatbot, ya tenemos mucho hecho, pero vamos a tener que cambiar 2 cosas, primero el modelo de lenguaje y el system prompt</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como modelo de lenguaje veo <code>HuggingFaceH4/zephyr-7b-beta</code>, pero vamos a usar <code>Qwen/Qwen2.5-72B-Instruct</code>, que es un modelo muy capaz.</p>
      <p>As√≠ que busca el texto <code>client = InferenceClient("HuggingFaceH4/zephyr-7b-beta")</code> y reempl√°zalo por <code>client = InferenceClient("Qwen/Qwen2.5-72B-Instruct")</code>, o espera que m√°s adelante pondr√© todo el c√≥digo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tambi√©n vamos a cambiar el system prompt, que por defecto es <code>You are a friendly Chatbot.</code>, pero como es un modelo entrenado en su mayor√≠a en ingl√©s, es probable que si le hablas en otro idioma te responda en ingl√©s, as√≠ que vamos a cambiarlo por <code>You are a friendly Chatbot. Always reply in the language in which the user is writing to you.</code>.</p>
      <p>As√≠ que busca el texto <code>gr.Textbox(value="You are a friendly Chatbot.", label="System message"),</code> y reempl√°zalo por <code>gr.Textbox(value="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.", label="System message"),</code>, o espera a que ahora voy a poner todo el c√≥digo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gradio</span> <span class="k">as</span> <span class="nn">gr</span>
<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>

<span class="sd">"""</span>
<span class="sd">For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference</span>
<span class="sd">"""</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">respond</span><span class="p">(</span>
    <span class="n">message</span><span class="p">,</span>
    <span class="n">history</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="n">system_message</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">system_message</span><span class="p">{closing_brace}]</span>

    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">history</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">val</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">val</span><span class="p">[</span><span class="mi">0</span><span class="p">]{closing_brace})</span>
        <span class="k">if</span> <span class="n">val</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">val</span><span class="p">[</span><span class="mi">1</span><span class="p">]{closing_brace})</span>

    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">message</span><span class="p">{closing_brace})</span>

    <span class="n">response</span> <span class="o">=</span> <span class="s2">""</span>

    <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">client</span><span class="o">.</span><span class="n">chat_completion</span><span class="p">(</span>
        <span class="n">messages</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
        <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span>

        <span class="n">response</span> <span class="o">+=</span> <span class="n">token</span>
        <span class="k">yield</span> <span class="n">response</span>
      
      
<span class="sd">"""</span>
<span class="sd">For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/gradio/chatinterface</span>
<span class="sd">"""</span>
<span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">ChatInterface</span><span class="p">(</span>
    <span class="n">respond</span><span class="p">,</span>
    <span class="n">additional_inputs</span><span class="o">=</span><span class="p">[</span>
        <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"System message"</span><span class="p">),</span>
        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">minimum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Max new tokens"</span><span class="p">),</span>
        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">minimum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Temperature"</span><span class="p">),</span>
        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span>
            <span class="n">minimum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
            <span class="n">maximum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">"Top-p (nucleus sampling)"</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">],</span>
<span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>
</pre></div>
</section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este es el archivo en el que estar√°n escritas las dependencias, pero para este caso va a ser muy sencillo:</p>
      <pre><code class="language-txt">huggingface_hub==0.25.2</code></pre>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="README.md">README.md<a class="anchor-link" href="#README.md"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este es el archivo en el que vamos a poner la informaci√≥n del espacio. En los spaces de HuggingFace, al inicio de los readmes, se pone un c√≥digo para que HuggingFace sepa c√≥mo mostrar la miniatura del espacio, qu√© fichero tiene que usar para ejecutar el c√≥digo, versi√≥n del sdk, etc.</p>
      <div class="highlight"><pre><span></span>---
title: SmolLM2
emoji: üí¨
colorFrom: yellow
colorTo: purple
sdk: gradio
sdk_version: 5.0.1
app_file: app.py
pinned: false
license: apache-2.0
<span class="gu">short_description: Gradio SmolLM2 chat</span>
<span class="gu">---</span>
      
An example chatbot using [<span class="nt">Gradio</span>](<span class="na">https://gradio.app</span>), [<span class="sb">`huggingface_hub`</span>](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index), and the [<span class="nt">Hugging Face Inference API</span>](<span class="na">https://huggingface.co/docs/api-inference/index</span>).
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Despliegue">Despliegue<a class="anchor-link" href="#Despliegue"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.</p>
      <p>As√≠ que cuando est√©n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend gradio - chatbot" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp" width="1882" height="863"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Backend">Backend<a class="anchor-link" href="#Backend"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Muy bien, hemos hecho un chatbot, pero no era la intenci√≥n, aqu√≠ hab√≠amos venido a hacer un backend! Para, para, f√≠jate lo que pone debajo del chatbot</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend gradio - Use via API" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp" width="1882" height="863"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver un texto <code>Use via API</code>, donde si pulsamos se nos abre un men√∫ con una API para poder usar el chatbot.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend gradio - API" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp" width="1882" height="863"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que nos da una documentaci√≥n de c√≥mo usar la API, tanto con Python, con JavaScript, como con bash.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Prueba-de-la-API">Prueba de la API<a class="anchor-link" href="#Prueba-de-la-API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usamos el c√≥digo de ejemplo de Python.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">gradio_client</span> <span class="kn">import</span> <span class="n">Client</span>',
          '',
          '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">"Maximofn/SmolLM2"</span><span class="p">)</span>',
          '<span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
          '		<span class="n">message</span><span class="o">=</span><span class="s2">"Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo"</span><span class="p">,</span>',
          '		<span class="n">system_message</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span>',
          '		<span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '		<span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '		<span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
          '		<span class="n">api_name</span><span class="o">=</span><span class="s2">"/chat"</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2.hf.space ‚úî',
          '¬°Hola M√°ximo! Mucho gusto, estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Estamos haciendo llamadas a la API de <code>InferenceClient</code> de HuggingFace, as√≠ que podr√≠amos pensar, ¬øPara qu√© hemos hecho un backend, si podemos llamar directamente a la API de HuggingFace? Pues lo vas a ver a continuaci√≥n.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
          '		<span class="n">message</span><span class="o">=</span><span class="s2">"¬øC√≥mo me llamo?"</span><span class="p">,</span>',
          '		<span class="n">system_message</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span>',
          '		<span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '		<span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '		<span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
          '		<span class="n">api_name</span><span class="o">=</span><span class="s2">"/chat"</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Tu nombre es M√°ximo. ¬øEs correcto?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>La plantilla de chat de Gradio maneja el historial por nosotros, de manera que cada vez que creamos un nuevo <code>cliente</code>, se crea un nuevo hilo de conversaci√≥n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a probar a crear un nuevo cliente, y ver si se crea un nuevo hilo de conversaci√≥n.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">gradio_client</span> <span class="kn">import</span> <span class="n">Client</span>',
          '',
          '<span class="n">new_client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">"Maximofn/SmolLM2"</span><span class="p">)</span>',
          '<span class="n">result</span> <span class="o">=</span> <span class="n">new_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
          '		<span class="n">message</span><span class="o">=</span><span class="s2">"Hola, ¬øc√≥mo est√°s? Me llamo Luis"</span><span class="p">,</span>',
          '		<span class="n">system_message</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span>',
          '		<span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '		<span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '		<span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
          '		<span class="n">api_name</span><span class="o">=</span><span class="s2">"/chat"</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2.hf.space ‚úî',
          'Hola Luis, estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? Es un gusto conocerte. ¬øEn qu√© puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora le volvemos a preguntar c√≥mo me llamo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">new_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
          '		<span class="n">message</span><span class="o">=</span><span class="s2">"¬øC√≥mo me llamo?"</span><span class="p">,</span>',
          '		<span class="n">system_message</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span>',
          '		<span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '		<span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '		<span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
          '		<span class="n">api_name</span><span class="o">=</span><span class="s2">"/chat"</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, tenemos dos clientes, cada uno con su propio hilo de conversaci√≥n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar-backend-con-FastAPI,-Langchain-y-Docker">Desplegar backend con FastAPI, Langchain y Docker<a class="anchor-link" href="#Desplegar-backend-con-FastAPI,-Langchain-y-Docker"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vamos a hacer lo mismo, crear un backend de un chatbot, con el mismo modelo, pero en este caso usando FastAPI, Langchain y Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear-space">Crear space<a class="anchor-link" href="#Crear-space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que crear un nuevo espacio, pero en este caso lo haremos de otra manera</p>
      <ul>
      <li>Ponemos un nombre, una descripci√≥n y elegimos la licencia.</li>
      <li>Elegimos Docker como el tipo de SDK. Al elegir Docker, nos aparecer√°n plantillas, as√≠ que elegimos una plantilla en blanco.</li>
      <li>Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t√∫ elige lo que mejor consideres.</li>
      <li>Y por √∫ltimo hay que elegir si queremos crear el espacio p√∫blico o privado.</li>
      </ul>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - create space" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp" width="945" height="753"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="C%C3%B3digo">C√≥digo<a class="anchor-link" href="#C%C3%B3digo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora, al crear el space, vemos que solo tenemos un archivo, el <code>README.md</code>. As√≠ que vamos a tener que crear todo el c√≥digo nosotros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a crear el c√≥digo de la aplicaci√≥n</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Empezamos con las librer√≠as necesarias</p>
      <div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">HTTPException</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>

<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
<span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">START</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">StateGraph</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="n">load_dotenv</span><span class="p">()</span>
      </pre></div>
      <p>Cargamos <code>fastapi</code> para poder crear las rutas de la API, <code>pydantic</code> para crear la plantilla de las querys, <code>huggingface_hub</code> para poder crear un modelo de lenguaje, <code>langchain</code> para poder indicarle al modelo si los mensajes son del chatbot o del usuario y <code>langgraph</code> para poder crear el chatbot.</p>
      <p>Adem√°s cargamos <code>os</code> y <code>dotenv</code> para poder cargar las variables de entorno.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cargamos el token de HuggingFace</p>
      <div class="highlight"><pre><span></span><span class="c1"># HuggingFace token</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"HUGGINGFACE_TOKEN"</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_TOKEN"</span><span class="p">))</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el modelo de lenguaje</p>
      <div class="highlight"><pre><span></span><span class="c1"># Initialize the HuggingFace model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_TOKEN"</span><span class="p">)</span>
<span class="p">)</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos ahora una funci√≥n para llamar al modelo</p>
      <div class="highlight"><pre><span></span><span class="c1"># Define the function that calls the model</span>
<span class="k">def</span> <span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Call the model with the given messages</span>

<span class="sd">    Args:</span>
<span class="sd">        state: MessagesState</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary containing the generated text and the thread ID</span>
<span class="sd">    """</span>
    <span class="c1"># Convert LangChain messages to HuggingFace format</span>
    <span class="n">hf_messages</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
            <span class="n">hf_messages</span><span class="o">.</span><span class="n">append</span><span class="p">({opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">{closing_brace})</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
            <span class="n">hf_messages</span><span class="o">.</span><span class="n">append</span><span class="p">({opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">{closing_brace})</span>

    <span class="c1"># Call the API</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">chat_completion</span><span class="p">(</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">hf_messages</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span>
    <span class="p">)</span>

    <span class="c1"># Convert the response to LangChain format</span>
    <span class="n">ai_message</span> <span class="o">=</span> <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ai_message</span><span class="p">]{closing_brace}</span>
</pre></div>
<p>Convertimos los mensajes de formato LangChain a formato HuggingFace, as√≠ podemos usar el modelo de lenguaje.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Definimos una plantilla para las queries</p>
      <div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">QueryRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">query</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">thread_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"default"</span>
</pre></div>
<p>Las queries van a tener un <code>query</code>, el mensaje del usuario, y un <code>thread_id</code>, que es el identificador del hilo de la conversaci√≥n y m√°s adelante explicaremos para qu√© lo usamos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un grafo de LangGraph</p>
      <div class="highlight"><pre><span></span><span class="c1"># Define the graph</span>
<span class="n">workflow</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">state_schema</span><span class="o">=</span><span class="n">MessagesState</span><span class="p">)</span>

<span class="c1"># Define the node in the graph</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"model"</span><span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"model"</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>

<span class="c1"># Add memory</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="n">graph_app</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
</pre></div>
<p>Con esto lo que hacemos es crear un grafo de LangGraph, que es una estructura de datos que nos permite crear un chatbot y que gestiona por nosotros el estado del chatbot, es decir, entre otras cosas, el historial de mensajes. As√≠ no lo tenemos que hacer nosotros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos la aplicaci√≥n de FastAPI</p>
      <div class="highlight"><pre><span></span><span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"LangChain FastAPI"</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"API to generate text using LangChain and LangGraph"</span><span class="p">)</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos los endpoints de la API</p>
      <div class="highlight"><pre><span></span><span class="c1"># Welcome endpoint</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"/"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">api_home</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""Welcome endpoint"""</span>
    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"detail"</span><span class="p">:</span> <span class="s2">"Welcome to FastAPI, Langchain, Docker tutorial"</span><span class="p">{closing_brace}</span>

<span class="c1"># Generate endpoint</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">"/generate"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">QueryRequest</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Endpoint to generate text using the language model</span>

<span class="sd">    Args:</span>
<span class="sd">        request: QueryRequest</span>
<span class="sd">        query: str</span>
<span class="sd">        thread_id: str = "default"</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary containing the generated text and the thread ID</span>
<span class="sd">    """</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Configure the thread ID</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">request</span><span class="o">.</span><span class="n">thread_id</span><span class="p">{closing_brace}{closing_brace}</span>

        <span class="c1"># Create the input message</span>
        <span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">request</span><span class="o">.</span><span class="n">query</span><span class="p">)]</span>

        <span class="c1"># Invoke the graph</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">graph_app</span><span class="o">.</span><span class="n">invoke</span><span class="p">({opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">{closing_brace},</span> <span class="n">config</span><span class="p">)</span>

        <span class="c1"># Get the model response</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>

        <span class="k">return</span> <span class="p">{opening_brace}</span>
            <span class="s2">"generated_text"</span><span class="p">:</span> <span class="n">response</span><span class="p">,</span>
            <span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">request</span><span class="o">.</span><span class="n">thread_id</span>
        <span class="p">{closing_brace}</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="n">status_code</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">detail</span><span class="o">=</span><span class="sa">f</span><span class="s2">"Error al generar texto: </span><span class="si">{opening_brace}</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<p>Hemos creado el endpoint <code>/</code> que nos devolver√° un texto cuando accedamos a la API, y el endpoint <code>/generate</code> que es el que usaremos para generar el texto.</p>
<p>Si nos fijamos en la funci√≥n <code>generate</code> tenemos la variable <code>config</code>, que es un diccionario que contiene el <code>thread_id</code>. Este <code>thread_id</code> es el que nos permite tener un historial de mensajes de cada usuario, de esta manera, diferentes usuarios pueden usar el mismo endpoint y tener su propio historial de mensajes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, tenemos el c√≥digo para que se pueda ejecutar la aplicaci√≥n</p>
      <div class="highlight"><pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">uvicorn</span>
    <span class="n">uvicorn</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">app</span><span class="p">,</span> <span class="n">host</span><span class="o">=</span><span class="s2">"0.0.0.0"</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">7860</span><span class="p">)</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a escribir todo el c√≥digo junto</p>
      <div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">HTTPException</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>

<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
<span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>
<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">START</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">StateGraph</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="n">load_dotenv</span><span class="p">()</span>

<span class="c1"># HuggingFace token</span>
<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"HUGGINGFACE_TOKEN"</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_TOKEN"</span><span class="p">))</span>

<span class="c1"># Initialize the HuggingFace model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_TOKEN"</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Define the function that calls the model</span>
<span class="k">def</span> <span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Call the model with the given messages</span>

<span class="sd">    Args:</span>
<span class="sd">        state: MessagesState</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary containing the generated text and the thread ID</span>
<span class="sd">    """</span>
    <span class="c1"># Convert LangChain messages to HuggingFace format</span>
    <span class="n">hf_messages</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
            <span class="n">hf_messages</span><span class="o">.</span><span class="n">append</span><span class="p">({opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">{closing_brace})</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
            <span class="n">hf_messages</span><span class="o">.</span><span class="n">append</span><span class="p">({opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">{closing_brace})</span>

    <span class="c1"># Call the API</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">chat_completion</span><span class="p">(</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">hf_messages</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span>
    <span class="p">)</span>

    <span class="c1"># Convert the response to LangChain format</span>
    <span class="n">ai_message</span> <span class="o">=</span> <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ai_message</span><span class="p">]{closing_brace}</span>

<span class="c1"># Define the graph</span>
<span class="n">workflow</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">state_schema</span><span class="o">=</span><span class="n">MessagesState</span><span class="p">)</span>

<span class="c1"># Define the node in the graph</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"model"</span><span class="p">)</span>
<span class="n">workflow</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"model"</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>

<span class="c1"># Add memory</span>
<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
<span class="n">graph_app</span> <span class="o">=</span> <span class="n">workflow</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>

<span class="c1"># Define the data model for the request</span>
<span class="k">class</span> <span class="nc">QueryRequest</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">query</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">thread_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">"default"</span>

<span class="c1"># Create the FastAPI application</span>
<span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"LangChain FastAPI"</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"API to generate text using LangChain and LangGraph"</span><span class="p">)</span>

<span class="c1"># Welcome endpoint</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"/"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">api_home</span><span class="p">():</span>
<span class="w">    </span><span class="sd">"""Welcome endpoint"""</span>
    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"detail"</span><span class="p">:</span> <span class="s2">"Welcome to FastAPI, Langchain, Docker tutorial"</span><span class="p">{closing_brace}</span>

<span class="c1"># Generate endpoint</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s2">"/generate"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">QueryRequest</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Endpoint to generate text using the language model</span>

<span class="sd">    Args:</span>
<span class="sd">        request: QueryRequest</span>
<span class="sd">        query: str</span>
<span class="sd">        thread_id: str = "default"</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary containing the generated text and the thread ID</span>
<span class="sd">    """</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Configure the thread ID</span>
        <span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">request</span><span class="o">.</span><span class="n">thread_id</span><span class="p">{closing_brace}{closing_brace}</span>

        <span class="c1"># Create the input message</span>
        <span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">request</span><span class="o">.</span><span class="n">query</span><span class="p">)]</span>

        <span class="c1"># Invoke the graph</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">graph_app</span><span class="o">.</span><span class="n">invoke</span><span class="p">({opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">{closing_brace},</span> <span class="n">config</span><span class="p">)</span>

        <span class="c1"># Get the model response</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>

        <span class="k">return</span> <span class="p">{opening_brace}</span>
            <span class="s2">"generated_text"</span><span class="p">:</span> <span class="n">response</span><span class="p">,</span>
            <span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">request</span><span class="o">.</span><span class="n">thread_id</span>
        <span class="p">{closing_brace}</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="n">status_code</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">detail</span><span class="o">=</span><span class="sa">f</span><span class="s2">"Error al generar texto: </span><span class="si">{opening_brace}</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">uvicorn</span>
    <span class="n">uvicorn</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">app</span><span class="p">,</span> <span class="n">host</span><span class="o">=</span><span class="s2">"0.0.0.0"</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">7860</span><span class="p">)</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dockerfile">Dockerfile<a class="anchor-link" href="#Dockerfile"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      ‚àë<section class="section-block-markdown-cell">
      <p>Ahora vemos c√≥mo crear el Dockerfile</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero indicamos desde qu√© imagen vamos a partir</p>
      <div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">python:3.13-slim</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos el directorio de trabajo</p>
      <div class="highlight"><pre><span></span><span class="k">RUN</span><span class="w"> </span>useradd<span class="w"> </span>-m<span class="w"> </span>-u<span class="w"> </span><span class="m">1000</span><span class="w"> </span>user
<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Copiamos el archivo con las dependencias e instalamos</p>
      <div class="highlight"><pre><span></span><span class="k">COPY</span><span class="w"> </span>--chown<span class="o">=</span>user<span class="w"> </span>./requirements.txt<span class="w"> </span>requirements.txt
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span>--upgrade<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Copiamos el resto del c√≥digo</p>
      <div class="highlight"><pre><span></span><span class="k">COPY</span><span class="w"> </span>--chown<span class="o">=</span>user<span class="w"> </span>.<span class="w"> </span>/app
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Exponemos el puerto 7860</p>
      <div class="highlight"><pre><span></span><span class="k">EXPOSE</span><span class="w"> </span><span class="s">7860</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos las variables de entorno</p>
<div class="highlight"><pre><span></span><span class="k">RUN</span><span class="w"> </span>--mount<span class="o">=</span><span class="nv">type</span><span class="o">=</span>secret,id<span class="o">=</span>HUGGINGFACE_TOKEN,mode<span class="o">=</span><span class="m">0444</span>,required<span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nb">test</span><span class="w"> </span>-f<span class="w"> </span>/run/secrets/HUGGINGFACE_TOKEN<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">"Secret exists!"</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, indicamos el comando para ejecutar la aplicaci√≥n</p>
      <div class="highlight"><pre><span></span><span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">"uvicorn"</span><span class="p">,</span><span class="w"> </span><span class="s2">"app:app"</span><span class="p">,</span><span class="w"> </span><span class="s2">"--host"</span><span class="p">,</span><span class="w"> </span><span class="s2">"0.0.0.0"</span><span class="p">,</span><span class="w"> </span><span class="s2">"--port"</span><span class="p">,</span><span class="w"> </span><span class="s2">"7860"</span><span class="p">]</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora lo ponemos todo junto</p>
      <div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">python:3.13-slim</span>
      
<span class="k">RUN</span><span class="w"> </span>useradd<span class="w"> </span>-m<span class="w"> </span>-u<span class="w"> </span><span class="m">1000</span><span class="w"> </span>user
<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>

<span class="k">COPY</span><span class="w"> </span>--chown<span class="o">=</span>user<span class="w"> </span>./requirements.txt<span class="w"> </span>requirements.txt
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span>--upgrade<span class="w"> </span>-r<span class="w"> </span>requirements.txt

<span class="k">COPY</span><span class="w"> </span>--chown<span class="o">=</span>user<span class="w"> </span>.<span class="w"> </span>/app

<span class="k">EXPOSE</span><span class="w"> </span><span class="s">7860</span>

<span class="k">RUN</span><span class="w"> </span>--mount<span class="o">=</span><span class="nv">type</span><span class="o">=</span>secret,id<span class="o">=</span>HUGGINGFACE_TOKEN,mode<span class="o">=</span><span class="m">0444</span>,required<span class="o">=</span><span class="nb">true</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nb">test</span><span class="w"> </span>-f<span class="w"> </span>/run/secrets/HUGGINGFACE_TOKEN<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">"Secret exists!"</span>

<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">"uvicorn"</span><span class="p">,</span><span class="w"> </span><span class="s2">"app:app"</span><span class="p">,</span><span class="w"> </span><span class="s2">"--host"</span><span class="p">,</span><span class="w"> </span><span class="s2">"0.0.0.0"</span><span class="p">,</span><span class="w"> </span><span class="s2">"--port"</span><span class="p">,</span><span class="w"> </span><span class="s2">"7860"</span><span class="p">]</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el archivo con las dependencias</p>
<pre><code class="language-txt">fastapi
uvicorn
requests
pydantic&gt;=2.0.0
langchain
langchain-huggingface
langchain-core
langgraph &gt; 0.2.27
python-dotenv.2.11</code></pre>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="README.md">README.md<a class="anchor-link" href="#README.md"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, creamos el archivo README.md con informaci√≥n del espacio y con las intrucciones para HugginFace</p>
<div class="highlight"><pre><span></span>---
title: SmolLM2 Backend
emoji: üìä
colorFrom: yellow
colorTo: red
sdk: docker
pinned: false
license: apache-2.0
short_description: Backend of SmolLM2 chat
<span class="gu">app_port: 7860</span>
<span class="gu">---</span>

<span class="gh"># SmolLM2 Backend</span>

This project implements a FastAPI API that uses LangChain and LangGraph to generate text with the Qwen2.5-72B-Instruct model from HuggingFace.

<span class="gu">## Configuration</span>

<span class="gu">### In HuggingFace Spaces</span>

This project is designed to run in HuggingFace Spaces. To configure it:

<span class="k">1.</span> Create a new Space in HuggingFace with SDK Docker
<span class="k">2.</span> Configure the <span class="sb">`HUGGINGFACE_TOKEN`</span> or <span class="sb">`HF_TOKEN`</span> environment variable in the Space configuration:
<span class="w">   </span><span class="k">-</span><span class="w"> </span>Go to the "Settings" tab of your Space
<span class="w">   </span><span class="k">-</span><span class="w"> </span>Scroll down to the "Repository secrets" section
<span class="w">   </span><span class="k">-</span><span class="w"> </span>Add a new variable with the name <span class="sb">`HUGGINGFACE_TOKEN`</span> and your token as the value
<span class="w">   </span><span class="k">-</span><span class="w"> </span>Save the changes

<span class="gu">### Local development</span>

For local development:

<span class="k">1.</span> Clone this repository
<span class="k">2.</span> Create a <span class="sb">`.env`</span> file in the project root with your HuggingFace token:
    ``
    HUGGINGFACE_TOKEN=your_token_here
    ``
<span class="k">3.</span> Install the dependencies:
    ``
    pip install -r requirements.txt
    ``

<span class="gu">## Local execution</span>

``bash
uvicorn app:app --reload
``

The API will be available at <span class="sb">`http://localhost:8000`</span>.

<span class="gu">## Endpoints</span>

<span class="gu">### GET `/`</span>

Welcome endpoint that returns a greeting message.

<span class="gu">### POST `/generate`</span>

Endpoint to generate text using the language model.

<span class="gs">**Request parameters:**</span>
``json
{opening_brace}
  "query": "Your question here",
  "thread_id": "optional_thread_identifier"
{closing_brace}
``

<span class="gs">**Response:**</span>
``json
{opening_brace}
  "generated_text": "Generated text by the model",
  "thread_id": "thread identifier"
{closing_brace}
``

<span class="gu">## Docker</span>

To run the application in a Docker container:

``bash
<span class="gh"># Build the image</span>
docker build -t smollm2-backend .

<span class="gh"># Run the container</span>
docker run -p 8000:8000 --env-file .env smollm2-backend
``

<span class="gu">## API documentation</span>

The interactive API documentation is available at:
<span class="k">-</span><span class="w"> </span>Swagger UI: <span class="sb">`http://localhost:8000/docs`</span>
<span class="k">-</span><span class="w"> </span>ReDoc: <span class="sb">`http://localhost:8000/redoc`</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Token-de-HuggingFace">Token de HuggingFace<a class="anchor-link" href="#A%C3%B1adir-el-token-a-los-secrets-del-espacio"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h3>
      <p>Si te has fijado en el c√≥digo y en el Dockerfile hemos usado un token de HuggingFace, as√≠ que vamos a tener que crear uno. En nuestra cuenta de HuggingFace creamos un <a href="https://huggingface.co/settings/tokens/new?tokenType=fineGrained">nuevo token</a>, le ponemos un nombre y le damos los siguientes permisos:</p>
      <ul>
      <li>Read access to contents of all repos under your personal namespace</li>
      <li>Read access to contents of all repos under your personal namespacev</li>
      <li>Make calls to inference providers</li>
      <li>Make calls to Inference Endpoints</li>
      </ul>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - token" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-token.webp" width="1878" height="1216"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="A%C3%B1adir-el-token-a-los-secrets-del-espacio">A√±adir el token a los secrets del espacio<a class="anchor-link" href="#A%C3%B1adir-el-token-a-los-secrets-del-espacio"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que ya tenemos el token, necesitamos a√±adirlo al espacio. En la parte de arriba de la app, podremos ver un bot√≥n llamado <code>Settings</code>, lo pulsamos y podremos ver la secci√≥n de configuraci√≥n del espacio.</p>
      <p>Si bajamos, podremos ver una secci√≥n en la que podemos a√±adir <code>Variables</code> y <code>Secrets</code>. En este caso, como estamos a√±adiendo un token, lo vamos a a√±adir a los <code>Secrets</code>.</p>
      <p>Le ponemos el nombre <code>HUGGINGFACE_TOKEN</code> y el valor del token.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Despliegue">Despliegue<a class="anchor-link" href="#Despliegue"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.</p>
      <p>As√≠ que cuando est√©n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.</p>
      <p>En este caso, solo hemos construido un backend, por lo que lo que vamos a ver al entrar al espacio es lo que definimos en el endpoint <code>/</code></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - space" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-space.webp" width="2832" height="1360"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="URL-del-backend">URL del backend<a class="anchor-link" href="#URL-del-backend"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Necesitamos saber la URL del backend para poder hacer llamadas a la API. Para ello, tenemos que pulsar en los tres puntos de la parte superior derecha para ver las opciones</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - options" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-options.webp" width="1878" height="1216"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el men√∫ que se despliega pulsamos en <code>Embed this Spade</code>, se nos abrir√° una ventana en la que indica c√≥mo embeber el espacio con un iframe y adem√°s nos dar√° la URL del espacio.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - embed" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-embed.webp" width="1926" height="864"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora nos vamos a esa URL, veremos lo mismo que en el espacio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Documentaci%C3%B3n">Documentaci√≥n<a class="anchor-link" href="#Documentaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>FastAPI, a parte de ser una API rapid√≠sima, tiene otra gran ventaja, y es que genera documentaci√≥n de manera autom√°tica.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si a√±adimos <code>/docs</code> a la URL que vimos antes, podremos ver la documentaci√≥n de la API con <code>Swagger UI</code>.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - swagger doc" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-swagger-doc.webp" width="2834" height="1352"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tambi√©n podemos a√±adir <code>/redoc</code> a la URL para ver la documentaci√≥n con <code>ReDoc</code>.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - redoc doc" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-redoc.webp" width="2834" height="1384"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Prueba-de-la-API">Prueba de la API<a class="anchor-link" href="#Prueba-de-la-API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Lo bueno de la documentaci√≥n <code>Swagger UI</code> es que nos permite probar la API directamente desde el navegador.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A√±adimos <code>/docs</code> a la URL que obtuvimos, abrimos el desplegable del endpoint <code>/generate</code> y le damos a <code>Try it out</code>, modificamos el valor de la <code>query</code> y del <code>thread_id</code> y pulsamos en <code>Execute</code>.</p>
      <p>En el primer caso voy a poner</p>
      <ul>
      <li><strong>query</strong>: Hola, ¬øC√≥mo est√°s? Soy M√°ximo</li>
      <li><strong>thread_id</strong>: user1</li>
      </ul>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - test API" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-test-API.webp" width="2720" height="1334"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Recibimos la siguiente respuesta <code>¬°Hola M√°ximo! Estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?</code></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker -response 1 - user1" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user1.webp" width="2720" height="1282"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a probar ahora la misma pregunta, pero con un <code>thread_id</code> diferente, en este caso <code>user2</code>.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - query 1 - user2" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-query1-user2.webp" width="2720" height="1336"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Y nos responde esto <code>¬°Hola Luis! Estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?</code></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - response 1 - user2" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user2.webp" width="2720" height="1224"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora pedimos nuestro nombre con los dos usuarios y obtenemos esto</p>
      <ul>
      <li>Para el usuario <strong>user1</strong>: <code>Te llamas M√°ximo. ¬øHay algo m√°s en lo que pueda ayudarte?</code></li>
      <li>Para el usuario <strong>user2</strong>: <code>Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte hoy, Luis?</code></li>
      </ul>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - response 2 - user1" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user1.webp" width="2720" height="1224"/></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="backend docker - response 2 - user2" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user2.webp" width="2720" height="1214"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar-backend-con-Gradio-y-modelo-corriendo-en-el-servidor">Desplegar backend con Gradio y modelo corriendo en el servidor<a class="anchor-link" href="#Desplegar-backend-con-Gradio-y-modelo-corriendo-en-el-servidor"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los dos backends que hemos creado en realidad no est√°n corriendo un modelo, sino que est√°n haciendo llamadas a Inference Endpoints de HuggingFace. Pero puede que queramos que todo corra en el servidor, incluso el modelo. Puede ser que hayas hecho un fine-tuning de un LLM para tu caso de uso, por lo que ya no puedes hacer llamadas a Inference Endpoints.</p>
      <p>As√≠ que vamos a ver c√≥mo modificar el c√≥digo de los dos backends para correr un modelo en el servidor y no hacer llamadas a Inference Endpoints.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear-Space">Crear Space<a class="anchor-link" href="#Crear-Space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripci√≥n, seleccionamos Gradio como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW m√°s b√°sico y gratuito, y seleccionamos si lo hacemos privado o p√∫blico.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="C%C3%B3digo">C√≥digo<a class="anchor-link" href="#C%C3%B3digo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que hacer cambios en <code>app.py</code> y en <code>requirements.txt</code> para que en lugar de hacer llamadas a Inference Endpoints, se ejecute el modelo localmente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los cambios que tenemos que hacer son</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importar <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code> de la librer√≠a <code>transformers</code> e importar <code>torch</code></p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En lugar de crear un modelo mediante <code>InferenceClient</code> lo creamos con <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code></p>
<div class="highlight"><pre><span></span><span class="c1"># Cargar el modelo y el tokenizer</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"HuggingFaceTB/SmolLM2-1.7B-Instruct"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span>
<span class="p">)</span>
</pre></div>
      <p>Utilizo <code>HuggingFaceTB/SmolLM2-1.7B-Instruct</code> porque es un modelo bastante capaz de solo 1.7B de par√°metros. Como he elegido el HW m√°s b√°sico no puedo usar modelos muy grandes. T√∫, si quieres usar un modelo m√°s grande tienes dos opciones, usar el HW gratuito y aceptar que la inferencia va a ser m√°s lenta, o usar un HW m√°s potente, pero de pago.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Modificar la funci√≥n <code>respond</code> para que construya el prompt con la estructura necesaria por la librer√≠a <code>transformers</code>, tokenizar el prompt, hacer la inferencia y destokenizar la respuesta.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">respond</span><span class="p">(</span>
    <span class="n">message</span><span class="p">,</span>
    <span class="n">history</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="n">system_message</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Construir el prompt con el formato correcto</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"&lt;|system|&gt;</span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">system_message</span><span class="si">{closing_brace}</span><span class="s2">&lt;/s&gt;</span><span class="se">\n</span><span class="s2">"</span>

    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">history</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">val</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"&lt;|user|&gt;</span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">val</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">{closing_brace}</span><span class="s2">&lt;/s&gt;</span><span class="se">\n</span><span class="s2">"</span>
        <span class="k">if</span> <span class="n">val</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"&lt;|assistant|&gt;</span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">val</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">{closing_brace}</span><span class="s2">&lt;/s&gt;</span><span class="se">\n</span><span class="s2">"</span>

    <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"&lt;|user|&gt;</span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">message</span><span class="si">{closing_brace}</span><span class="s2">&lt;/s&gt;</span><span class="se">\n</span><span class="s2">&lt;|assistant|&gt;</span><span class="se">\n</span><span class="s2">"</span>

    <span class="c1"># Tokenizar el prompt</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Generar la respuesta</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>

    <span class="c1"># Decodificar la respuesta</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Extraer solo la parte de la respuesta del asistente</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"&lt;|assistant|&gt;</span><span class="se">\n</span><span class="s2">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="k">yield</span> <span class="n">response</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A continuaci√≥n dejo todo el c√≥digo</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gradio</span> <span class="k">as</span> <span class="nn">gr</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="sd">"""</span>
<span class="sd">For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference</span>
<span class="sd">"""</span>

<span class="c1"># Cargar el modelo y el tokenizer</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"HuggingFaceTB/SmolLM2-1.7B-Instruct"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">respond</span><span class="p">(</span>
    <span class="n">message</span><span class="p">,</span>
    <span class="n">history</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="n">system_message</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">,</span>
    <span class="n">top_p</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># Construir el prompt con el formato correcto</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"&lt;|system|&gt;</span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">system_message</span><span class="si">{closing_brace}</span><span class="s2">&lt;/s&gt;</span><span class="se">\n</span><span class="s2">"</span>

    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">history</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">val</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"&lt;|user|&gt;</span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">val</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">{closing_brace}</span><span class="s2">&lt;/s&gt;</span><span class="se">\n</span><span class="s2">"</span>
        <span class="k">if</span> <span class="n">val</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"&lt;|assistant|&gt;</span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">val</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">{closing_brace}</span><span class="s2">&lt;/s&gt;</span><span class="se">\n</span><span class="s2">"</span>

    <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"&lt;|user|&gt;</span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">message</span><span class="si">{closing_brace}</span><span class="s2">&lt;/s&gt;</span><span class="se">\n</span><span class="s2">&lt;|assistant|&gt;</span><span class="se">\n</span><span class="s2">"</span>

    <span class="c1"># Tokenizar el prompt</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Generar la respuesta</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>

    <span class="c1"># Decodificar la respuesta</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Extraer solo la parte de la respuesta del asistente</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"&lt;|assistant|&gt;</span><span class="se">\n</span><span class="s2">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="k">yield</span> <span class="n">response</span>


<span class="sd">"""</span>
<span class="sd">For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/gradio/chatinterface</span>
<span class="sd">"""</span>
<span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">ChatInterface</span><span class="p">(</span>
    <span class="n">respond</span><span class="p">,</span>
    <span class="n">additional_inputs</span><span class="o">=</span><span class="p">[</span>
        <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span> 
            <span class="n">label</span><span class="o">=</span><span class="s2">"System message"</span>
        <span class="p">),</span>
        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">minimum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Max new tokens"</span><span class="p">),</span>
        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">minimum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Temperature"</span><span class="p">),</span>
        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span>
            <span class="n">minimum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
            <span class="n">maximum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">"Top-p (nucleus sampling)"</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">],</span>
<span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 27" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este archivo hay que a√±adir las nuevas librer√≠as que vamos a usar, en este caso <code>transformers</code>, <code>accelerate</code> y <code>torch</code>. El archivo entero quedar√≠a:</p>
<pre><code class="language-txt">huggingface_hub==0.25.2
gradio&gt;=4.0.0
transformers&gt;=4.36.0
torch&gt;=2.0.0
accelerate&gt;=0.25.0</code></pre>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Prueba-de-la-API">Prueba de la API<a class="anchor-link" href="#Prueba-de-la-API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 28" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Desplegamos el space y probamos directamente la API.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">gradio_client</span> <span class="kn">import</span> <span class="n">Client</span>',
          '',
          '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">"Maximofn/SmolLM2_localModel"</span><span class="p">)</span>',
          '<span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
          '		<span class="n">message</span><span class="o">=</span><span class="s2">"Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo"</span><span class="p">,</span>',
          '		<span class="n">system_message</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span>',
          '		<span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '		<span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '		<span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
          '		<span class="n">api_name</span><span class="o">=</span><span class="s2">"/chat"</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî',
          'Hola M√°ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en d√≠a. ¬øC√≥mo puedo servirte?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Me sorprende lo r√°pido que responde el modelo estando en un servidor sin GPU.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar-backend-con-FastAPI,-Langchain-y-Docker-y-modelo-corriendo-en-el-servidor">Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor<a class="anchor-link" href="#Desplegar-backend-con-FastAPI,-Langchain-y-Docker-y-modelo-corriendo-en-el-servidor"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 29" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora hacemos lo mismo que antes, pero con FastAPI, LangChain y Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear-Space">Crear Space<a class="anchor-link" href="#Crear-Space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripci√≥n, seleccionamos Docker como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW m√°s b√°sico y gratuito, y seleccionamos si lo hacemos privado o p√∫blico.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="C%C3%B3digo">C√≥digo<a class="anchor-link" href="#C%C3%B3digo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 32" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya no importamos <code>InferenceClient</code> y ahora importamos <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code> de la librer√≠a <code>transformers</code> e importamos <code>torch</code>.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos el modelo y el tokenizer con <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># Initialize the model and tokenizer</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Cargando modelo y tokenizer..."</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"HuggingFaceTB/SmolLM2-1.7B-Instruct"</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c1"># Load the model in BF16 format for better performance and lower memory usage</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">"cuda"</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Usando GPU para el modelo..."</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">model_name</span><span class="p">,</span>
            <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span>
            <span class="n">low_cpu_mem_usage</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Usando CPU para el modelo..."</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="n">model_name</span><span class="p">,</span>
            <span class="n">device_map</span><span class="o">=</span><span class="p">{opening_brace}</span><span class="s2">""</span><span class="p">:</span> <span class="n">device</span><span class="p">{closing_brace},</span>
            <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Modelo cargado exitosamente en: </span><span class="si">{opening_brace}</span><span class="n">device</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al cargar el modelo: </span><span class="si">{opening_brace}</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">raise</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Redefinimos la funci√≥n <code>call_model</code> para que haga la inferencia con el modelo local.</p>
<div class="highlight"><pre><span></span><span class="c1"># Define the function that calls the model</span>
<span class="k">def</span> <span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Call the model with the given messages</span>

<span class="sd">    Args:</span>
<span class="sd">        state: MessagesState</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary containing the generated text and the thread ID</span>
<span class="sd">    """</span>
    <span class="c1"># Convert LangChain messages to chat format</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">msg</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">{closing_brace})</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">({opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">msg</span><span class="o">.</span><span class="n">content</span><span class="p">{closing_brace})</span>

    <span class="c1"># Prepare the input using the chat template</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Generate response</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>  <span class="c1"># Increase the number of tokens for longer responses</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>

    <span class="c1"># Decode and clean the response</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Extract only the assistant's response (after the last user message)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"Assistant:"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="c1"># Convert the response to LangChain format</span>
    <span class="n">ai_message</span> <span class="o">=</span> <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">ai_message</span><span class="p">]{closing_brace}</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que quitar <code>langchain-huggingface</code> y a√±adir <code>transformers</code>, <code>accelerate</code> y <code>torch</code> en el archivo <code>requirements.txt</code>. El archivo quedar√≠a:</p>
<pre><code class="language-txt">fastapi
uvicorn
requests
pydantic&gt;=2.0.0
langchain&gt;=0.1.0
langchain-core&gt;=0.1.10
langgraph&gt;=0.2.27
python-dotenv&gt;=1.0.0
transformers&gt;=4.36.0
torch&gt;=2.0.0
accelerate&gt;=0.26.0</code></pre>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dockerfile">Dockerfile<a class="anchor-link" href="#Dockerfile"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya no necesitamos tener <code>RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true</code> porque como el modelo va a estar en el servidor y no vamos a hacer llamadas a Inference Endpoints, no necesitamos el token. El archivo quedar√≠a:</p>
<div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">python:3.13-slim</span>

<span class="k">RUN</span><span class="w"> </span>useradd<span class="w"> </span>-m<span class="w"> </span>-u<span class="w"> </span><span class="m">1000</span><span class="w"> </span>user
<span class="k">WORKDIR</span><span class="w"> </span><span class="s">/app</span>

<span class="k">COPY</span><span class="w"> </span>--chown<span class="o">=</span>user<span class="w"> </span>./requirements.txt<span class="w"> </span>requirements.txt
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span>--upgrade<span class="w"> </span>-r<span class="w"> </span>requirements.txt

<span class="k">COPY</span><span class="w"> </span>--chown<span class="o">=</span>user<span class="w"> </span>.<span class="w"> </span>/app

<span class="k">EXPOSE</span><span class="w"> </span><span class="s">7860</span>

<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">"uvicorn"</span><span class="p">,</span><span class="w"> </span><span class="s2">"app:app"</span><span class="p">,</span><span class="w"> </span><span class="s2">"--host"</span><span class="p">,</span><span class="w"> </span><span class="s2">"0.0.0.0"</span><span class="p">,</span><span class="w"> </span><span class="s2">"--port"</span><span class="p">,</span><span class="w"> </span><span class="s2">"7860"</span><span class="p">]</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Prueba-de-la-API">Prueba de la API<a class="anchor-link" href="#Prueba-de-la-API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Desplegamos el space y probamos la API. En este caso lo voy a probar directamente desde python.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">requests</span>',
          '',
          '<span class="n">url</span> <span class="o">=</span> <span class="s2">"https://maximofn-smollm2-backend-localmodel.hf.space/generate"</span>',
          '<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>',
          '    <span class="s2">"query"</span><span class="p">:</span> <span class="s2">"Hola, ¬øc√≥mo est√°s?"</span><span class="p">,</span>',
          '    <span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"user1"</span>',
          '<span class="p">}</span>',
          '',
          '<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>',
          '<span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>',
          '    <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="s2">"Respuesta:"</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s2">"generated_text"</span><span class="p">])</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="s2">"Thread ID:"</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s2">"thread_id"</span><span class="p">])</span>',
          '<span class="k">else</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="s2">"Error:"</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Respuesta: system',
          'You are a friendly Chatbot. Always reply in the language in which the user is writing to you.',
          'user',
          'Hola, ¬øc√≥mo est√°s?',
          'assistant',
          'Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.',
          'Thread ID: user1',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Este tarda un poco m√°s que el anterior. En realidad tarda lo normal para un modelo ejecut√°ndose en un servidor sin GPU. Lo raro es cuando lo desplegamos en Gradio. No s√© qu√© har√° HuggingFace por detr√°s, o tal vez ha sido coincidencia</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Conclusiones">Conclusiones<a class="anchor-link" href="#Conclusiones"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos visto c√≥mo crear una backend con un LLM, tanto haciendo llamadas al Inference Endpoint de HuggingFace, como haciendo llamadas a un modelo corriendo localmente. Hemos visto c√≥mo hacerlo con Gradio o con FastAPI, Langchain y Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A partir de aqu√≠ tienes el conocimiento para poder desplegar tus propios modelos, incluso aunque no sean LLMs, podr√≠an ser modelos multimodales. A partir de aqu√≠ puedes hacer lo que quieras.</p>
      </section>
      






    </div> -->

  </section>

</PostLayout>
