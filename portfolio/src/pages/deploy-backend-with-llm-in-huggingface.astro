---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Desplegar backend con LLM en HuggingFace';
const end_url = 'deploy-backend-with-llm-in-huggingface';
const description = '¿Quieres desplegar un backend con tu propio LLM? En este post te explico cómo hacerlo con HuggingFace Spaces, FastAPI, Langchain y Docker.';
const keywords = 'hugging face, fastapi, langchain, docker, backend, llm';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=960
    image_height=720
    image_extension=webp
    article_date=2025-03-02+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Desplegar backend con Gradio"><h2>Desplegar backend con Gradio</h2></a>
      <a class="anchor-link" href="#Crear space"><h3>Crear space</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>Código</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#README.md"><h4>README.md</h4></a>
      <a class="anchor-link" href="#Despliegue"><h3>Despliegue</h3></a>
      <a class="anchor-link" href="#Backend"><h3>Backend</h3></a>
      <a class="anchor-link" href="#Prueba de la API"><h3>Prueba de la API</h3></a>
      <a class="anchor-link" href="#Desplegar backend con FastAPI, Langchain y Docker"><h2>Desplegar backend con FastAPI, Langchain y Docker</h2></a>
      <a class="anchor-link" href="#Crear space"><h3>Crear space</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>Código</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#Dockerfile"><h4>Dockerfile</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#README.md"><h4>README.md</h4></a>
      <a class="anchor-link" href="#Token de HuggingFace"><h3>Token de HuggingFace</h3></a>
      <a class="anchor-link" href="#Anadir el token a los secrets del espacio"><h3>Añadir el token a los secrets del espacio</h3></a>
      <a class="anchor-link" href="#Despliegue"><h3>Despliegue</h3></a>
      <a class="anchor-link" href="#URL del backend"><h3>URL del backend</h3></a>
      <a class="anchor-link" href="#Documentacion"><h3>Documentación</h3></a>
      <a class="anchor-link" href="#Prueba de la API"><h3>Prueba de la API</h3></a>
      <a class="anchor-link" href="#Desplegar backend con Gradio y modelo corriendo en el servidor"><h2>Desplegar backend con Gradio y modelo corriendo en el servidor</h2></a>
      <a class="anchor-link" href="#Crear Space"><h3>Crear Space</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>Código</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#Prueba de la API"><h4>Prueba de la API</h4></a>
      <a class="anchor-link" href="#Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor"><h2>Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor</h2></a>
      <a class="anchor-link" href="#Crear Space"><h3>Crear Space</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>Código</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#Dockerfile"><h4>Dockerfile</h4></a>
      <a class="anchor-link" href="#Prueba de la API"><h3>Prueba de la API</h3></a>
      <a class="anchor-link" href="#Conclusiones"><h2>Conclusiones</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este post vamos a ver cómo desplegar un backend en HuggingFace. Vamos a ver cómo hacerlo de dos maneras, mediante la forma común, creando una aplicación con Gradio, y mediante una opción diferente usando FastAPI, Langchain y Docker</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para ambos casos va a ser necesario tener una cuenta en HuggingFace, ya que vamos a desplegar el backend en un space de HuggingFace.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar backend con Gradio">Desplegar backend con Gradio<a class="anchor-link" href="#Desplegar backend con Gradio"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear space">Crear space<a class="anchor-link" href="#Crear space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero de todo, creamos un nuevo espacio en Hugging Face.</p>
      <ul>
        <li>Ponemos un nombre, una descripción y elegimos la licencia.</li>
        <li>Elegimos Gradio como el tipo de SDK. Al elegir Gradio, nos aparecerán plantillas, así que elegimos la plantilla de chatbot.</li>
        <li>Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero tú elige lo que mejor consideres.</li>
        <li>Y por último hay que elegir si queremos crear el espacio público o privado.</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp" alt="backend gradio - create space">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">Código<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al crear el space, podemos clonarlo o podemos ver los archivos en la propia página de HuggingFace. Podemos ver que se han creado 3 archivos, <code>app.py</code>, <code>requirements.txt</code> y <code>README.md</code>. Así que vamos a ver qué poner en cada uno</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aquí tenemos el código de la aplicación. Como hemos elegido la plantilla de chatbot, ya tenemos mucho hecho, pero vamos a tener que cambiar 2 cosas, primero el modelo de lenguaje y el system prompt</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como modelo de lenguaje veo <code>HuggingFaceH4/zephyr-7b-beta</code>, pero vamos a usar <code>Qwen/Qwen2.5-72B-Instruct</code>, que es un modelo muy capaz.</p>
      <p>Así que busca el texto <code>client = InferenceClient(&quot;HuggingFaceH4/zephyr-7b-beta&quot;)</code> y reemplázalo por <code>client = InferenceClient(&quot;Qwen/Qwen2.5-72B-Instruct&quot;)</code>, o espera que más adelante pondré todo el código.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>También vamos a cambiar el system prompt, que por defecto es <code>You are a friendly Chatbot.</code>, pero como es un modelo entrenado en su mayoría en inglés, es probable que si le hablas en otro idioma te responda en inglés, así que vamos a cambiarlo por <code>You are a friendly Chatbot. Always reply in the language in which the user is writing to you.</code>.</p>
      <p>Así que busca el texto <code>gr.Textbox(value=&quot;You are a friendly Chatbot.&quot;, label=&quot;System message&quot;),</code> y reemplázalo por <code>gr.Textbox(value=&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;, label=&quot;System message&quot;),</code>, o espera a que ahora voy a poner todo el código.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">import gradio as gr<br>from huggingface_hub import InferenceClient<br><br>"""<br>For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference<br>"""<br>client = InferenceClient("Qwen/Qwen2.5-72B-Instruct")<br><br><br>def respond(<br>&#x20;&#x20;message,<br>&#x20;&#x20;history: list[tuple[str, str]],<br>&#x20;&#x20;system_message,<br>&#x20;&#x20;max_tokens,<br>&#x20;&#x20;temperature,<br>&#x20;&#x20;top_p,<br>):<br>&#x20;&#x20;messages = [{opening_brace}"role": "system", "content": system_message{closing_brace}]<br><br>&#x20;&#x20;for val in history:<br>&#x20;&#x20;&#x20;&#x20;if val[0]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;messages.append({opening_brace}"role": "user", "content": val[0]{closing_brace})<br>&#x20;&#x20;&#x20;&#x20;if val[1]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;messages.append({opening_brace}"role": "assistant", "content": val[1]{closing_brace})<br><br>&#x20;&#x20;messages.append({opening_brace}"role": "user", "content": message{closing_brace})<br><br>&#x20;&#x20;response = ""<br><br>&#x20;&#x20;for message in client.chat_completion(<br>&#x20;&#x20;&#x20;&#x20;messages,<br>&#x20;&#x20;&#x20;&#x20;max_tokens=max_tokens,<br>&#x20;&#x20;&#x20;&#x20;stream=True,<br>&#x20;&#x20;&#x20;&#x20;temperature=temperature,<br>&#x20;&#x20;&#x20;&#x20;top_p=top_p,<br>&#x20;&#x20;):<br>&#x20;&#x20;&#x20;&#x20;token = message.choices[0].delta.content<br><br>&#x20;&#x20;&#x20;&#x20;response += token<br>&#x20;&#x20;&#x20;&#x20;yield response<br><br><br>"""<br>For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/gradio/chatinterface<br>"""<br>demo = gr.ChatInterface(<br>&#x20;&#x20;respond,<br>&#x20;&#x20;additional_inputs=[<br>&#x20;&#x20;&#x20;&#x20;gr.Textbox(value="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.", label="System message"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;minimum=0.1,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;maximum=1.0,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;value=0.95,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;step=0.05,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;label="Top-p (nucleus sampling)",<br>&#x20;&#x20;&#x20;&#x20;),<br>&#x20;&#x20;],<br>)<br><br><br>if __name__ == "__main__":<br>&#x20;&#x20;demo.launch()</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este es el archivo en el que estarán escritas las dependencias, pero para este caso va a ser muy sencillo:</p>
      <div class='highlight'><pre><code class="language-txt">huggingface_hub==0.25.2</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="README.md">README.md<a class="anchor-link" href="#README.md"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este es el archivo en el que vamos a poner la información del espacio. En los spaces de HuggingFace, al inicio de los readmes, se pone un código para que HuggingFace sepa cómo mostrar la miniatura del espacio, qué fichero tiene que usar para ejecutar el código, versión del sdk, etc.</p>
      <div class='highlight'><pre><code class="language-md">---
      title: SmolLM2
      emoji: 💬
      colorFrom: yellow
      colorTo: purple
      sdk: gradio
      sdk_version: 5.0.1
      app_file: app.py
      pinned: false
      license: apache-2.0
      short_description: Gradio SmolLM2 chat
      ---
      
      An example chatbot using [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index), and the [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Despliegue">Despliegue<a class="anchor-link" href="#Despliegue"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.</p>
      <p>Así que cuando estén los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp" alt="backend gradio - chatbot">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Backend">Backend<a class="anchor-link" href="#Backend"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Muy bien, hemos hecho un chatbot, pero no era la intención, aquí habíamos venido a hacer un backend! Para, para, fíjate lo que pone debajo del chatbot</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp" alt="backend gradio - Use via API">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver un texto <code>Use via API</code>, donde si pulsamos se nos abre un menú con una API para poder usar el chatbot.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp" alt="backend gradio - API">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que nos da una documentación de cómo usar la API, tanto con Python, con JavaScript, como con bash.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Prueba de la API">Prueba de la API<a class="anchor-link" href="#Prueba de la API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usamos el código de ejemplo de Python.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2&quot;</span><span class="p">)</span>',
      '<span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;Hola, ¿cómo estás? Me llamo Máximo&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2.hf.space ✔',
          '¡Hola Máximo! Mucho gusto, estoy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Estamos haciendo llamadas a la API de <code>InferenceClient</code> de HuggingFace, así que podríamos pensar, ¿Para qué hemos hecho un backend, si podemos llamar directamente a la API de HuggingFace? Pues lo vas a ver a continuación.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;¿Cómo me llamo?&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Tu nombre es Máximo. ¿Es correcto?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>La plantilla de chat de Gradio maneja el historial por nosotros, de manera que cada vez que creamos un nuevo <code>cliente</code>, se crea un nuevo hilo de conversación.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a probar a crear un nuevo cliente, y ver si se crea un nuevo hilo de conversación.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">new_client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2&quot;</span><span class="p">)</span>',
      '<span class="n">result</span> <span class="o">=</span> <span class="n">new_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;Hola, ¿cómo estás? Me llamo Luis&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2.hf.space ✔',
          'Hola Luis, estoy muy bien, gracias por preguntar. ¿Cómo estás tú? Es un gusto conocerte. ¿En qué puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora le volvemos a preguntar cómo me llamo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">new_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;¿Cómo me llamo?&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Te llamas Luis. ¿Hay algo más en lo que pueda ayudarte?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, tenemos dos clientes, cada uno con su propio hilo de conversación.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar backend con FastAPI, Langchain y Docker">Desplegar backend con FastAPI, Langchain y Docker<a class="anchor-link" href="#Desplegar backend con FastAPI, Langchain y Docker"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vamos a hacer lo mismo, crear un backend de un chatbot, con el mismo modelo, pero en este caso usando FastAPI, Langchain y Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear space">Crear space<a class="anchor-link" href="#Crear space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que crear un nuevo espacio, pero en este caso lo haremos de otra manera</p>
      <ul>
        <li>Ponemos un nombre, una descripción y elegimos la licencia.</li>
        <li>Elegimos Docker como el tipo de SDK. Al elegir Docker, nos aparecerán plantillas, así que elegimos una plantilla en blanco.</li>
        <li>Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero tú elige lo que mejor consideres.</li>
        <li>Y por último hay que elegir si queremos crear el espacio público o privado.</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp" alt="backend docker - create space">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">Código<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora, al crear el space, vemos que solo tenemos un archivo, el <code>README.md</code>. Así que vamos a tener que crear todo el código nosotros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a crear el código de la aplicación</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Empezamos con las librerías necesarias</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from fastapi import FastAPI, HTTPException<br>from pydantic import BaseModel<br>from huggingface_hub import InferenceClient<br><br>from langchain_core.messages import HumanMessage, AIMessage<br>from langgraph.checkpoint.memory import MemorySaver<br>from langgraph.graph import START, MessagesState, StateGraph<br><br>import os<br>from dotenv import load_dotenv<br>load_dotenv()</code></pre></div>
            </section>
      <p>Cargamos <code>fastapi</code> para poder crear las rutas de la API, <code>pydantic</code> para crear la plantilla de las querys, <code>huggingface_hub</code> para poder crear un modelo de lenguaje, <code>langchain</code> para poder indicarle al modelo si los mensajes son del chatbot o del usuario y <code>langgraph</code> para poder crear el chatbot.</p>
      <p>Además cargamos <code>os</code> y <code>dotenv</code> para poder cargar las variables de entorno.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cargamos el token de HuggingFace</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># HuggingFace token<br>HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el modelo de lenguaje</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Initialize the HuggingFace model<br>model = InferenceClient(<br>&#x20;&#x20;model="Qwen/Qwen2.5-72B-Instruct",<br>&#x20;&#x20;api_key=os.getenv("HUGGINGFACE_TOKEN")<br>)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos ahora una función para llamar al modelo</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Define the function that calls the model<br>def call_model(state: MessagesState):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Call the model with the given messages<br><br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;state: MessagesState<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;# Convert LangChain messages to HuggingFace format<br>&#x20;&#x20;hf_messages = []<br>&#x20;&#x20;for msg in state["messages"]:<br>&#x20;&#x20;&#x20;&#x20;if isinstance(msg, HumanMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;hf_messages.append({opening_brace}"role": "user", "content": msg.content{closing_brace})<br>&#x20;&#x20;&#x20;&#x20;elif isinstance(msg, AIMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;hf_messages.append({opening_brace}"role": "assistant", "content": msg.content{closing_brace})<br>&#x20;&#x20;<br>&#x20;&#x20;# Call the API<br>&#x20;&#x20;response = model.chat_completion(<br>&#x20;&#x20;&#x20;&#x20;messages=hf_messages,<br>&#x20;&#x20;&#x20;&#x20;temperature=0.5,<br>&#x20;&#x20;&#x20;&#x20;max_tokens=64,<br>&#x20;&#x20;&#x20;&#x20;top_p=0.7<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Convert the response to LangChain format<br>&#x20;&#x20;ai_message = AIMessage(content=response.choices[0].message.content)<br>&#x20;&#x20;return {opening_brace}"messages": state["messages"] + [ai_message]{closing_brace}</code></pre></div>
            </section>
      <p>Convertimos los mensajes de formato LangChain a formato HuggingFace, así podemos usar el modelo de lenguaje.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Definimos una plantilla para las queries</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">class QueryRequest(BaseModel):<br>&#x20;&#x20;query: str<br>&#x20;&#x20;thread_id: str = "default"</code></pre></div>
            </section>
      <p>Las queries van a tener un <code>query</code>, el mensaje del usuario, y un <code>thread_id</code>, que es el identificador del hilo de la conversación y más adelante explicaremos para qué lo usamos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un grafo de LangGraph</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Define the graph<br>workflow = StateGraph(state_schema=MessagesState)<br><br># Define the node in the graph<br>workflow.add_edge(START, "model")<br>workflow.add_node("model", call_model)<br><br># Add memory<br>memory = MemorySaver()<br>graph_app = workflow.compile(checkpointer=memory)</code></pre></div>
            </section>
      <p>Con esto lo que hacemos es crear un grafo de LangGraph, que es una estructura de datos que nos permite crear un chatbot y que gestiona por nosotros el estado del chatbot, es decir, entre otras cosas, el historial de mensajes. Así no lo tenemos que hacer nosotros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos la aplicación de FastAPI</p>
      <div class='highlight'><pre><code class="language-python">app = FastAPI(title="LangChain FastAPI", description="API to generate text using LangChain and LangGraph")</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos los endpoints de la API</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Welcome endpoint<br>@app.get("/")<br>async def api_home():<br>&#x20;&#x20;"""Welcome endpoint"""<br>&#x20;&#x20;return {opening_brace}"detail": "Welcome to FastAPI, Langchain, Docker tutorial"{closing_brace}<br><br># Generate endpoint<br>@app.post("/generate")<br>async def generate(request: QueryRequest):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Endpoint to generate text using the language model<br>&#x20;&#x20;<br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;request: QueryRequest<br>&#x20;&#x20;&#x20;&#x20;query: str<br>&#x20;&#x20;&#x20;&#x20;thread_id: str = "default"<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;try:<br>&#x20;&#x20;&#x20;&#x20;# Configure the thread ID<br>&#x20;&#x20;&#x20;&#x20;config = {opening_brace}"configurable": {opening_brace}"thread_id": request.thread_id{closing_brace}{closing_brace}<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Create the input message<br>&#x20;&#x20;&#x20;&#x20;input_messages = [HumanMessage(content=request.query)]<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Invoke the graph<br>&#x20;&#x20;&#x20;&#x20;output = graph_app.invoke({opening_brace}"messages": input_messages{closing_brace}, config)<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Get the model response<br>&#x20;&#x20;&#x20;&#x20;response = output["messages"][-1].content<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;return {opening_brace}<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;"generated_text": response,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;"thread_id": request.thread_id<br>&#x20;&#x20;&#x20;&#x20;{closing_brace}<br>&#x20;&#x20;except Exception as e:<br>&#x20;&#x20;&#x20;&#x20;raise HTTPException(status_code=500, detail=f"Error al generar texto: {opening_brace}str(e){closing_brace}")</code></pre></div>
            </section>
      <p>Hemos creado el endpoint <code>/</code> que nos devolverá un texto cuando accedamos a la API, y el endpoint <code>/generate</code> que es el que usaremos para generar el texto.</p>
      <p>Si nos fijamos en la función <code>generate</code> tenemos la variable <code>config</code>, que es un diccionario que contiene el <code>thread_id</code>. Este <code>thread_id</code> es el que nos permite tener un historial de mensajes de cada usuario, de esta manera, diferentes usuarios pueden usar el mismo endpoint y tener su propio historial de mensajes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por último, tenemos el código para que se pueda ejecutar la aplicación</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">if __name__ == "__main__":<br>&#x20;&#x20;import uvicorn<br>&#x20;&#x20;uvicorn.run(app, host="0.0.0.0", port=7860)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a escribir todo el código junto</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from fastapi import FastAPI, HTTPException<br>from pydantic import BaseModel<br>from huggingface_hub import InferenceClient<br><br>from langchain_core.messages import HumanMessage, AIMessage<br>from langgraph.checkpoint.memory import MemorySaver<br>from langgraph.graph import START, MessagesState, StateGraph<br><br>import os<br>from dotenv import load_dotenv<br>load_dotenv()<br><br># HuggingFace token<br>HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))<br><br># Initialize the HuggingFace model<br>model = InferenceClient(<br>&#x20;&#x20;model="Qwen/Qwen2.5-72B-Instruct",<br>&#x20;&#x20;api_key=os.getenv("HUGGINGFACE_TOKEN")<br>)<br><br># Define the function that calls the model<br>def call_model(state: MessagesState):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Call the model with the given messages<br><br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;state: MessagesState<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;# Convert LangChain messages to HuggingFace format<br>&#x20;&#x20;hf_messages = []<br>&#x20;&#x20;for msg in state["messages"]:<br>&#x20;&#x20;&#x20;&#x20;if isinstance(msg, HumanMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;hf_messages.append({opening_brace}"role": "user", "content": msg.content{closing_brace})<br>&#x20;&#x20;&#x20;&#x20;elif isinstance(msg, AIMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;hf_messages.append({opening_brace}"role": "assistant", "content": msg.content{closing_brace})<br>&#x20;&#x20;<br>&#x20;&#x20;# Call the API<br>&#x20;&#x20;response = model.chat_completion(<br>&#x20;&#x20;&#x20;&#x20;messages=hf_messages,<br>&#x20;&#x20;&#x20;&#x20;temperature=0.5,<br>&#x20;&#x20;&#x20;&#x20;max_tokens=64,<br>&#x20;&#x20;&#x20;&#x20;top_p=0.7<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Convert the response to LangChain format<br>&#x20;&#x20;ai_message = AIMessage(content=response.choices[0].message.content)<br>&#x20;&#x20;return {opening_brace}"messages": state["messages"] + [ai_message]{closing_brace}<br><br># Define the graph<br>workflow = StateGraph(state_schema=MessagesState)<br><br># Define the node in the graph<br>workflow.add_edge(START, "model")<br>workflow.add_node("model", call_model)<br><br># Add memory<br>memory = MemorySaver()<br>graph_app = workflow.compile(checkpointer=memory)<br><br># Define the data model for the request<br>class QueryRequest(BaseModel):<br>&#x20;&#x20;query: str<br>&#x20;&#x20;thread_id: str = "default"<br><br># Create the FastAPI application<br>app = FastAPI(title="LangChain FastAPI", description="API to generate text using LangChain and LangGraph")<br><br># Welcome endpoint<br>@app.get("/")<br>async def api_home():<br>&#x20;&#x20;"""Welcome endpoint"""<br>&#x20;&#x20;return {opening_brace}"detail": "Welcome to FastAPI, Langchain, Docker tutorial"{closing_brace}<br><br># Generate endpoint<br>@app.post("/generate")<br>async def generate(request: QueryRequest):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Endpoint to generate text using the language model<br>&#x20;&#x20;<br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;request: QueryRequest<br>&#x20;&#x20;&#x20;&#x20;query: str<br>&#x20;&#x20;&#x20;&#x20;thread_id: str = "default"<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;try:<br>&#x20;&#x20;&#x20;&#x20;# Configure the thread ID<br>&#x20;&#x20;&#x20;&#x20;config = {opening_brace}"configurable": {opening_brace}"thread_id": request.thread_id{closing_brace}{closing_brace}<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Create the input message<br>&#x20;&#x20;&#x20;&#x20;input_messages = [HumanMessage(content=request.query)]<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Invoke the graph<br>&#x20;&#x20;&#x20;&#x20;output = graph_app.invoke({opening_brace}"messages": input_messages{closing_brace}, config)<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Get the model response<br>&#x20;&#x20;&#x20;&#x20;response = output["messages"][-1].content<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;return {opening_brace}<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;"generated_text": response,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;"thread_id": request.thread_id<br>&#x20;&#x20;&#x20;&#x20;{closing_brace}<br>&#x20;&#x20;except Exception as e:<br>&#x20;&#x20;&#x20;&#x20;raise HTTPException(status_code=500, detail=f"Error al generar texto: {opening_brace}str(e){closing_brace}")<br><br>if __name__ == "__main__":<br>&#x20;&#x20;import uvicorn<br>&#x20;&#x20;uvicorn.run(app, host="0.0.0.0", port=7860)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dockerfile">Dockerfile<a class="anchor-link" href="#Dockerfile"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vemos cómo crear el Dockerfile</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero indicamos desde qué imagen vamos a partir</p>
      <div class='highlight'><pre><code class="language-dockerfile">FROM python:3.13-slim</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos el directorio de trabajo</p>
      <div class='highlight'><pre><code class="language-dockerfile">RUN useradd -m -u 1000 user
      WORKDIR /app</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Copiamos el archivo con las dependencias e instalamos</p>
      <div class='highlight'><pre><code class="language-dockerfile">COPY --chown=user ./requirements.txt requirements.txt
      RUN pip install --no-cache-dir --upgrade -r requirements.txt</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Copiamos el resto del código</p>
      <div class='highlight'><pre><code class="language-dockerfile">COPY --chown=user . /app</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Exponemos el puerto 7860</p>
      <div class='highlight'><pre><code class="language-dockerfile">EXPOSE 7860</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos las variables de entorno</p>
      <div class='highlight'><pre><code class="language-dockerfile">RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \
          test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Secret exists!"</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por último, indicamos el comando para ejecutar la aplicación</p>
      <div class='highlight'><pre><code class="language-dockerfile">CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora lo ponemos todo junto</p>
      <div class='highlight'><pre><code class="language-dockerfile">FROM python:3.13-slim
      
      RUN useradd -m -u 1000 user
      WORKDIR /app
      
      COPY --chown=user ./requirements.txt requirements.txt
      RUN pip install --no-cache-dir --upgrade -r requirements.txt
      
      COPY --chown=user . /app
      
      EXPOSE 7860
      
      RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \
          test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Secret exists!"
      
      CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el archivo con las dependencias</p>
      <div class='highlight'><pre><code class="language-txt">fastapi
      uvicorn
      requests
      pydantic&gt;=2.0.0
      langchain
      langchain-huggingface
      langchain-core
      langgraph &gt; 0.2.27
      python-dotenv.2.11</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="README.md">README.md<a class="anchor-link" href="#README.md"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por último, creamos el archivo README.md con información del espacio y con las intrucciones para HugginFace</p>
      <div class='highlight'><pre><code class="language-md">---
      title: SmolLM2 Backend
      emoji: 📊
      colorFrom: yellow
      colorTo: red
      sdk: docker
      pinned: false
      license: apache-2.0
      short_description: Backend of SmolLM2 chat
      app_port: 7860
      ---
      
      # SmolLM2 Backend
      
      This project implements a FastAPI API that uses LangChain and LangGraph to generate text with the Qwen2.5-72B-Instruct model from HuggingFace.
      
      ## Configuration
      
      ### In HuggingFace Spaces
      
      This project is designed to run in HuggingFace Spaces. To configure it:
      
      1. Create a new Space in HuggingFace with SDK Docker
      2. Configure the `HUGGINGFACE_TOKEN` or `HF_TOKEN` environment variable in the Space configuration:
         - Go to the "Settings" tab of your Space
         - Scroll down to the "Repository secrets" section
         - Add a new variable with the name `HUGGINGFACE_TOKEN` and your token as the value
         - Save the changes
      
      ### Local development
      
      For local development:
      
      1. Clone this repository
      2. Create a `.env` file in the project root with your HuggingFace token:
         ``
         HUGGINGFACE_TOKEN=your_token_here
         ``
      3. Install the dependencies:
         ``
         pip install -r requirements.txt
         ``
      
      ## Local execution
      
      ``bash
      uvicorn app:app --reload
      ``
      
      The API will be available at `http://localhost:8000`.
      
      ## Endpoints
      
      ### GET `/`
      
      Welcome endpoint that returns a greeting message.
      
      ### POST `/generate`
      
      Endpoint to generate text using the language model.
      
      **Request parameters:**
      ``json
      {opening_brace}
        "query": "Your question here",
        "thread_id": "optional_thread_identifier"
      {closing_brace}
      ``
      
      **Response:**
      ``json
      {opening_brace}
        "generated_text": "Generated text by the model",
        "thread_id": "thread identifier"
      {closing_brace}
      ``
      
      ## Docker
      
      To run the application in a Docker container:
      
      ``bash
      # Build the image
      docker build -t smollm2-backend .
      
      # Run the container
      docker run -p 8000:8000 --env-file .env smollm2-backend
      ``
      
      ## API documentation
      
      The interactive API documentation is available at:
      - Swagger UI: `http://localhost:8000/docs`
      - ReDoc: `http://localhost:8000/redoc`</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Token de HuggingFace">Token de HuggingFace<a class="anchor-link" href="#Token de HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h3>
      <p>Si te has fijado en el código y en el Dockerfile hemos usado un token de HuggingFace, así que vamos a tener que crear uno. En nuestra cuenta de HuggingFace creamos un <a href="https://huggingface.co/settings/tokens/new?tokenType=fineGrained" target="_blank" rel="nofollow noreferrer">nuevo token</a>, le ponemos un nombre y le damos los siguientes permisos:</p>
      <ul>
        <li>Read access to contents of all repos under your personal namespace</li>
        <li>Read access to contents of all repos under your personal namespacev</li>
        <li>Make calls to inference providers</li>
        <li>Make calls to Inference Endpoints</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-token.webp" alt="backend docker - token">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Anadir el token a los secrets del espacio">Añadir el token a los secrets del espacio<a class="anchor-link" href="#Anadir el token a los secrets del espacio"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que ya tenemos el token, necesitamos añadirlo al espacio. En la parte de arriba de la app, podremos ver un botón llamado <code>Settings</code>, lo pulsamos y podremos ver la sección de configuración del espacio.</p>
      <p>Si bajamos, podremos ver una sección en la que podemos añadir <code>Variables</code> y <code>Secrets</code>. En este caso, como estamos añadiendo un token, lo vamos a añadir a los <code>Secrets</code>.</p>
      <p>Le ponemos el nombre <code>HUGGINGFACE_TOKEN</code> y el valor del token.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Despliegue">Despliegue<a class="anchor-link" href="#Despliegue"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.</p>
      <p>Así que cuando estén los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.</p>
      <p>En este caso, solo hemos construido un backend, por lo que lo que vamos a ver al entrar al espacio es lo que definimos en el endpoint <code>/</code></p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-space.webp" alt="backend docker - space">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="URL del backend">URL del backend<a class="anchor-link" href="#URL del backend"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Necesitamos saber la URL del backend para poder hacer llamadas a la API. Para ello, tenemos que pulsar en los tres puntos de la parte superior derecha para ver las opciones</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-options.webp" alt="backend docker - options">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el menú que se despliega pulsamos en <code>Embed this Spade</code>, se nos abrirá una ventana en la que indica cómo embeber el espacio con un iframe y además nos dará la URL del espacio.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-embed.webp" alt="backend docker - embed">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora nos vamos a esa URL, veremos lo mismo que en el espacio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Documentacion">Documentación<a class="anchor-link" href="#Documentacion"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>FastAPI, a parte de ser una API rapidísima, tiene otra gran ventaja, y es que genera documentación de manera automática.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si añadimos <code>/docs</code> a la URL que vimos antes, podremos ver la documentación de la API con <code>Swagger UI</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-swagger-doc.webp" alt="backend docker - swagger doc">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>También podemos añadir <code>/redoc</code> a la URL para ver la documentación con <code>ReDoc</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-redoc.webp" alt="backend docker - redoc doc">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Prueba de la API">Prueba de la API<a class="anchor-link" href="#Prueba de la API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Lo bueno de la documentación <code>Swagger UI</code> es que nos permite probar la API directamente desde el navegador.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Añadimos <code>/docs</code> a la URL que obtuvimos, abrimos el desplegable del endpoint <code>/generate</code> y le damos a <code>Try it out</code>, modificamos el valor de la <code>query</code> y del <code>thread_id</code> y pulsamos en <code>Execute</code>.</p>
      <p>En el primer caso voy a poner</p>
      <ul>
        <li><strong>query</strong>: Hola, ¿Cómo estás? Soy Máximo</li>
        <li><strong>thread_id</strong>: user1</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-test-API.webp" alt="backend docker - test API">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Recibimos la siguiente respuesta <code>¡Hola Máximo! Estoy muy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?</code></p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user1.webp" alt="backend docker -response 1 - user1">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a probar ahora la misma pregunta, pero con un <code>thread_id</code> diferente, en este caso <code>user2</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-query1-user2.webp" alt="backend docker - query 1 - user2">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Y nos responde esto <code>¡Hola Luis! Estoy muy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?</code></p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user2.webp" alt="backend docker - response 1 - user2">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora pedimos nuestro nombre con los dos usuarios y obtenemos esto</p>
      <ul>
        <li>Para el usuario <strong>user1</strong>: <code>Te llamas Máximo. ¿Hay algo más en lo que pueda ayudarte?</code></li>
        <li>Para el usuario <strong>user2</strong>: <code>Te llamas Luis. ¿Hay algo más en lo que pueda ayudarte hoy, Luis?</code></li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user1.webp" alt="backend docker - response 2 - user1">
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user2.webp" alt="backend docker - response 2 - user2">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar backend con Gradio y modelo corriendo en el servidor">Desplegar backend con Gradio y modelo corriendo en el servidor<a class="anchor-link" href="#Desplegar backend con Gradio y modelo corriendo en el servidor"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los dos backends que hemos creado en realidad no están corriendo un modelo, sino que están haciendo llamadas a Inference Endpoints de HuggingFace. Pero puede que queramos que todo corra en el servidor, incluso el modelo. Puede ser que hayas hecho un fine-tuning de un LLM para tu caso de uso, por lo que ya no puedes hacer llamadas a Inference Endpoints.</p>
      <p>Así que vamos a ver cómo modificar el código de los dos backends para correr un modelo en el servidor y no hacer llamadas a Inference Endpoints.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear Space">Crear Space<a class="anchor-link" href="#Crear Space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripción, seleccionamos Gradio como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW más básico y gratuito, y seleccionamos si lo hacemos privado o público.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">Código<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que hacer cambios en <code>app.py</code> y en <code>requirements.txt</code> para que en lugar de hacer llamadas a Inference Endpoints, se ejecute el modelo localmente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los cambios que tenemos que hacer son</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importar <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code> de la librería <code>transformers</code> e importar <code>torch</code></p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer<br>import torch</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En lugar de crear un modelo mediante <code>InferenceClient</code> lo creamos con <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code></p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Cargar el modelo y el tokenizer<br>model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"<br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>model = AutoModelForCausalLM.from_pretrained(<br>&#x20;&#x20;model_name,<br>&#x20;&#x20;torch_dtype=torch.float16,<br>&#x20;&#x20;device_map="auto"<br>)</code></pre></div>
            </section>
      <p>Utilizo <code>HuggingFaceTB/SmolLM2-1.7B-Instruct</code> porque es un modelo bastante capaz de solo 1.7B de parámetros. Como he elegido el HW más básico no puedo usar modelos muy grandes. Tú, si quieres usar un modelo más grande tienes dos opciones, usar el HW gratuito y aceptar que la inferencia va a ser más lenta, o usar un HW más potente, pero de pago.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Modificar la función <code>respond</code> para que construya el prompt con la estructura necesaria por la librería <code>transformers</code>, tokenizar el prompt, hacer la inferencia y destokenizar la respuesta.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">def respond(<br>&#x20;&#x20;message,<br>&#x20;&#x20;history: list[tuple[str, str]],<br>&#x20;&#x20;system_message,<br>&#x20;&#x20;max_tokens,<br>&#x20;&#x20;temperature,<br>&#x20;&#x20;top_p,<br>):<br>&#x20;&#x20;# Construir el prompt con el formato correcto<br>&#x20;&#x20;prompt = f"&lt;|system|&gt;\n{opening_brace}system_message{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;for val in history:<br>&#x20;&#x20;&#x20;&#x20;if val[0]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;prompt += f"&lt;|user|&gt;\n{opening_brace}val[0]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;&#x20;&#x20;if val[1]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;prompt += f"&lt;|assistant|&gt;\n{opening_brace}val[1]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;prompt += f"&lt;|user|&gt;\n{opening_brace}message{closing_brace}&lt;/s&gt;\n&lt;|assistant|&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;# Tokenizar el prompt<br>&#x20;&#x20;inputs = tokenizer(prompt, return_tensors="pt").to(model.device)<br>&#x20;&#x20;<br>&#x20;&#x20;# Generar la respuesta<br>&#x20;&#x20;outputs = model.generate(<br>&#x20;&#x20;&#x20;&#x20;**inputs,<br>&#x20;&#x20;&#x20;&#x20;max_new_tokens=max_tokens,<br>&#x20;&#x20;&#x20;&#x20;temperature=temperature,<br>&#x20;&#x20;&#x20;&#x20;top_p=top_p,<br>&#x20;&#x20;&#x20;&#x20;do_sample=True,<br>&#x20;&#x20;&#x20;&#x20;pad_token_id=tokenizer.eos_token_id<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Decodificar la respuesta<br>&#x20;&#x20;response = tokenizer.decode(outputs[0], skip_special_tokens=True)<br>&#x20;&#x20;<br>&#x20;&#x20;# Extraer solo la parte de la respuesta del asistente<br>&#x20;&#x20;response = response.split("&lt;|assistant|&gt;\n")[-1].strip()<br>&#x20;&#x20;<br>&#x20;&#x20;yield response</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A continuación dejo todo el código</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">import gradio as gr<br>from transformers import AutoModelForCausalLM, AutoTokenizer<br>import torch<br><br>"""<br>For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference<br>"""<br><br># Cargar el modelo y el tokenizer<br>model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"<br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>model = AutoModelForCausalLM.from_pretrained(<br>&#x20;&#x20;model_name,<br>&#x20;&#x20;torch_dtype=torch.float16,<br>&#x20;&#x20;device_map="auto"<br>)<br><br>def respond(<br>&#x20;&#x20;message,<br>&#x20;&#x20;history: list[tuple[str, str]],<br>&#x20;&#x20;system_message,<br>&#x20;&#x20;max_tokens,<br>&#x20;&#x20;temperature,<br>&#x20;&#x20;top_p,<br>):<br>&#x20;&#x20;# Construir el prompt con el formato correcto<br>&#x20;&#x20;prompt = f"&lt;|system|&gt;\n{opening_brace}system_message{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;for val in history:<br>&#x20;&#x20;&#x20;&#x20;if val[0]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;prompt += f"&lt;|user|&gt;\n{opening_brace}val[0]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;&#x20;&#x20;if val[1]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;prompt += f"&lt;|assistant|&gt;\n{opening_brace}val[1]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;prompt += f"&lt;|user|&gt;\n{opening_brace}message{closing_brace}&lt;/s&gt;\n&lt;|assistant|&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;# Tokenizar el prompt<br>&#x20;&#x20;inputs = tokenizer(prompt, return_tensors="pt").to(model.device)<br>&#x20;&#x20;<br>&#x20;&#x20;# Generar la respuesta<br>&#x20;&#x20;outputs = model.generate(<br>&#x20;&#x20;&#x20;&#x20;**inputs,<br>&#x20;&#x20;&#x20;&#x20;max_new_tokens=max_tokens,<br>&#x20;&#x20;&#x20;&#x20;temperature=temperature,<br>&#x20;&#x20;&#x20;&#x20;top_p=top_p,<br>&#x20;&#x20;&#x20;&#x20;do_sample=True,<br>&#x20;&#x20;&#x20;&#x20;pad_token_id=tokenizer.eos_token_id<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Decodificar la respuesta<br>&#x20;&#x20;response = tokenizer.decode(outputs[0], skip_special_tokens=True)<br>&#x20;&#x20;<br>&#x20;&#x20;# Extraer solo la parte de la respuesta del asistente<br>&#x20;&#x20;response = response.split("&lt;|assistant|&gt;\n")[-1].strip()<br>&#x20;&#x20;<br>&#x20;&#x20;yield response<br><br><br>"""<br>For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/gradio/chatinterface<br>"""<br>demo = gr.ChatInterface(<br>&#x20;&#x20;respond,<br>&#x20;&#x20;additional_inputs=[<br>&#x20;&#x20;&#x20;&#x20;gr.Textbox(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;value="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.", <br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;label="System message"<br>&#x20;&#x20;&#x20;&#x20;),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;minimum=0.1,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;maximum=1.0,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;value=0.95,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;step=0.05,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;label="Top-p (nucleus sampling)",<br>&#x20;&#x20;&#x20;&#x20;),<br>&#x20;&#x20;],<br>)<br><br><br>if __name__ == "__main__":<br>&#x20;&#x20;demo.launch()</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 27" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este archivo hay que añadir las nuevas librerías que vamos a usar, en este caso <code>transformers</code>, <code>accelerate</code> y <code>torch</code>. El archivo entero quedaría:</p>
      <div class='highlight'><pre><code class="language-txt">huggingface_hub==0.25.2
      gradio&gt;=4.0.0
      transformers&gt;=4.36.0
      torch&gt;=2.0.0
      accelerate&gt;=0.25.0</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Prueba de la API">Prueba de la API<a class="anchor-link" href="#Prueba de la API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 28" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Desplegamos el space y probamos directamente la API.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2_localModel&quot;</span><span class="p">)</span>',
      '<span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;Hola, ¿cómo estás? Me llamo Máximo&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2-localmodel.hf.space ✔',
          'Hola Máximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en día. ¿Cómo puedo servirte?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Me sorprende lo rápido que responde el modelo estando en un servidor sin GPU.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor">Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor<a class="anchor-link" href="#Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 29" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora hacemos lo mismo que antes, pero con FastAPI, LangChain y Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear Space">Crear Space<a class="anchor-link" href="#Crear Space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripción, seleccionamos Docker como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW más básico y gratuito, y seleccionamos si lo hacemos privado o público.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">Código<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 32" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya no importamos <code>InferenceClient</code> y ahora importamos <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code> de la librería <code>transformers</code> e importamos <code>torch</code>.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer<br>import torch</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos el modelo y el tokenizer con <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code>.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Initialize the model and tokenizer<br>print("Cargando modelo y tokenizer...")<br>device = "cuda" if torch.cuda.is_available() else "cpu"<br>model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"<br><br>try:<br>&#x20;&#x20;# Load the model in BF16 format for better performance and lower memory usage<br>&#x20;&#x20;tokenizer = AutoTokenizer.from_pretrained(model_name)<br>&#x20;&#x20;<br>&#x20;&#x20;if device == "cuda":<br>&#x20;&#x20;&#x20;&#x20;print("Usando GPU para el modelo...")<br>&#x20;&#x20;&#x20;&#x20;model = AutoModelForCausalLM.from_pretrained(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;model_name,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;torch_dtype=torch.bfloat16,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device_map="auto",<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;low_cpu_mem_usage=True<br>&#x20;&#x20;&#x20;&#x20;)<br>&#x20;&#x20;else:<br>&#x20;&#x20;&#x20;&#x20;print("Usando CPU para el modelo...")<br>&#x20;&#x20;&#x20;&#x20;model = AutoModelForCausalLM.from_pretrained(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;model_name,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device_map={opening_brace}"": device{closing_brace},<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;torch_dtype=torch.float32<br>&#x20;&#x20;&#x20;&#x20;)<br><br>&#x20;&#x20;print(f"Modelo cargado exitosamente en: {opening_brace}device{closing_brace}")<br>except Exception as e:<br>&#x20;&#x20;print(f"Error al cargar el modelo: {opening_brace}str(e){closing_brace}")<br>&#x20;&#x20;raise</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Redefinimos la función <code>call_model</code> para que haga la inferencia con el modelo local.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Define the function that calls the model<br>def call_model(state: MessagesState):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Call the model with the given messages<br><br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;state: MessagesState<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;# Convert LangChain messages to chat format<br>&#x20;&#x20;messages = []<br>&#x20;&#x20;for msg in state["messages"]:<br>&#x20;&#x20;&#x20;&#x20;if isinstance(msg, HumanMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;messages.append({opening_brace}"role": "user", "content": msg.content{closing_brace})<br>&#x20;&#x20;&#x20;&#x20;elif isinstance(msg, AIMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;messages.append({opening_brace}"role": "assistant", "content": msg.content{closing_brace})<br>&#x20;&#x20;<br>&#x20;&#x20;# Prepare the input using the chat template<br>&#x20;&#x20;input_text = tokenizer.apply_chat_template(messages, tokenize=False)<br>&#x20;&#x20;inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)<br>&#x20;&#x20;<br>&#x20;&#x20;# Generate response<br>&#x20;&#x20;outputs = model.generate(<br>&#x20;&#x20;&#x20;&#x20;inputs,<br>&#x20;&#x20;&#x20;&#x20;max_new_tokens=512,  # Increase the number of tokens for longer responses<br>&#x20;&#x20;&#x20;&#x20;temperature=0.7,<br>&#x20;&#x20;&#x20;&#x20;top_p=0.9,<br>&#x20;&#x20;&#x20;&#x20;do_sample=True,<br>&#x20;&#x20;&#x20;&#x20;pad_token_id=tokenizer.eos_token_id<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Decode and clean the response<br>&#x20;&#x20;response = tokenizer.decode(outputs[0], skip_special_tokens=True)<br>&#x20;&#x20;# Extract only the assistant&#39;s response (after the last user message)<br>&#x20;&#x20;response = response.split("Assistant:")[-1].strip()<br>&#x20;&#x20;<br>&#x20;&#x20;# Convert the response to LangChain format<br>&#x20;&#x20;ai_message = AIMessage(content=response)<br>&#x20;&#x20;return {opening_brace}"messages": state["messages"] + [ai_message]{closing_brace}</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que quitar <code>langchain-huggingface</code> y añadir <code>transformers</code>, <code>accelerate</code> y <code>torch</code> en el archivo <code>requirements.txt</code>. El archivo quedaría:</p>
      <div class='highlight'><pre><code class="language-txt">fastapi
      uvicorn
      requests
      pydantic&gt;=2.0.0
      langchain&gt;=0.1.0
      langchain-core&gt;=0.1.10
      langgraph&gt;=0.2.27
      python-dotenv&gt;=1.0.0
      transformers&gt;=4.36.0
      torch&gt;=2.0.0
      accelerate&gt;=0.26.0</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dockerfile">Dockerfile<a class="anchor-link" href="#Dockerfile"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya no necesitamos tener <code>RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true</code> porque como el modelo va a estar en el servidor y no vamos a hacer llamadas a Inference Endpoints, no necesitamos el token. El archivo quedaría:</p>
      <div class='highlight'><pre><code class="language-dockerfile">FROM python:3.13-slim
      
      RUN useradd -m -u 1000 user
      WORKDIR /app
      
      COPY --chown=user ./requirements.txt requirements.txt
      RUN pip install --no-cache-dir --upgrade -r requirements.txt
      
      COPY --chown=user . /app
      
      EXPOSE 7860
      
      CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Prueba de la API">Prueba de la API<a class="anchor-link" href="#Prueba de la API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Desplegamos el space y probamos la API. En este caso lo voy a probar directamente desde python.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>',
      '<span class="w"> </span>',
      '<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://maximofn-smollm2-backend-localmodel.hf.space/generate&quot;</span>',
      '<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>',
      '<span class="w">    </span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;Hola, ¿cómo estás?&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;user1&quot;</span>',
      '<span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>',
      '<span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>',
      '<span class="w">    </span><span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Respuesta:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;generated_text&quot;</span><span class="p">])</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Thread ID:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;thread_id&quot;</span><span class="p">])</span>',
      '<span class="k">else</span><span class="p">:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error:&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Respuesta: system',
          'You are a friendly Chatbot. Always reply in the language in which the user is writing to you.',
          'user',
          'Hola, ¿cómo estás?',
          'assistant',
          'Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.',
          'Thread ID: user1',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Este tarda un poco más que el anterior. En realidad tarda lo normal para un modelo ejecutándose en un servidor sin GPU. Lo raro es cuando lo desplegamos en Gradio. No sé qué hará HuggingFace por detrás, o tal vez ha sido coincidencia</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Conclusiones">Conclusiones<a class="anchor-link" href="#Conclusiones"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos visto cómo crear una backend con un LLM, tanto haciendo llamadas al Inference Endpoint de HuggingFace, como haciendo llamadas a un modelo corriendo localmente. Hemos visto cómo hacerlo con Gradio o con FastAPI, Langchain y Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A partir de aquí tienes el conocimiento para poder desplegar tus propios modelos, incluso aunque no sean LLMs, podrían ser modelos multimodales. A partir de aquí puedes hacer lo que quieras.</p>
      </section>







    </div>

  </section>

</PostLayout>
