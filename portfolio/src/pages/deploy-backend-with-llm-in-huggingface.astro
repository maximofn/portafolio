---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Desplegar backend con LLM en HuggingFace';
const end_url = 'deploy-backend-with-llm-in-huggingface';
const description = '¬øQuieres desplegar un backend con tu propio LLM? En este post te explico c√≥mo hacerlo con HuggingFace Spaces, FastAPI, Langchain y Docker.';
const keywords = 'hugging face, fastapi, langchain, docker, backend, llm';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=960
    image_height=720
    image_extension=webp
    article_date=2025-03-02+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Desplegar backend con Gradio"><h2>Desplegar backend con Gradio</h2></a>
      <a class="anchor-link" href="#Crear space"><h3>Crear space</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#README.md"><h4>README.md</h4></a>
      <a class="anchor-link" href="#Despliegue"><h3>Despliegue</h3></a>
      <a class="anchor-link" href="#Backend"><h3>Backend</h3></a>
      <a class="anchor-link" href="#Prueba de la API"><h3>Prueba de la API</h3></a>
      <a class="anchor-link" href="#Desplegar backend con FastAPI, Langchain y Docker"><h2>Desplegar backend con FastAPI, Langchain y Docker</h2></a>
      <a class="anchor-link" href="#Crear space"><h3>Crear space</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#Dockerfile"><h4>Dockerfile</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#README.md"><h4>README.md</h4></a>
      <a class="anchor-link" href="#Token de HuggingFace"><h3>Token de HuggingFace</h3></a>
      <a class="anchor-link" href="#Anadir el token a los secrets del espacio"><h3>A√±adir el token a los secrets del espacio</h3></a>
      <a class="anchor-link" href="#Despliegue"><h3>Despliegue</h3></a>
      <a class="anchor-link" href="#URL del backend"><h3>URL del backend</h3></a>
      <a class="anchor-link" href="#Documentacion"><h3>Documentaci√≥n</h3></a>
      <a class="anchor-link" href="#Prueba de la API"><h3>Prueba de la API</h3></a>
      <a class="anchor-link" href="#Desplegar backend con Gradio y modelo corriendo en el servidor"><h2>Desplegar backend con Gradio y modelo corriendo en el servidor</h2></a>
      <a class="anchor-link" href="#Crear Space"><h3>Crear Space</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#Prueba de la API"><h4>Prueba de la API</h4></a>
      <a class="anchor-link" href="#Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor"><h2>Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor</h2></a>
      <a class="anchor-link" href="#Crear Space"><h3>Crear Space</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#Dockerfile"><h4>Dockerfile</h4></a>
      <a class="anchor-link" href="#Prueba de la API"><h3>Prueba de la API</h3></a>
      <a class="anchor-link" href="#Conclusiones"><h2>Conclusiones</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este post vamos a ver c√≥mo desplegar un backend en HuggingFace. Vamos a ver c√≥mo hacerlo de dos maneras, mediante la forma com√∫n, creando una aplicaci√≥n con Gradio, y mediante una opci√≥n diferente usando FastAPI, Langchain y Docker</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para ambos casos va a ser necesario tener una cuenta en HuggingFace, ya que vamos a desplegar el backend en un space de HuggingFace.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar backend con Gradio">Desplegar backend con Gradio<a class="anchor-link" href="#Desplegar backend con Gradio"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear space">Crear space<a class="anchor-link" href="#Crear space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero de todo, creamos un nuevo espacio en Hugging Face.</p>
      <ul>
        <li>Ponemos un nombre, una descripci√≥n y elegimos la licencia.</li>
        <li>Elegimos Gradio como el tipo de SDK. Al elegir Gradio, nos aparecer√°n plantillas, as√≠ que elegimos la plantilla de chatbot.</li>
        <li>Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t√∫ elige lo que mejor consideres.</li>
        <li>Y por √∫ltimo hay que elegir si queremos crear el espacio p√∫blico o privado.</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp" alt="backend gradio - create space">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">C√≥digo<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al crear el space, podemos clonarlo o podemos ver los archivos en la propia p√°gina de HuggingFace. Podemos ver que se han creado 3 archivos, <code>app.py</code>, <code>requirements.txt</code> y <code>README.md</code>. As√≠ que vamos a ver qu√© poner en cada uno</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aqu√≠ tenemos el c√≥digo de la aplicaci√≥n. Como hemos elegido la plantilla de chatbot, ya tenemos mucho hecho, pero vamos a tener que cambiar 2 cosas, primero el modelo de lenguaje y el system prompt</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como modelo de lenguaje veo <code>HuggingFaceH4/zephyr-7b-beta</code>, pero vamos a usar <code>Qwen/Qwen2.5-72B-Instruct</code>, que es un modelo muy capaz.</p>
      <p>As√≠ que busca el texto <code>client = InferenceClient(&quot;HuggingFaceH4/zephyr-7b-beta&quot;)</code> y reempl√°zalo por <code>client = InferenceClient(&quot;Qwen/Qwen2.5-72B-Instruct&quot;)</code>, o espera que m√°s adelante pondr√© todo el c√≥digo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tambi√©n vamos a cambiar el system prompt, que por defecto es <code>You are a friendly Chatbot.</code>, pero como es un modelo entrenado en su mayor√≠a en ingl√©s, es probable que si le hablas en otro idioma te responda en ingl√©s, as√≠ que vamos a cambiarlo por <code>You are a friendly Chatbot. Always reply in the language in which the user is writing to you.</code>.</p>
      <p>As√≠ que busca el texto <code>gr.Textbox(value=&quot;You are a friendly Chatbot.&quot;, label=&quot;System message&quot;),</code> y reempl√°zalo por <code>gr.Textbox(value=&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;, label=&quot;System message&quot;),</code>, o espera a que ahora voy a poner todo el c√≥digo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">import gradio as gr<br>from huggingface_hub import InferenceClient<br><br>"""<br>For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference<br>"""<br>client = InferenceClient("Qwen/Qwen2.5-72B-Instruct")<br><br><br>def respond(<br>&#x20;&#x20;message,<br>&#x20;&#x20;history: list[tuple[str, str]],<br>&#x20;&#x20;system_message,<br>&#x20;&#x20;max_tokens,<br>&#x20;&#x20;temperature,<br>&#x20;&#x20;top_p,<br>):<br>&#x20;&#x20;messages = [{opening_brace}"role": "system", "content": system_message{closing_brace}]<br><br>&#x20;&#x20;for val in history:<br>&#x20;&#x20;&#x20;&#x20;if val[0]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;messages.append({opening_brace}"role": "user", "content": val[0]{closing_brace})<br>&#x20;&#x20;&#x20;&#x20;if val[1]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;messages.append({opening_brace}"role": "assistant", "content": val[1]{closing_brace})<br><br>&#x20;&#x20;messages.append({opening_brace}"role": "user", "content": message{closing_brace})<br><br>&#x20;&#x20;response = ""<br><br>&#x20;&#x20;for message in client.chat_completion(<br>&#x20;&#x20;&#x20;&#x20;messages,<br>&#x20;&#x20;&#x20;&#x20;max_tokens=max_tokens,<br>&#x20;&#x20;&#x20;&#x20;stream=True,<br>&#x20;&#x20;&#x20;&#x20;temperature=temperature,<br>&#x20;&#x20;&#x20;&#x20;top_p=top_p,<br>&#x20;&#x20;):<br>&#x20;&#x20;&#x20;&#x20;token = message.choices[0].delta.content<br><br>&#x20;&#x20;&#x20;&#x20;response += token<br>&#x20;&#x20;&#x20;&#x20;yield response<br><br><br>"""<br>For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/gradio/chatinterface<br>"""<br>demo = gr.ChatInterface(<br>&#x20;&#x20;respond,<br>&#x20;&#x20;additional_inputs=[<br>&#x20;&#x20;&#x20;&#x20;gr.Textbox(value="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.", label="System message"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;minimum=0.1,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;maximum=1.0,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;value=0.95,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;step=0.05,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;label="Top-p (nucleus sampling)",<br>&#x20;&#x20;&#x20;&#x20;),<br>&#x20;&#x20;],<br>)<br><br><br>if __name__ == "__main__":<br>&#x20;&#x20;demo.launch()</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este es el archivo en el que estar√°n escritas las dependencias, pero para este caso va a ser muy sencillo:</p>
      <div class='highlight'><pre><code class="language-txt">huggingface_hub==0.25.2</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="README.md">README.md<a class="anchor-link" href="#README.md"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este es el archivo en el que vamos a poner la informaci√≥n del espacio. En los spaces de HuggingFace, al inicio de los readmes, se pone un c√≥digo para que HuggingFace sepa c√≥mo mostrar la miniatura del espacio, qu√© fichero tiene que usar para ejecutar el c√≥digo, versi√≥n del sdk, etc.</p>
      <div class='highlight'><pre><code class="language-md">---
      title: SmolLM2
      emoji: üí¨
      colorFrom: yellow
      colorTo: purple
      sdk: gradio
      sdk_version: 5.0.1
      app_file: app.py
      pinned: false
      license: apache-2.0
      short_description: Gradio SmolLM2 chat
      ---
      
      An example chatbot using [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index), and the [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Despliegue">Despliegue<a class="anchor-link" href="#Despliegue"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.</p>
      <p>As√≠ que cuando est√©n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp" alt="backend gradio - chatbot">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Backend">Backend<a class="anchor-link" href="#Backend"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Muy bien, hemos hecho un chatbot, pero no era la intenci√≥n, aqu√≠ hab√≠amos venido a hacer un backend! Para, para, f√≠jate lo que pone debajo del chatbot</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp" alt="backend gradio - Use via API">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver un texto <code>Use via API</code>, donde si pulsamos se nos abre un men√∫ con una API para poder usar el chatbot.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp" alt="backend gradio - API">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que nos da una documentaci√≥n de c√≥mo usar la API, tanto con Python, con JavaScript, como con bash.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Prueba de la API">Prueba de la API<a class="anchor-link" href="#Prueba de la API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usamos el c√≥digo de ejemplo de Python.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2&quot;</span><span class="p">)</span>',
      '<span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2.hf.space ‚úî',
          '¬°Hola M√°ximo! Mucho gusto, estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Estamos haciendo llamadas a la API de <code>InferenceClient</code> de HuggingFace, as√≠ que podr√≠amos pensar, ¬øPara qu√© hemos hecho un backend, si podemos llamar directamente a la API de HuggingFace? Pues lo vas a ver a continuaci√≥n.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;¬øC√≥mo me llamo?&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Tu nombre es M√°ximo. ¬øEs correcto?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>La plantilla de chat de Gradio maneja el historial por nosotros, de manera que cada vez que creamos un nuevo <code>cliente</code>, se crea un nuevo hilo de conversaci√≥n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a probar a crear un nuevo cliente, y ver si se crea un nuevo hilo de conversaci√≥n.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">new_client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2&quot;</span><span class="p">)</span>',
      '<span class="n">result</span> <span class="o">=</span> <span class="n">new_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;Hola, ¬øc√≥mo est√°s? Me llamo Luis&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2.hf.space ‚úî',
          'Hola Luis, estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? Es un gusto conocerte. ¬øEn qu√© puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora le volvemos a preguntar c√≥mo me llamo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">new_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;¬øC√≥mo me llamo?&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, tenemos dos clientes, cada uno con su propio hilo de conversaci√≥n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar backend con FastAPI, Langchain y Docker">Desplegar backend con FastAPI, Langchain y Docker<a class="anchor-link" href="#Desplegar backend con FastAPI, Langchain y Docker"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vamos a hacer lo mismo, crear un backend de un chatbot, con el mismo modelo, pero en este caso usando FastAPI, Langchain y Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear space">Crear space<a class="anchor-link" href="#Crear space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que crear un nuevo espacio, pero en este caso lo haremos de otra manera</p>
      <ul>
        <li>Ponemos un nombre, una descripci√≥n y elegimos la licencia.</li>
        <li>Elegimos Docker como el tipo de SDK. Al elegir Docker, nos aparecer√°n plantillas, as√≠ que elegimos una plantilla en blanco.</li>
        <li>Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t√∫ elige lo que mejor consideres.</li>
        <li>Y por √∫ltimo hay que elegir si queremos crear el espacio p√∫blico o privado.</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp" alt="backend docker - create space">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">C√≥digo<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora, al crear el space, vemos que solo tenemos un archivo, el <code>README.md</code>. As√≠ que vamos a tener que crear todo el c√≥digo nosotros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a crear el c√≥digo de la aplicaci√≥n</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Empezamos con las librer√≠as necesarias</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from fastapi import FastAPI, HTTPException<br>from pydantic import BaseModel<br>from huggingface_hub import InferenceClient<br><br>from langchain_core.messages import HumanMessage, AIMessage<br>from langgraph.checkpoint.memory import MemorySaver<br>from langgraph.graph import START, MessagesState, StateGraph<br><br>import os<br>from dotenv import load_dotenv<br>load_dotenv()</code></pre></div>
            </section>
      <p>Cargamos <code>fastapi</code> para poder crear las rutas de la API, <code>pydantic</code> para crear la plantilla de las querys, <code>huggingface_hub</code> para poder crear un modelo de lenguaje, <code>langchain</code> para poder indicarle al modelo si los mensajes son del chatbot o del usuario y <code>langgraph</code> para poder crear el chatbot.</p>
      <p>Adem√°s cargamos <code>os</code> y <code>dotenv</code> para poder cargar las variables de entorno.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cargamos el token de HuggingFace</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># HuggingFace token<br>HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el modelo de lenguaje</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Initialize the HuggingFace model<br>model = InferenceClient(<br>&#x20;&#x20;model="Qwen/Qwen2.5-72B-Instruct",<br>&#x20;&#x20;api_key=os.getenv("HUGGINGFACE_TOKEN")<br>)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos ahora una funci√≥n para llamar al modelo</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Define the function that calls the model<br>def call_model(state: MessagesState):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Call the model with the given messages<br><br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;state: MessagesState<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;# Convert LangChain messages to HuggingFace format<br>&#x20;&#x20;hf_messages = []<br>&#x20;&#x20;for msg in state["messages"]:<br>&#x20;&#x20;&#x20;&#x20;if isinstance(msg, HumanMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;hf_messages.append({opening_brace}"role": "user", "content": msg.content{closing_brace})<br>&#x20;&#x20;&#x20;&#x20;elif isinstance(msg, AIMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;hf_messages.append({opening_brace}"role": "assistant", "content": msg.content{closing_brace})<br>&#x20;&#x20;<br>&#x20;&#x20;# Call the API<br>&#x20;&#x20;response = model.chat_completion(<br>&#x20;&#x20;&#x20;&#x20;messages=hf_messages,<br>&#x20;&#x20;&#x20;&#x20;temperature=0.5,<br>&#x20;&#x20;&#x20;&#x20;max_tokens=64,<br>&#x20;&#x20;&#x20;&#x20;top_p=0.7<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Convert the response to LangChain format<br>&#x20;&#x20;ai_message = AIMessage(content=response.choices[0].message.content)<br>&#x20;&#x20;return {opening_brace}"messages": state["messages"] + [ai_message]{closing_brace}</code></pre></div>
            </section>
      <p>Convertimos los mensajes de formato LangChain a formato HuggingFace, as√≠ podemos usar el modelo de lenguaje.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Definimos una plantilla para las queries</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">class QueryRequest(BaseModel):<br>&#x20;&#x20;query: str<br>&#x20;&#x20;thread_id: str = "default"</code></pre></div>
            </section>
      <p>Las queries van a tener un <code>query</code>, el mensaje del usuario, y un <code>thread_id</code>, que es el identificador del hilo de la conversaci√≥n y m√°s adelante explicaremos para qu√© lo usamos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un grafo de LangGraph</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Define the graph<br>workflow = StateGraph(state_schema=MessagesState)<br><br># Define the node in the graph<br>workflow.add_edge(START, "model")<br>workflow.add_node("model", call_model)<br><br># Add memory<br>memory = MemorySaver()<br>graph_app = workflow.compile(checkpointer=memory)</code></pre></div>
            </section>
      <p>Con esto lo que hacemos es crear un grafo de LangGraph, que es una estructura de datos que nos permite crear un chatbot y que gestiona por nosotros el estado del chatbot, es decir, entre otras cosas, el historial de mensajes. As√≠ no lo tenemos que hacer nosotros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos la aplicaci√≥n de FastAPI</p>
      <div class='highlight'><pre><code class="language-python">app = FastAPI(title="LangChain FastAPI", description="API to generate text using LangChain and LangGraph")</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos los endpoints de la API</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Welcome endpoint<br>@app.get("/")<br>async def api_home():<br>&#x20;&#x20;"""Welcome endpoint"""<br>&#x20;&#x20;return {opening_brace}"detail": "Welcome to FastAPI, Langchain, Docker tutorial"{closing_brace}<br><br># Generate endpoint<br>@app.post("/generate")<br>async def generate(request: QueryRequest):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Endpoint to generate text using the language model<br>&#x20;&#x20;<br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;request: QueryRequest<br>&#x20;&#x20;&#x20;&#x20;query: str<br>&#x20;&#x20;&#x20;&#x20;thread_id: str = "default"<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;try:<br>&#x20;&#x20;&#x20;&#x20;# Configure the thread ID<br>&#x20;&#x20;&#x20;&#x20;config = {opening_brace}"configurable": {opening_brace}"thread_id": request.thread_id{closing_brace}{closing_brace}<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Create the input message<br>&#x20;&#x20;&#x20;&#x20;input_messages = [HumanMessage(content=request.query)]<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Invoke the graph<br>&#x20;&#x20;&#x20;&#x20;output = graph_app.invoke({opening_brace}"messages": input_messages{closing_brace}, config)<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Get the model response<br>&#x20;&#x20;&#x20;&#x20;response = output["messages"][-1].content<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;return {opening_brace}<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;"generated_text": response,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;"thread_id": request.thread_id<br>&#x20;&#x20;&#x20;&#x20;{closing_brace}<br>&#x20;&#x20;except Exception as e:<br>&#x20;&#x20;&#x20;&#x20;raise HTTPException(status_code=500, detail=f"Error al generar texto: {opening_brace}str(e){closing_brace}")</code></pre></div>
            </section>
      <p>Hemos creado el endpoint <code>/</code> que nos devolver√° un texto cuando accedamos a la API, y el endpoint <code>/generate</code> que es el que usaremos para generar el texto.</p>
      <p>Si nos fijamos en la funci√≥n <code>generate</code> tenemos la variable <code>config</code>, que es un diccionario que contiene el <code>thread_id</code>. Este <code>thread_id</code> es el que nos permite tener un historial de mensajes de cada usuario, de esta manera, diferentes usuarios pueden usar el mismo endpoint y tener su propio historial de mensajes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, tenemos el c√≥digo para que se pueda ejecutar la aplicaci√≥n</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">if __name__ == "__main__":<br>&#x20;&#x20;import uvicorn<br>&#x20;&#x20;uvicorn.run(app, host="0.0.0.0", port=7860)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a escribir todo el c√≥digo junto</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from fastapi import FastAPI, HTTPException<br>from pydantic import BaseModel<br>from huggingface_hub import InferenceClient<br><br>from langchain_core.messages import HumanMessage, AIMessage<br>from langgraph.checkpoint.memory import MemorySaver<br>from langgraph.graph import START, MessagesState, StateGraph<br><br>import os<br>from dotenv import load_dotenv<br>load_dotenv()<br><br># HuggingFace token<br>HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))<br><br># Initialize the HuggingFace model<br>model = InferenceClient(<br>&#x20;&#x20;model="Qwen/Qwen2.5-72B-Instruct",<br>&#x20;&#x20;api_key=os.getenv("HUGGINGFACE_TOKEN")<br>)<br><br># Define the function that calls the model<br>def call_model(state: MessagesState):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Call the model with the given messages<br><br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;state: MessagesState<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;# Convert LangChain messages to HuggingFace format<br>&#x20;&#x20;hf_messages = []<br>&#x20;&#x20;for msg in state["messages"]:<br>&#x20;&#x20;&#x20;&#x20;if isinstance(msg, HumanMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;hf_messages.append({opening_brace}"role": "user", "content": msg.content{closing_brace})<br>&#x20;&#x20;&#x20;&#x20;elif isinstance(msg, AIMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;hf_messages.append({opening_brace}"role": "assistant", "content": msg.content{closing_brace})<br>&#x20;&#x20;<br>&#x20;&#x20;# Call the API<br>&#x20;&#x20;response = model.chat_completion(<br>&#x20;&#x20;&#x20;&#x20;messages=hf_messages,<br>&#x20;&#x20;&#x20;&#x20;temperature=0.5,<br>&#x20;&#x20;&#x20;&#x20;max_tokens=64,<br>&#x20;&#x20;&#x20;&#x20;top_p=0.7<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Convert the response to LangChain format<br>&#x20;&#x20;ai_message = AIMessage(content=response.choices[0].message.content)<br>&#x20;&#x20;return {opening_brace}"messages": state["messages"] + [ai_message]{closing_brace}<br><br># Define the graph<br>workflow = StateGraph(state_schema=MessagesState)<br><br># Define the node in the graph<br>workflow.add_edge(START, "model")<br>workflow.add_node("model", call_model)<br><br># Add memory<br>memory = MemorySaver()<br>graph_app = workflow.compile(checkpointer=memory)<br><br># Define the data model for the request<br>class QueryRequest(BaseModel):<br>&#x20;&#x20;query: str<br>&#x20;&#x20;thread_id: str = "default"<br><br># Create the FastAPI application<br>app = FastAPI(title="LangChain FastAPI", description="API to generate text using LangChain and LangGraph")<br><br># Welcome endpoint<br>@app.get("/")<br>async def api_home():<br>&#x20;&#x20;"""Welcome endpoint"""<br>&#x20;&#x20;return {opening_brace}"detail": "Welcome to FastAPI, Langchain, Docker tutorial"{closing_brace}<br><br># Generate endpoint<br>@app.post("/generate")<br>async def generate(request: QueryRequest):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Endpoint to generate text using the language model<br>&#x20;&#x20;<br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;request: QueryRequest<br>&#x20;&#x20;&#x20;&#x20;query: str<br>&#x20;&#x20;&#x20;&#x20;thread_id: str = "default"<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;try:<br>&#x20;&#x20;&#x20;&#x20;# Configure the thread ID<br>&#x20;&#x20;&#x20;&#x20;config = {opening_brace}"configurable": {opening_brace}"thread_id": request.thread_id{closing_brace}{closing_brace}<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Create the input message<br>&#x20;&#x20;&#x20;&#x20;input_messages = [HumanMessage(content=request.query)]<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Invoke the graph<br>&#x20;&#x20;&#x20;&#x20;output = graph_app.invoke({opening_brace}"messages": input_messages{closing_brace}, config)<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;# Get the model response<br>&#x20;&#x20;&#x20;&#x20;response = output["messages"][-1].content<br>&#x20;&#x20;&#x20;&#x20;<br>&#x20;&#x20;&#x20;&#x20;return {opening_brace}<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;"generated_text": response,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;"thread_id": request.thread_id<br>&#x20;&#x20;&#x20;&#x20;{closing_brace}<br>&#x20;&#x20;except Exception as e:<br>&#x20;&#x20;&#x20;&#x20;raise HTTPException(status_code=500, detail=f"Error al generar texto: {opening_brace}str(e){closing_brace}")<br><br>if __name__ == "__main__":<br>&#x20;&#x20;import uvicorn<br>&#x20;&#x20;uvicorn.run(app, host="0.0.0.0", port=7860)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dockerfile">Dockerfile<a class="anchor-link" href="#Dockerfile"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vemos c√≥mo crear el Dockerfile</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero indicamos desde qu√© imagen vamos a partir</p>
      <div class='highlight'><pre><code class="language-dockerfile">FROM python:3.13-slim</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos el directorio de trabajo</p>
      <div class='highlight'><pre><code class="language-dockerfile">RUN useradd -m -u 1000 user
      WORKDIR /app</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Copiamos el archivo con las dependencias e instalamos</p>
      <div class='highlight'><pre><code class="language-dockerfile">COPY --chown=user ./requirements.txt requirements.txt
      RUN pip install --no-cache-dir --upgrade -r requirements.txt</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Copiamos el resto del c√≥digo</p>
      <div class='highlight'><pre><code class="language-dockerfile">COPY --chown=user . /app</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Exponemos el puerto 7860</p>
      <div class='highlight'><pre><code class="language-dockerfile">EXPOSE 7860</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos las variables de entorno</p>
      <div class='highlight'><pre><code class="language-dockerfile">RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \
          test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Secret exists!"</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, indicamos el comando para ejecutar la aplicaci√≥n</p>
      <div class='highlight'><pre><code class="language-dockerfile">CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora lo ponemos todo junto</p>
      <div class='highlight'><pre><code class="language-dockerfile">FROM python:3.13-slim
      
      RUN useradd -m -u 1000 user
      WORKDIR /app
      
      COPY --chown=user ./requirements.txt requirements.txt
      RUN pip install --no-cache-dir --upgrade -r requirements.txt
      
      COPY --chown=user . /app
      
      EXPOSE 7860
      
      RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \
          test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Secret exists!"
      
      CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el archivo con las dependencias</p>
      <div class='highlight'><pre><code class="language-txt">fastapi
      uvicorn
      requests
      pydantic&gt;=2.0.0
      langchain
      langchain-huggingface
      langchain-core
      langgraph &gt; 0.2.27
      python-dotenv.2.11</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="README.md">README.md<a class="anchor-link" href="#README.md"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, creamos el archivo README.md con informaci√≥n del espacio y con las intrucciones para HugginFace</p>
      <div class='highlight'><pre><code class="language-md">---
      title: SmolLM2 Backend
      emoji: üìä
      colorFrom: yellow
      colorTo: red
      sdk: docker
      pinned: false
      license: apache-2.0
      short_description: Backend of SmolLM2 chat
      app_port: 7860
      ---
      
      # SmolLM2 Backend
      
      This project implements a FastAPI API that uses LangChain and LangGraph to generate text with the Qwen2.5-72B-Instruct model from HuggingFace.
      
      ## Configuration
      
      ### In HuggingFace Spaces
      
      This project is designed to run in HuggingFace Spaces. To configure it:
      
      1. Create a new Space in HuggingFace with SDK Docker
      2. Configure the `HUGGINGFACE_TOKEN` or `HF_TOKEN` environment variable in the Space configuration:
         - Go to the "Settings" tab of your Space
         - Scroll down to the "Repository secrets" section
         - Add a new variable with the name `HUGGINGFACE_TOKEN` and your token as the value
         - Save the changes
      
      ### Local development
      
      For local development:
      
      1. Clone this repository
      2. Create a `.env` file in the project root with your HuggingFace token:
         ``
         HUGGINGFACE_TOKEN=your_token_here
         ``
      3. Install the dependencies:
         ``
         pip install -r requirements.txt
         ``
      
      ## Local execution
      
      ``bash
      uvicorn app:app --reload
      ``
      
      The API will be available at `http://localhost:8000`.
      
      ## Endpoints
      
      ### GET `/`
      
      Welcome endpoint that returns a greeting message.
      
      ### POST `/generate`
      
      Endpoint to generate text using the language model.
      
      **Request parameters:**
      ``json
      {opening_brace}
        "query": "Your question here",
        "thread_id": "optional_thread_identifier"
      {closing_brace}
      ``
      
      **Response:**
      ``json
      {opening_brace}
        "generated_text": "Generated text by the model",
        "thread_id": "thread identifier"
      {closing_brace}
      ``
      
      ## Docker
      
      To run the application in a Docker container:
      
      ``bash
      # Build the image
      docker build -t smollm2-backend .
      
      # Run the container
      docker run -p 8000:8000 --env-file .env smollm2-backend
      ``
      
      ## API documentation
      
      The interactive API documentation is available at:
      - Swagger UI: `http://localhost:8000/docs`
      - ReDoc: `http://localhost:8000/redoc`</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Token de HuggingFace">Token de HuggingFace<a class="anchor-link" href="#Token de HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h3>
      <p>Si te has fijado en el c√≥digo y en el Dockerfile hemos usado un token de HuggingFace, as√≠ que vamos a tener que crear uno. En nuestra cuenta de HuggingFace creamos un <a href="https://huggingface.co/settings/tokens/new?tokenType=fineGrained" target="_blank" rel="nofollow noreferrer">nuevo token</a>, le ponemos un nombre y le damos los siguientes permisos:</p>
      <ul>
        <li>Read access to contents of all repos under your personal namespace</li>
        <li>Read access to contents of all repos under your personal namespacev</li>
        <li>Make calls to inference providers</li>
        <li>Make calls to Inference Endpoints</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-token.webp" alt="backend docker - token">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Anadir el token a los secrets del espacio">A√±adir el token a los secrets del espacio<a class="anchor-link" href="#Anadir el token a los secrets del espacio"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que ya tenemos el token, necesitamos a√±adirlo al espacio. En la parte de arriba de la app, podremos ver un bot√≥n llamado <code>Settings</code>, lo pulsamos y podremos ver la secci√≥n de configuraci√≥n del espacio.</p>
      <p>Si bajamos, podremos ver una secci√≥n en la que podemos a√±adir <code>Variables</code> y <code>Secrets</code>. En este caso, como estamos a√±adiendo un token, lo vamos a a√±adir a los <code>Secrets</code>.</p>
      <p>Le ponemos el nombre <code>HUGGINGFACE_TOKEN</code> y el valor del token.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Despliegue">Despliegue<a class="anchor-link" href="#Despliegue"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.</p>
      <p>As√≠ que cuando est√©n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.</p>
      <p>En este caso, solo hemos construido un backend, por lo que lo que vamos a ver al entrar al espacio es lo que definimos en el endpoint <code>/</code></p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-space.webp" alt="backend docker - space">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="URL del backend">URL del backend<a class="anchor-link" href="#URL del backend"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Necesitamos saber la URL del backend para poder hacer llamadas a la API. Para ello, tenemos que pulsar en los tres puntos de la parte superior derecha para ver las opciones</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-options.webp" alt="backend docker - options">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el men√∫ que se despliega pulsamos en <code>Embed this Spade</code>, se nos abrir√° una ventana en la que indica c√≥mo embeber el espacio con un iframe y adem√°s nos dar√° la URL del espacio.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-embed.webp" alt="backend docker - embed">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora nos vamos a esa URL, veremos lo mismo que en el espacio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Documentacion">Documentaci√≥n<a class="anchor-link" href="#Documentacion"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>FastAPI, a parte de ser una API rapid√≠sima, tiene otra gran ventaja, y es que genera documentaci√≥n de manera autom√°tica.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si a√±adimos <code>/docs</code> a la URL que vimos antes, podremos ver la documentaci√≥n de la API con <code>Swagger UI</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-swagger-doc.webp" alt="backend docker - swagger doc">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tambi√©n podemos a√±adir <code>/redoc</code> a la URL para ver la documentaci√≥n con <code>ReDoc</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-redoc.webp" alt="backend docker - redoc doc">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Prueba de la API">Prueba de la API<a class="anchor-link" href="#Prueba de la API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Lo bueno de la documentaci√≥n <code>Swagger UI</code> es que nos permite probar la API directamente desde el navegador.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A√±adimos <code>/docs</code> a la URL que obtuvimos, abrimos el desplegable del endpoint <code>/generate</code> y le damos a <code>Try it out</code>, modificamos el valor de la <code>query</code> y del <code>thread_id</code> y pulsamos en <code>Execute</code>.</p>
      <p>En el primer caso voy a poner</p>
      <ul>
        <li><strong>query</strong>: Hola, ¬øC√≥mo est√°s? Soy M√°ximo</li>
        <li><strong>thread_id</strong>: user1</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-test-API.webp" alt="backend docker - test API">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Recibimos la siguiente respuesta <code>¬°Hola M√°ximo! Estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?</code></p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user1.webp" alt="backend docker -response 1 - user1">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a probar ahora la misma pregunta, pero con un <code>thread_id</code> diferente, en este caso <code>user2</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-query1-user2.webp" alt="backend docker - query 1 - user2">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Y nos responde esto <code>¬°Hola Luis! Estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?</code></p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user2.webp" alt="backend docker - response 1 - user2">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora pedimos nuestro nombre con los dos usuarios y obtenemos esto</p>
      <ul>
        <li>Para el usuario <strong>user1</strong>: <code>Te llamas M√°ximo. ¬øHay algo m√°s en lo que pueda ayudarte?</code></li>
        <li>Para el usuario <strong>user2</strong>: <code>Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte hoy, Luis?</code></li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user1.webp" alt="backend docker - response 2 - user1">
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user2.webp" alt="backend docker - response 2 - user2">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar backend con Gradio y modelo corriendo en el servidor">Desplegar backend con Gradio y modelo corriendo en el servidor<a class="anchor-link" href="#Desplegar backend con Gradio y modelo corriendo en el servidor"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los dos backends que hemos creado en realidad no est√°n corriendo un modelo, sino que est√°n haciendo llamadas a Inference Endpoints de HuggingFace. Pero puede que queramos que todo corra en el servidor, incluso el modelo. Puede ser que hayas hecho un fine-tuning de un LLM para tu caso de uso, por lo que ya no puedes hacer llamadas a Inference Endpoints.</p>
      <p>As√≠ que vamos a ver c√≥mo modificar el c√≥digo de los dos backends para correr un modelo en el servidor y no hacer llamadas a Inference Endpoints.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear Space">Crear Space<a class="anchor-link" href="#Crear Space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripci√≥n, seleccionamos Gradio como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW m√°s b√°sico y gratuito, y seleccionamos si lo hacemos privado o p√∫blico.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">C√≥digo<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que hacer cambios en <code>app.py</code> y en <code>requirements.txt</code> para que en lugar de hacer llamadas a Inference Endpoints, se ejecute el modelo localmente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los cambios que tenemos que hacer son</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importar <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code> de la librer√≠a <code>transformers</code> e importar <code>torch</code></p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer<br>import torch</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En lugar de crear un modelo mediante <code>InferenceClient</code> lo creamos con <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code></p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Cargar el modelo y el tokenizer<br>model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"<br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>model = AutoModelForCausalLM.from_pretrained(<br>&#x20;&#x20;model_name,<br>&#x20;&#x20;torch_dtype=torch.float16,<br>&#x20;&#x20;device_map="auto"<br>)</code></pre></div>
            </section>
      <p>Utilizo <code>HuggingFaceTB/SmolLM2-1.7B-Instruct</code> porque es un modelo bastante capaz de solo 1.7B de par√°metros. Como he elegido el HW m√°s b√°sico no puedo usar modelos muy grandes. T√∫, si quieres usar un modelo m√°s grande tienes dos opciones, usar el HW gratuito y aceptar que la inferencia va a ser m√°s lenta, o usar un HW m√°s potente, pero de pago.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Modificar la funci√≥n <code>respond</code> para que construya el prompt con la estructura necesaria por la librer√≠a <code>transformers</code>, tokenizar el prompt, hacer la inferencia y destokenizar la respuesta.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">def respond(<br>&#x20;&#x20;message,<br>&#x20;&#x20;history: list[tuple[str, str]],<br>&#x20;&#x20;system_message,<br>&#x20;&#x20;max_tokens,<br>&#x20;&#x20;temperature,<br>&#x20;&#x20;top_p,<br>):<br>&#x20;&#x20;# Construir el prompt con el formato correcto<br>&#x20;&#x20;prompt = f"&lt;|system|&gt;\n{opening_brace}system_message{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;for val in history:<br>&#x20;&#x20;&#x20;&#x20;if val[0]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;prompt += f"&lt;|user|&gt;\n{opening_brace}val[0]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;&#x20;&#x20;if val[1]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;prompt += f"&lt;|assistant|&gt;\n{opening_brace}val[1]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;prompt += f"&lt;|user|&gt;\n{opening_brace}message{closing_brace}&lt;/s&gt;\n&lt;|assistant|&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;# Tokenizar el prompt<br>&#x20;&#x20;inputs = tokenizer(prompt, return_tensors="pt").to(model.device)<br>&#x20;&#x20;<br>&#x20;&#x20;# Generar la respuesta<br>&#x20;&#x20;outputs = model.generate(<br>&#x20;&#x20;&#x20;&#x20;**inputs,<br>&#x20;&#x20;&#x20;&#x20;max_new_tokens=max_tokens,<br>&#x20;&#x20;&#x20;&#x20;temperature=temperature,<br>&#x20;&#x20;&#x20;&#x20;top_p=top_p,<br>&#x20;&#x20;&#x20;&#x20;do_sample=True,<br>&#x20;&#x20;&#x20;&#x20;pad_token_id=tokenizer.eos_token_id<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Decodificar la respuesta<br>&#x20;&#x20;response = tokenizer.decode(outputs[0], skip_special_tokens=True)<br>&#x20;&#x20;<br>&#x20;&#x20;# Extraer solo la parte de la respuesta del asistente<br>&#x20;&#x20;response = response.split("&lt;|assistant|&gt;\n")[-1].strip()<br>&#x20;&#x20;<br>&#x20;&#x20;yield response</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A continuaci√≥n dejo todo el c√≥digo</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">import gradio as gr<br>from transformers import AutoModelForCausalLM, AutoTokenizer<br>import torch<br><br>"""<br>For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference<br>"""<br><br># Cargar el modelo y el tokenizer<br>model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"<br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>model = AutoModelForCausalLM.from_pretrained(<br>&#x20;&#x20;model_name,<br>&#x20;&#x20;torch_dtype=torch.float16,<br>&#x20;&#x20;device_map="auto"<br>)<br><br>def respond(<br>&#x20;&#x20;message,<br>&#x20;&#x20;history: list[tuple[str, str]],<br>&#x20;&#x20;system_message,<br>&#x20;&#x20;max_tokens,<br>&#x20;&#x20;temperature,<br>&#x20;&#x20;top_p,<br>):<br>&#x20;&#x20;# Construir el prompt con el formato correcto<br>&#x20;&#x20;prompt = f"&lt;|system|&gt;\n{opening_brace}system_message{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;for val in history:<br>&#x20;&#x20;&#x20;&#x20;if val[0]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;prompt += f"&lt;|user|&gt;\n{opening_brace}val[0]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;&#x20;&#x20;if val[1]:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;prompt += f"&lt;|assistant|&gt;\n{opening_brace}val[1]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;prompt += f"&lt;|user|&gt;\n{opening_brace}message{closing_brace}&lt;/s&gt;\n&lt;|assistant|&gt;\n"<br>&#x20;&#x20;<br>&#x20;&#x20;# Tokenizar el prompt<br>&#x20;&#x20;inputs = tokenizer(prompt, return_tensors="pt").to(model.device)<br>&#x20;&#x20;<br>&#x20;&#x20;# Generar la respuesta<br>&#x20;&#x20;outputs = model.generate(<br>&#x20;&#x20;&#x20;&#x20;**inputs,<br>&#x20;&#x20;&#x20;&#x20;max_new_tokens=max_tokens,<br>&#x20;&#x20;&#x20;&#x20;temperature=temperature,<br>&#x20;&#x20;&#x20;&#x20;top_p=top_p,<br>&#x20;&#x20;&#x20;&#x20;do_sample=True,<br>&#x20;&#x20;&#x20;&#x20;pad_token_id=tokenizer.eos_token_id<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Decodificar la respuesta<br>&#x20;&#x20;response = tokenizer.decode(outputs[0], skip_special_tokens=True)<br>&#x20;&#x20;<br>&#x20;&#x20;# Extraer solo la parte de la respuesta del asistente<br>&#x20;&#x20;response = response.split("&lt;|assistant|&gt;\n")[-1].strip()<br>&#x20;&#x20;<br>&#x20;&#x20;yield response<br><br><br>"""<br>For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/gradio/chatinterface<br>"""<br>demo = gr.ChatInterface(<br>&#x20;&#x20;respond,<br>&#x20;&#x20;additional_inputs=[<br>&#x20;&#x20;&#x20;&#x20;gr.Textbox(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;value="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.", <br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;label="System message"<br>&#x20;&#x20;&#x20;&#x20;),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="Max new tokens"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label="Temperature"),<br>&#x20;&#x20;&#x20;&#x20;gr.Slider(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;minimum=0.1,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;maximum=1.0,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;value=0.95,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;step=0.05,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;label="Top-p (nucleus sampling)",<br>&#x20;&#x20;&#x20;&#x20;),<br>&#x20;&#x20;],<br>)<br><br><br>if __name__ == "__main__":<br>&#x20;&#x20;demo.launch()</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 27" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este archivo hay que a√±adir las nuevas librer√≠as que vamos a usar, en este caso <code>transformers</code>, <code>accelerate</code> y <code>torch</code>. El archivo entero quedar√≠a:</p>
      <div class='highlight'><pre><code class="language-txt">huggingface_hub==0.25.2
      gradio&gt;=4.0.0
      transformers&gt;=4.36.0
      torch&gt;=2.0.0
      accelerate&gt;=0.25.0</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Prueba de la API">Prueba de la API<a class="anchor-link" href="#Prueba de la API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 28" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Desplegamos el space y probamos directamente la API.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2_localModel&quot;</span><span class="p">)</span>',
      '<span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî',
          'Hola M√°ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en d√≠a. ¬øC√≥mo puedo servirte?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Me sorprende lo r√°pido que responde el modelo estando en un servidor sin GPU.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor">Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor<a class="anchor-link" href="#Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 29" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora hacemos lo mismo que antes, pero con FastAPI, LangChain y Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Crear Space">Crear Space<a class="anchor-link" href="#Crear Space"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripci√≥n, seleccionamos Docker como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW m√°s b√°sico y gratuito, y seleccionamos si lo hacemos privado o p√∫blico.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">C√≥digo<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 32" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya no importamos <code>InferenceClient</code> y ahora importamos <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code> de la librer√≠a <code>transformers</code> e importamos <code>torch</code>.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer<br>import torch</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos el modelo y el tokenizer con <code>AutoModelForCausalLM</code> y <code>AutoTokenizer</code>.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Initialize the model and tokenizer<br>print("Cargando modelo y tokenizer...")<br>device = "cuda" if torch.cuda.is_available() else "cpu"<br>model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"<br><br>try:<br>&#x20;&#x20;# Load the model in BF16 format for better performance and lower memory usage<br>&#x20;&#x20;tokenizer = AutoTokenizer.from_pretrained(model_name)<br>&#x20;&#x20;<br>&#x20;&#x20;if device == "cuda":<br>&#x20;&#x20;&#x20;&#x20;print("Usando GPU para el modelo...")<br>&#x20;&#x20;&#x20;&#x20;model = AutoModelForCausalLM.from_pretrained(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;model_name,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;torch_dtype=torch.bfloat16,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device_map="auto",<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;low_cpu_mem_usage=True<br>&#x20;&#x20;&#x20;&#x20;)<br>&#x20;&#x20;else:<br>&#x20;&#x20;&#x20;&#x20;print("Usando CPU para el modelo...")<br>&#x20;&#x20;&#x20;&#x20;model = AutoModelForCausalLM.from_pretrained(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;model_name,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device_map={opening_brace}"": device{closing_brace},<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;torch_dtype=torch.float32<br>&#x20;&#x20;&#x20;&#x20;)<br><br>&#x20;&#x20;print(f"Modelo cargado exitosamente en: {opening_brace}device{closing_brace}")<br>except Exception as e:<br>&#x20;&#x20;print(f"Error al cargar el modelo: {opening_brace}str(e){closing_brace}")<br>&#x20;&#x20;raise</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Redefinimos la funci√≥n <code>call_model</code> para que haga la inferencia con el modelo local.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Define the function that calls the model<br>def call_model(state: MessagesState):<br>&#x20;&#x20;"""<br>&#x20;&#x20;Call the model with the given messages<br><br>&#x20;&#x20;Args:<br>&#x20;&#x20;&#x20;&#x20;state: MessagesState<br><br>&#x20;&#x20;Returns:<br>&#x20;&#x20;&#x20;&#x20;dict: A dictionary containing the generated text and the thread ID<br>&#x20;&#x20;"""<br>&#x20;&#x20;# Convert LangChain messages to chat format<br>&#x20;&#x20;messages = []<br>&#x20;&#x20;for msg in state["messages"]:<br>&#x20;&#x20;&#x20;&#x20;if isinstance(msg, HumanMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;messages.append({opening_brace}"role": "user", "content": msg.content{closing_brace})<br>&#x20;&#x20;&#x20;&#x20;elif isinstance(msg, AIMessage):<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;messages.append({opening_brace}"role": "assistant", "content": msg.content{closing_brace})<br>&#x20;&#x20;<br>&#x20;&#x20;# Prepare the input using the chat template<br>&#x20;&#x20;input_text = tokenizer.apply_chat_template(messages, tokenize=False)<br>&#x20;&#x20;inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)<br>&#x20;&#x20;<br>&#x20;&#x20;# Generate response<br>&#x20;&#x20;outputs = model.generate(<br>&#x20;&#x20;&#x20;&#x20;inputs,<br>&#x20;&#x20;&#x20;&#x20;max_new_tokens=512,  # Increase the number of tokens for longer responses<br>&#x20;&#x20;&#x20;&#x20;temperature=0.7,<br>&#x20;&#x20;&#x20;&#x20;top_p=0.9,<br>&#x20;&#x20;&#x20;&#x20;do_sample=True,<br>&#x20;&#x20;&#x20;&#x20;pad_token_id=tokenizer.eos_token_id<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Decode and clean the response<br>&#x20;&#x20;response = tokenizer.decode(outputs[0], skip_special_tokens=True)<br>&#x20;&#x20;# Extract only the assistant&#39;s response (after the last user message)<br>&#x20;&#x20;response = response.split("Assistant:")[-1].strip()<br>&#x20;&#x20;<br>&#x20;&#x20;# Convert the response to LangChain format<br>&#x20;&#x20;ai_message = AIMessage(content=response)<br>&#x20;&#x20;return {opening_brace}"messages": state["messages"] + [ai_message]{closing_brace}</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que quitar <code>langchain-huggingface</code> y a√±adir <code>transformers</code>, <code>accelerate</code> y <code>torch</code> en el archivo <code>requirements.txt</code>. El archivo quedar√≠a:</p>
      <div class='highlight'><pre><code class="language-txt">fastapi
      uvicorn
      requests
      pydantic&gt;=2.0.0
      langchain&gt;=0.1.0
      langchain-core&gt;=0.1.10
      langgraph&gt;=0.2.27
      python-dotenv&gt;=1.0.0
      transformers&gt;=4.36.0
      torch&gt;=2.0.0
      accelerate&gt;=0.26.0</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dockerfile">Dockerfile<a class="anchor-link" href="#Dockerfile"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya no necesitamos tener <code>RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true</code> porque como el modelo va a estar en el servidor y no vamos a hacer llamadas a Inference Endpoints, no necesitamos el token. El archivo quedar√≠a:</p>
      <div class='highlight'><pre><code class="language-dockerfile">FROM python:3.13-slim
      
      RUN useradd -m -u 1000 user
      WORKDIR /app
      
      COPY --chown=user ./requirements.txt requirements.txt
      RUN pip install --no-cache-dir --upgrade -r requirements.txt
      
      COPY --chown=user . /app
      
      EXPOSE 7860
      
      CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Prueba de la API">Prueba de la API<a class="anchor-link" href="#Prueba de la API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Desplegamos el space y probamos la API. En este caso lo voy a probar directamente desde python.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>',
      '<span class="w"> </span>',
      '<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://maximofn-smollm2-backend-localmodel.hf.space/generate&quot;</span>',
      '<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>',
      '<span class="w">    </span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;Hola, ¬øc√≥mo est√°s?&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;user1&quot;</span>',
      '<span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>',
      '<span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>',
      '<span class="w">    </span><span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Respuesta:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;generated_text&quot;</span><span class="p">])</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Thread ID:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;thread_id&quot;</span><span class="p">])</span>',
      '<span class="k">else</span><span class="p">:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error:&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Respuesta: system',
          'You are a friendly Chatbot. Always reply in the language in which the user is writing to you.',
          'user',
          'Hola, ¬øc√≥mo est√°s?',
          'assistant',
          'Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.',
          'Thread ID: user1',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Este tarda un poco m√°s que el anterior. En realidad tarda lo normal para un modelo ejecut√°ndose en un servidor sin GPU. Lo raro es cuando lo desplegamos en Gradio. No s√© qu√© har√° HuggingFace por detr√°s, o tal vez ha sido coincidencia</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Conclusiones">Conclusiones<a class="anchor-link" href="#Conclusiones"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos visto c√≥mo crear una backend con un LLM, tanto haciendo llamadas al Inference Endpoint de HuggingFace, como haciendo llamadas a un modelo corriendo localmente. Hemos visto c√≥mo hacerlo con Gradio o con FastAPI, Langchain y Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A partir de aqu√≠ tienes el conocimiento para poder desplegar tus propios modelos, incluso aunque no sean LLMs, podr√≠an ser modelos multimodales. A partir de aqu√≠ puedes hacer lo que quieras.</p>
      </section>







    </div>

  </section>

</PostLayout>
