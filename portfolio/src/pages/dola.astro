---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'DoLa ‚Äì Decoding by Contrasting Layers Improves Factuality in Large Language Models';
const end_url = 'dola';
const description = '¬øAlguna vez has hablado con un LLM y te ha respondido algo que suena como si hubiera estado bebiendo caf√© de m√°quina durante toda la noche? üòÇ ¬°Eso es lo que llamamos una alucinaci√≥n en el mundo de los LLMs! Pero no te preocupes, porque no es que tu modelo de lenguaje est√© loco (aunque a veces puede parecerlo ü§™). La verdad es que los LLMs pueden ser un poco... creativos cuando se trata de generar texto. Pero gracias a DoLa, un m√©todo que utiliza capas de contraste para mejorar la factibilidad de los LLMs, podemos evitar que nuestros modelos de lenguaje se conviertan en escritores de ciencia ficci√≥n üòÇ. En este post, te explicar√© c√≥mo funciona DoLa y te mostrar√© un ejemplo de c√≥digo para que puedas entender mejor c√≥mo hacer que tus LLMs sean m√°s fiables y menos propensos a inventar historias. ¬°Vamos a salvar a nuestros LLMs de la locura y hacer que sean m√°s √∫tiles! üöÄ';
const keywords = 'dola, decodificaci√≥n por capas contrastantes, factibilidad, grandes modelos de lenguaje, transformers, hugging face, nlp, procesamiento de lenguaje natural, aprendizaje autom√°tico, inteligencia artificial';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-08-01+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Metodo"><h2>M√©todo</h2></a>
      <a class="anchor-link" href="#Seleccion dinamica de la capa prematura"><h2>Selecci√≥n din√°mica de la capa prematura</h2></a>
      <a class="anchor-link" href="#Contraste de las predicciones"><h2>Contraste de las predicciones</h2></a>
      <a class="anchor-link" href="#Penalizacion por repeticion"><h2>Penalizaci√≥n por repetici√≥n</h2></a>
      <a class="anchor-link" href="#Implementacion con transformers"><h2>Implementaci√≥n con transformers</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models">DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models<a class="anchor-link" href="#DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aunque a medida que los LLMs van aumentando de tama√±o y van surgiendo nuevas capacidades, tenemos un problema y son las alucionaciones. Los autores del paper <a href="https://arxiv.org/abs/2309.03883" target="_blank" rel="nofollow noreferrer">DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</a> proponen un m√©todo para evitar este problema.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Proponen un enfoque de decodificaci√≥n contrastiva, donde la probabilidad de salida de la siguiente palabra se obtiene de la diferencia en los logits entre una capa superior y una inferior. Al enfatizar el conocimiento de las capas superiores y restarle importancia al de las inferiores, podemos hacer que los LM sean m√°s factuales y, por lo tanto, reducir las alucinaciones.</p>
      <p>En la siguiente figura se muestra esta idea. Mientras que <code>Seattle</code> mantiene una alta probabilidad en todas las capas, la probabilidad de la respuesta correcta <code>Olympia</code> aumenta despu√©s de que las capas superiores inyecten m√°s conocimiento factual. Contrastar las diferencias entre las distintas capas puede revelar la respuesta correcta en este caso</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure1.webp" width="748" height="431" alt="DoLa-figure1">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Metodo">M√©todo<a class="anchor-link" href="#Metodo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Un LLM consiste en una capa de embedding, varios transformers secuenciales y a continuaci√≥n una capa de salida. Lo que proponen es medir la salida de cada transformer mediante la divergencia de Jensen-Shannon (JSD)</p>
      <p>En la siguiente figura se puede ver esta medida a la salida de cada transformer para una frase de entrada al LLM. Cada columna corresponde a un token de la frase.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure2.webp" width="972" height="403" alt="DoLa-figure2">
      <p>Se pueden observar dos patrones</p>
      <ul>
        <li>El primero ocurre cuando se predicen entidades de nombre o fechas importantes, como <code>Wole Soyinka</code> y <code>1986</code>, que requieren conocimiento factual. Se puede ver que la JSD calculada sigue siendo extremadamente alta en las capas superiores. Este patr√≥n indica que el modelo sigue cambiando sus predicciones en las √∫ltimas capas, y potencialmente inyectando m√°s conocimiento factual en las predicciones</li>
      </ul>
      <ul>
        <li>El segundo ocurre cuando se predicen palabras funcionales, como <code>was</code>, <code>the</code>, <code>to</code>, <code>in</code>, y los tokens copiados de la pregunta de entrada, como <code>first Nigerian</code>, <code>Nobel Prize</code>. Cuando se predicen estos tokens "f√°ciles", podemos observar que la JSD se vuelve muy peque√±a a partir de las capas intermedias. Este hallazgo indica que el modelo ya ha decidido qu√© token generar en las capas intermedias, y mantiene las distribuciones de salida casi sin cambios en las capas superiores. Este hallazgo tambi√©n es consistente con las suposiciones en los LLM de salida temprana <code>Schuster et al., 2022</code></li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cuando la predicci√≥n de la siguiente palabra requiere conocimiento factual, el LLM parece cambiar las predicciones en las capas superiores. Contrastar las capas antes y despu√©s de un cambio repentino puede, por lo tanto, amplificar el conocimiento que emerge de las capas superiores y hacer que el modelo se base m√°s en su conocimiento interno factual. Adem√°s, esta evoluci√≥n de la informaci√≥n parece variar de un token a otro</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Su m√©todo requiere seleccionar con precisi√≥n la capa prematura que contiene informaci√≥n plausible pero menos factual, que puede no estar siempre en la misma capa temprana. Por lo tanto, proponen encontrar esa capa prematura mediante una selecci√≥n din√°mica de la capa prematura como se ve en la siguiente imagen</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure3.webp" width="837" height="457" alt="DoLa-figure3">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Seleccion dinamica de la capa prematura">Selecci√≥n din√°mica de la capa prematura<a class="anchor-link" href="#Seleccion dinamica de la capa prematura"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para seleccionar la capa prematura, calculan la divergencia de Jensen-Shannon (JSD) entre las capas intermedias y la final. La capa prematura se selecciona como la capa con la JSD m√°s alta</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Sin embargo, como este proceso puede ser un poco lento, lo que hacen es agrupar varias capas para hacer menos c√°lculos</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Contraste de las predicciones">Contraste de las predicciones<a class="anchor-link" href="#Contraste de las predicciones"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos la √∫ltima capa (capa madura) y la capa prematura, podemos contrastar las predicciones de ambas capas. Para ello, calculan la probabilidad logar√≠tmica del siguiente token en la capa madura y la prematura. A continuaci√≥n restan la probabilidad logar√≠tmica de la capa prematura a la de la capa madura, as√≠ le dan m√°s importancia al conocimiento de la capa madura</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Penalizacion por repeticion">Penalizaci√≥n por repetici√≥n<a class="anchor-link" href="#Penalizacion por repeticion"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La motivaci√≥n de DoLa es restar importancia al conocimiento ling√º√≠stico de las capas inferiores y amplificar el conocimiento factual del mundo real. Sin embargo, esto puede dar lugar a que el modelo genere p√°rrafos gramaticalmente incorrectos</p>
      <p>Emp√≠ricamente no han observado ese problema, pero han encontrado que la distribuci√≥n DoLa resultante a veces tiene una mayor tendencia a repetir frases generadas previamente, especialmente durante la generaci√≥n de largas secuencias de razonamiento en la cadena de pensamiento</p>
      <p>As√≠ que incluyen una penalizaci√≥n por repetici√≥n introducida en <code>Keskar et al. (2019)</code> con <code>Œ∏ = 1.2</code> durante la decodificaci√≥n</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Implementacion con transformers">Implementaci√≥n con transformers<a class="anchor-link" href="#Implementacion con transformers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver c√≥mo implementar DoLa con la librer√≠a <code>transformers</code> de Hugging Face. Para obtener m√°s informaci√≥n de c√≥mo aplicar DoLa con la librer√≠a <code>transformers</code> puedes consultar el siguiente <a href="https://huggingface.co/docs/transformers/main/en/generation_strategies#dola-decoding" target="_blank" rel="nofollow noreferrer">enlace</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero nos logueamos en el Hub, porque vamos a usar Llama 3 8B, para poder usarlo hay que pedir permiso a Meta, as√≠ que para poder descargarlo hay que estar logueado para que sepa qui√©n lo est√° descargando</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora instanciamos el tokenizador y el modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Asignamos un valor fijo de semilla para la reproducibilidad del ejemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Generamos los tokens de entrada al LLM</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;What does Darth Vader say to Luke in &quot;The Empire Strikes Back&quot;?&#39;</span>',
      '<span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Answer with a short answer.</span><span class="se">\n\n</span><span class="s2">Question: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Answer: &quot;</span>',
      '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Generamos ahora la entrada vanilla, es decir, sin aplicar DoLa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generate_kwargs</span><span class="o">=</span><span class="p">{</span>',
      '<span class="w">    </span><span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="kc">None</span>',
      '<span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">vanilla_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">vanilla_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&quot;No, I am your father.&quot; (Note: This is a famous misquote. The actual quote is &quot;No, I am your father&quot; is not in the movie. The correct quote is &quot;No, I am your father.&quot; is not',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que sabe que hay un error famoso, pero no consigue decir la frase correcta</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora aplicando DoLa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dola_high_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span> <span class="n">dola_layers</span><span class="o">=</span><span class="s1">&#39;high&#39;</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">dola_high_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&quot;No, I am your father.&quot; (Note: This is one of the most famous lines in movie history, and it&#x27;s often misquoted as &quot;Luke, I am your father.&quot;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora s√≠ consigue dar la frase correcta y el <a href="https://www.bbc.co.uk/bitesize/articles/zc38kty" target="_blank" rel="nofollow noreferrer">error famoso</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a hacer otra prueba con otro ejemplo, reinicio el notebook y uso otro modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">&quot;huggyllama/llama-7b&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Asignamos un valor fijo de semilla para la reproducibilidad del ejemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Le escribo una nueva pregunta</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;On what date was the Declaration of Independence officially signed?&quot;</span>',
      '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Generamos la salida vanilla</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generate_kwargs</span><span class="o">=</span><span class="p">{</span>',
      '<span class="w">    </span><span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="kc">None</span>',
      '<span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">vanilla_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">vanilla_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The Declaration of Independence was signed on July 4, 1776.',
          'What was the date of the signing of the Declaration of Independence?',
          'The Declaration of Independence was signed on July 4,',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, genera mal la salida, ya que aunque se celebra el 4 de julio, en realidad fue firmada el <a href="https://www.nps.gov/inde/learn/historyculture/resources-declarationofindependence.htm" target="_blank" rel="nofollow noreferrer">2 de agosto</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a probar ahora con DoLa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dola_high_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span> <span class="n">dola_layers</span><span class="o">=</span><span class="s1">&#39;high&#39;</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">dola_high_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'July 4, 1776. This is the most well-known date in U.S. history. The day has been celebrated with parades, barbeques, fireworks and festivals for hundreds of years.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Sigue sin generar una salida correcta, as√≠ que vamos a indicarle que solo contraste la capa final con las capas 28 y 30</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dola_high_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span> <span class="n">dola_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span><span class="mi">30</span><span class="p">],</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">dola_high_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'It was officially signed on 2 August 1776, when 56 members of the Second Continental Congress put their John Hancocks to the Declaration. The 2-page document had been written in 17',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora s√≠ consigue generar la respuesta correcta</p>
      </section>







    </div>

  </section>

</PostLayout>
