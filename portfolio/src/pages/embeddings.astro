---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Embeddings';
const end_url = 'embeddings';
const description = 'Descubre el poder de los embeddings';
const keywords = 'embeddings, nlp, procesamiento de lenguaje natural, transformers, huggingface, bert, word2vec, glove';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/embeddings.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2023-12-09+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Ordinal encoding"><h2>Ordinal encoding</h2></a>
      <a class="anchor-link" href="#One hot encoding"><h2>One hot encoding</h2></a>
      <a class="anchor-link" href="#Word embeddings"><h2>Word embeddings</h2></a>
      <a class="anchor-link" href="#Similitud entre palabras"><h3>Similitud entre palabras</h3></a>
      <a class="anchor-link" href="#Ejemplo con embeddings de OpenAI"><h3>Ejemplo con embeddings de OpenAI</h3></a>
      <a class="anchor-link" href="#Ejemplo con embeddings de HuggingFace"><h3>Ejemplo con embeddings de HuggingFace</h3></a>
      <a class="anchor-link" href="#Operaciones con palabras"><h3>Operaciones con palabras</h3></a>
      <a class="anchor-link" href="#Tipos de Word Embeddings"><h3>Tipos de Word Embeddings</h3></a>
      <a class="anchor-link" href="#Word2Vec"><h4>Word2Vec</h4></a>
      <a class="anchor-link" href="#CBOW"><h5>CBOW</h5></a>
      <a class="anchor-link" href="#Skip-gram"><h5>Skip-gram</h5></a>
      <a class="anchor-link" href="#GloVe"><h4>GloVe</h4></a>
      <a class="anchor-link" href="#FastText"><h4>FastText</h4></a>
      <a class="anchor-link" href="#Limitaciones de los word embeddings"><h4>Limitaciones de los word embeddings</h4></a>
      <a class="anchor-link" href="#Sentence embeddings"><h2>Sentence embeddings</h2></a>
      <a class="anchor-link" href="#ELMo"><h3>ELMo</h3></a>
      <a class="anchor-link" href="#InferSent"><h3>InferSent</h3></a>
      <a class="anchor-link" href="#Sentence-BERT"><h3>Sentence-BERT</h3></a>
      <a class="anchor-link" href="#Entrenamiento de un modelo word2vec con gensim"><h2>Entrenamiento de un modelo word2vec con gensim</h2></a>
      <a class="anchor-link" href="#Descarga del dataset"><h3>Descarga del dataset</h3></a>
      <a class="anchor-link" href="#Limpieza del dataset"><h3>Limpieza del dataset</h3></a>
      <a class="anchor-link" href="#Entrenamiento del modelo word2vec"><h3>Entrenamiento del modelo word2vec</h3></a>
      <a class="anchor-link" href="#Evaluacion del modelo word2vec"><h3>Evaluación del modelo word2vec</h3></a>
      <a class="anchor-link" href="#Visualizacion de los embeddings"><h3>Visualización de los embeddings</h3></a>
      <a class="anchor-link" href="#Uso de modelos preentrenados con HuggingFace"><h2>Uso de modelos preentrenados con HuggingFace</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En un anterior post sobre <a href="https://maximofn.com/tokens/">tokens</a>, ya vimos la representación mínima de cada palabra. Que corresponde a darle un número a la mínima división de cada palabra.</p>
      <p>Sin embargo los transformers y por tanto los LLMs, no representan así la información de las palabras, sino que lo hacen mediante <code>embeddings</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver primero dos formas de representar las palabras, el <code>ordinal encoding</code> y el <code>one hot encoding</code>. Y viendo los problemas de estos dos tipos de representaciones podremos llegar hasta los <code>word embeddings</code> y los <code>sentence embeddings</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Además vamos a ver un ejemplo de cómo entrenar un modelo de <code>word embeddings</code> con la librería <code>gensim</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Y por último veremos cómo usar modelos preentrenados de <code>embeddings</code> con la librería <code>transformers</code> de HuggingFace.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Ordinal encoding">Ordinal encoding<a class="anchor-link" href="#Ordinal encoding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Esta es la manera más básica de representar las palabras dentro de los transformers. Consiste en darle un número a cada palabra, o quedarnos con los números que ya tienen asignados los tokens.</p>
      <p>Sin embargo, este tipo de representación tiene dos problemas</p>
      <ul>
        <li>Imaginemos que mesa corresponde al token 3, gato al token 1 y perro al token 2. Se podría llegar a suponer que <code>mesa = gato + perro</code>, pero no es así. No existe esa relación entre esas palabras. Incluso podríamos pensar que adjudicando los tokens correctos sí podría llegar a darse este tipo de relaciones. Sin embargo, este pensamiento se viene abajo con las palabras que tienen más de un significado, como por ejemplo la palabra <code>banco</code></li>
      </ul>
      <ul>
        <li>El segundo problema es que las redes neuronales internamente hacen muchos cálculos numéricos, por lo que podría darse el caso de que si mesa tiene el token 3, tenga internamente más importancia que la palabra gato que tiene el token 1.</li>
      </ul>
      <p>De modo que este tipo de representación de las palabras se puede descartar muy rápidamente</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="One hot encoding">One hot encoding<a class="anchor-link" href="#One hot encoding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aquí lo que se hace es usar vectores de <code>N</code> dimensiones. Por ejemplo vimos que OpenAI tiene un vocabulario de <code>100277</code> tokens distintos. Por lo que si usamos <code>one hot encoding</code>, cada palabra se representaría con un vector de <code>100277</code> dimensiones.</p>
      <p>Sin embargo, el one hot encoding tiene otros dos grandes problemas</p>
      <ul>
        <li>No tiene en cuenta la relación entre las palabras. Por lo que si tenemos dos palabras que son sinónimos, como por ejemplo <code>gato</code> y <code>felino</code>, tendríamos dos vectores distintos para representarlas.</li>
      </ul>
      <p>En el lenguaje la relación entre las palabras es muy importante, y no tener en cuenta esta relación es un gran problema.</p>
      <ul>
        <li>El segundo problema es que los vectores son muy grandes. Si tenemos un vocabulario de <code>100277</code> tokens, cada palabra se representaría con un vector de <code>100277</code> dimensiones. Esto hace que los vectores sean muy grandes y que los cálculos sean muy costosos. Además, estos vectores van a ser todo ceros, excepto en la posición que corresponda al token de la palabra. Por lo que la mayoría de los cálculos van a ser multiplicaciones por cero, que son cálculos que no aportan nada. Así que vamos a tener un montón de memoria asignada a vectores en los que solo se tiene un <code>1</code> en una posición determinada.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Word embeddings">Word embeddings<a class="anchor-link" href="#Word embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Con los word embeddings se intenta solucionar los problemas de los dos tipos de representaciones anteriores. Para ello se usan vectores de <code>N</code> dimensiones, pero en este caso no se usan vectores de 100277 dimensiones, sino que se usan vectores de muchas menos dimensiones. Por ejemplo veremos que OpenAI usa <code>1536</code> dimensiones.</p>
      <p>Cada una de las dimensiones de estos vectores representan una característica de la palabra. Por ejemplo una de las dimensiones podría representar si la palabra es un verbo o un sustantivo. Otra dimensión podría representar si la palabra es un animal o no. Otra dimensión podría representar si la palabra es un nombre propio o no. Y así sucesivamente.</p>
      <p>Sin embargo estas características no se definen a mano, sino que se aprenden de forma automática. Durante el entrenamiento de los transformers, se van ajustando los valores de cada una de las dimensiones de los vectores, de modo que se aprenden las características de cada una de las palabras.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al hacer que cada una de las dimensiones de las palabras represente una característica de la palabra, se consigue que las palabras que tengan características similares, tengan vectores similares. Por ejemplo las palabras <code>gato</code> y <code>felino</code> tendrán vectores muy similares, ya que ambas son animales. Y las palabras <code>mesa</code> y <code>silla</code> tendrán vectores similares, ya que ambas son muebles.</p>
      <p>En la siguiente imagen podemos ver una representación de 3 dimensiones de palabras, y podemos ver que todas las palabras relacionadas con <code>school</code> están cerca, todas las palabras relacionadas con <code>food</code> están cerca y todas las palabras relacionadas con <code>ball</code> están cerca.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/word_embedding_3_dimmension.webp" alt="word_embedding_3_dimmension">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tener que cada una de las dimensiones de los vectores represente una característica de la palabra, consigue que podamos hacer operaciones con palabras. Por ejemplo si a la palabra <code>rey</code> se le resta la palabra <code>hombre</code> y se le suma la palabra <code>mujer</code>, obtenemos una palabra muy parecida a la palabra <code>reina</code>. Más adelante lo comprobaremos con un ejemplo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Similitud entre palabras">Similitud entre palabras<a class="anchor-link" href="#Similitud entre palabras"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como cada una de las palabras se representa mediante un vector de <code>N</code> dimensiones, podemos calcular la similitud entre dos palabras. Para ello se usa la función de similitud del coseno o <code>cosine similarity</code>.</p>
      <p>Si dos palabras están cercanas en el espacio vectorial, quiere decir que el álgulo que hay entre sus vectores es pequeño, por lo que su coseno es cercano a 1. Si hay un ángulo de 90 grados entre los vectores, el coseno es 0, es decir que no hay similitud entre las palabras. Y si hay un ángulo de 180 grados entre los vectores, el coseno es -1, es decir que las palabras son opuestas.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/cosine_similarity.webp" alt="cosine similarity">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Ejemplo con embeddings de OpenAI">Ejemplo con embeddings de OpenAI<a class="anchor-link" href="#Ejemplo con embeddings de OpenAI"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que sabemos lo que son los <code>embeddings</code>, veamos unos ejemplos con los <code>embeddings</code> que nos proporciona la <code>API</code> de <code>OpenAI</code>.</p>
      <p>Para ello, primero tenemos que tener instalado el paquete de <code>OpenAI</code></p>
      <div class='highlight'><pre><code class="language-bash">pip install openai</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importamos las librerías necesarias</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Usamos una <code>API key</code> de OpenAI. Para ello, nos dirigimos a la página de <a href="https://openai.com/" target="_blank" rel="nofollow noreferrer">OpenAI</a>, y nos registramos. Una vez registrados, nos dirigimos a la sección de <a href="https://platform.openai.com/api-keys">API Keys</a>, y creamos una nueva <code>API Key</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif" alt="open ai api key">
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">api_key</span> <span class="o">=</span> <span class="s2">&quot;Pon aquí tu API key&quot;</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Seleccionamos que modelo de embeddings queremos usar. En este caso vamos a usar <code>text-embedding-ada-002</code> que es el que recomienda <code>OpenAI</code> en su documentación de <a href="https://platform.openai.com/docs/guides/embeddings/" target="_blank" rel="nofollow noreferrer">embeddings</a>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_openai</span> <span class="o">=</span> <span class="s2">&quot;text-embedding-ada-002&quot;</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un cliente de la <code>API</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">client_openai</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span> <span class="n">organization</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver cómo son los <code>embeddings</code> de la palabra <code>Rey</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;Rey&quot;</span>',
      '<span class="n">embedding_openai</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding_openai</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_openai</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(torch.Size([1536]),',
          'tensor([-0.0103, -0.0005, -0.0189,  ..., -0.0009, -0.0226,  0.0045]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, obtenemos un vector de <code>1536</code> dimensiones</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Ejemplo con embeddings de HuggingFace">Ejemplo con embeddings de HuggingFace<a class="anchor-link" href="#Ejemplo con embeddings de HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como la generación de embeddings de OpenAI es de pago, vamos a ver cómo usar los embeddings de HuggingFace, que son gratuitos. Para ello primero tenemos que asegurarnos de tener instalada la librería <code>sentence-transformers</code></p>
      <div class='highlight'><pre><code class="language-bash">pip install -U sentence-transformers</code></pre></div>
      <p>Y ahora comenzamos a generar los embeddings de las palabras</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero importamos la librería</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos un modelo de <code>embeddings</code> de <code>HuggingFace</code>. Usamos <code>paraphrase-MiniLM-L6-v2</code> porque es un modelo pequeño y rápido, pero que da buenos resultados, y ahora para nuestro ejemplo nos basta.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;paraphrase-MiniLM-L6-v2&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Y ya podemos generar los <code>embeddings</code> de las palabras</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Rey&#39;</span><span class="p">]</span>',
      '<span class="n">embedding_huggingface</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding_huggingface</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_huggingface</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '((1, 384),',
          'array([ 4.99837071e-01, -7.60397986e-02,  5.47384083e-01,  1.89465046e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.21713984e-01, -1.01025246e-01,  6.44087136e-01,  4.91398573e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.73571329e-02, -2.77234882e-01,  4.34713453e-01, -1.06284058e+00,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.44114518e-01,  8.98794234e-01,  4.74923879e-01, -7.48904228e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.84665376e-01, -1.75070837e-01,  5.92192829e-01, -1.02512836e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;9.45721626e-01,  2.43777707e-01,  3.91995460e-01,  3.35530996e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.58333105e-01,  1.18869759e-01,  5.31717360e-01, -1.21750660e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.45580745e-01, -7.63889611e-01, -3.19075316e-01,  2.55386919e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.06407446e-01, -8.99556637e-01,  6.34190366e-02, -2.96231866e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.22994244e-01,  7.44934231e-02, -4.49327320e-01, -2.71379113e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.88012260e-01, -2.82730222e-01,  2.50365853e-01,  3.06314558e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5.01561277e-02, -5.73592126e-01, -4.93096076e-02, -2.54629493e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.45663840e-01, -1.54654181e-03,  1.85357735e-01,  2.49421135e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.80077875e-01, -2.99735814e-01,  7.34686375e-01,  9.35385004e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-8.64403173e-02,  5.90056717e-01,  9.62065995e-01, -3.89911681e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.52635378e-01,  1.10802782e+00, -4.28262979e-01,  8.98583114e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.79768258e-01, -7.25559890e-01,  4.38431054e-01,  6.08255446e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.06222546e+00,  1.86217821e-03,  5.23232877e-01, -5.59782684e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.08870542e+00, -1.29855171e-01, -1.34669527e-01,  4.24595959e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.99118191e-01, -2.53481418e-01, -1.82368979e-01,  9.74772453e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-7.66527832e-01,  2.02146843e-01, -9.27186012e-01, -3.72025579e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.51360565e-01,  3.66043419e-01,  3.58169287e-01, -5.50914466e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.87659878e-01,  2.67650932e-01, -1.30100116e-01, -9.08647776e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.58671075e-01, -4.44935560e-01, -1.43231079e-01, -2.83272982e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.21463636e-02,  1.98998764e-01, -9.47986841e-02,  1.74529219e+00,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.71559617e-01,  5.96294463e-01,  1.38505893e-02,  3.90956283e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.46427560e-01,  2.63105750e-01,  2.64972121e-01, -2.67196923e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.54366294e-02,  9.39224422e-01,  3.35206270e-01, -1.99105024e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.06340271e-01,  3.83643419e-01,  4.37904626e-01,  8.92579079e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.86432815e-01, -2.59302586e-01, -6.39415443e-01,  1.21703267e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;6.44594133e-01,  2.56335083e-02,  5.53315282e-02,  5.85618019e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.03075497e-01, -4.17360187e-01,  5.00189543e-01,  4.23062295e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-7.62073815e-01, -4.36184794e-01, -4.13090199e-01, -2.14746520e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.76077414e-01, -1.51846036e-02, -6.51694953e-01,  2.05930993e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.73996288e-01,  1.14034235e-01, -7.40544260e-01,  1.98710993e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-6.66027904e-01,  3.00016254e-01, -4.03109461e-01,  1.85078502e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.27183425e-01,  4.19003010e-01,  1.16863050e-01, -4.33366179e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.62291127e-01,  6.25310719e-01, -3.34749371e-01,  3.18448655e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-9.09660235e-02,  3.58690947e-01,  1.23402506e-01, -5.08333087e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.18513209e-01,  5.83032072e-01, -8.37822199e-01, -1.52947128e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5.07765234e-01, -2.90990144e-01, -2.56464798e-02,  5.69117546e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.43118417e-01, -3.27799052e-01, -1.70862004e-01,  4.14014012e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.74694878e-01,  5.15708327e-01,  3.21234539e-02,  1.55380607e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.21141332e-01, -1.72114551e-01,  6.43211603e-01, -3.89207341e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.29103401e-01,  4.13877398e-01, -9.22305062e-02, -4.54976231e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.50242126e+00, -2.81573564e-01,  1.70057654e-01,  4.53076512e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.25060362e-01, -1.33391351e-01,  5.40394569e-03,  3.71117502e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.29107875e-01,  1.35897202e-02,  2.44936779e-01,  1.04574718e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.65612388e-01,  4.33572650e-01, -4.09719855e-01, -2.95067448e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.26362443e-02, -7.43583977e-01, -7.35885441e-01, -1.35508239e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.12558493e-01, -5.46157181e-01,  7.55161867e-02, -3.57991695e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.20607555e-01,  5.53125329e-02, -3.23110700e-01,  4.88573104e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.07487953e+00,  1.72190830e-01,  8.48749802e-02,  5.73584400e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.06147277e-01,  3.26699704e-01,  5.09487510e-01, -2.60940105e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.85459042e-01,  3.15197736e-01, -8.84049162e-02, -2.14854136e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.04228538e-01, -3.53874594e-01,  3.30587216e-02, -2.04278827e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.45132256e-01, -4.05272096e-01,  9.07981098e-01, -1.70708492e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.62848401e-01, -3.17223936e-01,  1.53909430e-01,  7.24429131e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.27339968e-01, -1.16330147e+00, -9.58504915e-01,  4.87008452e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.30886355e-01, -1.40117988e-01,  7.84571916e-02, -2.93157458e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.00778294e+00,  1.34625390e-01, -4.66320179e-02,  6.51122704e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.50451362e-02, -2.15500608e-01, -2.42915586e-01, -3.21900517e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.94186682e-01,  4.71027017e-01,  1.56058431e-01,  1.30854800e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.84257025e-01, -1.44421116e-01, -7.09840000e-01, -1.80235609e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-8.30230191e-02,  9.08326149e-01, -8.22497830e-02,  1.46948382e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.41326815e-01,  3.81170362e-01, -6.37023628e-01,  1.70148894e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.00046806e-01,  5.70729785e-02, -1.09820545e+00, -1.03613675e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-6.21219516e-01,  4.55532551e-01,  1.86942443e-01, -2.04409719e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.81394243e-01, -7.88963258e-01,  2.19068691e-01, -3.62780124e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.41522694e-01, -1.73794985e-01, -4.00943428e-01,  5.01900315e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.53949839e-01,  1.03774257e-01, -1.66873619e-01, -4.63893116e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.78147718e-01,  4.85655308e-01, -3.02978605e-02, -5.67060888e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.68107373e-01, -6.57559693e-01, -5.02855539e-01, -1.94635347e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-9.58659649e-01, -4.97986436e-01,  1.33874401e-01,  3.09395105e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.52993363e-01,  7.43827343e-01, -1.87271550e-01, -6.11483693e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.08927953e+00, -2.30332208e-03,  2.11169615e-01, -3.46892715e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.32458824e-01,  2.07640216e-01, -4.10387546e-01,  3.12181324e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.69687408e-01,  8.62928331e-01,  2.40735337e-01, -3.65841389e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;6.84210837e-01,  3.45884450e-02,  5.63964128e-01,  2.39361122e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.10872793e-01, -6.34638309e-01, -9.07931089e-01, -6.35836497e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.20288679e-01,  2.59186536e-01, -4.45540816e-01,  6.33085072e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.97424471e-01,  7.51152515e-01, -2.68558711e-01, -4.39288855e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.13556695e-01, -1.89288303e-01,  5.81856608e-01,  4.75860722e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.60344616e-01, -2.96180040e-01,  2.91323394e-01,  1.34404674e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.22037649e-01,  4.19363379e-02, -3.87936801e-01, -9.25336123e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.28307915e-01, -1.74257740e-01, -1.52818128e-01,  4.31716293e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.12064430e-01,  2.98252910e-01,  9.86064151e-02,  3.84781063e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;6.68018535e-02, -2.29525566e-01, -8.20755959e-03,  5.17108142e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-6.66776478e-01, -1.38897672e-01,  4.68370765e-01, -2.14766636e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.43549764e-01,  2.25854263e-01, -1.92763060e-02,  2.78505355e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.39088053e-01, -9.69757214e-02, -2.71263003e-01,  1.05703615e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.14365645e-01,  4.16649908e-01,  4.18699026e-01, -1.76222697e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.08620593e-01, -5.79392374e-01, -1.68948188e-01, -1.77841976e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5.69338985e-02,  2.12916449e-01,  4.24367547e-01, -7.13860095e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;8.28932896e-02, -2.40542665e-01, -5.94049037e-01,  4.09415931e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.01326215e+00, -5.71239054e-01,  4.35258061e-01, -3.64619821e-01],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;dtype=float32))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos obtenemos un vector de <code>384</code> dimensiones. En este caso se obtiene un vector de esta dimensión porque se ha usado el modelo <code>paraphrase-MiniLM-L6-v2</code>. Si usamos otro modelo, obtendremos vectores de otra dimensión.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Operaciones con palabras">Operaciones con palabras<a class="anchor-link" href="#Operaciones con palabras"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a obtener los embeddings de las palabras <code>rey</code>, <code>hombre</code>, <code>mujer</code> y <code>reina</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai_rey</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;rey&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_hombre</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;hombre&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_mujer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;mujer&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;reina&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai_reina</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_openai_reina</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(torch.Size([1536]),',
          'tensor([-0.0110, -0.0084, -0.0115,  ...,  0.0082, -0.0096, -0.0024]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a obtener el embedding resultante de restarle a <code>rey</code> el embedding de <code>hombre</code> y sumarle el embedding de <code>mujer</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai</span> <span class="o">=</span> <span class="n">embedding_openai_rey</span> <span class="o">-</span> <span class="n">embedding_openai_hombre</span> <span class="o">+</span> <span class="n">embedding_openai_mujer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_openai</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(torch.Size([1536]),',
          'tensor([-0.0226, -0.0323,  0.0017,  ...,  0.0014, -0.0290, -0.0188]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Por último comparamos el resultado obtenido con el embedding de <code>reina</code>. Para ello usamos la función de <code>cosine_similarity</code> que nos proporciona la librería <code>pytorch</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">similarity_openai</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding_openai</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">embedding_openai_reina</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;similarity_openai: </span><span class="si">{</span><span class="n">similarity_openai</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'similarity_openai: 0.7564167976379395',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos es un valor muy cercano a 1, por lo que podemos decir que el resultado obtenido es muy parecido al embedding de <code>reina</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si usamos palabras en inglés, obtenemos un resultado más cercano a 1</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai_rey</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_hombre</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_mujer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;queen&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai</span> <span class="o">=</span> <span class="n">embedding_openai_rey</span> <span class="o">-</span> <span class="n">embedding_openai_hombre</span> <span class="o">+</span> <span class="n">embedding_openai_mujer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">similarity_openai</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding_openai</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">embedding_openai_reina</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;similarity_openai: </span><span class="si">{</span><span class="n">similarity_openai</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'similarity_openai: tensor([0.8849])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Esto es normal, ya que el modelo de OpenAI ha sido entrenado con más textos en inglés que en español</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tipos de Word Embeddings">Tipos de Word Embeddings<a class="anchor-link" href="#Tipos de Word Embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Existen varios tipos de word embeddings, y cada uno de ellos tiene sus ventajas e inconvenientes. Vamos a ver los más importantes</p>
      <ul>
        <li>Word2Vec</li>
        <li>GloVe</li>
        <li>FastText</li>
        <li>BERT</li>
        <li>GPT-2</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Word2Vec">Word2Vec<a class="anchor-link" href="#Word2Vec"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Word2Vec es un algoritmo que se usa para crear word embeddings. Este algoritmo fue creado por Google en 2013, y es uno de los algoritmos más usados para crear word embeddings.</p>
      <p>Tiene dos variantes, <code>CBOW</code> y <code>Skip-gram</code>. <code>CBOW</code> es más rápido de entrenar, mientras que <code>Skip-gram</code> es más preciso. Vamos a ver cómo funciona cada uno de ellos</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="CBOW">CBOW<a class="anchor-link" href="#CBOW"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>CBOW</code> o <code>Continuous Bag of Words</code> es un algoritmo que se usa para predecir una palabra a partir de las palabras que la rodean. Por ejemplo si tenemos la frase <code>El gato es un animal</code>, el algoritmo intentará predecir la palabra <code>gato</code> a partir de las palabras que la rodean, en este caso <code>El</code>, <code>es</code>, <code>un</code> y <code>animal</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/cbow.webp" alt="CBOW">
      <p>En esta arquitectura, el modelo predice cuál es la palabra más probable en el contexto dado. Por lo tanto, las palabras que tienen la misma probabilidad de aparecer se consideran similares y, por lo tanto, se acercan más en el espacio dimensional.</p>
      <p>Supongamos que en una oración reemplazamos <code>barco</code> con <code>bote</code>, entonces el modelo predice la probabilidad para ambos y si resulta ser similar entonces podemos considerar que las palabras son similares.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Skip-gram">Skip-gram<a class="anchor-link" href="#Skip-gram"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Skip-gram</code> o <code>Skip-gram with Negative Sampling</code> es un algoritmo que se usa para predecir las palabras que rodean a una palabra. Por ejemplo si tenemos la frase <code>El gato es un animal</code>, el algoritmo intentará predecir las palabras <code>El</code>, <code>es</code>, <code>un</code> y <code>animal</code> a partir de la palabra <code>gato</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Skip-gram.webp" alt="Skip-gram">
      <p>Esta arquitectura es similar a la de CBOW, pero en cambio el modelo funciona al revés. El modelo predice el contexto usando la palabra dada. Por lo tanto, las palabras que tienen el mismo contexto se consideran similares y, por lo tanto, se acercan más en el espacio dimensional.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="GloVe">GloVe<a class="anchor-link" href="#GloVe"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>GloVe</code> o <code>Global Vectors for Word Representation</code> es un algoritmo que se usa para crear word embeddings. Este algoritmo fue creado por la Universidad de Stanford en 2014.</p>
      <p>Word2Vec ignora el hecho de que algunas palabras de contexto se producen con más frecuencia que otras y también solo tienen en cuenta el contexto local y por lo tanto, no captura el contexto global.</p>
      <p>Este algoritmo usa una matriz de co-ocurrencia para crear los word embeddings. Esta matriz de co-ocurrencia es una matriz que contiene el número de veces que aparece cada palabra junto a cada una de las otras palabras del vocabulario.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="FastText">FastText<a class="anchor-link" href="#FastText"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>FastText</code> es un algoritmo que se usa para crear word embeddings. Este algoritmo fue creado por Facebook en 2016.</p>
      <p>Una de las principales desventajas de <code>Word2Vec</code> y <code>GloVe</code> es que no pueden codificar palabras desconocidas o fuera del vocabulario.</p>
      <p>Entonces, para lidiar con este problema, Facebook propuso un modelo <code>FastText</code>. Es una extensión de <code>Word2Vec</code> y sigue el mismo modelo <code>Skip-gram</code> y <code>CBOW</code>. Pero a diferencia de <code>Word2Vec</code> que alimenta palabras enteras en la red neuronal, <code>FastText</code> primero divide las palabras en varias subpalabras (o <code>n-grams</code>) y luego las alimenta a la red neuronal.</p>
      <p>Por ejemplo, si el valor de <code>n</code> es 3 y la palabra es <code>manzana</code> entonces su tri-gram será [<code>&#x3C;ma</code>, <code>man</code>, <code>anz</code>, <code>nza</code>, <code>zan</code>, <code>ana</code>, <code>na&#x3E;</code>] y su embedding de palabras será la suma de la representación vectorial de estos tri-grams. Aquí, los hiperparámetros <code>min_n</code> y <code>max_n</code> se consideran como 3 y los caracteres <code>&#x3C;</code> y <code>&#x3E;</code> representan el comienzo y el final de la palabra.</p>
      <p>Por lo tanto, utilizando esta metodología, las palabras desconocidas se pueden representar en forma vectorial, ya que tienen una alta probabilidad de que sus <code>n-grams</code> también estén presentes en otras palabras.</p>
      <p>Este algoritmo es una mejora de <code>Word2Vec</code>, ya que además de tener en cuenta las palabras que rodean a una palabra, también tiene en cuenta los <code>n-grams</code> de la palabra. Por ejemplo si tenemos la palabra <code>gato</code>, también tiene en cuenta los <code>n-gramas</code> de la palabra, en este caso <code>ga</code>, <code>at</code> y <code>to</code>, para <code>n = 2</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Limitaciones de los word embeddings">Limitaciones de los word embeddings<a class="anchor-link" href="#Limitaciones de los word embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Las técnicas de word embedding han dado un resultado decente, pero el problema es que el enfoque no es lo suficientemente preciso. No tienen en cuenta el orden de las palabras en las que aparecen, lo que conduce a la pérdida de la comprensión sintáctica y semántica de la oración.</p>
      <p>Por ejemplo, <code>Vas allí para enseñar, no a jugar</code> Y <code>Vas allí a jugar, no a enseñar</code> Ambas oraciones tendrán la misma representación en el espacio vectorial, pero no significan lo mismo.</p>
      <p>Además, el modelo de word embedding no puede dar resultados satisfactorios en una gran cantidad de datos de texto, ya que la misma palabra puede tener un significado diferente en una oración diferente según el contexto de la oración.</p>
      <p>Por ejemplo, <code>Voy a sentarme en el banco</code> Y <code>Voy a hacer gestiones en el banco</code> En ambas oraciones, la palabra <code>banco</code> tiene diferentes significados.</p>
      <p>Por lo tanto, requerimos un tipo de representación que pueda retener el significado contextual de la palabra presente en una oración.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Sentence embeddings">Sentence embeddings<a class="anchor-link" href="#Sentence embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El sentence embedding es similar al word embedding, pero en lugar de palabras, codifican toda la oración en la representación vectorial.</p>
      <p>Una forma simple de obtener sentence embedding es promediando los word embedding de todas las palabras presentes en la oración. Pero no son lo suficientemente precisos.</p>
      <p>Algunos de los modelos más avanzados para sentence embedding son <code>ELMo</code>, <code>InferSent</code> y <code>Sentence-BERT</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="ELMo">ELMo<a class="anchor-link" href="#ELMo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>ELMo</code> o <code>Embeddings from Language Models</code> es un modelo de sentence embedding que fue creado por la Universidad de Allen en 2018. Utiliza una red LSTM profunda bidireccional para producir representación vectorial. <code>ELMo</code> puede representar las palabras desconocidas o fuera del vocabulario en forma vectorial ya que está basado en caracteres.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="InferSent">InferSent<a class="anchor-link" href="#InferSent"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>InferSent</code> es un modelo de sentence embedding que fue creado por Facebook en 2017. Utiliza una red LSTM profunda bidireccional para producir representación vectorial. <code>InferSent</code> puede representar las palabras desconocidas o fuera del vocabulario en forma vectorial, ya que está basado en caracteres. Las oraciones están codificadas en una representación vectorial de 4096 dimensiones.</p>
      <p>La capacitación del modelo se realiza en el conjunto de datos de Stanford Natural Language Inference (<code>SNLI</code>). Este conjunto de datos está etiquetado y escrito por humanos para alrededor de 500K pares de oraciones.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sentence-BERT">Sentence-BERT<a class="anchor-link" href="#Sentence-BERT"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Sentence-BERT</code> es un modelo de sentence embedding que fue creado por la Universidad de Londres en 2019. Utiliza una red LSTM profunda bidireccional para producir representación vectorial. <code>Sentence-BERT</code> puede representar las palabras desconocidas o fuera del vocabulario en forma vectorial ya que está basado en caracteres. Las oraciones están codificadas en una representación vectorial de 768 dimensiones.</p>
      <p>El modelo de NLP de última generación <code>BERT</code> es excelente en las tareas de Similitud Textual Semántica, pero el problema es que tomaría mucho tiempo para un corpus enorme (65 horas para 10.000 oraciones), ya que requiere que ambas oraciones se introduzcan en la red y esto aumenta el cálculo por un factor enorme.</p>
      <p>Por lo tanto, <code>Sentence-BERT</code> es una modificación del modelo <code>BERT</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Entrenamiento de un modelo word2vec con gensim">Entrenamiento de un modelo word2vec con gensim<a class="anchor-link" href="#Entrenamiento de un modelo word2vec con gensim"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para descargar el dataset que vamos a usar hay que instalar la librería <code>dataset</code> de huggingface:</p>
      <div class='highlight'><pre><code class="language-bash">pip install datasets</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para entrenar el modelo de embeddings vamos a usar la librería <code>gensim</code>. Para instalarla con Conda usamos</p>
      <div class='highlight'><pre><code class="language-bash">conda install -c conda-forge gensim</code></pre></div>
      <p>Y para instalarla con pip usamos</p>
      <div class='highlight'><pre><code class="language-bash">pip install gensim</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para limpiar el dataset que nos hemos descargado vamos a usar expresiones regulares, que normalmente ya está instalada en Python, y <code>nltk</code> que es una librería de procesamiento de lenguaje natural. Para instalarla con Conda usamos</p>
      <div class='highlight'><pre><code class="language-bash">conda install -c anaconda nltk</code></pre></div>
      <p>Y para instalarlo con pip usamos</p>
      <div class='highlight'><pre><code class="language-bash">pip install nltk</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos todo instalado, podemos importar las librerías que vamos a usar:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gensim.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Word2Vec</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">gensim.parsing.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">strip_punctuation</span><span class="p">,</span> <span class="n">strip_numeric</span><span class="p">,</span> <span class="n">strip_short</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.corpus</span><span class="w"> </span><span class="kn">import</span> <span class="n">stopwords</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">word_tokenize</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Descarga del dataset">Descarga del dataset<a class="anchor-link" href="#Descarga del dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a descargar un dataset de textos procedentes de la wikipedia en español, para ello ejecutamos lo siguiente:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>',
      '<span class="w"> </span>',
      '<span class="n">dataset_corpus</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;large_spanish_corpus&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;all_wikis&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver cómo es</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset_corpus</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'DatasetDict(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;train: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 28109484',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, el dataset tiene más de 28 millones de textos. Vamos a ver alguno de ellos:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset_corpus</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x27;¡Bienvenidos!&#x27;,',
          '&#x27;Ir a los contenidos»&#x27;,',
          '&#x27;= Contenidos =&#x27;,',
          '&#x27;&#x27;,',
          '&#x27;Portada&#x27;,',
          '&#x27;Tercera Lengua más hablada en el mundo.&#x27;,',
          '&#x27;La segunda en número de habitantes en el mundo occidental.&#x27;,',
          '&#x27;La de mayor proyección y crecimiento día a día.&#x27;,',
          '&#x27;El español es, hoy en día, nombrado en cada vez más contextos, tomando realce internacional como lengua de cultura y civilización siempre de mayor envergadura.&#x27;,',
          '&#x27;Ejemplo de ello es que la comunidad minoritaria más hablada en los Estados Unidos es precisamente la que habla idioma español.&#x27;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como hay muchos ejemplos vamos a crear un subset de 10 millones de ejemplos para poder trabajar más rápido:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset</span> <span class="o">=</span> <span class="n">dataset_corpus</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10000000</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Limpieza del dataset">Limpieza del dataset<a class="anchor-link" href="#Limpieza del dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora nos descargamos las <code>stopwords</code> de <code>nltk</code>, que son palabras que no aportan información y que vamos a eliminar de los textos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>',
      '<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[nltk_data] Downloading package stopwords to',
          '[nltk_data]     /home/wallabot/nltk_data...',
          '[nltk_data]   Package stopwords is already up-to-date!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'True',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vamos a descargar los <code>punkt</code> de <code>nltk</code>, que es un <code>tokenizer</code> que nos va a permitir separar los textos en frases</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[nltk_data] Downloading package punkt to /home/wallabot/nltk_data...',
          '[nltk_data]   Package punkt is already up-to-date!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'True',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una función para limpiar los datos. Esta función va a:</p>
      <ul>
        <li>Pasar el texto a minúsculas</li>
        <li>Eliminar las URLs</li>
        <li>Eliminar las menciones a redes sociales como <code>@twitter</code> y <code>#hashtag</code></li>
        <li>Eliminar los signos de puntuación</li>
        <li>Eliminar los números</li>
        <li>Eliminar las palabras cortas</li>
        <li>Eliminar las stop words</li>
      </ul>
      <p>Como estamos usando un dataset de huggingface, los textos están en formato <code>dict</code>, así que devolvemos un diccionario.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">clean_text</span><span class="p">(</span><span class="n">sentence_batch</span><span class="p">):</span>',
      '<span class="w">    </span><span class="c1"># extrae el texto de la entrada</span>',
      '<span class="w">    </span><span class="n">text_list</span> <span class="o">=</span> <span class="n">sentence_batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="n">cleaned_text_list</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">text_list</span><span class="p">:</span>',
      '<span class="w">        </span><span class="c1"># Convierte el texto a minúsculas</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina URLs</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;http\S+|www\S+|https\S+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">MULTILINE</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina las menciones @ y &#39;#&#39; de las redes sociales</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\@\w+|\#\w+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina los caracteres de puntuación</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">strip_punctuation</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina los números</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">strip_numeric</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina las palabras cortas</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">strip_short</span><span class="p">(</span><span class="n">text</span><span class="p">,</span><span class="n">minsize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina las palabras comunes (stop words)</span>',
      '<span class="w">        </span><span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;spanish&#39;</span><span class="p">))</span>',
      '<span class="w">        </span><span class="n">word_tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
      '<span class="w">        </span><span class="n">filtered_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="n">cleaned_text_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filtered_text</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="c1"># Devuelve el texto limpio</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">cleaned_text_list</span><span class="p">}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aplicamos la función a los datos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentences_corpus</span> <span class="o">=</span> <span class="n">subset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">clean_text</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Map:   0%|          | 0/10000000 [00:00&amp;lt;?, ? examples/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a guardar el dataset filtrado en un fichero para no tener que volver a ejecutar el proceso de limpieza</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentences_corpus</span><span class="o">.</span><span class="n">save_to_disk</span><span class="p">(</span><span class="s2">&quot;sentences_corpus&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Saving the dataset (0/4 shards):   0%|          | 0/15000000 [00:00&amp;lt;?, ? examples/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Para cargarlo podemos hacer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_from_disk</span>',
      '<span class="n">sentences_corpus</span> <span class="o">=</span> <span class="n">load_from_disk</span><span class="p">(</span><span class="s1">&#39;sentences_corpus&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora lo que vamos a tener es una lista de listas, donde cada lista es una frase tokenizada y sin stopwords. Es decir, tenemos una lista de frases, y cada frase es una lista de palabras. Vamos a ver cómo es:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;La frase &quot;</span><span class="si">{</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&quot; se convierte en la lista de palabras &quot;</span><span class="si">{</span><span class="n">sentences_corpus</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'La frase &quot;¡Bienvenidos!&quot; se convierte en la lista de palabras &quot;[&#x27;¡bienvenidos&#x27;]&quot;',
          'La frase &quot;Ir a los contenidos»&quot; se convierte en la lista de palabras &quot;[&#x27;ir&#x27;, &#x27;contenidos&#x27;, &#x27;»&#x27;]&quot;',
          'La frase &quot;= Contenidos =&quot; se convierte en la lista de palabras &quot;[&#x27;contenidos&#x27;]&quot;',
          'La frase &quot;&quot; se convierte en la lista de palabras &quot;[]&quot;',
          'La frase &quot;Portada&quot; se convierte en la lista de palabras &quot;[&#x27;portada&#x27;]&quot;',
          'La frase &quot;Tercera Lengua más hablada en el mundo.&quot; se convierte en la lista de palabras &quot;[&#x27;tercera&#x27;, &#x27;lengua&#x27;, &#x27;hablada&#x27;, &#x27;mundo&#x27;]&quot;',
          'La frase &quot;La segunda en número de habitantes en el mundo occidental.&quot; se convierte en la lista de palabras &quot;[&#x27;segunda&#x27;, &#x27;número&#x27;, &#x27;habitantes&#x27;, &#x27;mundo&#x27;, &#x27;occidental&#x27;]&quot;',
          'La frase &quot;La de mayor proyección y crecimiento día a día.&quot; se convierte en la lista de palabras &quot;[&#x27;mayor&#x27;, &#x27;proyección&#x27;, &#x27;crecimiento&#x27;, &#x27;día&#x27;, &#x27;día&#x27;]&quot;',
          'La frase &quot;El español es, hoy en día, nombrado en cada vez más contextos, tomando realce internacional como lengua de cultura y civilización siempre de mayor envergadura.&quot; se convierte en la lista de palabras &quot;[&#x27;español&#x27;, &#x27;hoy&#x27;, &#x27;día&#x27;, &#x27;nombrado&#x27;, &#x27;cada&#x27;, &#x27;vez&#x27;, &#x27;contextos&#x27;, &#x27;tomando&#x27;, &#x27;realce&#x27;, &#x27;internacional&#x27;, &#x27;lengua&#x27;, &#x27;cultura&#x27;, &#x27;civilización&#x27;, &#x27;siempre&#x27;, &#x27;mayor&#x27;, &#x27;envergadura&#x27;]&quot;',
          'La frase &quot;Ejemplo de ello es que la comunidad minoritaria más hablada en los Estados Unidos es precisamente la que habla idioma español.&quot; se convierte en la lista de palabras &quot;[&#x27;ejemplo&#x27;, &#x27;ello&#x27;, &#x27;comunidad&#x27;, &#x27;minoritaria&#x27;, &#x27;hablada&#x27;, &#x27;unidos&#x27;, &#x27;precisamente&#x27;, &#x27;habla&#x27;, &#x27;idioma&#x27;, &#x27;español&#x27;]&quot;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Entrenamiento del modelo word2vec">Entrenamiento del modelo word2vec<a class="anchor-link" href="#Entrenamiento del modelo word2vec"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a entrenar un modelo de embeddings que convertirá palabras en vectores. Para ello vamos a usar la librería <code>gensim</code> y su modelo <code>Word2Vec</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">sentences_corpus</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>',
      '<span class="n">dim_embedding</span> <span class="o">=</span> <span class="mi">100</span>',
      '<span class="n">window_size</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># 5 palabras a la izquierda y 5 palabras a la derecha</span>',
      '<span class="n">min_count</span> <span class="o">=</span> <span class="mi">5</span>   <span class="c1"># Ignora las palabras con frecuencia menor a 5</span>',
      '<span class="n">workers</span> <span class="o">=</span> <span class="mi">4</span>    <span class="c1"># Número de hilos de ejecución</span>',
      '<span class="n">sg</span> <span class="o">=</span> <span class="mi">1</span>        <span class="c1"># 0 para CBOW, 1 para Skip-gram</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="n">dim_embedding</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_count</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="n">sg</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Este modelo se ha entrenado en la CPU, ya qye <code>gensim</code> no tiene opción de realizar el entrenamiento en la GPU y aun así en mi ordenador ha tardado X minutos en entrenar el modelo. Aunque la dimensión del embedding que hemos elegido es de solo 100 (a diferencia del tamaño de los embeddings de openai que es de 1536), no es un tiempo demasiado grande, ya que el dataset tiene 10 millones de frases.</p>
      <p>Los grandes modelos de lenguaje son entrenados con datasets de miles de millones de frases, por lo que es normal que el entrenamiento de un modelo de embeddings con un dataset de 10 millones de frases tarde unos minutos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez entrenado el modelo, lo guardamos en un archivo para poder usarlo en el futuro</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;word2vec.model&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si lo quisieramos cargar en el futuro, podemos hacerlo con</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;word2vec.model&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Evaluacion del modelo word2vec">Evaluación del modelo word2vec<a class="anchor-link" href="#Evaluacion del modelo word2vec"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver las palabras más similares de algunas palabras</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;perro&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[(&#x27;gato&#x27;, 0.7948548197746277),',
          '(&#x27;perros&#x27;, 0.77247554063797),',
          '(&#x27;cachorro&#x27;, 0.7638891339302063),',
          '(&#x27;hámster&#x27;, 0.7540281414985657),',
          '(&#x27;caniche&#x27;, 0.7514827251434326),',
          '(&#x27;bobtail&#x27;, 0.7492328882217407),',
          '(&#x27;mastín&#x27;, 0.7491254210472107),',
          '(&#x27;lobo&#x27;, 0.7312178611755371),',
          '(&#x27;semental&#x27;, 0.7292628288269043),',
          '(&#x27;sabueso&#x27;, 0.7290207147598267)]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;gato&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[(&#x27;conejo&#x27;, 0.8148329854011536),',
          '(&#x27;zorro&#x27;, 0.8109457492828369),',
          '(&#x27;perro&#x27;, 0.7948548793792725),',
          '(&#x27;lobo&#x27;, 0.7878773808479309),',
          '(&#x27;ardilla&#x27;, 0.7860757112503052),',
          '(&#x27;mapache&#x27;, 0.7817519307136536),',
          '(&#x27;huiña&#x27;, 0.766639232635498),',
          '(&#x27;oso&#x27;, 0.7656188011169434),',
          '(&#x27;mono&#x27;, 0.7633568644523621),',
          '(&#x27;camaleón&#x27;, 0.7623056769371033)]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vamos a ver el ejemplo en el que comprobamos la similitud de la palabra <code>reina</code> con el resultado de a la palabra <code>rey</code> le restamos la palabra <code>hombre</code> y le sumamos la palabra <code>mujer</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_hombre</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;hombre&#39;</span><span class="p">]</span>',
      '<span class="n">embedding_mujer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;mujer&#39;</span><span class="p">]</span>',
      '<span class="n">embedding_rey</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;rey&#39;</span><span class="p">]</span>',
      '<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;reina&#39;</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_rey</span> <span class="o">-</span> <span class="n">embedding_hombre</span> <span class="o">+</span> <span class="n">embedding_mujer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding_reina</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">embedding_reina</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="n">similarity</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'tensor([0.8156])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, hay bastante similitud</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Visualizacion de los embeddings">Visualización de los embeddings<a class="anchor-link" href="#Visualizacion de los embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a visualizar los embedding, para ello primero obtenemos los vectores y las palabras del modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vectors</span>',
      '<span class="n">words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como la dimensión de los embeddings es 100, para poder visualizarlos en 2 o 3 dimensiones tenemos que reducir la dimensión. Para ello vamos a usar <code>PCA</code> (más rápido) o <code>TSNE</code> (más preciso) de <code>sklearn</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>',
      '<span class="w"> </span>',
      '<span class="n">dimmesions</span> <span class="o">=</span> <span class="mi">2</span>',
      '<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">dimmesions</span><span class="p">)</span>',
      '<span class="n">reduced_embeddings_PCA</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">TSNE</span>',
      '<span class="w"> </span>',
      '<span class="n">dimmesions</span> <span class="o">=</span> <span class="mi">2</span>',
      '<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">dimmesions</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>',
      '<span class="n">reduced_embeddings_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[t-SNE] Computing 121 nearest neighbors...',
          '[t-SNE] Indexed 493923 samples in 0.013s...',
          '[t-SNE] Computed neighbors for 493923 samples in 377.143s...',
          '[t-SNE] Computed conditional probabilities for sample 1000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 2000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 3000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 4000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 5000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 6000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 7000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 8000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 9000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 10000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 11000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 12000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 13000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 14000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 15000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 16000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 17000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 18000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 19000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 20000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 21000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 22000 / 493923',
          '...',
          '[t-SNE] Computed conditional probabilities for sample 493923 / 493923',
          '[t-SNE] Mean sigma: 0.275311',
          '[t-SNE] KL divergence after 250 iterations with early exaggeration: 117.413788',
          '[t-SNE] KL divergence after 300 iterations: 5.774648',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora los visualizamos en 2 dimensiones con <code>matplotlib</code>. Vamos a visualizar la reducción de dimensionalidad que hemos hecho con <code>PCA</code> y con <code>TSNE</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>',
      '<span class="w"> </span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>',
      '<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">200</span><span class="p">]):</span> <span class="c1"># Limitar a las primeras 200 palabras</span>',
      '<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>',
      '<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Embeddings (PCA)&#39;</span><span class="p">)</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;Figure size 1000x1000 with 1 Axes&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>',
      '<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">200</span><span class="p">]):</span> <span class="c1"># Limitar a las primeras 200 palabras</span>',
      '<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>',
      '<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>',
      '<span class="w">                 </span><span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;Figure size 1000x1000 with 1 Axes&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Uso de modelos preentrenados con HuggingFace">Uso de modelos preentrenados con HuggingFace<a class="anchor-link" href="#Uso de modelos preentrenados con HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para usar modelos preentrenados de <code>embeddings</code> vamos a usar la librería <code>transformers</code> de <code>huggingface</code>. Para instalarla con Conda usamos</p>
      <div class='highlight'><pre><code class="language-bash">conda install -c conda-forge transformers</code></pre></div>
      <p>Y para instalarlo con pip usamos</p>
      <div class='highlight'><pre><code class="language-bash">pip install transformers</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Con la tarea <code>feature-extraction</code> de <code>huggingface</code> podemos usar modelos preentrenados para obtener los embeddings de las palabras. Para ello primero importamos la librería necesaria</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a obtener los <code>embeddings</code> de <code>BERT</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>',
      '<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;feature-extraction&quot;</span><span class="p">,</span><span class="n">framework</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span><span class="n">model</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver los <code>embeddings</code> de la palabra <code>rey</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;rey&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">embedding</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([3, 768])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos obtenemos un vector de <code>768</code> dimensiones, es decir, los <code>embeddings</code> de <code>BERT</code> tienen <code>768</code> dimensiones. Por otro lado, vemos que tiene 3 vectores de <code>embeddings</code>, esto es porque <code>BERT</code> añade un token al principio y otro al final de la frase, por lo que a nosotros solo nos interesa el vector del medio</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a volver a hacer el ejemplo en el que comprobamos la similitud de la palabra <code>reina</code> con el resultado de restarle a la palabra <code>rey</code> la palabra <code>hombre</code> y sumarle la palabra <code>mujer</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_hombre</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="n">embedding_mujer</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="n">embedding_rey</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;queen&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_rey</span> <span class="o">-</span> <span class="n">embedding_hombre</span> <span class="o">+</span> <span class="n">embedding_mujer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver la similitud</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding_reina</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">embedding_reina</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="n">similarity</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '/tmp/ipykernel_33343/4248442045.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).',
          '&#x20;&#x20;embedding = torch.tensor(embedding).unsqueeze(0)',
          '/tmp/ipykernel_33343/4248442045.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).',
          '&#x20;&#x20;embedding_reina = torch.tensor(embedding_reina).unsqueeze(0)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '0.742547333240509',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Usando los <code>embeddings</code> de <code>BERT</code> también obtenemos un resultado muy cercano a 1</p>
      </section>







    </div>

  </section>

</PostLayout>
