---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Medida de similitud entre embeddings';
const end_url = 'embeddings-similarity';
const description = 'Discover how similarity is measured between embeddings, the basis of the attention mechanism of transformers and RAG algorithms';
const keywords = 'embeddings, similarity, cosine similarity, L2 similarity, dot product similarity';
const languaje = 'EN';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/embeddings-similarity.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2023-12-18+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Similarity-by-square-L2"><h2>Similarity by square L2</h2></a>
      <a class="anchor-link" href="#Cosine-similarity"><h2>Cosine similarity</h2></a>
      <a class="anchor-link" href="#Scalar-product-similarity"><h2>Scalar product similarity</h2></a>
      <a class="anchor-link" href="#Which-similarity-system-to-use"><h2>Which similarity system to use</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Measure-of-similarity-between-embeddings">Measure of similarity between embeddings<a class="anchor-link" href="#Measure-of-similarity-between-embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now that we have seen what <a href="https://maximofn.com/embeddings">embeddings</a> are, we know that we can measure the similarity between two words by measuring the similarity between their embeddings. In the <a href="https://maximofn.com/embeddings">embeddings</a> post we saw the example of using the cosine similarity measure, but there are other similarity measures we can use, L2 square, scalar product similarity, cosine similarity, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
      <p>In this post we are going to look at these three that we have named</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Similarity-by-square-L2">Similarity by square L2<a class="anchor-link" href="#Similarity-by-square-L2"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This similarity is derived from the Euclidean distance, which is the straight line distance between two points in a multidimensional space, which is calculated with the Pythagorean theorem.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Euclidean distance" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/distancia_euclidiana.webp" width="1200" height="961"/></p>
      <p>The Euclidean distance between two points $p$ and $q$ is calculated as:</p>
      $$
      d(p,q) = \sqrt{opening_brace}(p_1 - q_1)^2 + (p_2 - q_2)^2 + \cdots + (p_n - q_n)^2{closing_brace} = \sqrt{opening_brace}{opening_brace}sum_{opening_brace}i=1{closing_brace}^n (p_i - q_i)^2{closing_brace}
      $$
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The similarity times the square L2 is the square of the Euclidean distance, that is:</p>
      $$
      similarity(p,q) = d(p,q)^2 = \sum_{opening_brace}i=1{closing_brace}^n (p_i - q_i)^2
      $$
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Cosine-similarity">Cosine similarity<a class="anchor-link" href="#Cosine-similarity"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we remember what we learned about sines and cosines in school, we will remember that when two vectors have an angle of 0ยบ between them, their cosine is 1, when the angle between them is 90ยบ, their cosine is 0 and when the angle is 180ยบ, their cosine is -1.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="cosine similarity" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/dot-product.gif" width="960" height="540"/></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="cosine similarity" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/dot-product-unity.gif" width="498" height="498"/></p>
      <p>Therefore, we can use the cosine of the angle between two vectors to measure their similarity. It can be shown that the cosine of the angle between two vectors is equal to the scalar product of the two vectors divided by the product of their moduli. It is not the purpose of this post to demonstrate it, but if you want you can see the demonstration <a href="https://www.wwwinsights.com/wp-content/uploads/2023/05/image-11-1024x694.png" target="_blank" rel="nofollow noreferrer">here</a>.</p>
      $$
      similarity(U,V) = \frac{opening_brace}U \V{closing_brace}{opening_brace}V{closing_brace}{opening_brace}V{closing_brace}.
      $$
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Scalar-product-similarity">Scalar product similarity<a class="anchor-link" href="#Scalar-product-similarity"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The scalar product similarity is the scalar product of two vectors.</p>
      $$
      similarity(U,V) = U \cdot V
      $$<p>As we have written the cosine similarity formula, when the length of the vectors is 1, i.e., they are normalized, the cosine similarity is equal to the scalar product similarity.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>So, what is the similarity by the scalar product for? Well, to measure the similarity between two vectors that are not normalized, that is, that do not have length 1.</p>
      <p>For example, youtube, in order to create embeddings of its videos, makes the embeddings of the videos it classifies with higher quality longer than those of the videos it classifies with lower quality.</p>
      <p>This way, when a user does a search, the similarity by product scalar will give higher similarity to higher quality videos, so it will give the user the higher quality videos first.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Which-similarity-system-to-use">Which similarity system to use<a class="anchor-link" href="#Which-similarity-system-to-use"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To choose the similarity system we are going to use, we must take into account the space in which we are working.</p>
      <ul>
      <li>If we are working in a high dimensional space, with normalized embeddings, cosine similarity works best. For example OpenAI generates normalized embeddings, so cosine similarity works best.</li>
      <li>If we are working in a classification system, where the distance between two classes is important, the similarity by L2 square works best.</li>
      <li>If we are working in a recommender system, where the length of the vectors is important, the scalar product similarity works best.</li>
      </ul>
      </section>
      






    </div>

  </section>

</PostLayout>
