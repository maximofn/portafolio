---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Complete Guide to FastRTC: Create a Real-Time AI Voice Chat with Python';
const end_url = 'fastrtc';
const description = 'Learn how to use FastRTC to create real-time AI voice applications in Python. Follow our step-by-step guide to build a voice chatbot with LLMs, from basic installation to integrating it with a phone call.';
const keywords = 'fastrtc, real-time, ai, application, phone, python, voice chat, llm';
const languaje = 'EN';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/fastrtc-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=960
    image_height=720
    image_extension=webp
    article_date=2025-03-08+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Main-Features-of-FastRTC"><h2>Main Features of FastRTC</h2></a>
      <a class="anchor-link" href="#Installation"><h2>Installation</h2></a>
      <a class="anchor-link" href="#Getting-Started"><h2>Getting Started</h2></a>
      <a class="anchor-link" href="#Leveling-Up:-Voice-Chat-with-LLM"><h2>Leveling Up: Voice Chat with LLM</h2></a>
      <a class="anchor-link" href="#Phone-Call"><h2>Phone Call</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="FastRTC:-The-Real-Time-Communication-Library-for-Python">FastRTC: The Real-Time Communication Library for Python<a class="anchor-link" href="#FastRTC:-The-Real-Time-Communication-Library-for-Python"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In recent months, we have seen significant advancements in real-time voice models, with entire companies being founded around both open-source and closed models. Some key milestones include:</p>
      <ul>
      <li><code>OpenAI</code> and <code>Google</code> launched their live multimodal APIs for ChatGPT and Gemini. OpenAI even launched a phone number <code>1-800-ChatGPT</code>!</li>
      <li><code>Kyutai</code> launched <a href="https://huggingface.co/kyutai" target="_blank" rel="nofollow noreferrer">Moshi</a>, a fully open-source audio-to-audio LLM.</li>
      <li><code>Alibaba</code> launched <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct">Qwen2-Audio</a>, an open-source LLM that natively understands audio.</li>
      <li><code>Fixie.ai</code> launched <a href="https://huggingface.co/fixie-ai/ultravox-v0_5-llama-3_3-70b">Ultravox</a>, another open-source LLM that also natively understands audio.</li>
      <li><code>ElevenLabs</code> raised 180 million dollars in its Series C.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Despite this explosion in models and funding, it remains difficult to build real-time AI applications that stream audio and video, especially in Python.</p>
      <ul>
      <li>Machine learning engineers may not have experience with the necessary technologies to build real-time applications, such as <code>WebRTC</code>.</li>
      <li>Even code assistance tools like <code>Cursor</code> and <code>Copilot</code> struggle to write Python code that supports real-time audio/video applications.</li>
      </ul>
      <p>That's why the announcement of <code>FastRTC</code>, the real-time communication library for Python, is exciting. The library is designed to make it easy to build real-time audio and video AI applications entirely in Python!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Main-Features-of-FastRTC">Main Features of FastRTC<a class="anchor-link" href="#Main-Features-of-FastRTC"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <ul>
      <li>üó£Ô∏è Automatic voice detection and built-in turn taking, so you only have to worry about the user response logic.</li>
      <li>üíª Automatic UI - Built-in Gradio UI enabled for WebRTC for testing (or deployment to production!).</li>
      <li>üìû Phone call - Use <code>fastphone()</code> to get a <strong>free</strong> phone number to call your audio stream (HF token required).</li>
      <li>‚ö°Ô∏è Support for <code>WebRTC</code> and <code>Websocket</code>.</li>
      <li>üí™ Customizable - You can mount the stream in any <code>FastAPI</code> application to serve a custom UI and deploy beyond <code>Gradio</code>.</li>
      <li>üß∞ Many utilities for <code>text-to-speech</code>, <code>speech-to-text</code>, <code>stop detection</code> to help you get started.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Installation">Installation<a class="anchor-link" href="#Installation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To be able to use <code>FastRTC</code>, you first need to install the library:</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>fastrtc</pre></div>
      
      <p>But if we want to install the pause detection, speech-to-text, and text-to-speech functionalities, we need to install some additional dependencies:</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="s2">"fastrtc[vad, stt, tts]"</span></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Getting-Started">Getting Started<a class="anchor-link" href="#Getting-Started"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We will start by building the <code>hello world</code> of real-time audio: echoing what the user says. In <code>FastRTC</code>, this is as simple as:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">fastrtc</span> <span class="kn">import</span> <span class="n">Stream</span><span class="p">,</span> <span class="n">ReplyOnPause</span>',
          '<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
          '',
          '<span class="k">def</span> <span class="nf">echo</span><span class="p">(</span><span class="n">audio</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>',
          '    <span class="k">yield</span> <span class="n">audio</span>',
          '',
          '<span class="n">stream</span> <span class="o">=</span> <span class="n">Stream</span><span class="p">(</span><span class="n">ReplyOnPause</span><span class="p">(</span><span class="n">echo</span><span class="p">),</span> <span class="n">modality</span><span class="o">=</span><span class="s2">"audio"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"send-receive"</span><span class="p">)</span>',
          '<span class="n">stream</span><span class="o">.</span><span class="n">ui</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '* Running on local URL:  http://127.0.0.1:7872',
          'To create a public link, set `share=True` in `launch()`.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>When we go to the link that Gradio suggests, we first have to give permissions to the browser to access the microphone. Next, this will appear:
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="fastrct - hello world - init" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/fastRTC%20-%20hello%20world%20-%20init.webp" width="550" height="361"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we click on the tab to the right of the word <code>Record</code>, we can select the microphone we want to use.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>When we press the <code>Record</code> button, everything we say will be repeated by the application. That is, it captures the audio, detects when we have stopped speaking, and repeats it.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's break it down:</p>
      <ul>
      <li><code>ReplyOnPause</code> will handle voice detection and turn-taking for you. You only need to worry about the logic for responding to the user. You have to pass it the function that will manage the input audio. In our case, it's the <code>echo</code> function, which captures the input audio and returns it as a stream using <code>yield</code>, which many people don't know, but is a generator, meaning it's a Python method for creating iterators. If you want to learn more about <code>yield</code>, you can read my post on <a href="https://www.maximofn.com/python#6.5.-Generators">Python</a>. Any generator that returns an audio tuple (represented as <code>(sample_rate, audio_data)</code>) will work.</li>
      <li>The <code>Stream</code> class will build a Gradio UI for you to quickly test your stream. Once you have finished prototyping, you can deploy your Stream as a production-ready FastAPI application in a single line of code.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Here we can see an example from the creators of <code>FastRTC</code>
      <video controls="" src="https://github.com/user-attachments/assets/fcf2d30e-3e98-47c9-8dc3-23340784c441"></video></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Leveling-Up:-Voice-Chat-with-LLM">Leveling Up: Voice Chat with LLM<a class="anchor-link" href="#Leveling-Up:-Voice-Chat-with-LLM"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The next level is to use an LLM to respond to the user. <code>FastRTC</code> comes with built-in <code>speech-to-text</code> and <code>text-to-speech</code> capabilities, so working with LLMs is really easy. Let's modify our <code>echo</code> function accordingly:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">fastrtc</span> <span class="kn">import</span> <span class="n">ReplyOnPause</span><span class="p">,</span> <span class="n">Stream</span><span class="p">,</span> <span class="n">get_stt_model</span><span class="p">,</span> <span class="n">get_tts_model</span>',
          '<span class="kn">from</span> <span class="nn">gradio_client</span> <span class="kn">import</span> <span class="n">Client</span>',
          '',
          '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">"Maximofn/SmolLM2_localModel"</span><span class="p">)</span>',
          '<span class="n">stt_model</span> <span class="o">=</span> <span class="n">get_stt_model</span><span class="p">()</span>',
          '<span class="n">tts_model</span> <span class="o">=</span> <span class="n">get_tts_model</span><span class="p">()</span>',
          '',
          '<span class="k">def</span> <span class="nf">echo</span><span class="p">(</span><span class="n">audio</span><span class="p">):</span>',
          '    <span class="n">prompt</span> <span class="o">=</span> <span class="n">stt_model</span><span class="o">.</span><span class="n">stt</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>',
          '    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
          '            <span class="n">message</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>',
          '            <span class="n">system_message</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span>',
          '            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '            <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
          '            <span class="n">api_name</span><span class="o">=</span><span class="s2">"/chat"</span>',
          '    <span class="p">)</span>',
          '    <span class="n">prompt</span> <span class="o">=</span> <span class="n">response</span>',
          '    <span class="k">for</span> <span class="n">audio_chunk</span> <span class="ow">in</span> <span class="n">tts_model</span><span class="o">.</span><span class="n">stream_tts_sync</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>',
          '        <span class="k">yield</span> <span class="n">audio_chunk</span>',
          '',
          '<span class="n">stream</span> <span class="o">=</span> <span class="n">Stream</span><span class="p">(</span><span class="n">ReplyOnPause</span><span class="p">(</span><span class="n">echo</span><span class="p">),</span> <span class="n">modality</span><span class="o">=</span><span class="s2">"audio"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"send-receive"</span><span class="p">)</span>',
          '<span class="n">stream</span><span class="o">.</span><span class="n">ui</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî',
          '* Running on local URL:  http://127.0.0.1:7871',
          'To create a public link, set `share=True` in `launch()`.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As a <code>speech-to-text</code> model, use <code>Moonshine</code>, which supposedly only supports English, but I have tested it in Spanish and it understands well.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As a language model, we will use the model I deployed in a backend on Hugging Face and wrote about in the post <a href="https://www.maximofn.com/en/deploy-backend-with-llm-in-huggingface">Deploying a Backend with LLM on HuggingFace</a>. It uses the LLM <code>HuggingFaceTB/SmolLM2-1.7B-Instruct</code>, which is a small model since it's running on a backend with CPU, but it works quite well.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As a <code>text-to-speech</code> model, use <code>Kokoro</code>, which does have options to speak in other languages, but is not yet implemented in the <code>FastRTC</code> library.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we are very interested in using <code>speech-to-speech</code> and <code>text-to-speech</code> models in other languages, we could implement them ourselves, because the greatest potential of <code>FastRTC</code> lies in the real-time communication layer, but I won't go into that now.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now if we test the code we just wrote, we can have a voice chatbot in real time.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Phone-Call">Phone Call<a class="anchor-link" href="#Leveling-Up:-Voice-Chat-with-LLM"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      We generated a script because it doesn't always work in a Jupyter Notebook.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">%%writefile</span> fastrtc_phone_demo.py',
      '',
      '<span class="kn">from</span> <span class="nn">fastrtc</span> <span class="kn">import</span> <span class="n">ReplyOnPause</span><span class="p">,</span> <span class="n">Stream</span><span class="p">,</span> <span class="n">get_stt_model</span><span class="p">,</span> <span class="n">get_tts_model</span>',
      '<span class="kn">import</span> <span class="nn">gradio</span>',
      '<span class="kn">from</span> <span class="nn">gradio_client</span> <span class="kn">import</span> <span class="n">Client</span>',
      '<span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">from</span> <span class="nn">gradio.networking</span> <span class="kn">import</span> <span class="n">setup_tunnel</span> <span class="k">as</span> <span class="n">original_setup_tunnel</span>',
      '<span class="kn">import</span> <span class="nn">socket</span>',
      '',
      '<span class="c1"># Monkey patch setup_tunnel para que acepte el par√°metro adicional</span>',
      '<span class="k">def</span> <span class="nf">patched_setup_tunnel</span><span class="p">(</span><span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">share_token</span><span class="p">,</span> <span class="n">share_server_address</span><span class="p">,</span> <span class="n">share_server_tls_certificate</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>',
      '    <span class="k">return</span> <span class="n">original_setup_tunnel</span><span class="p">(</span><span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">share_token</span><span class="p">,</span> <span class="n">share_server_address</span><span class="p">,</span> <span class="n">share_server_tls_certificate</span><span class="p">)</span>',
      '',
      '<span class="c1"># Replace the original function with our patched version</span>',
      '<span class="n">gradio</span><span class="o">.</span><span class="n">networking</span><span class="o">.</span><span class="n">setup_tunnel</span> <span class="o">=</span> <span class="n">patched_setup_tunnel</span>',
      '',
      '<span class="c1"># Get the token from the environment variable</span>',
      '<span class="n">HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN"</span><span class="p">)</span>',
      '',
      '<span class="c1"># Initialize the LLM client</span>',
      '<span class="n">llm_client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">"Maximofn/SmolLM2_localModel"</span><span class="p">)</span>',
      '',
      '<span class="c1"># Initialize the STT and TTS models</span>',
      '<span class="n">stt_model</span> <span class="o">=</span> <span class="n">get_stt_model</span><span class="p">()</span>',
      '<span class="n">tts_model</span> <span class="o">=</span> <span class="n">get_tts_model</span><span class="p">()</span>',
      '',
      '<span class="c1"># Define the echo function</span>',
      '<span class="k">def</span> <span class="nf">echo</span><span class="p">(</span><span class="n">audio</span><span class="p">):</span>',
      '    <span class="c1"># Convert the audio to text</span>',
      '    <span class="n">prompt</span> <span class="o">=</span> <span class="n">stt_model</span><span class="o">.</span><span class="n">stt</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>',
      '',
      '    <span class="c1"># Generate the response</span>',
      '    <span class="n">response</span> <span class="o">=</span> <span class="n">llm_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '            <span class="n">message</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>',
      '            <span class="n">system_message</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span>',
      '            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '            <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '            <span class="n">api_name</span><span class="o">=</span><span class="s2">"/chat"</span>',
      '    <span class="p">)</span>',
      '    ',
      '    <span class="c1"># Convert the response to audio</span>',
      '    <span class="n">prompt</span> <span class="o">=</span> <span class="n">response</span>',
      '',
      '    <span class="c1"># Stream the audio</span>',
      '    <span class="k">for</span> <span class="n">audio_chunk</span> <span class="ow">in</span> <span class="n">tts_model</span><span class="o">.</span><span class="n">stream_tts_sync</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>',
      '        <span class="k">yield</span> <span class="n">audio_chunk</span>',
      '',
      '<span class="k">def</span> <span class="nf">find_free_port</span><span class="p">(</span><span class="n">start_port</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">max_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">):</span>',
      '<span class="w">    </span><span class="sd">"""Find the first free port starting from start_port."""</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Searching for a free port starting from </span><span class="si">{</span><span class="n">start_port</span><span class="si">}</span><span class="s2">..."</span><span class="p">)</span>',
      '    <span class="k">for</span> <span class="n">port</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_port</span><span class="p">,</span> <span class="n">max_port</span><span class="p">):</span>',
      '        <span class="k">with</span> <span class="n">socket</span><span class="o">.</span><span class="n">socket</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">AF_INET</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">SOCK_STREAM</span><span class="p">)</span> <span class="k">as</span> <span class="n">sock</span><span class="p">:</span>',
      '            <span class="n">result</span> <span class="o">=</span> <span class="n">sock</span><span class="o">.</span><span class="n">connect_ex</span><span class="p">((</span><span class="s1">\'127.0.0.1\'</span><span class="p">,</span> <span class="n">port</span><span class="p">))</span>',
      '            <span class="k">if</span> <span class="n">result</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># If result != 0, the port is free</span>',
      '                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Free port found: </span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '                <span class="k">return</span> <span class="n">port</span>',
      '    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"No free port found between </span><span class="si">{</span><span class="n">start_port</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">max_port</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '    ',
      '<span class="n">free_port</span> <span class="o">=</span> <span class="n">find_free_port</span><span class="p">()</span>    <span class="c1"># Search for a free port</span>',
      '',
      '<span class="n">stream</span> <span class="o">=</span> <span class="n">Stream</span><span class="p">(</span><span class="n">ReplyOnPause</span><span class="p">(</span><span class="n">echo</span><span class="p">),</span> <span class="n">modality</span><span class="o">=</span><span class="s2">"audio"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"send-receive"</span><span class="p">)</span>',
      '<span class="n">stream</span><span class="o">.</span><span class="n">fastphone</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="n">free_port</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>




































































      
      <section class="section-block-markdown-cell">
      <p>We explain the code</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The part</p>
<div class="highlight"><pre><span></span><span class="c1"># Monkey patch setup_tunnel para que acepte el par√°metro adicional</span>
<span class="k">def</span> <span class="nf">patched_setup_tunnel</span><span class="p">(</span><span class="n">host, port, share_token, share_server_address, share_server_tls_certificate=None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">original_setup_tunnel</span><span class="p">(</span><span class="n">host, port, share_token, share_server_address, share_server_tls_certificate</span><span class="p">)</span>
<span class="n">    </span>
<span class="c1"># Replace the original function with our patched version</span>
<span class="n">gradio.networking.setup_tunnel</span> <span class="c1">=</span> <span class="n">patched_setup_tunnel</span>
</pre></div>
      <p>It is necessary because <code>FastRTC</code> is written for an older version of <code>gradio</code> that does not support the <code>share_server_address</code> parameter in the <code>setup_tunnel</code> method. So we patch it to accept the additional parameter.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As a Hugging Face token is required, we obtain it from the environment variable <code>HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN</code>.</p>
<div class="highlight"><pre><span></span><span class="c1"># Get the token from the environment variable</span>
<span class="n">HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN"</span><span class="p">)</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The language models, the <code>speech-to-text</code> model, and the <code>text-to-speech</code> model are created below, along with the <code>echo</code> function that will handle the input and output audio.</p>
<div class="highlight"><pre><span></span><span class="c1"># Initialize the LLM client</span>
<span class="n">llm_client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">"Maximofn/SmolLM2_localModel"</span><span class="p">)</span>

<span class="c1"># Initialize the STT and TTS models</span>
<span class="n">stt_model</span> <span class="o">=</span> <span class="n">get_stt_model</span><span class="p">()</span>
<span class="n">tts_model</span> <span class="o">=</span> <span class="n">get_tts_model</span><span class="p">()</span>

<span class="c1"># Define the echo function</span>
<span class="k">def</span> <span class="nf">echo</span><span class="p">(</span><span class="n">audio</span><span class="p">):</span>
    <span class="c1"># Convert the audio to text</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">stt_model</span><span class="o">.</span><span class="n">stt</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>

    <span class="c1"># Generate the response</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">llm_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
            <span class="n">message</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">system_message</span><span class="o">=</span><span class="s2">"You are a friendly Chatbot. Always reply in the language in which the user is writing to you."</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
            <span class="n">api_name</span><span class="o">=</span><span class="s2">"/chat"</span>
    <span class="p">)</span>

    <span class="c1"># Convert the response to audio</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">response</span>

    <span class="c1"># Stream the audio</span>
    <span class="k">for</span> <span class="n">audio_chunk</span> <span class="ow">in</span> <span class="n">tts_model</span><span class="o">.</span><span class="n">stream_tts_sync</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">audio_chunk</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As before we have used the port <code>8000</code>, if it says it is occupied, we create a function to find a free port and we find one.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">find_free_port</span><span class="p">(</span><span class="n">start_port</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">max_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Find the first free port starting from start_port."""</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Searching for a free port starting from </span><span class="si">{opening_brace}</span><span class="n">start_port</span><span class="si">{closing_brace}</span><span class="s2">..."</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">port</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_port</span><span class="p">,</span> <span class="n">max_port</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">socket</span><span class="o">.</span><span class="n">socket</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">AF_INET</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">SOCK_STREAM</span><span class="p">)</span> <span class="k">as</span> <span class="n">sock</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">sock</span><span class="o">.</span><span class="n">connect_ex</span><span class="p">((</span><span class="s1">'127.0.0.1'</span><span class="p">,</span> <span class="n">port</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">result</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># If result != 0, the port is free</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Free port found: </span><span class="si">{opening_brace}</span><span class="n">port</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">port</span>
      <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"No free port found between </span><span class="si">{opening_brace}</span><span class="n">start_port</span><span class="si">{closing_brace}</span><span class="s2"> and </span><span class="si">{opening_brace}</span><span class="n">max_port</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">free_port</span> <span class="o">=</span> <span class="n">find_free_port</span><span class="p">()</span>    <span class="c1"># Search for a free port</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the stream and now we use <code>stream.fastphone()</code> to get a free phone number to call your stream, instead of <code>stream.ui.launch()</code> that we used before to create the graphical interface.</p>
<div class="highlight"><pre><span></span><span class="n">stream</span> <span class="o">=</span> <span class="n">Stream</span><span class="p">(</span><span class="n">ReplyOnPause</span><span class="p">(</span><span class="n">echo</span><span class="p">),</span> <span class="n">modality</span><span class="o">=</span><span class="s2">"audio"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"send-receive"</span><span class="p">)</span>
<span class="n">stream</span><span class="o">.</span><span class="n">fastphone</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="n">free_port</span><span class="p">)</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we run it, we will see something like this:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">!</span>python<span class="w"> </span>fastrtc_phone_demo.py',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî',
          '<span class="ansi-green-fg">INFO</span>:	  Warming up STT model.',
          '<span class="ansi-green-fg">INFO</span>:	  STT model warmed up.',
          '<span class="ansi-green-fg">INFO</span>:	  Warming up VAD model.',
          '<span class="ansi-green-fg">INFO</span>:	  VAD model warmed up.',
          'Searching for a free port starting from 8000...',
          'Free port found: 8004',
          '<span class="ansi-green-fg">INFO</span>:     Started server process [<span class="ansi-cyan-fg">24029</span>]',
          '<span class="ansi-green-fg">INFO</span>:     Waiting for application startup.',
          '<span class="ansi-green-fg">INFO</span>:	  Visit <span class="ansi-cyan-fg">https://fastrtc.org/userguide/api/</span> for WebRTC or Websocket API docs.',
          '<span class="ansi-green-fg">INFO</span>:     Application startup complete.',
          '<span class="ansi-green-fg">INFO</span>:     Uvicorn running on <span class="ansi-bold">http://127.0.0.1:8004</span> (Press CTRL+C to quit)',
          '<span class="ansi-green-fg">INFO</span>:	  Your FastPhone is now live! Call <span class="ansi-cyan-fg">+1 877-713-4471</span> and use code <span class="ansi-cyan-fg">994514</span> to connect to your stream.',
          '<span class="ansi-green-fg">INFO</span>:	  You have <span class="ansi-cyan-fg">30:00</span> minutes remaining in your quota (Resetting on <span class="ansi-cyan-fg">2025-04-07</span>)',
          '<span class="ansi-green-fg">INFO</span>:	  Visit <span class="ansi-cyan-fg">https://fastrtc.org/userguide/audio/#telephone-integration</span> for information on making your handler compatible with phone usage.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see that it appears</p>
<div class="highlight"><pre><span></span>INFO:<span class="w">	  </span>Your<span class="w"> </span>FastPhone<span class="w"> </span>is<span class="w"> </span>now<span class="w"> </span>live!<span class="w"> </span>Call<span class="w"> </span>+1<span class="w"> </span><span class="m">877</span>-713-4471<span class="w"> </span>and<span class="w"> </span>use<span class="w"> </span>code<span class="w"> </span><span class="m">994514</span><span class="w"> </span>to<span class="w"> </span>connect<span class="w"> </span>to<span class="w"> </span>your<span class="w"> </span>stream.
INFO:<span class="w">	  </span>You<span class="w"> </span>have<span class="w"> </span><span class="m">30</span>:00<span class="w"> </span>minutes<span class="w"> </span>remaining<span class="w"> </span><span class="k">in</span><span class="w"> </span>your<span class="w"> </span>quota<span class="w"> </span><span class="o">(</span>Resetting<span class="w"> </span>on<span class="w"> </span><span class="m">2025</span>-04-07<span class="o">)</span>
</pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we go to <a href="https://fastrtc.org/userguide/audio/#telephone-integration" target="_blank" rel="nofollow noreferrer">Telephone Integration</a> in the <code>FastRTC</code> documentation, we will see that it uses <a href="https://www.twilio.com/">twilio</a> to make the call. It has an option to configure a local number from the United States, Dublin, Frankfurt, Tokyo, Singapore, Sydney, and S√£o Paulo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>I tried making the call from Spain (which is going to be quite expensive for me) and it works, but it's slow. I called, entered the code, and waited for the agent to connect, but since it was taking too long, I hung up.</p>
      </section>
      






    </div>

  </section>

</PostLayout>
