---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'GPT-2 â€“ Language Models are Unsupervised Multitask Learners';
const end_url = 'gpt2';
const description = 'Unlock the power of text generation with GPT-2, the latest open model from OpenAI ðŸ’¸! ðŸš€ In this post, I take you by the hand through the architecture behind this model, and show you how to fine-tune it ðŸ˜œ, code included. Read more and find out how GPT-2 can make your words more interesting than a human\'s ðŸ’¬ (or at least, than a bored human\'s) ðŸ˜‰';
const keywords = 'gpt2, openai, text generation, fine-tuning, nlp, natural language processing';
const languaje = 'EN';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT2.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=800
    image_height=436
    image_extension=webp
    article_date=2024-07-09+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Paper"><h2>Paper</h2></a>
      <a class="anchor-link" href="#Architecture"><h2>Architecture</h2></a>
      <a class="anchor-link" href="#Summary of the paper"><h2>Summary of the paper</h2></a>
      <a class="anchor-link" href="#Text Generation"><h2>Text Generation</h2></a>
      <a class="anchor-link" href="#Text Generation with Pipeline"><h3>Text Generation with Pipeline</h3></a>
      <a class="anchor-link" href="#Text Generation with Automodel"><h3>Text Generation with Automodel</h3></a>
      <a class="anchor-link" href="#Generate Text Token by Token"><h3>Generate Text Token by Token</h3></a>
      <a class="anchor-link" href="#Greedy search"><h4>Greedy search</h4></a>
      <a class="anchor-link" href="#Architecture of the models available in Hugging Face"><h2>Architecture of the models available in Hugging Face</h2></a>
      <a class="anchor-link" href="#GPT2Model"><h3>GPT2Model</h3></a>
      <a class="anchor-link" href="#GPT2LMHeadModel"><h3>GPT2LMHeadModel</h3></a>
      <a class="anchor-link" href="#GPT2ForSequenceClassification"><h3>GPT2ForSequenceClassification</h3></a>
      <a class="anchor-link" href="#GPT2ForQuestionAnswering"><h3>GPT2ForQuestionAnswering</h3></a>
      <a class="anchor-link" href="#GPT2ForTokenClassification"><h3>GPT2ForTokenClassification</h3></a>
      <a class="anchor-link" href="#Fine tuning GPT-2"><h2>Fine tuning GPT-2</h2></a>
      <a class="anchor-link" href="#Fine tuning for text generation"><h3>Fine tuning for text generation</h3></a>
      <a class="anchor-link" href="#Calculation of the Loss"><h4>Calculation of the Loss</h4></a>
      <a class="anchor-link" href="#Dataset"><h4>Dataset</h4></a>
      <a class="anchor-link" href="#Model Instance"><h4>Model Instance</h4></a>
      <a class="anchor-link" href="#Pytorch dataset"><h4>Pytorch dataset</h4></a>
      <a class="anchor-link" href="#Dataloader"><h4>Dataloader</h4></a>
      <a class="anchor-link" href="#Training"><h4>Training</h4></a>
      <a class="anchor-link" href="#Inference"><h4>Inference</h4></a>
      <a class="anchor-link" href="#Fine tuning GPT-2 for sentence classification"><h3>Fine tuning GPT-2 for sentence classification</h3></a>
      <a class="anchor-link" href="#Dataset"><h4>Dataset</h4></a>
      <a class="anchor-link" href="#Tokenizer"><h4>Tokenizer</h4></a>
      <a class="anchor-link" href="#Model"><h4>Model</h4></a>
      <a class="anchor-link" href="#Evaluation"><h4>Evaluation</h4></a>
      <a class="anchor-link" href="#Trainer"><h4>Trainer</h4></a>
      <a class="anchor-link" href="#Training"><h4>Training</h4></a>
      <a class="anchor-link" href="#Inference"><h4>Inference</h4></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Paper">Paper<a class="anchor-link" href="#Paper"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="nofollow noreferrer">Language Models are Unsupervised Multitask Learners</a> is the GPT-2 paper. This is the second version of the model <a href="https://maximofn.com/gpt1/">GPT-1</a> that we have already seen.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Architecture">Architecture<a class="anchor-link" href="#Architecture"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Before discussing the architecture of GPT-2, let's recall how the architecture of GPT-1 was.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_architecture.webp" alt="gpt1 architecture">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In GPT-2, a transformer-based architecture is used, just like in <a href="https://maximofn.com/gpt1/">GPT-1</a>, with the following sizes</p>
      <table>
        <thead>
          <tr>
            <th>Parameters</th>
            <th>Layers</th>
            <th>d_model</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>117M</td>
            <td>12</td>
            <td>768</td>
          </tr>
          <tr>
            <td>345M</td>
            <td>24</td>
            <td>1024</td>
          </tr>
          <tr>
            <td>1542M</td>
            <td>48</td>
            <td>1600</td>
          </tr>
        </tbody>
      </table>
      <p>The smallest model is equivalent to the original GPT, and the second smallest is equivalent to the largest BERT model. The largest model has more than an order of magnitude more parameters than GPT.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In addition, the following modifications were made to the architecture</p>
      <ul>
        <li>A normalization layer is added before the attention block. This can help stabilize the model's training and improve its ability to learn deeper representations. By normalizing the inputs of each block, variability in the outputs is reduced and model training is facilitated.</li>
        <li>An additional normalization has been added after the final self-attention block. This can help reduce variability in the model's outputs and improve its stability.</li>
        <li>In most models, the weights of the layers are initialized randomly, following a normal or uniform distribution. However, in the case of GPT-2, the authors decided to use a modified initialization that takes into account the depth of the model. The idea behind this modified initialization is that as the model becomes deeper, the signal flowing through the residual layers tends to weaken. This is because each residual layer adds to the original input, which can cause the signal to attenuate with the depth of the model. To counteract this effect, they decided to scale the weights of the residual layers during initialization by a factor of 1/âˆšN, where N is the number of residual layers. This means that as the model becomes deeper, the weights of the residual layers become smaller. This initialization trick can help stabilize the training of the model and improve its ability to learn deeper representations. By scaling the weights of the residual layers, variability in the outputs of each layer is reduced, and the flow of the signal through the model is facilitated. In summary, the modified initialization in GPT-2 is used to counteract the attenuation effect of the signal in the residual layers, which helps stabilize the training of the model and improve its ability to learn deeper representations.</li>
        <li>The vocabulary size has expanded to 50,257. This means that the model can learn to represent a wider set of words and tokens.</li>
        <li>The context size has been increased from 512 to 1024 tokens. This allows the model to take into account a broader context when generating text.</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_vs_GPT2_architecture.webp" alt="GPT1 vs GPT-2 architecture">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Summary of the paper">Summary of the paper<a class="anchor-link" href="#Summary of the paper"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The most interesting ideas from the paper are:</p>
      <ul>
        <li>For the pre-training of the model, they considered using a diverse and almost unlimited source of text, such as web scraping from Common Crawl. However, they found that there was text of very poor quality. So they used the WebText dataset, which also came from web scraping but with a quality filter, such as the number of outbound links from Reddit, etc. They also removed text coming from Wikipedia, as it could be duplicated on other pages.* They used a BPE tokenizer that we already explained in a <a href="https://maximofn.com/bpe/">post</a> previously</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Text Generation">Text Generation<a class="anchor-link" href="#Text Generation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's see how to generate text with a pretrained GPT-2</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To generate text, we will use the model from the <a href="https://huggingface.co/openai-community/gpt2" target="_blank" rel="nofollow noreferrer">GPT-2</a> repository of Hugging Face.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Text Generation with Pipeline">Text Generation with Pipeline<a class="anchor-link" href="#Text Generation with Pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 37" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>With this model, we can already use the transformers pipeline</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-xl&quot;</span>',
      '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;text-generation&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>',
      '<span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">&quot;Hello, I&#39;m a language model,&quot;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
      '<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">o</span><span class="p">[</span><span class="s1">&#39;generated_text&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to &#x27;longest_first&#x27; truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Output 1: Hello, I&#x27;m a language model, and I want to change the way you read',
          'A little in today&#x27;s post I want to talk about',
          'Output 2: Hello, I&#x27;m a language model, with two roles: the language model and the lexicographer-semantics expert. The language models are going',
          'Output 3: Hello, I&#x27;m a language model, and this is your brain. Here is your brain, and all this data that&#x27;s stored in there, that',
          'Output 4: Hello, I&#x27;m a language model, and I like to talk... I want to help you talk to your customers',
          'Are you using language model',
          'Output 5: Hello, I&#x27;m a language model, I&#x27;m gonna tell you about what type of language you&#x27;re using. We all know a language like this,',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Text Generation with Automodel">Text Generation with Automodel<a class="anchor-link" href="#Text Generation with Automodel"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 38" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>But if we want to use <code>Automodel</code>, we can do the following</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-xl&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Just like with <a href="https://maximofn.com/gpt1/#Text-generation">GPT-1</a> we can import <code>GPT2Tokenizer</code> and <code>AutoTokenizer</code>. This is because in the GPT-2 <a href="https://huggingface.co/openai-community/gpt2">model card</a> it is indicated that <code>GPT2Tokenizer</code> should be used, but in the post about the <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> library we explain that <code>AutoTokenizer</code> should be used to load the tokenizer. So let's try both.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-xl&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, I&#39;m a language model,&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="n">input_auto_tokens</span> <span class="o">=</span> <span class="n">auto_tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, I&#39;m a language model,&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input auto tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_auto_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'input tokens:',
          '&#x7B;&#x27;input_ids&#x27;: tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])&#x7D;',
          'input auto tokens:',
          '&#x7B;&#x27;input_ids&#x27;: tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1, 1]])&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As can be seen with the two tokenizers, the same tokens are obtained. So, to make the code more general, so that if the checkpoints change, the code doesn't have to be changed, we will use <code>AutoTokenizer</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We then create the device, the tokenizer, and the model</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-xl&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now that we have instantiated the model, let's see how many parameters it has</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of parameters: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">params</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Number of parameters: 1558M',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As we can see, we have loaded the model with 1.5B parameters, but if we wanted to load the other models, we would have to do</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">checkpoints_small</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">model_small</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_small</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of parameters of small model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_small</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoints_medium</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-medium&quot;</span>',
      '<span class="n">model_medium</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_medium</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of parameters of medium model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_medium</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoints_large</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-large&quot;</span>',
      '<span class="n">model_large</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_large</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of parameters of large model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_large</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoints_xl</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-xl&quot;</span>',
      '<span class="n">model_xl</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_xl</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of parameters of xl model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_xl</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Number of parameters of small model: 124M',
          'Number of parameters of medium model: 355M',
          'Number of parameters of large model: 774M',
          'Number of parameters of xl model: 1558M',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We create the input tokens for the model</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_sentence</span> <span class="o">=</span> <span class="s2">&quot;Hello, I&#39;m a language model,&quot;</span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;input_ids&#x27;: tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device=&#x27;cuda:0&#x27;)&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We pass it to the model to generate the output tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">output_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'output tokens:',
          'tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   314,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1101,  1016,   284,  1037,   345,   351,   534,  1917,    13,   198]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We decode the tokens to obtain the output sentence</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decoded output: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'decoded output:',
          'Hello, I&#x27;m a language model, and I&#x27;m going to help you with your problem.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We have already managed to generate text with GPT-2</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Generate Text Token by Token">Generate Text Token by Token<a class="anchor-link" href="#Generate Text Token by Token"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 39" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Greedy search">Greedy search<a class="anchor-link" href="#Greedy search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 40" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We have used <code>model.generate</code> to generate the output tokens all at once, but let's see how to generate them one by one. For this, instead of using <code>model.generate</code>, we will use <code>model</code>, which actually calls the <code>model.forward</code> method.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ 6.6288,  5.1421, -0.8002,  ..., -6.3998, -4.4113,  1.8240],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 2.7250,  1.9371, -1.2293,  ..., -5.0979, -5.1617,  2.2694],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 2.6891,  4.3089, -1.6074,  ..., -7.6321, -2.0448,  0.4042],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 6.0513,  3.8020, -2.8080,  ..., -6.7754, -8.3176,  1.1541],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 6.8402,  5.6952,  0.2002,  ..., -9.1281, -6.7818,  2.7576],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 1.0255, -0.2201, -2.5484,  ..., -6.2137, -7.2322,  0.1665]]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;UnsafeViewBackward0&amp;gt;), past_key_values=((tensor([[[[ 0.4779,  0.7671, -0.7532,  ..., -0.3551,  0.4590,  0.3073],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.2034, -0.6033,  0.2484,  ...,  0.7760, -0.3546,  0.0198],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.1968, -0.9029,  0.5570,  ...,  0.9985, -0.5028, -0.3508],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.5007, -0.4009,  0.1604,  ..., -0.3693, -0.1158,  0.1320],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.4854, -0.1369,  0.7377,  ..., -0.8043, -0.1054,  0.0871],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.1610, -0.8358, -0.5534,  ...,  0.9951, -0.3085,  0.4574]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[[ 0.6288, -0.1374, -0.3467,  ..., -1.0003, -1.1518,  0.3114],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-1.7269,  1.2920, -0.0734,  ...,  1.0572,  1.4698, -2.0412],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.2714, -0.0670, -0.4769,  ...,  0.6305,  0.6890, -0.8158],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.0499, -0.0721,  0.4580,  ...,  0.6797,  0.2331,  0.0210],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.1894,  0.2077,  0.6722,  ...,  0.6938,  0.2104, -0.0574],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.3661, -0.0218,  0.2618,  ...,  0.8750,  1.2205, -0.6103]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[[ 0.5964,  1.1178,  0.3604,  ...,  0.8426,  0.4881, -0.4094],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.3186, -0.3953,  0.2687,  ..., -0.1110, -0.5640,  0.5900],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.2092,  0.3898, -0.6061,  ..., -0.2859, -0.3136, -0.1002],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.0539,  0.8941,  0.3423,  ..., -0.6326, -0.1053, -0.6679],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.5628,  0.6687, -0.2720,  ..., -0.1073, -0.9792, -0.0302]]]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;PermuteBackward0&amp;gt;))), hidden_states=None, attentions=None, cross_attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see that it outputs a lot of data, let's first look at the keys of the output.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'odict_keys([&#x27;logits&#x27;, &#x27;past_key_values&#x27;])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>In this case we only have the logits of the model, let's check their size</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="w"> </span>',
      '<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 8, 50257])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Let's see how many tokens we had at the input</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 8])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Well, at the output we have the same number of logits as at the input. This is normal.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We obtain the logits from the last position of the output</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">nex_token_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([50257])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>There is a total of 50257 logits, meaning there is a vocabulary of 50257 tokens and we need to determine which token has the highest probability. To do this, we first calculate the softmax.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">softmax_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([50257])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Once we have calculated the softmax, we obtain the most likely token by finding the one with the highest probability, that is, the one with the highest value after the softmax.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(tensor(0.1732, device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;MaxBackward0&amp;gt;),',
          'tensor(290, device=&#x27;cuda:0&#x27;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We have obtained the following token, now we decode it</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27; and&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We obtained the following token using the greedy method, that is, the token with the highest probability. But we already saw in the post about the transformers library the <a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaciÃ³n-de-texto">ways to generate text</a> that can be done <code>sampling</code>, <code>top-k</code>, <code>top-p</code>, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's put everything into a function and see what comes out if we generate a few tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="w">    </span><span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="w">    </span><span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>',
      '<span class="w">        </span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
      '<span class="w">        </span><span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">generated_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now we generate text</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generate_greedy_text</span><span class="p">(</span><span class="s2">&quot;Hello, I&#39;m a language model,&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&quot;Hello, I&#x27;m a language model, and I&#x27;m going to help you with your problem.\n\n\nI&#x27;m going to help you&quot;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>The output is quite repetitive as was already seen in the <a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">ways of generating texts</a>. However, it is still a better output than what we obtained with <a href="https://maximofn.com/gpt1/#Generaci%C3%B3n-de-texto">GPT-1</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Architecture of the models available in Hugging Face">Architecture of the models available in Hugging Face<a class="anchor-link" href="#Architecture of the models available in Hugging Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 41" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we go to the Hugging Face documentation for <a href="https://huggingface.co/docs/transformers/en/model_doc/gpt2" target="_blank" rel="nofollow noreferrer">GPT2</a> we can see that we have the options <code>GPT2Model</code>, <code>GPT2LMHeadModel</code>, <code>GPT2ForSequenceClassification</code>, <code>GPT2ForQuestionAnswering</code>, <code>GPT2ForTokenClassification</code>. Let's take a look at them.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2Model">GPT2Model<a class="anchor-link" href="#GPT2Model"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 42" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This is the base model, that is, the transformer decoder.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Model</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2Model(',
          '&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As can be seen from the output, a tensor of dimension 768, which is the dimension of the embeddings for the small model. If we had used the model <code>openai-community/gpt2-xl</code>, we would have obtained an output of 1600.</p>
      <p>Depending on the task you want to perform, you would now need to add more layers.</p>
      <p>We can add them manually, but the weights of those layers would be initialized randomly. Whereas if we use Hugging Face models with these layers, the weights are pretrained.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2LMHeadModel">GPT2LMHeadModel<a class="anchor-link" href="#GPT2LMHeadModel"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 43" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>It is the one we used before to generate text</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2LMHeadModel</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2LMHeadModel(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(lm_head): Linear(in_features=768, out_features=50257, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As can be seen, it is the same model as before, only at the end a linear layer has been added with an input of 768 (the embeddings) and an output of 50257, which corresponds to the vocabulary size.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2ForSequenceClassification">GPT2ForSequenceClassification<a class="anchor-link" href="#GPT2ForSequenceClassification"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 44" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This option is for classifying text sequences, in this case we have to specify with <code>num_labels</code> the number of classes we want to classify.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2ForSequenceClassification</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
      '<span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;score.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(score): Linear(in_features=768, out_features=5, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now, instead of having an output of 50257, we have an output of 5, which is the number we introduced in <code>num_labels</code> and is the number of classes we want to classify.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2ForQuestionAnswering">GPT2ForQuestionAnswering<a class="anchor-link" href="#GPT2ForQuestionAnswering"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 45" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In the post of <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> we explain that, in this mode, a context is passed to the model along with a question about the context and it returns the answer.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2ForQuestionAnswering</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;qa_outputs.bias&#x27;, &#x27;qa_outputs.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2ForQuestionAnswering(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(qa_outputs): Linear(in_features=768, out_features=2, bias=True)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see that the output gives us a two-dimensional tensor</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2ForTokenClassification">GPT2ForTokenClassification<a class="anchor-link" href="#GPT2ForTokenClassification"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 46" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We also discussed in the <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> post what token classification was, explaining that it classified which category each token belonged to. We need to pass the number of classes we want to classify with <code>num_labels</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2ForTokenClassification</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
      '<span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;classifier.bias&#x27;, &#x27;classifier.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2ForTokenClassification(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;(classifier): Linear(in_features=768, out_features=5, bias=True)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>At the output, we get the five classes that we specified with <code>num_labels</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Fine tuning GPT-2">Fine tuning GPT-2<a class="anchor-link" href="#Fine tuning GPT-2"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 47" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Fine tuning for text generation">Fine tuning for text generation<a class="anchor-link" href="#Fine tuning for text generation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 48" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>First, let's see how the training would be done with pure Pytorch</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Calculation of the Loss">Calculation of the Loss<a class="anchor-link" href="#Calculation of the Loss"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 49" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Before starting to fine-tune GPT-2, let's look at something. Previously, when we obtained the model's output, we did this</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ 6.6288,  5.1421, -0.8002,  ..., -6.3998, -4.4113,  1.8240],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 2.7250,  1.9371, -1.2293,  ..., -5.0979, -5.1617,  2.2694],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 2.6891,  4.3089, -1.6074,  ..., -7.6321, -2.0448,  0.4042],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 6.0513,  3.8020, -2.8080,  ..., -6.7754, -8.3176,  1.1541],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 6.8402,  5.6952,  0.2002,  ..., -9.1281, -6.7818,  2.7576],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 1.0255, -0.2201, -2.5484,  ..., -6.2137, -7.2322,  0.1665]]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;UnsafeViewBackward0&amp;gt;), past_key_values=((tensor([[[[ 0.4779,  0.7671, -0.7532,  ..., -0.3551,  0.4590,  0.3073],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.2034, -0.6033,  0.2484,  ...,  0.7760, -0.3546,  0.0198],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.1968, -0.9029,  0.5570,  ...,  0.9985, -0.5028, -0.3508],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.5007, -0.4009,  0.1604,  ..., -0.3693, -0.1158,  0.1320],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.4854, -0.1369,  0.7377,  ..., -0.8043, -0.1054,  0.0871],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.1610, -0.8358, -0.5534,  ...,  0.9951, -0.3085,  0.4574]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[[ 0.6288, -0.1374, -0.3467,  ..., -1.0003, -1.1518,  0.3114],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-1.7269,  1.2920, -0.0734,  ...,  1.0572,  1.4698, -2.0412],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.2714, -0.0670, -0.4769,  ...,  0.6305,  0.6890, -0.8158],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.0499, -0.0721,  0.4580,  ...,  0.6797,  0.2331,  0.0210],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[-0.1894,  0.2077,  0.6722,  ...,  0.6938,  0.2104, -0.0574],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.3661, -0.0218,  0.2618,  ...,  0.8750,  1.2205, -0.6103]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[[ 0.5964,  1.1178,  0.3604,  ...,  0.8426,  0.4881, -0.4094],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.3186, -0.3953,  0.2687,  ..., -0.1110, -0.5640,  0.5900],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.2092,  0.3898, -0.6061,  ..., -0.2859, -0.3136, -0.1002],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.0539,  0.8941,  0.3423,  ..., -0.6326, -0.1053, -0.6679],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ 0.5628,  0.6687, -0.2720,  ..., -0.1073, -0.9792, -0.0302]]]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;PermuteBackward0&amp;gt;))), hidden_states=None, attentions=None, cross_attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We can see that we get <code>loss=None</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'None',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Since we will need the loss to perform fine-tuning, let's see how to obtain it.</p>
      <p>If we go to the documentation of the <a href="https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel.forward" target="_blank" rel="nofollow noreferrer">forward</a> method of <code>GPT2LMHeadModel</code>, we can see that it states that the output is an object of type <code>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</code>. So, if we go to the documentation of <a href="https://huggingface.co/docs/transformers/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions">transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a>, we can see that it states that it returns <code>loss</code> if <code>labels</code> are passed to the <code>forward</code> method.</p>
      <p>If we go to the source code of the <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1277" target="_blank" rel="nofollow noreferrer">forward</a> method, we see this block of code</p>
      <p>```python</p>
      <p>loss = None</p>
      <p>if labels is not None:</p>
      <p>labels = labels.to(lm_logits.device)</p>
      <p>shift_logits = lm_logits[..., :-1, :].contiguous()</p>
      <p>shift_labels = labels[..., 1:].contiguous()</p>
      <p>loss_fct = CrossEntropyLoss()</p>
      <p>loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))```</p>
      <p>That is, the <code>loss</code> is calculated as follows</p>
      <ul>
        <li>Logits and labels shift: The first part is to shift the logits (<code>lm_logits</code>) and the labels (<code>labels</code>) so that the <code>tokens < n</code> predict <code>n</code>, meaning from position <code>n</code> it predicts the next token based on the previous ones.</li>
        <li>CrossEntropyLoss: An instance of the loss function <code>CrossEntropyLoss()</code> is created.</li>
        <li>Flatten tokens: Next, the logits and labels are flattened using <code>view(-1, shift_logits.size(-1))</code> and <code>view(-1)</code>, respectively. This is done to ensure that the logits and labels have the same shape for the loss function.</li>
        <li>Calculation of the loss: Finally, the loss is calculated using the <code>CrossEntropyLoss()</code> function with the flattened logits and flattened labels as inputs.</li>
      </ul>
      <p>In summary, the <code>loss</code> is calculated as the cross-entropy loss between the shifted and flattened logits and the shifted and flattened labels.</p>
      <p>Therefore, if we pass the labels to the <code>forward</code> method, it will return the <code>loss</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'tensor(3.8028, device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;NllLossBackward0&amp;gt;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 53" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>For the training, we are going to use an English jokes dataset <a href="https://huggingface.co/datasets/Maximofn/short-jokes-dataset" target="_blank" rel="nofollow noreferrer">short-jokes-dataset</a>, which is a dataset with 231 thousand English jokes.</p>
      <blockquote>
      <p>We restart the notebook to avoid issues with the GPU memory</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We download the dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>',
      '<span class="w"> </span>',
      '<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Maximofn/short-jokes-dataset&quot;</span><span class="p">)</span>',
      '<span class="n">jokes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'DatasetDict(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;train: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;ID&#x27;, &#x27;Joke&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 231657',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Let's take a look at it a bit</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">jokes</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;ID&#x27;: 1,',
          '&#x27;Joke&#x27;: &#x27;[me narrating a documentary about narrators] &quot;I can\&#x27;t hear what they\&#x27;re saying cuz I\&#x27;m talking&quot;&#x27;&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Model Instance">Model Instance<a class="anchor-link" href="#Model Instance"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 54" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To be able to use the <code>xl</code> model, that is, the one with 1.5B parameters, I switch it to FP16 to avoid running out of memory.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-xl&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Pytorch dataset">Pytorch dataset<a class="anchor-link" href="#Pytorch dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 55" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create a Dataset class in Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span>',
      '<span class="w"> </span>',
      '<span class="k">class</span><span class="w"> </span><span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">&quot;JOKE: &quot;</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">&quot;&amp;lt;|endoftext|&amp;gt;&quot;</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '<span class="w">        </span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '<span class="w">        </span><span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">])</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '<span class="w">        </span><span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">&quot;Joke&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '<span class="w">        </span><span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="w">        </span><span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>We instantiate it</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see an example</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
      '<span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'JOKE: Why can&#x27;t Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&amp;lt;|endoftext|&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '(torch.Size([1, 22]), torch.Size([1, 22]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dataloader">Dataloader<a class="anchor-link" href="#Dataloader"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 56" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We now create a DataLoader from Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>',
      '<span class="w"> </span>',
      '<span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>',
      '<span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see a batch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentences</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">))</span>',
      '<span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(1, torch.Size([1, 1, 36]), torch.Size([1, 1, 36]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Training">Training<a class="anchor-link" href="#Training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 57" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>',
      '<span class="w"> </span>',
      '<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>',
      '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>',
      '<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-6</span>',
      '<span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>',
      '<span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>',
      '<span class="w"> </span>',
      '<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>',
      '<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>',
      '<span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
      '<span class="w"> </span>',
      '<span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>',
      '<span class="w"> </span>',
      '<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="w"> </span>',
      '<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>',
      '<span class="w">    </span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> started&quot;</span> <span class="o">+</span> <span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Training&quot;</span><span class="p">)</span>',
      '<span class="w">    </span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>',
      '<span class="w">        </span>',
      '<span class="w">        </span><span class="c1">#################### &quot;Fit as many joke sequences into MAX_SEQ_LEN sequence as possible&quot; logic start ####</span>',
      '<span class="w">        </span><span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
      '<span class="w">            </span><span class="k">continue</span>',
      '<span class="w">        </span>',
      '<span class="w">        </span><span class="c1"># The first joke sequence in the sequence</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>',
      '<span class="w">            </span><span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
      '<span class="w">            </span><span class="k">continue</span>',
      '<span class="w">        </span><span class="k">else</span><span class="p">:</span>',
      '<span class="w">            </span><span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>',
      '<span class="w">            </span><span class="c1"># as the start for next sequence </span>',
      '<span class="w">            </span><span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
      '<span class="w">                </span><span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>',
      '<span class="w">                </span><span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
      '<span class="w">            </span><span class="k">else</span><span class="p">:</span>',
      '<span class="w">                </span><span class="c1">#Add the joke to sequence, continue and try to add more</span>',
      '<span class="w">                </span><span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="w">                </span><span class="k">continue</span>',
      '<span class="w">        </span><span class="c1">################## Sequence ready, process it trough the model ##################</span>',
      '<span class="w">            </span>',
      '<span class="w">        </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>',
      '<span class="w">        </span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
      '<span class="w">        </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>',
      '<span class="w">                    </span>',
      '<span class="w">        </span><span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>',
      '<span class="w">            </span><span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>',
      '<span class="w">            </span><span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>',
      '<span class="w">            </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>',
      '<span class="w">            </span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>',
      '<span class="w">            </span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
      '<span class="w">            </span><span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]})</span>',
      '<span class="w">        </span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
      '<span class="w">        </span><span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>',
      '<span class="w">            </span><span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 0 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training:   0%|          | 0/231657 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231657/231657 [32:29&amp;lt;00:00, 118.83it/s, loss=3.1, lr=2.31e-7]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 1 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231657/231657 [32:34&amp;lt;00:00, 118.55it/s, loss=2.19, lr=4.62e-7]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 2 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231657/231657 [32:36&amp;lt;00:00, 118.42it/s, loss=2.42, lr=6.93e-7]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 3 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231657/231657 [32:23&amp;lt;00:00, 119.18it/s, loss=2.16, lr=9.25e-7]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 4 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231657/231657 [32:22&amp;lt;00:00, 119.25it/s, loss=2.1, lr=1.16e-6]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>',
      '<span class="w"> </span>',
      '<span class="n">losses_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>',
      '<span class="n">lrs_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lrs</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_np</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs_np</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;learning rate&#39;</span><span class="p">)</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;Figure size 1200x600 with 1 Axes&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Inference">Inference<a class="anchor-link" href="#Inference"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 58" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's see how well the model tells jokes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">&quot;JOKE:&quot;</span>',
      '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
      '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'decoded joke:',
          'JOKE:!!!!!!!!!!!!!!!!!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>You can see that you pass it a sequence with the word <code>joke</code> and it returns a joke. But if you return another sequence, it does not.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">&quot;My dog is cute and&quot;</span>',
      '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
      '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'decoded joke:',
          'My dog is cute and!!!!!!!!!!!!!!!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Fine tuning GPT-2 for sentence classification">Fine tuning GPT-2 for sentence classification<a class="anchor-link" href="#Fine tuning GPT-2 for sentence classification"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 59" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now we are going to do a training with the Hugging Face libraries</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 60" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We are going to use the <code>imdb</code> dataset for sentiment classification into positive and negative.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>',
      '<span class="w"> </span>',
      '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;imdb&quot;</span><span class="p">)</span>',
      '<span class="n">dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'DatasetDict(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;train: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;text&#x27;, &#x27;label&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 25000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x20;&#x20;&#x20;&#x20;test: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;text&#x27;, &#x27;label&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 25000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x20;&#x20;&#x20;&#x20;unsupervised: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;text&#x27;, &#x27;label&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 50000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Let's take a look at it a bit</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">info</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'DatasetInfo(description=&#x27;&#x27;, citation=&#x27;&#x27;, homepage=&#x27;&#x27;, license=&#x27;&#x27;, features=&#x7B;&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)&#x7D;, post_processed=None, supervised_keys=None, task_templates=None, builder_name=&#x27;parquet&#x27;, dataset_name=&#x27;imdb&#x27;, config_name=&#x27;plain_text&#x27;, version=0.0.0, splits=&#x7B;&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=33435948, num_examples=25000, shard_lengths=None, dataset_name=&#x27;imdb&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=32653810, num_examples=25000, shard_lengths=None, dataset_name=&#x27;imdb&#x27;), &#x27;unsupervised&#x27;: SplitInfo(name=&#x27;unsupervised&#x27;, num_bytes=67113044, num_examples=50000, shard_lengths=None, dataset_name=&#x27;imdb&#x27;)&#x7D;, download_checksums=&#x7B;&#x27;hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet&#x27;: &#x7B;&#x27;num_bytes&#x27;: 20979968, &#x27;checksum&#x27;: None&#x7D;, &#x27;hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet&#x27;: &#x7B;&#x27;num_bytes&#x27;: 20470363, &#x27;checksum&#x27;: None&#x7D;, &#x27;hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet&#x27;: &#x7B;&#x27;num_bytes&#x27;: 41996509, &#x27;checksum&#x27;: None&#x7D;&#x7D;, download_size=83446840, post_processing_size=None, dataset_size=133202802, size_in_bytes=216649642)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Let's take a look at the features this dataset has.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">features</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None),',
          '&#x27;label&#x27;: ClassLabel(names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>The dataset contains strings and classes. Additionally, there are two types of classes, <code>pos</code> and <code>neg</code>. We will create a variable with the number of classes.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">num_clases</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">))</span>',
      '<span class="n">num_clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '2',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 61" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Tokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">bos_token</span><span class="o">=</span><span class="s1">&#39;&amp;lt;|startoftext|&amp;gt;&#39;</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="s1">&#39;&amp;lt;|endoftext|&amp;gt;&#39;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s1">&#39;&amp;lt;|pad|&amp;gt;&#39;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now that we have a tokenizer, we can tokenize the dataset, since the model only understands tokens.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Model">Model<a class="anchor-link" href="#Model"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 62" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We instantiate the model</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2ForSequenceClassification</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_clases</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>',
      '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;score.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Evaluation">Evaluation<a class="anchor-link" href="#Evaluation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 63" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create an evaluation metric</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>',
      '<span class="w"> </span>',
      '<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '<span class="w">    </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Trainer">Trainer<a class="anchor-link" href="#Trainer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 64" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the trainer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>',
      '<span class="w"> </span>',
      '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./results&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span>',
      '<span class="w">    </span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">],</span>',
      '<span class="w">    </span><span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Training">Training<a class="anchor-link" href="#Training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 65" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We train</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;IPython.core.display.HTML object&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'TrainOutput(global_step=4689, training_loss=0.04045845954294626, metrics=&#x7B;&#x27;train_runtime&#x27;: 5271.3532, &#x27;train_samples_per_second&#x27;: 14.228, &#x27;train_steps_per_second&#x27;: 0.89, &#x27;total_flos&#x27;: 3.91945125888e+16, &#x27;train_loss&#x27;: 0.04045845954294626, &#x27;epoch&#x27;: 3.0&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Inference">Inference<a class="anchor-link" href="#Inference"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 66" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We test the model after training it.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">get_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">prediction</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="s2">&quot;positive&quot;</span> <span class="k">if</span> <span class="n">prediction</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;negative&quot;</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;I hate this movie!&quot;</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">get_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'negative',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>

















    </div>

  </section>

</PostLayout>
