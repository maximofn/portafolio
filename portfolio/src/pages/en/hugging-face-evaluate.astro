---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Evaluate';
const end_url = 'hugging-face-evaluate';
const description = 'Forget sleepless nights calculating metrics and losing your mind with evaluate for your NLP models! ðŸ˜© Hugging Face\'s evaluate library is the answer to your prayers, allowing you to evaluate the performance of your models easily and quickly ðŸš€. With evaluate, you can say goodbye to manual calculations and hello to full automation ðŸ¤— leaving you more time to focus on what really matters ðŸ¤”: improving your models and revolutionizing the world of AI ðŸ’»';
const keywords = 'hugging face, evaluate, nlp, evaluation, metrics, performance, automation';
const languaje = 'EN';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/evaluate_logo_mockup.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=960
    image_height=720
    image_extension=png
    article_date=2021-04-29+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Installation"><h2>Installation</h2></a>
      <a class="anchor-link" href="#Type-of-evaluations"><h2>Type of evaluations</h2></a>
      <a class="anchor-link" href="#Load"><h2>Load</h2></a>
      <a class="anchor-link" href="#Community-module-loading"><h3>Community module loading</h3></a>
      <a class="anchor-link" href="#List-of-available-modules"><h3>List of available modules</h3></a>
      <a class="anchor-link" href="#Module-attributes"><h2>Module attributes</h2></a>
      <a class="anchor-link" href="#Execution"><h2>Execution</h2></a>
      <a class="anchor-link" href="#All-in-one"><h3>All in one</h3></a>
      <a class="anchor-link" href="#Incremental"><h3>Incremental</h3></a>
      <a class="anchor-link" href="#Combination-of-several-evaluations"><h2>Combination of several evaluations</h2></a>
      <a class="anchor-link" href="#Save-results"><h2>Save results</h2></a>
      <a class="anchor-link" href="#Upload-results-to-the-hub"><h2>Upload results to the hub</h2></a>
      <a class="anchor-link" href="#Evaluator"><h2>Evaluator</h2></a>
      <a class="anchor-link" href="#Display"><h2>Display</h2></a>
      <a class="anchor-link" href="#Evaluating-the-model-on-a-set-of-tasks"><h2>Evaluating the model on a set of tasks</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Hugging-Face-evaluate">Hugging Face evaluate<a class="anchor-link" href="#Hugging-Face-evaluate"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The <code>Evaluate</code> library of <code>Hugging Face</code> is a library to easily evaluate models and datasets.</p>
      <p>With a single line of code, you have access to dozens of evaluation methods for different domains (NLP, computer vision, reinforcement learning and more). Whether on your local machine, or in a distributed training setup, you can evaluate models in a consistent and reproducible manner.</p>
      <p>A complete list of available metrics can be obtained from the <a href="https://huggingface.co/evaluate-metric" target="_blank" rel="nofollow noreferrer">evaluate</a> page in Hugging Face. Each metric has a dedicated Hugging Face <code>Space</code> with an interactive demonstration on how to use the metric and a documentation card detailing the limitations and use of the metrics.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
      <h2 id="Installation">Installation<a class="anchor-link" href="#Installation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To install the library it is necessary to do the following</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>evaluate
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Type-of-evaluations">Type of evaluations<a class="anchor-link" href="#Type-of-evaluations"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Several types of evaluations are available</p>
      <ul>
      <li>Metric: A metric is used to evaluate the performance of a model and usually includes model predictions and ground truth labels.</li>
      <li><code>comparison</code>: Used to compare two models. This can be done, for example, by comparing their predictions with ground truth labels.</li>
      <li>Measurement: The dataset is as important as the model trained on it. With measurements the properties of a dataset can be investigated.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Load">Load<a class="anchor-link" href="#Load"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Each <code>metric</code>, <code>comparison</code> or <code>measurement</code> can be loaded with the method load</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">evaluate</span>',
          '',
          '<span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
          '<span class="n">accuracy</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'EvaluationModule(name: "accuracy", module_type: "metric", features: {\'predictions\': Value(dtype=\'int32\', id=None), \'references\': Value(dtype=\'int32\', id=None)}, usage: """',
          'Args:',
          '    predictions (`list` of `int`): Predicted labels.',
          '    references (`list` of `int`): Ground truth labels.',
          '    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.',
          '    sample_weight (`list` of `float`): Sample weights Defaults to None.',
          'Returns:',
          '    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.',
          'Examples:',
          '    Example 1-A simple example',
          '        &gt;&gt;&gt; accuracy_metric = evaluate.load("accuracy")',
          '        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])',
          '        &gt;&gt;&gt; print(results)',
          '        {\'accuracy\': 0.5}',
          '    Example 2-The same as Example 1, except with `normalize` set to `False`.',
          '        &gt;&gt;&gt; accuracy_metric = evaluate.load("accuracy")',
          '        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)',
          '        &gt;&gt;&gt; print(results)',
          '        {\'accuracy\': 3.0}',
          '    Example 3-The same as Example 1, except with `sample_weight` set.',
          '        &gt;&gt;&gt; accuracy_metric = evaluate.load("accuracy")',
          '        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])',
          '        &gt;&gt;&gt; print(results)',
          '        {\'accuracy\': 0.8778625954198473}',
          '""", stored examples: 0)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>If you want to be sure to load the type of metric you want, whether <code>metric</code>, <code>comparison</code> or <code>measurement</code> type, you can do it by adding the <code>module_type</code> parameter</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">evaluate</span>',
          '',
          '<span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">"metric"</span><span class="p">)</span>',
          '<span class="n">word_length</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"word_length"</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">"measurement"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[nltk_data] Downloading package punkt to',
          '[nltk_data]     /home/maximo.fernandez/nltk_data...',
          '[nltk_data]   Package punkt is already up-to-date!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Community-module-loading">Community module loading<a class="anchor-link" href="#Community-module-loading"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In addition to the modules offered by the library, you can also upload models uploaded by someone else to the Hugging Face hub.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">element_count</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"lvwerra/element_count"</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">"measurement"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <h3 id="List-of-available-modules">List of available modules<a class="anchor-link" href="#List-of-available-modules"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we want to get a list of all the available modules we have to use the <code>list_evaluation_modules</code> method, in which we can put search filters</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">element_count</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"lvwerra/element_count"</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">"measurement"</span><span class="p">)</span>',
          '</span><span class="n">evaluate</span><span class="o">.</span><span class="n">list_evaluation_modules</span><span class="p">(</span>',
          '  <span class="n">module_type</span><span class="o">=</span><span class="s2">"comparison"</span><span class="p">,</span>',
          '  <span class="n">include_community</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
          '  <span class="n">with_details</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[{\'name\': \'ncoop57/levenshtein_distance\',',
          '  \'type\': \'comparison\',',
          '  \'community\': True,',
          '  \'likes\': 0},',
          ' {\'name\': \'kaleidophon/almost_stochastic_order\',',
          '  \'type\': \'comparison\',',
          '  \'community\': True,',
          '  \'likes\': 1}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Module-attributes">Module attributes<a class="anchor-link" href="#Module-attributes"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>All evaluation modules come with a variety of useful attributes that help you use the module.</p>
      <table>
      <thead>
      <tr>
      <th>Attribute</th>
      <th>Description</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>description</td>
      <td>A brief description of the evaluation module.</td>
      </tr>
      <tr>
      <td>citation</td>
      <td>A BibTex string to cite when available.</td>
      </tr>
      <tr>
      <td>features</td>
      <td>A Features object that defines the input format.</td>
      </tr>
      <tr>
      <td>inputs_description</td>
      <td>This is equivalent to the module documentation string.</td>
      </tr>
      <tr>
      <td>homepage</td>
      <td>The home page of the module.</td>
      </tr>
      <tr>
      <td>license</td>
      <td>The module license.</td>
      </tr>
      <tr>
      <td>codebase_urls</td>
      <td>Link to the code behind the module.</td>
      </tr>
      <tr>
      <td>reference_urls</td>
      <td>additional reference URLs.</td>
      </tr>
      </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's take a look at some of them</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
          '</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"description: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">description</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">citation: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">citation</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">features: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">features</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">inputs_description: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">inputs_description</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">homepage: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">homepage</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">license: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">license</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">codebase_urls: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">codebase_urls</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">reference_urls: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">reference_urls</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'description: ',
          'Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:',
          'Accuracy = (TP + TN) / (TP + TN + FP + FN)',
          ' Where:',
          'TP: True positive',
          'TN: True negative',
          'FP: False positive',
          'FN: False negative',
          'citation: ',
          '@article{scikit-learn,',
          '  title={Scikit-learn: Machine Learning in {P}ython},',
          '  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.',
          '         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.',
          '         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and',
          '         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},',
          '  journal={Journal of Machine Learning Research},',
          '  volume={12},',
          '  pages={2825--2830},',
          '  year={2011}',
          '}',
          'features: {\'predictions\': Value(dtype=\'int32\', id=None), \'references\': Value(dtype=\'int32\', id=None)}',
          'inputs_description: ',
          'Args:',
          '    predictions (`list` of `int`): Predicted labels.',
          '    references (`list` of `int`): Ground truth labels.',
          '    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.',
          '    sample_weight (`list` of `float`): Sample weights Defaults to None.',
          'Returns:',
          '    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.',
          'Examples:',
          '    Example 1-A simple example',
          '        &gt;&gt;&gt; accuracy_metric = evaluate.load("accuracy")',
          '        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])',
          '        &gt;&gt;&gt; print(results)',
          '        {\'accuracy\': 0.5}',
          '    Example 2-The same as Example 1, except with `normalize` set to `False`.',
          '        &gt;&gt;&gt; accuracy_metric = evaluate.load("accuracy")',
          '        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)',
          '        &gt;&gt;&gt; print(results)',
          '        {\'accuracy\': 3.0}',
          '    Example 3-The same as Example 1, except with `sample_weight` set.',
          '        &gt;&gt;&gt; accuracy_metric = evaluate.load("accuracy")',
          '        &gt;&gt;&gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])',
          '        &gt;&gt;&gt; print(results)',
          '        {\'accuracy\': 0.8778625954198473}',
          'homepage: ',
          'license: ',
          'codebase_urls: []',
          'reference_urls: [\'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html']',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Execution">Execution<a class="anchor-link" href="#Execution"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now that we know how the evaluation module works and what it should contain, let's use it. When it comes to calculating the evaluation, there are two main ways to do it:</p>
      <ul>
      <li>All in one</li>
      <li>Incremental</li>
      </ul>
      <p>In the incremental approach, the required entries are added to the module with <code>EvaluationModule.add()</code> or <code>EvaluationModule.add_batch()</code> and the score is computed at the end with <code>EvaluationModule.compute()</code>. Alternatively, all entries can be passed at once to <code>compute()</code>.</p>
      <p>Let's look at these two approaches.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="All-in-one">All in one<a class="anchor-link" href="#All-in-one"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Once we have all the predictions and ground truth we can calculate the metric. Once we have a module defined, we pass it the predictions and ground truth using the <code>compute()</code> method.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
          '</span><span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>',
          '<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>',
          '',
          '<span class="n">accuracy_value</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>',
          '<span class="n">accuracy_value</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'accuracy\': 0.5}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Incremental">Incremental<a class="anchor-link" href="#Incremental"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In many evaluation processes, predictions are built iteratively, as in a for loop. In that case, you could store the predictions and ground truth in a list and at the end pass them to <code>compute()</code>.</p>
      <p>However with the <code>add()</code> and <code>add_batch()</code> methods you can avoid the step of storing the predictions.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If you have all the predictions of a single batch you must use the <code>add()</code> method.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">ref</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]):</span>',
          '    <span class="n">accuracy</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">references</span><span class="o">=</span><span class="n">ref</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">pred</span><span class="p">)</span>',
          '<span class="n">accuracy_value</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>',
          '<span class="n">accuracy_value</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'accuracy\': 0.5}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>However, when you have predictions of several batches you have to use the <code>add_batch()</code> method.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">refs</span><span class="p">,</span> <span class="n">preds</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]):</span>',
          '    <span class="n">accuracy</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">references</span><span class="o">=</span><span class="n">refs</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>',
          '<span class="n">accuracy_value</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>',
          '<span class="n">accuracy_value</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'accuracy\': 0.5}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Combination-of-several-evaluations">Combination of several evaluations<a class="anchor-link" href="#Combination-of-several-evaluations"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Often, one wants to evaluate not only a single metric, but also a variety of different metrics that capture different aspects of a model. For example, for classification it is often a good idea to calculate <code>F1</code>, <code>recall</code> and <code>accuracy</code> in addition to <code>accuracy</code> to get a better picture of model performance. <code>Evaluate</code> allows one to load a bunch of metrics and call them sequentially. However, the most convenient way is to use the <code>combine()</code> function to group them.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">clasification_metrics</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">combine</span><span class="p">([</span><span class="s2">"accuracy"</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">,</span> <span class="s2">"recall"</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">clasification_metrics</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">combine</span><span class="p">([</span><span class="s2">"accuracy"</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">,</span> <span class="s2">"recall"</span><span class="p">])</span>',
          '</span><span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>',
          '<span class="n">targets</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>',
          '',
          '<span class="n">clasification_metrics</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'accuracy\': 0.6666666666666666,',
          ' \'f1\': 0.6666666666666666,',
          ' \'precision\': 1.0,',
          ' \'recall\': 0.5}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Save-results">Save results<a class="anchor-link" href="#Save-results"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 27" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We can save the evaluation results in a file with the <code>save()</code> method by passing a file name. We can pass parameters such as the experiment number</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">references</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>',
          '<span class="n">targets</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>',
          '',
          '<span class="n">result</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>',
          '',
          '<span class="n">hyperparams</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"model"</span><span class="p">:</span> <span class="s2">"bert-base-uncased"</span><span class="p">}</span>',
          '<span class="n">evaluate</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"./results/"</span><span class="p">,</span> <span class="n">experiment</span><span class="o">=</span><span class="s2">"run 42"</span><span class="p">,</span> <span class="o">**</span><span class="n">result</span><span class="p">,</span> <span class="o">**</span><span class="n">hyperparams</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'PosixPath(\'results/result-2024_04_25-17_45_41.json\')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As we can see we have had to create a <code>hyperparams</code> variable to pass it to the <code>save()</code> method. This normally will not be necessary because we will already have those of the model that we are training.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This will create a <code>json</code> with all the information</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">pathlib</span>',
          '',
          '<span class="n">path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="s2">"./results/"</span><span class="p">)</span>',
          '<span class="n">files</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">"*"</span><span class="p">))</span>',
          '<span class="n">files</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[PosixPath(\'results/result-2024_04_25-17_45_41.json\')]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">json</span>',
          '<span class="n">result_file</span> <span class="o">=</span> <span class="n">files</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>',
          '<span class="n">result_json</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">result_file</span><span class="p">)</span><span class="o">.</span><span class="n">read_text</span><span class="p">()</span>',
          '<span class="n">result_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">result_json</span><span class="p">)</span>',
          '<span class="n">result_dict</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'experiment\': \'run 42\',',
          ' \'accuracy\': 0.5,',
          ' \'model\': \'bert-base-uncased\',',
          ' \'_timestamp\': \'2024-04-25T17:45:41.218287\',',
          ' \'_git_commit_hash\': \'8725338b6bf9c97274685df41b2ee6e11319a735\',',
          ' \'_evaluate_version\': \'0.4.1\',',
          ' \'_python_version\': \'3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]\',',
          ' \'_interpreter_path\': \'/home/maximo.fernandez/miniconda3/envs/nlp/bin/python\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Upload-results-to-the-hub">Upload results to the hub<a class="anchor-link" href="#Upload-results-to-the-hub"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 28" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In case we are training a model, we can upload to the model card of the model the results of the evaluation with the <code>push_to_hub()</code> method. In this way they will appear in the model page.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Evaluator">Evaluator<a class="anchor-link" href="#Evaluator"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 29" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we have a model, a dataset and a metric, we can do inference for the whole dataset and pass the predictions and the actual labels to the evaluator to return the metric and thus obtain the model metrics.</p>
      <p>Or we can give everything to the library and let it do the work for us. Using the <code>evaluator()</code> method, we pass it the model, the dataset and the metric and the method does everything for us.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>First we define the model, the dataset and the metric</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      <span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">evaluator</span>',
      '      <span class="kn">import</span> <span class="nn">evaluate</span>',
      '      ',
      '      <span class="n">model_pipeline</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-classification"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"lvwerra/distilbert-imdb"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '      <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"imdb"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"test"</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>Now we pass everything to <code>evaluator()</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
          '<span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">evaluator</span>',
          '<span class="kn">import</span> <span class="nn">evaluate</span>',
          '',
          '<span class="n">model_pipeline</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-classification"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"lvwerra/distilbert-imdb"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"imdb"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"test"</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
          '<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
          '</span><span class="n">task_evaluator</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="s2">"text-classification"</span><span class="p">)</span>',
          '',
          '<span class="n">results</span> <span class="o">=</span> <span class="n">task_evaluator</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">model_or_pipeline</span><span class="o">=</span><span class="n">model_pipeline</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span>',
          '                       <span class="n">label_mapping</span><span class="o">=</span><span class="p">{</span><span class="s2">"NEGATIVE"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">"POSITIVE"</span><span class="p">:</span> <span class="mi">1</span><span class="p">},)</span>',
          '<span class="n">results</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'accuracy\': 0.933,',
          ' \'total_time_in_seconds\': 29.43192940400013,',
          ' \'samples_per_second\': 33.97670557962431,',
          ' \'latency_in_seconds\': 0.02943192940400013}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Thanks to the evaluator we were able to obtain the model metrics without having to make the inference ourselves.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Display">Display<a class="anchor-link" href="#Display"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Sometimes we get different metrics for different models, which makes it difficult to compare them, so graphs make it easier.</p>
      <p>The <code>Evaluate</code> library offers different visualizations through the <code>visualization()</code> method. We have to pass the data to it as a list of dictionaries, where each dictionary has to have the same keys</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In order to use this function it is necessary to have the <code>matplotlib</code> library installed.</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>matplotlib
      </pre></div>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">evaluate</span>
      <span class="kn">from</span> <span class="nn">evaluate.visualization</span> <span class="kn">import</span> <span class="n">radar_plot</span>
      
      <span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
         <span class="p">{opening_brace}</span><span class="s2">"accuracy"</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span> <span class="s2">"latency_in_seconds"</span><span class="p">:</span> <span class="mf">33.6</span><span class="p">{closing_brace},</span>
         <span class="p">{opening_brace}</span><span class="s2">"accuracy"</span><span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">:</span> <span class="mf">0.87</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">:</span> <span class="mf">0.91</span><span class="p">,</span> <span class="s2">"latency_in_seconds"</span><span class="p">:</span> <span class="mf">11.2</span><span class="p">{closing_brace},</span>
         <span class="p">{opening_brace}</span><span class="s2">"accuracy"</span><span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">:</span> <span class="mf">0.78</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">:</span> <span class="mf">0.88</span><span class="p">,</span> <span class="s2">"latency_in_seconds"</span><span class="p">:</span> <span class="mf">87.6</span><span class="p">{closing_brace},</span> 
         <span class="p">{opening_brace}</span><span class="s2">"accuracy"</span><span class="p">:</span> <span class="mf">0.88</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">:</span> <span class="mf">0.78</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">:</span> <span class="mf">0.81</span><span class="p">,</span> <span class="s2">"latency_in_seconds"</span><span class="p">:</span> <span class="mf">101.6</span><span class="p">{closing_brace}</span>
         <span class="p">]</span>
      
      <span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Model 1"</span><span class="p">,</span> <span class="s2">"Model 2"</span><span class="p">,</span> <span class="s2">"Model 3"</span><span class="p">,</span> <span class="s2">"Model 4"</span><span class="p">]</span>
      
      <span class="n">plot</span> <span class="o">=</span> <span class="n">radar_plot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">model_names</span><span class="o">=</span><span class="n">model_names</span><span class="p">)</span>
      <span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/tmp/ipykernel_10271/263559674.py:14: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
        plot.show()
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/hugging-face-evaluate0.webp" width="1026" height="507" alt="image hugging-face-evaluate 1" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We can now visually compare the 4 models and choose the optimal one, based on one or several metrics</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Evaluating-the-model-on-a-set-of-tasks">Evaluating the model on a set of tasks<a class="anchor-link" href="#Evaluating-the-model-on-a-set-of-tasks"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We can evaluate a model, for example, for different datasets. For this we can use the <code>evaluation_suite</code> method. For example we are going to create an evaluator that evaluates a model on the <code>imdb</code> and <code>sst2</code> datasets. We are going to see these datasets, for that we use the <code>load_dataset_builder</code> method so we don't have to download the complete dataset.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">evaluate</span>',
          '<span class="kn">from</span> <span class="nn">evaluate.visualization</span> <span class="kn">import</span> <span class="n">radar_plot</span>',
          '',
          '<span class="n">data</span> <span class="o">=</span> <span class="p">[</span>',
          '   <span class="p">{</span><span class="s2">"accuracy"</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span> <span class="s2">"latency_in_seconds"</span><span class="p">:</span> <span class="mf">33.6</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"accuracy"</span><span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">:</span> <span class="mf">0.87</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">:</span> <span class="mf">0.91</span><span class="p">,</span> <span class="s2">"latency_in_seconds"</span><span class="p">:</span> <span class="mf">11.2</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"accuracy"</span><span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">:</span> <span class="mf">0.78</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">:</span> <span class="mf">0.88</span><span class="p">,</span> <span class="s2">"latency_in_seconds"</span><span class="p">:</span> <span class="mf">87.6</span><span class="p">},</span> ',
          '   <span class="p">{</span><span class="s2">"accuracy"</span><span class="p">:</span> <span class="mf">0.88</span><span class="p">,</span> <span class="s2">"precision"</span><span class="p">:</span> <span class="mf">0.78</span><span class="p">,</span> <span class="s2">"f1"</span><span class="p">:</span> <span class="mf">0.81</span><span class="p">,</span> <span class="s2">"latency_in_seconds"</span><span class="p">:</span> <span class="mf">101.6</span><span class="p">}</span>',
          '   <span class="p">]</span>',
          '',
          '<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Model 1"</span><span class="p">,</span> <span class="s2">"Model 2"</span><span class="p">,</span> <span class="s2">"Model 3"</span><span class="p">,</span> <span class="s2">"Model 4"</span><span class="p">]</span>',
          '',
          '<span class="n">plot</span> <span class="o">=</span> <span class="n">radar_plot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">model_names</span><span class="o">=</span><span class="n">model_names</span><span class="p">)</span>',
          '<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset_builder</span>',
          '<span class="n">imdb</span> <span class="o">=</span> <span class="n">load_dataset_builder</span><span class="p">(</span><span class="s2">"imdb"</span><span class="p">)</span>',
          '<span class="n">imdb</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">features</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '/tmp/ipykernel_10271/263559674.py:14: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown',
          '  plot.show()',
          '{\'text\': Value(dtype=\'string\', id=None),',
          ' \'label\': ClassLabel(names=[\'neg\', \'pos\'], id=None)}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset_builder</span>',
          '<span class="n">sst2</span> <span class="o">=</span> <span class="n">load_dataset_builder</span><span class="p">(</span><span class="s2">"sst2"</span><span class="p">)</span>',
          '<span class="n">sst2</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">features</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'idx\': Value(dtype=\'int32\', id=None),',
          ' \'sentence\': Value(dtype=\'string\', id=None),',
          ' \'label\': ClassLabel(names=[\'negative\', \'positive\'], id=None)}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As we can see, with the <code>imdb</code> dataset we need to take the <code>text</code> column to get the text and the <code>label</code> column to get the target. With the <code>sst2</code> dataset we need to take the <code>sentence</code> column to get the text and the <code>label</code> column to get the target.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the evaluator for the two datasets</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">evaluate</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate.evaluation_suite</span> <span class="kn">import</span> <span class="n">SubTask</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">Suite</span><span class="p">(</span><span class="n">evaluate</span><span class="o">.</span><span class="n">EvaluationSuite</span><span class="p">):</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>',
      '              <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>',
      '      ',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">suite</span> <span class="o">=</span> <span class="p">[</span>',
      '                  <span class="n">SubTask</span><span class="p">(</span>',
      '                      <span class="n">task_type</span><span class="o">=</span><span class="s2">"text-classification"</span><span class="p">,</span>',
      '                      <span class="n">data</span><span class="o">=</span><span class="s2">"imdb"</span><span class="p">,</span>',
      '                      <span class="n">split</span><span class="o">=</span><span class="s2">"test[:1]"</span><span class="p">,</span>',
      '                      <span class="n">args_for_task</span><span class="o">=</span><span class="p">{</span>',
      '                          <span class="s2">"metric"</span><span class="p">:</span> <span class="s2">"accuracy"</span><span class="p">,</span>',
      '                          <span class="s2">"input_column"</span><span class="p">:</span> <span class="s2">"text"</span><span class="p">,</span>',
      '                          <span class="s2">"label_column"</span><span class="p">:</span> <span class="s2">"label"</span><span class="p">,</span>',
      '                          <span class="s2">"label_mapping"</span><span class="p">:</span> <span class="p">{</span>',
      '                              <span class="s2">"LABEL_0"</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>',
      '                              <span class="s2">"LABEL_1"</span><span class="p">:</span> <span class="mf">1.0</span>',
      '                          <span class="p">}</span>',
      '                      <span class="p">}</span>',
      '                  <span class="p">),</span>',
      '                  <span class="n">SubTask</span><span class="p">(</span>',
      '                      <span class="n">task_type</span><span class="o">=</span><span class="s2">"text-classification"</span><span class="p">,</span>',
      '                      <span class="n">data</span><span class="o">=</span><span class="s2">"sst2"</span><span class="p">,</span>',
      '                      <span class="n">split</span><span class="o">=</span><span class="s2">"test[:1]"</span><span class="p">,</span>',
      '                      <span class="n">args_for_task</span><span class="o">=</span><span class="p">{</span>',
      '                          <span class="s2">"metric"</span><span class="p">:</span> <span class="s2">"accuracy"</span><span class="p">,</span>',
      '                          <span class="s2">"input_column"</span><span class="p">:</span> <span class="s2">"sentence"</span><span class="p">,</span>',
      '                          <span class="s2">"label_column"</span><span class="p">:</span> <span class="s2">"label"</span><span class="p">,</span>',
      '                          <span class="s2">"label_mapping"</span><span class="p">:</span> <span class="p">{</span>',
      '                              <span class="s2">"LABEL_0"</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>',
      '                              <span class="s2">"LABEL_1"</span><span class="p">:</span> <span class="mf">1.0</span>',
      '                          <span class="p">}</span>',
      '                      <span class="p">}</span>',
      '                  <span class="p">)</span>',
      '              <span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











































      
      <section class="section-block-markdown-cell">
      <p>It can be seen in <code>split="test[:1]",</code> that we only take one example of the subset of test for this notebook and that the execution does not take too long</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora evaluamos con el modelo <code>huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">EvaluationSuite</span>
      <span class="n">suite</span> <span class="o">=</span> <span class="n">EvaluationSuite</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'mathemakitten/sentiment-evaluation-suite'</span><span class="p">)</span>
      <span class="n">results</span> <span class="o">=</span> <span class="n">suite</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">"huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli"</span><span class="p">)</span>
      <span class="n">results</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>`data` is a preloaded Dataset! Ignoring `subset` and `split`.
      `data` is a preloaded Dataset! Ignoring `subset` and `split`.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[4]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>[{opening_brace}'accuracy': 0.3,
        'total_time_in_seconds': 1.4153412349987775,
        'samples_per_second': 7.06543394110088,
        'latency_in_seconds': 0.14153412349987776,
        'task_name': 'imdb',
        'data_preprocessor': '&lt;function Suite.__init__.&lt;locals&gt;.&lt;lambda&gt; at 0x7f3ff27a5080&gt;'{closing_brace},
       {opening_brace}'accuracy': 0.0,
        'total_time_in_seconds': 0.1323430729971733,
        'samples_per_second': 75.56118936586572,
        'latency_in_seconds': 0.013234307299717328,
        'task_name': 'sst2',
        'data_preprocessor': '&lt;function Suite.__init__.&lt;locals&gt;.&lt;lambda&gt; at 0x7f3f2a9cc720&gt;'}]</pre>
      </div>
      </div>
      </div>
      </section>
      






    </div>

  </section>

</PostLayout>
