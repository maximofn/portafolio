---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Optimun';
const end_url = 'hugging-face-optimun';
const description = 'Attention, slow PyTorch models! üêå Optimun, the Hugging Face library, comes to the rescue to speed up your workouts and inferences. With Optimun, you can forget about speed issues and enjoy more speed and efficiency üïíÔ∏è. And best of all, it\'s PyTorch compatible - go on, give your models a boost with Optimun! üíª';
const keywords = 'hugging face, optimun, pytorch, transformers, training, inference, speed, efficiency';
const languaje = 'EN';
const image_path = 'https://images.maximofn.com/huggingface_optimun.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=2388
    image_height=884
    image_extension=webp
    article_date=2024-06-01+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Installation"><h2>Installation</h2></a>
      <a class="anchor-link" href="#BetterTransformer"><h2>BetterTransformer</h2></a>
      <a class="anchor-link" href="#Inference with Automodel"><h3>Inference with Automodel</h3></a>
      <a class="anchor-link" href="#Inference with Pipeline"><h3>Inference with Pipeline</h3></a>
      <a class="anchor-link" href="#Training"><h3>Training</h3></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Optimun</code> is an extension of the <a href="https://www.maximofn.com/hugging-face-transformers/">Transformers</a> library that provides a set of performance optimization tools for training and inference of models, on specific hardware, with maximum efficiency.</p>
      <p>The AI ecosystem is evolving rapidly, and every day more specialized hardware emerges along with its own optimizations. Therefore, <code>Optimum</code> allows users to efficiently utilize any of this HW with the same ease as <a href="https://www.maximofn.com/hugging-face-transformers/">Transformers</a>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Optimun</code> allows optimization for the following HW platforms:</p>
      <ul>
        <li>Nvidia</li>
        <li>AMD</li>
        <li>Intel</li>
        <li>AWS</li>
        <li>TPU</li>
        <li>Havana</li>
        <li>FuriousAI</li>
      </ul>
      <p>In addition, it offers acceleration for the following open source integrations</p>
      <ul>
        <li>ONNX runtime</li>
        <li>Exporters: Export Pytorch or TensorFlow models to different formats such as ONNX or TFLite</li>
        <li>BetterTransformer</li>
        <li>Torch FX</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Installation">Installation<a class="anchor-link" href="#Installation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To install <code>Optimun</code> simply run:</p>
      <div class='highlight'><pre><code class="language-bash">pip install optimum</code></pre></div>
      <p>But if you want to install with support for all HW platforms, you can do it like this</p>
      <table>
        <thead>
          <tr>
            <th>Accelerator</th>
            <th>Installation</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>ONNX Runtime</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]</code></td>
          </tr>
          <tr>
            <td>Intel Neural Compressor</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]</code></td>
          </tr>
          <tr>
            <td>OpenVINO</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[openvino]</code></td>
          </tr>
          <tr>
            <td>AMD Instinct GPUs and Ryzen AI NPU</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[amd]</code></td>
          </tr>
          <tr>
            <td>AWS Trainum & Inferentia</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[neuronx]</code></td>
          </tr>
          <tr>
            <td>Habana Gaudi Processor (HPU)</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[habana]</code></td>
          </tr>
          <tr>
            <td>FuriosaAI</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[furiosa]</code></td>
          </tr>
        </tbody>
      </table>
      <p>The flags <code>--upgrade --upgrade-strategy eager</code> are necessary to ensure that the different packages are updated to the latest possible version.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Since most people use Pytorch on Nvidia GPUs, and especially since I have an Nvidia GPU, this post will only discuss the use of <code>Optimun</code> with Nvidia GPUs and Pytorch.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="BetterTransformer">BetterTransformer<a class="anchor-link" href="#BetterTransformer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>BetterTransformer is a native PyTorch optimization to achieve an acceleration of x1.25 to x4 in the inference of Transformer-based models.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>BetterTransformer is an API that allows leveraging modern hardware features to accelerate the training and inference of transformer models in PyTorch, using more efficient attention implementations and the <code>fast path</code> of the native <code>nn.TransformerEncoderLayer</code>.</p>
      <p>BetterTransformer uses two types of accelerations:</p>
      <ol>
        <li><code>Flash Attention</code>: This is an implementation of <code>attention</code> that uses <code>sparse</code> to reduce computational complexity. Attention is one of the most expensive operations in transformer models, and <code>Flash Attention</code> makes it more efficient.</li>
        <li><code>Memory-Efficient Attention</code>: This is another implementation of attention that uses the <code>scaled_dot_product_attention</code> function from PyTorch. This function is more memory-efficient than the standard attention implementation in PyTorch.</li>
      </ol>
      <p>In addition, version 2.0 of PyTorch includes a native scaled dot product attention (SDPA) operator as part of <code>torch.nn.functional</code></p>
      <p><code>Optimun</code> provides this functionality with the library <code>Transformers</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Inference with Automodel">Inference with Automodel<a class="anchor-link" href="#Inference with Automodel"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>First, let's see how normal inference would work with <code>Transformers</code> and <code>Automodel</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">sentence_output</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now we see how it would be optimized with <code>BetterTransformer</code> and <code>Optimun</code></p>
      <p>What we need to do is convert the model using the <code>transform</code> method of <code>BetterTransformer</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.bettertransformer</span><span class="w"> </span><span class="kn">import</span> <span class="n">BetterTransformer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Convert the model to a BetterTransformer model</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">sentence_output</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Inference with Pipeline">Inference with Pipeline<a class="anchor-link" href="#Inference with Pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Just like before, we first see how normal inference would be done with <code>Transformers</code> and <code>Pipeline</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>',
      '<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;I am a student at [MASK] University.&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;score&#x27;: 0.05116177722811699,',
          '&#x20;&#x20;&#x27;token&#x27;: 8422,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;stanford&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at stanford university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.04033993184566498,',
          '&#x20;&#x20;&#x27;token&#x27;: 5765,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;harvard&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at harvard university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.03990468755364418,',
          '&#x20;&#x20;&#x27;token&#x27;: 7996,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;yale&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at yale university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.0361952930688858,',
          '&#x20;&#x20;&#x27;token&#x27;: 10921,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;cornell&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at cornell university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.03303057327866554,',
          '&#x20;&#x20;&#x27;token&#x27;: 9173,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;princeton&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at princeton university.&#x27;&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now we see how to optimize it, for this we use <code>pipeline</code> from <code>Optimun</code>, instead of the one from <code>Transformers</code>. Additionally, we need to specify that we want to use <code>bettertransformer</code> as the accelerator.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optimum.pipelines</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Use the BetterTransformer pipeline</span>',
      '<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;bettertransformer&quot;</span><span class="p">)</span>',
      '<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;I am a student at [MASK] University.&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.',
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)',
          '&#x20;&#x20;hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;score&#x27;: 0.05116180703043938,',
          '&#x20;&#x20;&#x27;token&#x27;: 8422,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;stanford&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at stanford university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.040340032428503036,',
          '&#x20;&#x20;&#x27;token&#x27;: 5765,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;harvard&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at harvard university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.039904672652482986,',
          '&#x20;&#x20;&#x27;token&#x27;: 7996,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;yale&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at yale university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.036195311695337296,',
          '&#x20;&#x20;&#x27;token&#x27;: 10921,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;cornell&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at cornell university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.03303062543272972,',
          '&#x20;&#x20;&#x27;token&#x27;: 9173,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;princeton&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at princeton university.&#x27;&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Training">Training<a class="anchor-link" href="#Training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>For the training with <code>Optimun</code> we do the same as with the inference with Automodel, we convert the model using the <code>transform</code> method of <code>BeterTransformer</code>.</p>
      <p>When we finish the training, we revert the model back to its original form using the <code>reverse</code> method of <code>BetterTransformer</code>, so that we can save it and upload it to the Hugging Face hub.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.bettertransformer</span><span class="w"> </span><span class="kn">import</span> <span class="n">BetterTransformer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Convert the model to a BetterTransformer model</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="c1">##############################################################################</span>',
      '<span class="c1"># do your training here</span>',
      '<span class="c1">##############################################################################</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Convert the model back to a Hugging Face model</span>',
      '<span class="n">model_hf</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">model_hf</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;fine_tuned_model&quot;</span><span class="p">)</span>',
      '<span class="n">model_hf</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;fine_tuned_model&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>































    </div>

  </section>

</PostLayout>
