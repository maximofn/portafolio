---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Tokenizers';
const end_url = 'hugging-face-tokenizers';
const description = 'üìÑ ‚û°Ô∏è üî§ Explore the power of the Hugging Face Tokenizers library for natural language processing in AI. Discover how this essential tool transforms text into structured data, optimizing AI model training with practical examples and Python code. Dive into the future of NLP with our expert guide';
const keywords = 'hugging face, tokenizers, natural language processing, nlp, artificial intelligence, ai, python';
const languaje = 'EN';
const image_path = "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Hugging%20Face's%20tokenizers%20library.webp";
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-02-26+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Installation"><h2>Installation</h2></a>
      <a class="anchor-link" href="#The-tokenization-pipeline"><h2>The tokenization pipeline</h2></a>
      <a class="anchor-link" href="#Standardization"><h3>Standardization</h3></a>
      <a class="anchor-link" href="#Pre-tokenization"><h3>Pre-tokenization</h3></a>
      <a class="anchor-link" href="#Tokenization"><h3>Tokenization</h3></a>
      <a class="anchor-link" href="#Model-training"><h4>Model training</h4></a>
      <a class="anchor-link" href="#Model-training-with-the-train-method"><h5>Model training with the <code>train</code> method</h5></a>
      <a class="anchor-link" href="#Model-training-with-train_from_iterator-method"><h5>Model training with <code>train_from_iterator</code> method</h5></a>
      <a class="anchor-link" href="#Training-the-model-with-the-train_from_iterator-method-from-a-Hugging-Face-dataset"><h5>Training the model with the <code>train_from_iterator</code> method from a Hugging Face dataset</h5></a>
      <a class="anchor-link" href="#Saving-the-model"><h4>Saving the model</h4></a>
      <a class="anchor-link" href="#Loading-the-pre-trained-model"><h4>Loading the pre-trained model</h4></a>
      <a class="anchor-link" href="#Post-processing"><h3>Post processing</h3></a>
      <a class="anchor-link" href="#Encoding"><h3>Encoding</h3></a>
      <a class="anchor-link" href="#Decoding"><h3>Decoding</h3></a>
      <a class="anchor-link" href="#BERT-tokenizer"><h2>BERT tokenizer</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Hugging-Face-tokenizers">Hugging Face tokenizers<a class="anchor-link" href="#Hugging-Face-tokenizers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The Hugging Face <code>tokenizers</code> library provides an implementation of today`s most commonly used tokenizers, focusing on performance and versatility. In the post <a href="https://maximofn.com/tokens/">tokens</a> we already saw the importance of tokens when processing text, since computers do not understand words, but numbers. Therefore, it is necessary to convert words to numbers so that language models can process them.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
      <h2 id="Installation">Installation<a class="anchor-link" href="#Installation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To install <code>tokenizers</code> with pip:</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>tokenizers
      <span class="sb">```</span>
      
      to<span class="w"> </span>install<span class="w"> </span><span class="sb">`</span>tokenizers<span class="sb">`</span><span class="w"> </span>with<span class="w"> </span>conda:
      
      <span class="sb">````</span>bash
      conda<span class="w"> </span>install<span class="w"> </span>conda-forge::tokenizers
      <span class="sb">```</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="The-tokenization-pipeline">The tokenization pipeline<a class="anchor-link" href="#The-tokenization-pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To tokenize a sequence, <code>Tokenizer.encode</code> is used, which performs the following steps:</p>
      <ul>
      <li>Standardization</li>
      <li>pre-tokenization</li>
      <li>Tokenization</li>
      <li>Post-tokenization</li>
      </ul>
      <p>Let's take a look at each one</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>For this post we are going to use the dataset <a href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/" target="_blank" rel="nofollow noreferrer">wikitext-103</a></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="o">!</span>wget<span class="w"> </span>https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>--2024-02-26 08:14:11--  https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz
      Resolving dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)... 23.200.169.125
      Connecting to dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)|23.200.169.125|:443... connected.
      HTTP request sent, awaiting response... </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>200 OK
      Length: 189603606 (181M) [application/x-gzip]
      Saving to: ‚Äòwikitext-103.tar.gz‚Äô
      
      wikitext-103.tar.gz 100%[===================&gt;] 180,82M  6,42MB/s    in 30s     
      
      2024-02-26 08:14:42 (5,95 MB/s) - ‚Äòwikitext-103.tar.gz‚Äô saved [189603606/189603606]
      
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">!</span>wget<span class="w"> </span>https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz',
          '</span><span class="o">!</span>tar<span class="w"> </span>-xvzf<span class="w"> </span>wikitext-103.tar.gz',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '--2024-02-26 08:14:11--  https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz',
          'Resolving dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)... 23.200.169.125',
          'Connecting to dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)|23.200.169.125|:443... connected.',
          'HTTP request sent, awaiting response... ',
          'wikitext-103/',
          'wikitext-103/wiki.test.tokens',
          'wikitext-103/wiki.valid.tokens',
          'wikitext-103/README.txt',
          'wikitext-103/LICENSE.txt',
          'wikitext-103/wiki.train.tokens',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">!</span>rm<span class="w"> </span>wikitext-103.tar.gz',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <h3 id="Standardization">Standardization<a class="anchor-link" href="#Standardization"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Normalization are operations that are applied to the text before tokenization, such as removing whitespace, converting to lowercase, removing special characters, etc. The following normalizations are implemented in Hugging Face:</p>
      <table>
      <thead>
      <tr>
      <th>Normalizaci√≥n</th>
      <th>Descripci√≥n</th>
      <th>Ejemplo</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>NFD (Normalization for D)</td>
      <td>Los caracteres se descomponen por equivalencia can√≥nica</td>
      <td><code>√¢</code> (U+00E2) se descompone en <code>a</code> (U+0061) + <code>^</code> (U+0302)</td>
      </tr>
      <tr>
      <td>NFKD (Normalization Form KD)</td>
      <td>Los caracteres se descomponen por compatibilidad</td>
      <td><code>Ô¨Å</code> (U+FB01) se descompone en <code>f</code> (U+0066) + <code>i</code> (U+0069)</td>
      </tr>
      <tr>
      <td>NFC (Normalization Form C)</td>
      <td>Los caracteres se descomponen y luego se recomponen por equivalencia can√≥nica</td>
      <td><code>√¢</code> (U+00E2) se descompone en <code>a</code> (U+0061) + <code>^</code> (U+0302) y luego se recompone en <code>√¢</code> (U+00E2)</td>
      </tr>
      <tr>
      <td>NFKC (Normalization Form KC)</td>
      <td>Los caracteres se descomponen por compatibilidad y luego se recomponen por equivalencia can√≥nica</td>
      <td><code>Ô¨Å</code> (U+FB01) se descompone en <code>f</code> (U+0066) + <code>i</code> (U+0069) y luego se recompone en <code>f</code> (U+0066) + <code>i</code> (U+0069)</td>
      </tr>
      <tr>
      <td>Lowercase</td>
      <td>Convierte el texto a min√∫sculas</td>
      <td><code>Hello World</code> se convierte en <code>hello world</code></td>
      </tr>
      <tr>
      <td>Strip</td>
      <td>Elimina todos los espacios en blanco de los lados especificados (izquierdo, derecho o ambos) del texto</td>
      <td><code>Hello World</code> se convierte en <code>Hello World</code></td>
      </tr>
      <tr>
      <td>StripAccents</td>
      <td>Elimina todos los s√≠mbolos de acento en unicode (se utilizar√° con NFD por coherencia)</td>
      <td><code>√°</code> (U+00E1) se convierte en <code>a</code> (U+0061)</td>
      </tr>
      <tr>
      <td>Replace</td>
      <td>Sustituye una cadena personalizada o <a href="https://maximofn.com/regular-expressions/">regex</a> y la cambia por el contenido dado</td>
      <td><code>Hello World</code> se convierte en <code>Hello Universe</code></td>
      </tr>
      <tr>
      <td>BertNormalizer</td>
      <td>Proporciona una implementaci√≥n del Normalizador utilizado en el BERT original. Las opciones que se pueden configurar son <code>clean_text</code>, <code>handle_chinese_chars</code>, <code>strip_accents</code> y <code>lowercase</code></td>
      <td><code>Hello World</code> se convierte en <code>hello world</code></td>
      </tr>
      </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's create a normalizer to see how it works.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">!</span>rm<span class="w"> </span>wikitext-103.tar.gz',
          '</span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">normalizers</span>',
          '',
          '<span class="n">bert_normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">BertNormalizer</span><span class="p">()</span>',
          '',
          '<span class="n">input_text</span> <span class="o">=</span> <span class="s2">"H√©ll√≤ h√¥w are √º?"</span>',
          '<span class="n">normalized_text</span> <span class="o">=</span> <span class="n">bert_normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
          '<span class="n">normalized_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'hello how are u?\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>To use several normalizers we can use the <code>Sequence</code> method</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">custom_normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">normalizers</span><span class="o">.</span><span class="n">NFKC</span><span class="p">(),</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">BertNormalizer</span><span class="p">()])</span>',
          '',
          '<span class="n">normalized_text</span> <span class="o">=</span> <span class="n">custom_normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
          '<span class="n">normalized_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'hello how are u?\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>To modify the normalizer of a tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">tokenizers</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">BertWordPieceTokenizer</span><span class="p">()</span> <span class="c1"># or any other tokenizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">tokenizers</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">BertWordPieceTokenizer</span><span class="p">()</span> <span class="c1"># or any other tokenizer</span>',
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">custom_normalizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <h3 id="Pre-tokenization">Pre-tokenization<a class="anchor-link" href="#Pre-tokenization"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pretokenization is the act of breaking a text into smaller objects. The pretokenizer will split the text into "words" and the final tokens will be parts of those words.</p>
      <p>The PreTokenizer is responsible for splitting the input according to a set of rules. This preprocessing allows you to make sure that the tokenizer does not build tokens across multiple "splits". For example, if you don't want to have whitespace within a token, then you can have a pre tokenizer that splits in words from whitespace.</p>
      <p>The following pre tokenizers are implemented in Hugging Face</p>
      <table>
      <thead>
      <tr>
      <th>PreTokenizer</th>
      <th>Descripci√≥n</th>
      <th>Ejemplo</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>ByteLevel</td>
      <td>Divide en espacios en blanco mientras reasigna todos los bytes a un conjunto de caracteres visibles. Esta t√©cnica fue introducida por OpenAI con GPT-2 y tiene algunas propiedades m√°s o menos buenas: Como mapea sobre bytes, un tokenizador que utilice esto s√≥lo requiere 256 caracteres como alfabeto inicial (el n√∫mero de valores que puede tener un byte), frente a los m√°s de 130.000 caracteres Unicode. Una consecuencia del punto anterior es que es absolutamente innecesario tener un token desconocido usando esto ya que podemos representar cualquier cosa con 256 tokens. Para caracteres no ascii, se vuelve completamente ilegible, ¬°pero funciona!</td>
      <td><code>Hello my friend, how are you?</code> se divide en <code>Hello</code>, <code>ƒ†my</code>, <code>ƒ†friend</code>, <code>,</code>, <code>ƒ†how</code>, <code>ƒ†are</code>, <code>ƒ†you</code>, <code>?</code></td>
      </tr>
      <tr>
      <td>Whitespace</td>
      <td>Divide en l√≠mites de palabra usando la siguiente expresi√≥n regular: <code>\w+[^\w\s]+</code>. En mi post sobre <a href="https://maximofn.com/regular-expressions/">expresiones regulares</a> puedes entender qu√© hace</td>
      <td><code>Hello there!</code> se divide en <code>Hello</code>, <code>there</code>, <code>!</code></td>
      </tr>
      <tr>
      <td>WhitespaceSplit</td>
      <td>Se divide en cualquier car√°cter de espacio en blanco</td>
      <td><code>Hello there!</code> se divide en <code>Hello</code>, <code>there!</code></td>
      </tr>
      <tr>
      <td>Punctuation</td>
      <td>Aislar√° todos los caracteres de puntuaci√≥n</td>
      <td><code>Hello?</code> se divide en <code>Hello</code>, <code>?</code></td>
      </tr>
      <tr>
      <td>Metaspace</td>
      <td>Separa los espacios en blanco y los sustituye por un car√°cter especial "‚ñÅ" (U+2581)</td>
      <td><code>Hello there</code> se divide en <code>Hello</code>, <code>‚ñÅthere</code></td>
      </tr>
      <tr>
      <td>CharDelimiterSplit</td>
      <td>Divisiones en un car√°cter determinado</td>
      <td>Ejemplo con el caracter <code>x</code>: <code>Helloxthere</code> se divide en <code>Hello</code>, <code>there</code></td>
      </tr>
      <tr>
      <td>Digits</td>
      <td>Divide los n√∫meros de cualquier otro car√°cter</td>
      <td><code>Hello123there</code> se divide en <code>Hello</code>, <code>123</code>, <code>there</code></td>
      </tr>
      <tr>
      <td>Split</td>
      <td>Pretokenizador vers√°til que divide seg√∫n el patr√≥n y el comportamiento proporcionados. El patr√≥n se puede invertir si es necesario. El patr√≥n debe ser una cadena personalizada o una <a href="https://maximofn.com/regular-expressions/">regex</a>. El comportamiento debe ser <code>removed</code>, <code>isolated</code>, <code>merged_with_previous</code>, <code>merged_with_next</code>, <code>contiguous</code>. Para invertir se indica con un booleano</td>
      <td>Ejemplo con pattern=<code>" "</code>, behavior=<code>isolated</code>, invert=<code>False</code>: <code>Hello, how are you?</code> se divide en <code>Hello,</code>, <code></code>, <code>how</code>, <code></code>, <code>are</code>, <code></code>, <code>you?</code></td>
      </tr>
      </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's create a pre tokenizer to see how it works.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">tokenizers</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">BertWordPieceTokenizer</span><span class="p">()</span> <span class="c1"># or any other tokenizer</span>',
          '</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">custom_normalizer</span>',
          '</span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">pre_tokenizers</span>',
          '',
          '<span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Digits</span><span class="p">(</span><span class="n">individual_digits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="n">input_text</span> <span class="o">=</span> <span class="s2">"I paid $30 for the car"</span>',
          '<span class="n">pre_tokenized_text</span> <span class="o">=</span> <span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
          '<span class="n">pre_tokenized_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[(\'I paid $\', (0, 8)),',
          ' (\'3\', (8, 9)),',
          ' (\'0\', (9, 10)),',
          ' (\' for the car\', (10, 22))]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>To use several pre tokenizers we can use the <code>Sequence</code> method.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">custom_pre_tokenizer</span> <span class="o">=</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Whitespace</span><span class="p">(),</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Digits</span><span class="p">(</span><span class="n">individual_digits</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>',
          '',
          '<span class="n">pre_tokenized_text</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
          '<span class="n">pre_tokenized_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[(\'I\', (0, 1)),',
          ' (\'paid\', (2, 6)),',
          ' (\'$\', (7, 8)),',
          ' (\'3\', (8, 9)),',
          ' (\'0\', (9, 10)),',
          ' (\'for\', (11, 14)),',
          ' (\'the\', (15, 18)),',
          ' (\'car\', (19, 22))]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>To modify the pre tokenizer of a tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenization">Tokenization<a class="anchor-link" href="#Tokenization"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Once the input texts have been normalized and pretokenized, the tokenizer applies the model to the pretokens. This is the part of the process that must be trained with the corpus (or has already been trained if a pre-trained tokenizer is used).</p>
      <p>The function of the model is to divide the "words" into tokens using the rules it has learned. It is also responsible for assigning those tokens to their corresponding IDs in the model's vocabulary.</p>
      <p>The model has a vocabulary size, i.e., it has a finite number of tokens, so it has to decompose the words and assign them to one of those tokens.</p>
      <p>This model is passed when initializing the Tokenizer. Currently, the ü§ó Tokenizers library supports:</p>
      <table>
      <thead>
      <tr>
      <th>Modelo</th>
      <th>Descripci√≥n</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>WordLevel</td>
      <td>Este es el algoritmo "cl√°sico" de tokenizaci√≥n. Te permite simplemente asignar palabras a IDs sin nada sofisticado. Tiene la ventaja de ser muy f√°cil de usar y entender, pero requiere vocabularios extremadamente grandes para una buena cobertura. El uso de este modelo requiere el uso de un PreTokenizer. Este modelo no realiza ninguna elecci√≥n directamente, simplemente asigna tokens de entrada a IDs.</td>
      </tr>
      <tr>
      <td>BPE (Byte Pair Encoding)</td>
      <td>Uno de los algoritmos de tokenizaci√≥n de subpalabras m√°s populares. El Byte-Pair-Encoding funciona empezando con caracteres y fusionando los que se ven juntos con m√°s frecuencia, creando as√≠ nuevos tokens. A continuaci√≥n, trabaja de forma iterativa para construir nuevos tokens a partir de los pares m√°s frecuentes que ve en un corpus. BPE es capaz de construir palabras que nunca ha visto utilizando m√∫ltiples subpalabras y, por tanto, requiere vocabularios m√°s peque√±os, con menos posibilidades de tener palabras <code>unk</code> (desconocidas).</td>
      </tr>
      <tr>
      <td>WordPiece</td>
      <td>Se trata de un algoritmo de tokenizaci√≥n de subpalabras bastante similar a BPE, utilizado principalmente por Google en modelos como BERT. Utiliza un algoritmo codicioso que intenta construir primero palabras largas, dividi√©ndolas en varios tokens cuando no existen palabras completas en el vocabulario. A diferencia de BPE, que parte de los caracteres y construye tokens lo m√°s grandes posible. Utiliza el famoso prefijo ## para identificar los tokens que forman parte de una palabra (es decir, que no empiezan una palabra).</td>
      </tr>
      <tr>
      <td>Unigram</td>
      <td>Unigram es tambi√©n un algoritmo de tokenizaci√≥n de subpalabras, y funciona tratando de identificar el mejor conjunto de tokens de subpalabras para maximizar la probabilidad de una frase dada. Se diferencia de BPE en que no es un algoritmo determinista basado en un conjunto de reglas aplicadas secuencialmente. En su lugar, Unigram podr√° calcular m√∫ltiples formas de tokenizar, eligiendo la m√°s probable.</td>
      </tr>
      </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>When you create a tokenizer you have to pass it the model</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Unigram</span><span class="p">())</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Let's pass the normalizer and the pre tokenizer we have created to it</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Unigram</span><span class="p">())</span>',
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">custom_normalizer</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Now we have to train the model or load a pre-trained one. In this case we are going to train one with the corpus we have downloaded.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Model-training">Model training<a class="anchor-link" href="#Model-training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To train the model we have several types of <code>Trainer</code>s</p>
      <table>
      <thead>
      <tr>
      <th>Trainer</th>
      <th>Descripci√≥n</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>WordLevelTrainer</td>
      <td>Entrena un tokenizador WordLevel</td>
      </tr>
      <tr>
      <td>BpeTrainer</td>
      <td>Entrena un tokenizador BPE</td>
      </tr>
      <tr>
      <td>WordPieceTrainer</td>
      <td>Entrena un tokenizador WordPiece</td>
      </tr>
      <tr>
      <td>UnigramTrainer</td>
      <td>Entrena un tokenizador Unigram</td>
      </tr>
      </tbody>
      </table>
      <p>Almost all trainers have the same parameters, which are:</p>
      <ul>
      <li>vocab_size: The size of the final vocabulary, including all tokens and the alphabet.</li>
      <li>show_progress: show or not progress bars during training</li>
      <li>special_tokens: A list of special tokens that the model should be aware of.</li>
      </ul>
      <p>Apart from these parameters, each trainer has its own parameters, to see them see the <a href="https://huggingface.co/docs/tokenizers/api/trainers" target="_blank" rel="nofollow noreferrer">Trainers</a> documentation.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To train we have to create a <code>Trainer</code>, as the model we have created is a <code>Unigram</code> we will create a <code>UnigramTrainer</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Unigram</span><span class="p">())</span>',
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">custom_normalizer</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers.trainers</span> <span class="kn">import</span> <span class="n">trainers</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="o">.</span><span class="n">UnigramTrainer</span><span class="p">(</span>',
      '          <span class="n">vocab_size</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>',
      '          <span class="n">initial_alphabet</span><span class="o">=</span><span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">ByteLevel</span><span class="o">.</span><span class="n">alphabet</span><span class="p">(),</span>',
      '          <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">"&lt;PAD&gt;"</span><span class="p">,</span> <span class="s2">"&lt;BOS&gt;"</span><span class="p">,</span> <span class="s2">"&lt;EOS&gt;"</span><span class="p">],</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>












      
      <section class="section-block-markdown-cell">
      <p>Once we have created the <code>Trainer</code> there are two ways to enter, through the <code>train</code> method, to which a list of files is passed, or through the <code>train_from_iterator</code> method to which an iterator is passed.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Model-training-with-the-train-method">Model training with the <code>train</code> method<a class="anchor-link" href="#Model-training-with-the-train-method"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>First we create a list of files with the corpus</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
          '</span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Unigram</span><span class="p">())</span>',
          '</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">custom_normalizer</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
          '</span><span class="kn">from</span> <span class="nn">tokenizers.trainers</span> <span class="kn">import</span> <span class="n">trainers</span>',
          '',
          '<span class="n">trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="o">.</span><span class="n">UnigramTrainer</span><span class="p">(</span>',
          '    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>',
          '    <span class="n">initial_alphabet</span><span class="o">=</span><span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">ByteLevel</span><span class="o">.</span><span class="n">alphabet</span><span class="p">(),</span>',
          '    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">"&lt;PAD&gt;"</span><span class="p">,</span> <span class="s2">"&lt;BOS&gt;"</span><span class="p">,</span> <span class="s2">"&lt;EOS&gt;"</span><span class="p">],</span>',
          '<span class="p">)</span>',
          '</span><span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"wikitext-103/wiki.</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">.tokens"</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"test"</span><span class="p">,</span> <span class="s2">"train"</span><span class="p">,</span> <span class="s2">"valid"</span><span class="p">]]</span>',
          '<span class="n">files</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[\'wikitext-103/wiki.test.tokens\',',
          ' \'wikitext-103/wiki.train.tokens\',',
          ' \'wikitext-103/wiki.valid.tokens\']',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>And now we train the model</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Model-training-with-train_from_iterator-method">Model training with <code>train_from_iterator</code> method<a class="anchor-link" href="#Model-training-with-train_from_iterator-method"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>First we create a function that will return an iterator</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">iterator</span><span class="p">():</span>',
      '          <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>',
      '              <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>',
      '                  <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>',
      '                      <span class="k">yield</span> <span class="n">line</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>We now retrain the model</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">def</span> <span class="nf">iterator</span><span class="p">():</span>',
          '    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>',
          '        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>',
          '            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>',
          '                <span class="k">yield</span> <span class="n">line</span>',
          '</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">iterator</span><span class="p">(),</span> <span class="n">trainer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Training-the-model-with-the-train_from_iterator-method-from-a-Hugging-Face-dataset">Training the model with the <code>train_from_iterator</code> method from a Hugging Face dataset<a class="anchor-link" href="#Training-the-model-with-the-train_from_iterator-method-from-a-Hugging-Face-dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we had downloaded the Hugging Face dataset, we could have trained the model directly from the dataset.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">datasets</span>',
      '      ',
      '      <span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">"wikitext"</span><span class="p">,</span> <span class="s2">"wikitext-103-raw-v1"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"train+test+validation"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Now we can create an iterator</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">datasets</span>',
      '      ',
      '      <span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">"wikitext"</span><span class="p">,</span> <span class="s2">"wikitext-103-raw-v1"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"train+test+validation"</span><span class="p">)</span>',
      '<span></span><span class="k">def</span> <span class="nf">batch_iterator</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>',
      '          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>',
      '              <span class="k">yield</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">][</span><span class="s2">"text"</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>We retrain the model</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">datasets</span>',
          '',
          '<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">"wikitext"</span><span class="p">,</span> <span class="s2">"wikitext-103-raw-v1"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"train+test+validation"</span><span class="p">)</span>',
          '</span><span class="k">def</span> <span class="nf">batch_iterator</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>',
          '    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>',
          '        <span class="k">yield</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">][</span><span class="s2">"text"</span><span class="p">]</span>',
          '</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">batch_iterator</span><span class="p">(),</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Saving-the-model">Saving the model<a class="anchor-link" href="#Saving-the-model"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Once the model has been trained, it can be saved for future use. To save the model you must save it in a <code>json</code> file.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"wikitext-103-tokenizer.json"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <h4 id="Loading-the-pre-trained-model">Loading the pre-trained model<a class="anchor-link" href="#Loading-the-pre-trained-model"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 27" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We can load a pre-trained model from a <code>json</code> instead of having to train it.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"wikitext-103-tokenizer.json"</span><span class="p">)</span>',
          '</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s2">"wikitext-103-tokenizer.json"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;tokenizers.Tokenizer at 0x7f1dd7784a30&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We can also load a pre-trained model available in the Hugging Face Hub.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">\'bert-base-uncased\'</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;tokenizers.Tokenizer at 0x7f1d64a75e30&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Post-processing">Post processing<a class="anchor-link" href="#Post-processing"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 28" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We may want our tokenizer to automatically add special tokens, such as <code>[CLS]</code> or <code>[SEP]</code>.</p>
      <p>The following post processors are implemented in Hugging Face</p>
      <table>
      <thead>
      <tr>
      <th>PostProcesador</th>
      <th>Descripci√≥n</th>
      <th>Ejemplo</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>BertProcessing</td>
      <td>Este post-procesador se encarga de a√±adir los tokens especiales que necesita un modelo Bert (<code>SEP</code> y <code>CLS</code>)</td>
      <td><code>Hello, how are you?</code> se convierte en <code>[CLS]</code>, <code>Hello</code>, <code>,</code>, <code>how</code>, <code>are</code>, <code>you</code>, <code>?</code>, <code>[SEP]</code></td>
      </tr>
      <tr>
      <td>RobertaProcessing</td>
      <td>Este post-procesador se encarga de a√±adir los tokens especiales que necesita un modelo Roberta (<code>SEP</code> y <code>CLS</code>). Tambi√©n se encarga de recortar los offsets. Por defecto, el ByteLevel BPE puede incluir espacios en blanco en los tokens producidos. Si no desea que las compensaciones incluyan estos espacios en blanco, hay que inicializar este PostProcessor con <code>trim_offsets=True</code>.</td>
      <td><code>Hello, how are you?</code> se convierte en <code>&lt;s&gt;</code>, <code>Hello</code>, <code>,</code>, <code>how</code>, <code>are</code>, <code>you</code>, <code>?</code>, <code>&lt;/s&gt;</code></td>
      </tr>
      <tr>
      <td>ElectraProcessing</td>
      <td>A√±ade tokens especiales para ELECTRA</td>
      <td><code>Hello, how are you?</code> se convierte en <code>[CLS]</code>, <code>Hello</code>, <code>,</code>, <code>how</code>, <code>are</code>, <code>you</code>, <code>?</code>, <code>[SEP]</code></td>
      </tr>
      <tr>
      <td>TemplateProcessing</td>
      <td>Permite crear f√°cilmente una plantilla para el postprocesamiento, a√±adiendo tokens especiales y especificando el type_id de cada secuencia/token especial. La plantilla recibe dos cadenas que representan la secuencia √∫nica y el par de secuencias, as√≠ como un conjunto de tokens especiales a utilizar</td>
      <td>Example, when specifying a template with these values: single:<code>[CLS] $A [SEP]</code>, pair: <code>[CLS] $A [SEP] $B [SEP]</code>, special tokens: <code>[CLS]</code>, <code>[SEP]</code>. Input: (<code>I like this</code>, <code>but not this</code>), Output: <code>[CLS] I like this [SEP] but not this [SEP]</code></td>
      </tr>
      </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's create a post tokenizer to see how it works.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers.processors</span> <span class="kn">import</span> <span class="n">TemplateProcessing</span>',
      '      ',
      '      <span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>',
      '          <span class="n">single</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP]"</span><span class="p">,</span>',
      '          <span class="n">pair</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP] $B:1 [SEP]:1"</span><span class="p">,</span>',
      '          <span class="n">special_tokens</span><span class="o">=</span><span class="p">[(</span><span class="s2">"[CLS]"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">"[SEP]"</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>












      
      <section class="section-block-markdown-cell">
      <p>To modify the post tokenizer of a tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers.processors</span> <span class="kn">import</span> <span class="n">TemplateProcessing</span>',
      '      ',
      '      <span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>',
      '          <span class="n">single</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP]"</span><span class="p">,</span>',
      '          <span class="n">pair</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP] $B:1 [SEP]:1"</span><span class="p">,</span>',
      '          <span class="n">special_tokens</span><span class="o">=</span><span class="p">[(</span><span class="s2">"[CLS]"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">"[SEP]"</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">post_processor</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Let's see how it works</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">tokenizers.processors</span> <span class="kn">import</span> <span class="n">TemplateProcessing</span>',
          '',
          '<span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>',
          '    <span class="n">single</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP]"</span><span class="p">,</span>',
          '    <span class="n">pair</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP] $B:1 [SEP]:1"</span><span class="p">,</span>',
          '    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[(</span><span class="s2">"[CLS]"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">"[SEP]"</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span>',
          '<span class="p">)</span>',
          '</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">post_processor</span>',
          '</span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">"I paid $30 for the car"</span>',
          '<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
          '',
          '<span class="n">decoded_text</span><span class="o">.</span><span class="n">tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[\'[CLS]\', \'i\', \'paid\', \'$\', \'3\', \'0\', \'for\', \'the\', \'car\', \'[SEP]\']',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_text1</span> <span class="o">=</span> <span class="s2">"Hello, y\'all!"</span>',
          '<span class="n">input_text2</span> <span class="o">=</span> <span class="s2">"How are you?"</span>',
          '<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">input_text2</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="n">decoded_text</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[\'[CLS]\', \'hell\', \'o\', \',\', \'y\', "\'", \'all\', \'!\', \'[SEP]\', \'how\', \'are\', \'you\', \'?\', \'[SEP]\']',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>If we were to save the tokenizer now, the post tokenizer would be saved with it.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Encoding">Encoding<a class="anchor-link" href="#Encoding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 29" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Once we have the tokenizer trained, we can use it to tokenize texts.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">"I love tokenizers!"</span>',
      '      <span class="n">encoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Let's see what we get when we tokenize text</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">"I love tokenizers!"</span>',
          '<span class="n">encoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
          '</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded_text</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'tokenizers.Encoding',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We obtain an object of type <a href="https://huggingface.co/docs/tokenizers/api/encoding#tokenizers.Encoding" target="_blank" rel="nofollow noreferrer">Encoding</a>, which contains the tokens and token ids</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The <code>ids</code> are the <code>id</code>s of the tokens in the tokenizer vocabulary.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[1, 17, 383, 10694, 17, 3533, 3, 586, 2]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>The <code>tokens</code> are the tokens to which the <code>ids</code> are equivalent.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">encoded_text</span><span class="o">.</span><span class="n">tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[\'[CLS]\', \'i\', \'love\', \'token\', \'i\', \'zer\', \'s\', \'!\', \'[SEP]\']',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>If we have several sequences, we can code them all at the same time.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">input_text2</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">type_ids</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[\'[CLS]\', \'hell\', \'o\', \',\', \'y\', "\'", \'all\', \'!\', \'[SEP]\', \'how\', \'are\', \'you\', \'?\', \'[SEP]\']',
          '[1, 2215, 7, 5, 22, 26, 81, 586, 2, 98, 59, 213, 902, 2]',
          '[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>However, when you have several sequences it is better to use the <code>encode_batch</code> method.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">([</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">input_text2</span><span class="p">])</span>',
          '',
          '<span class="nb">type</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'list',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see that we get a list</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[\'[CLS]\', \'hell\', \'o\', \',\', \'y\', "\'", \'all\', \'!\', \'[SEP]\']',
          '[1, 2215, 7, 5, 22, 26, 81, 586, 2]',
          '[\'[CLS]\', \'how\', \'are\', \'you\', \'?\', \'[SEP]\']',
          '[1, 98, 59, 213, 902, 2]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Decoding">Decoding<a class="anchor-link" href="#Decoding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In addition to encoding input texts, a Tokenizer also has a method to decode, i.e. convert the IDs generated by its model back to a text. This is done by the methods <code>Tokenizer.decode</code> (for a predicted text) and <code>Tokenizer.decode_batch</code> (for a batch of predictions).</p>
      <p>The types of decoding that can be used are:</p>
      <table>
      <thead>
      <tr>
      <th>Decodificaci√≥n</th>
      <th>Descripci√≥n</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>BPEDecoder</td>
      <td>Revierte el modelo BPE</td>
      </tr>
      <tr>
      <td>ByteLevel</td>
      <td>Revierte el ByteLevel PreTokenizer. Este PreTokenizer codifica a nivel de byte, utilizando un conjunto de caracteres Unicode visibles para representar cada byte, por lo que necesitamos un Decoder para revertir este proceso y obtener algo legible de nuevo.</td>
      </tr>
      <tr>
      <td>CTC</td>
      <td>Revierte el modelo CTC</td>
      </tr>
      <tr>
      <td>Metaspace</td>
      <td>Revierte el PreTokenizer de Metaspace. Este PreTokenizer utiliza un identificador especial ‚ñÅ para identificar los espacios en blanco, por lo que este Decoder ayuda con la decodificaci√≥n de estos.</td>
      </tr>
      <tr>
      <td>WordPiece</td>
      <td>Revierte el modelo WordPiece. Este modelo utiliza un identificador especial ## para las subpalabras continuas, por lo que este decodificador ayuda a decodificarlas.</td>
      </tr>
      </tbody>
      </table>
      <p>The decoder will first convert the IDs into tokens (using the tokenizer vocabulary) and remove all special tokens, then join those tokens with blanks.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's create a decoder</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">decoders</span>',
      '      ',
      '      <span class="n">decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ByteLevel</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>We add it to the tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">decoders</span>',
      '      ',
      '      <span class="n">decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ByteLevel</span><span class="p">()</span>',
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>We decode</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">decoders</span>',
          '',
          '<span class="n">decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ByteLevel</span><span class="p">()</span>',
          '</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>',
          '</span><span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
          '',
          '<span class="n">input_text</span><span class="p">,</span> <span class="n">decoded_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(\'I love tokenizers!\', \'ilovetokenizers!\')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">decoded_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode_batch</span><span class="p">([</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="n">encoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">])</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">decoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">input_text2</span><span class="p">,</span> <span class="n">decoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Hello, y\'all! hello,y\'all!',
          'How are you? howareyou?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="BERT-tokenizer">BERT tokenizer<a class="anchor-link" href="#BERT-tokenizer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>With everything we have learned we are going to create the BERT tokenizer from scratch, first we create the tokenizer. Bert uses <code>WordPiece</code> as a model, so we pass it to the initializer of the tokenizer.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>',
      '      <span class="kn">from</span> <span class="nn">tokenizers.models</span> <span class="kn">import</span> <span class="n">WordPiece</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">"[UNK]"</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>BERT preprocesses texts by removing accents and lowercase letters. We also use a unicode normalizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>',
      '      <span class="kn">from</span> <span class="nn">tokenizers.models</span> <span class="kn">import</span> <span class="n">WordPiece</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">"[UNK]"</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">normalizers</span>',
      '      <span class="kn">from</span> <span class="nn">tokenizers.normalizers</span> <span class="kn">import</span> <span class="n">NFD</span><span class="p">,</span> <span class="n">Lowercase</span><span class="p">,</span> <span class="n">StripAccents</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">NFD</span><span class="p">(),</span> <span class="n">Lowercase</span><span class="p">(),</span> <span class="n">StripAccents</span><span class="p">()])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>The pretokenizer only splits whitespace and punctuation marks.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>',
      '      <span class="kn">from</span> <span class="nn">tokenizers.models</span> <span class="kn">import</span> <span class="n">WordPiece</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">"[UNK]"</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">normalizers</span>',
      '      <span class="kn">from</span> <span class="nn">tokenizers.normalizers</span> <span class="kn">import</span> <span class="n">NFD</span><span class="p">,</span> <span class="n">Lowercase</span><span class="p">,</span> <span class="n">StripAccents</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">NFD</span><span class="p">(),</span> <span class="n">Lowercase</span><span class="p">(),</span> <span class="n">StripAccents</span><span class="p">()])</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">Whitespace</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>And the post-processing uses the template that we saw in the previous section</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>',
      '      <span class="kn">from</span> <span class="nn">tokenizers.models</span> <span class="kn">import</span> <span class="n">WordPiece</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">"[UNK]"</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">normalizers</span>',
      '      <span class="kn">from</span> <span class="nn">tokenizers.normalizers</span> <span class="kn">import</span> <span class="n">NFD</span><span class="p">,</span> <span class="n">Lowercase</span><span class="p">,</span> <span class="n">StripAccents</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">NFD</span><span class="p">(),</span> <span class="n">Lowercase</span><span class="p">(),</span> <span class="n">StripAccents</span><span class="p">()])</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">Whitespace</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers.processors</span> <span class="kn">import</span> <span class="n">TemplateProcessing</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>',
      '          <span class="n">single</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP]"</span><span class="p">,</span>',
      '          <span class="n">pair</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP] $B:1 [SEP]:1"</span><span class="p">,</span>',
      '          <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span>',
      '              <span class="p">(</span><span class="s2">"[CLS]"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>',
      '              <span class="p">(</span><span class="s2">"[SEP]"</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>',
      '          <span class="p">],</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <p>We train the tokenizer with the dataset of wikitext-103</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>',
      '      <span class="kn">from</span> <span class="nn">tokenizers.models</span> <span class="kn">import</span> <span class="n">WordPiece</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">"[UNK]"</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">normalizers</span>',
      '      <span class="kn">from</span> <span class="nn">tokenizers.normalizers</span> <span class="kn">import</span> <span class="n">NFD</span><span class="p">,</span> <span class="n">Lowercase</span><span class="p">,</span> <span class="n">StripAccents</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">NFD</span><span class="p">(),</span> <span class="n">Lowercase</span><span class="p">(),</span> <span class="n">StripAccents</span><span class="p">()])</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">Whitespace</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers.processors</span> <span class="kn">import</span> <span class="n">TemplateProcessing</span>',
      '      ',
      '      <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>',
      '          <span class="n">single</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP]"</span><span class="p">,</span>',
      '          <span class="n">pair</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP] $B:1 [SEP]:1"</span><span class="p">,</span>',
      '          <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span>',
      '              <span class="p">(</span><span class="s2">"[CLS]"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>',
      '              <span class="p">(</span><span class="s2">"[SEP]"</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>',
      '          <span class="p">],</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">tokenizers.trainers</span> <span class="kn">import</span> <span class="n">WordPieceTrainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">WordPieceTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">30522</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">"[UNK]"</span><span class="p">,</span> <span class="s2">"[CLS]"</span><span class="p">,</span> <span class="s2">"[SEP]"</span><span class="p">,</span> <span class="s2">"[PAD]"</span><span class="p">,</span> <span class="s2">"[MASK]"</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>',
          '<span class="kn">from</span> <span class="nn">tokenizers.models</span> <span class="kn">import</span> <span class="n">WordPiece</span>',
          '',
          '<span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">"[UNK]"</span><span class="p">))</span>',
          '</span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">normalizers</span>',
          '<span class="kn">from</span> <span class="nn">tokenizers.normalizers</span> <span class="kn">import</span> <span class="n">NFD</span><span class="p">,</span> <span class="n">Lowercase</span><span class="p">,</span> <span class="n">StripAccents</span>',
          '',
          '<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">NFD</span><span class="p">(),</span> <span class="n">Lowercase</span><span class="p">(),</span> <span class="n">StripAccents</span><span class="p">()])</span>',
          '</span><span class="kn">from</span> <span class="nn">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">Whitespace</span>',
          '',
          '<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">tokenizers.processors</span> <span class="kn">import</span> <span class="n">TemplateProcessing</span>',
          '',
          '<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>',
          '    <span class="n">single</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP]"</span><span class="p">,</span>',
          '    <span class="n">pair</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP] $B:1 [SEP]:1"</span><span class="p">,</span>',
          '    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span>',
          '        <span class="p">(</span><span class="s2">"[CLS]"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>',
          '        <span class="p">(</span><span class="s2">"[SEP]"</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>',
          '    <span class="p">],</span>',
          '<span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">tokenizers.trainers</span> <span class="kn">import</span> <span class="n">WordPieceTrainer</span>',
          '',
          '<span class="n">trainer</span> <span class="o">=</span> <span class="n">WordPieceTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">30522</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">"[UNK]"</span><span class="p">,</span> <span class="s2">"[CLS]"</span><span class="p">,</span> <span class="s2">"[SEP]"</span><span class="p">,</span> <span class="s2">"[PAD]"</span><span class="p">,</span> <span class="s2">"[MASK]"</span><span class="p">])</span>',
          '</span><span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"wikitext-103/wiki.</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">.tokens"</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"test"</span><span class="p">,</span> <span class="s2">"train"</span><span class="p">,</span> <span class="s2">"valid"</span><span class="p">]]</span>',
          '<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now we test it</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">"I love tokenizers!"</span>',
          '',
          '<span class="n">encoded_text</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
          '<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"El texto de entrada \'</span><span class="si">{</span><span class="n">input_text</span><span class="si">}</span><span class="s2">\' se convierte en los tokens </span><span class="si">{</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, que tienen las ids </span><span class="si">{</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span><span class="si">}</span><span class="s2"> y luego se decodifica como \'</span><span class="si">{</span><span class="n">decoded_text</span><span class="si">}</span><span class="s2">\'"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'El texto de entrada \'I love tokenizers!\' se convierte en los tokens [\'[CLS]\', \'i\', \'love\', \'token\', \'##izers\', \'!\', \'[SEP]\'], que tienen las ids [1, 51, 2867, 25791, 12213, 5, 2] y luego se decodifica como \'i love token ##izers !\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      






    </div>

  </section>

</PostLayout>
