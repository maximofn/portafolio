---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Transformers';
const end_url = 'hugging-face-transformers';
const description = 'ü§ñ Transform your world with Hugging Face Transformers! üöÄ Ready to make magic with natural language? From super-fast techniques with pipeline üå™Ô∏è to ninja tricks with AutoModel ü•∑, this post takes you by the hand on an epic adventure into the NLP universe. Explore how to generate text that surprises, train models that dazzle, and share your creations on the Hugging Face Hub like a pro. Get ready to code and laugh, because the future of NLP is now and it\'s hilarious! üòÇ';
const keywords = 'Hugging Face, Transformers, NLP, Natural Language Processing, AutoModel, pipeline, fine-tuning, training, sharing, Hub';
const languaje = 'EN';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/HuggingFace%20Transformers.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1147
    image_height=644
    image_extension=webp
    article_date=2024-04-15+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Installation"><h2>Installation</h2></a>
      <a class="anchor-link" href="#Inference-with-%60pipeline"><h2>Inference with `pipeline</h2></a>
      <a class="anchor-link" href="#Tasks"><h3>Tasks</h3></a>
      <a class="anchor-link" href="#Use-of-pipeline"><h3>Use of <code>pipeline</code></h3></a>
      <a class="anchor-link" href="#How-pipeline-works"><h3>How <code>pipeline</code> works</h3></a>
      <a class="anchor-link" href="#Inference-with-AutoClass-and-pipeline."><h2>Inference with <code>AutoClass</code> and <code>pipeline</code>.</h2></a>
      <a class="anchor-link" href="#Tokenization-with-AutoTokenizer."><h3>Tokenization with <code>AutoTokenizer</code>.</h3></a>
      <a class="anchor-link" href="#AutoModel-Model"><h3><code>AutoModel</code> Model</h3></a>
      <a class="anchor-link" href="#AutoModelFor-Model"><h3><code>AutoModelFor</code> Model</h3></a>
      <a class="anchor-link" href="#Inference-with-AutoClass-only"><h2>Inference with <code>AutoClass</code> only</h2></a>
      <a class="anchor-link" href="#Generation-of-casual-text"><h3>Generation of casual text</h3></a>
      <a class="anchor-link" href="#Text-classification"><h3>Text classification</h3></a>
      <a class="anchor-link" href="#Classification-of-tokens"><h3>Classification of tokens</h3></a>
      <a class="anchor-link" href="#Question-answering"><h3>Question answering</h3></a>
      <a class="anchor-link" href="#Masked-language-modeling-(Masked-language-modeling)"><h3>Masked language modeling (Masked language modeling)</h3></a>
      <a class="anchor-link" href="#Model-customization"><h2>Model customization</h2></a>
      <a class="anchor-link" href="#Tokenization"><h2>Tokenization</h2></a>
      <a class="anchor-link" href="#Padding"><h3>Padding</h3></a>
      <a class="anchor-link" href="#Truncated"><h3>Truncated</h3></a>
      <a class="anchor-link" href="#Tensors"><h3>Tensors</h3></a>
      <a class="anchor-link" href="#Masks"><h3>Masks</h3></a>
      <a class="anchor-link" href="#Fast-Tokenizers"><h2>Fast Tokenizers</h2></a>
      <a class="anchor-link" href="#Text-generation-forms"><h2>Text generation forms</h2></a>
      <a class="anchor-link" href="#Greedy-Search"><h3>Greedy Search</h3></a>
      <a class="anchor-link" href="#Contrastive-Search"><h3>Contrastive Search</h3></a>
      <a class="anchor-link" href="#Multinomial-sampling"><h3>Multinomial sampling</h3></a>
      <a class="anchor-link" href="#Beam-search"><h3>Beam search</h3></a>
      <a class="anchor-link" href="#Beam-search-multinomial-sampling"><h3>Beam search multinomial sampling</h3></a>
      <a class="anchor-link" href="#Beam-search-n-grams-penalty"><h3>Beam search n-grams penalty</h3></a>
      <a class="anchor-link" href="#Beam-search-n-grams-penalty-return-sequences"><h3>Beam search n-grams penalty return sequences</h3></a>
      <a class="anchor-link" href="#Diverse-beam-search-decoding"><h3>Diverse beam search decoding</h3></a>
      <a class="anchor-link" href="#Speculative-Decoding"><h3>Speculative Decoding</h3></a>
      <a class="anchor-link" href="#Speculative-Decoding-randomness-control"><h3>Speculative Decoding randomness control</h3></a>
      <a class="anchor-link" href="#Sampling"><h3>Sampling</h3></a>
      <a class="anchor-link" href="#Sampling-temperature"><h3>Sampling temperature</h3></a>
      <a class="anchor-link" href="#Sampling-top-k"><h3>Sampling top-k</h3></a>
      <a class="anchor-link" href="#Sampling-top-p-(nucleus-sampling)"><h3>Sampling top-p (nucleus sampling)</h3></a>
      <a class="anchor-link" href="#Sampling-top-k-and-top-p"><h3>Sampling top-k and top-p</h3></a>
      <a class="anchor-link" href="#Effect-of-temperature-top-k-and-top-p"><h3>Effect of temperature, <code>top-k</code> y <code>top-p</code></h3></a>
      <a class="anchor-link" href="#Streaming"><h2>Streaming</h2></a>
      <a class="anchor-link" href="#Chat-templates"><h2>Chat templates</h2></a>
      <a class="anchor-link" href="#Context-tokenization"><h3>Context tokenization</h3></a>
      <a class="anchor-link" href="#Add-prompts-generation"><h3>Add prompts generation</h3></a>
      <a class="anchor-link" href="#Text-generation"><h3>Text generation</h3></a>
      <a class="anchor-link" href="#Text-generation-with-pipeline."><h3>Text generation with <code>pipeline</code>.</h3></a>
      <a class="anchor-link" href="#Train"><h2>Train</h2></a>
      <a class="anchor-link" href="#Dataset"><h3>Dataset</h3></a>
      <a class="anchor-link" href="#Tokenization"><h3>Tokenization</h3></a>
      <a class="anchor-link" href="#Model"><h3>Model</h3></a>
      <a class="anchor-link" href="#Evaluation-metrics"><h3>Evaluation metrics</h3></a>
      <a class="anchor-link" href="#Trainer"><h3>Trainer</h3></a>
      <a class="anchor-link" href="#Testing-the-model"><h3>Testing the model</h3></a>
      <a class="anchor-link" href="#Share-the-model-in-the-Hugging-Face-Hub"><h2>Share the model in the Hugging Face Hub</h2></a>
      <a class="anchor-link" href="#Logging"><h3>Logging</h3></a>
      <a class="anchor-link" href="#Up-once-trained"><h3>Up once trained</h3></a>
      <a class="anchor-link" href="#Climbing-while-training"><h3>Climbing while training</h3></a>
      <a class="anchor-link" href="#Hub-as-git-repository"><h2>Hub as git repository</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Hugging-Face-transformers">Hugging Face transformers<a class="anchor-link" href="#Hugging-Face-transformers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 57" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The <code>transformers</code> library from Hugging Face is one of the most popular libraries for working with language models. Its ease of use democratized the use of the <code>Transformer</code> architecture and made it possible to work with state-of-the-art language models without having to have a great deal of knowledge in the area.</p>
      <p>Between the <code>transformers</code> library, the model hub and its ease of use, the spaces and the ease of deploying demos, and new libraries like <code>datasets</code>, <code>accelerate</code>, <code>PEFT</code> and more, they have made Hugging Face one of the most important players in the AI scene at the moment. They call themselves "the GitHub of AI" and they certainly are.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
      <h2 id="Installation">Installation<a class="anchor-link" href="#Installation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 58" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To install transformers can be done with <code>pip</code>.</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>transformers
      <span class="sb">```</span>
      
      or<span class="w"> </span>with<span class="w"> </span><span class="sb">`</span>conda<span class="sb">`</span>.
      
      <span class="sb">````</span>bash
      conda<span class="w"> </span>install<span class="w"> </span>conda-forge::transformers
      <span class="sb">```</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In addition to the library you need to have a PyTorch or TensorFlow backend installed. That is, you need to have <code>torch</code> or <code>tensorflow</code> installed to be able to use <code>transformers</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Inference-with-%60pipeline">Inference with `pipeline<a class="anchor-link" href="#Inference-with-%60pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 59" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>With the <code>transformers</code> pipeline`s you can do inference with language models in a very simple way. This has the advantage that development is done much faster and prototyping can be done very easily. It also allows people who do not have much knowledge to be able to use the models.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>With <code>pipeline</code> you can do inference in a lot of different tasks. Each task has its own <code>pipeline</code> (NLP <code>pipeline</code>, vision <code>pipeline</code>, etc), but a general abstraction can be made using the <code>pipeline</code> class which takes care of selecting the appropriate <code>pipeline</code> for the task passed to it.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tasks">Tasks<a class="anchor-link" href="#Tasks"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 60" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As of this writing, the tasks that can be done with <code>pipeline</code> are:</p>
      <ul>
      <li><p>Audio:</p>
      <ul>
      <li>Audio classification<ul>
      <li>acoustic scene classification: label audio with a scene label ("office", "beach", "stadium")</li>
      <li>acoustic event detection: tag audio with a sound event tag ("car horn", "whale call", "glass breaking")</li>
      <li>labeling: labeling audio containing various sounds (birdsong, speaker identification in a meeting)</li>
      <li>music classification: labeling music with a genre label ("metal", "hip-hop", "country")</li>
      </ul>
      </li>
      </ul>
      </li>
      <li><p>Automatic speech recognition (ASR, audio speech recognition):</p>
      </li>
      <li><p>Computer vision</p>
      <ul>
      <li>Image classification</li>
      <li>Object detection</li>
      <li>Image segmentation</li>
      <li>Depth estimation</li>
      </ul>
      </li>
      </ul>
      <p>Natural language processing (NLP) * Natural language processing (NLP)</p>
      <ul>
      <li><p>Text classification</p>
      <ul>
      <li>sentiment analysis</li>
      <li>content classification</li>
      </ul>
      </li>
      <li><p>Classification of tokens</p>
      <ul>
      <li>Named Entity Recognition (NER): tag a token according to an entity category such as organization, person, location or date.</li>
      <li>part-of-speech (POS) tagging: tagging a token according to its part of speech, such as noun, verb or adjective. POS is useful to help translation systems understand how two identical words are grammatically different (e.g., "cut" as a noun versus "cut" as a verb).</li>
      </ul>
      </li>
      <li><p>Answers to questions</p>
      <ul>
      <li>extractive: given a question and some context, the answer is a fragment of text from the context that the model must extract.</li>
      <li>abstract: given a question and some context, the answer is generated from the context; this approach is handled by the Text2TextGenerationPipeline instead of the QuestionAnsweringPipeline shown below.</li>
      </ul>
      </li>
      <li><p>Summarize</p>
      <ul>
      <li>extractive: identifies and extracts the most important sentences from the original text</li>
      <li>abstracting: generates the objective summary (which may include new words not present in the input document) from the original text</li>
      </ul>
      </li>
      <li><p>Translation</p>
      </li>
      <li><p>Language modeling</p>
      <ul>
      <li>causal: the objective of the model is to predict the next token in a sequence, and future tokens are masked.</li>
      <li>masked: the objective of the model is to predict a masked token in a sequence with full access to the tokens in the sequence.</li>
      </ul>
      </li>
      <li><p>Multimodal</p>
      <ul>
      <li>Answers to document questions</li>
      </ul>
      </li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Use-of-pipeline">Use of <code>pipeline</code><a class="anchor-link" href="#Use-of-pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 61" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The easiest way to create a <code>pipeline</code> is simply to tell it the task we want it to solve using the <code>task</code> parameter. And the library will take care of selecting the best model for that task, download it and save it in the cache for future use.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).',
          'Using a pipeline without specifying a model name and revision in production is not recommended.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[2]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>[{opening_brace}'generated_text': 'Me encanta aprender de se r√©sistance davant que hiens que pr√©clase que ses encasas qu√©c√©nces. Se pr√©sentants cet en un croyne et cela d√©sirez'{closing_brace}]</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As you can see the generated text is in French, while I have introduced it in Spanish, so it is important to choose well the model. If you notice the library has taken the <code>openai-community/gpt2</code> model, which is a model trained mostly in English, and that when I put Spanish text in it, it got confused and generated a response in French.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We are going to use a model retrained in Spanish using the <code>model</code> parameter.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[2]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>[{opening_brace}'generated_text': 'Me encanta aprender de tus palabras, que con gran entusiasmo y con el mismo conocimiento como lo que t√∫ acabas escribiendo, te deseo de todo coraz√≥n todo el deseo de este d√≠a:\nY aunque tambi√©n haya personas a las que'{closing_brace}]</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now the generated text looks much better</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The <code>pipeline</code> class has many possible parameters, so to see all of them and learn more about the class I recommend you to read its <a href="https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/pipelines" target="_blank" rel="nofollow noreferrer">documentation</a>, but let's talk about one, because for deep learning it is very important and it is <code>device</code>. It defines the device (e.g. <code>cpu</code>, <code>cuda:1</code>, <code>mps</code> or an ordinal range of GPUs like <code>1</code>) on which the <code>pipeline</code> will be assigned.</p>
      <p>In my case, as I have a GPU I set <code>0</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
      
      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">generation</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">generation</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'generated_text'</span><span class="p">])</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de ustedes, a tal punto que he decidido escribir algunos de nuestros contenidos en este blog, el cual ha sido de gran utilidad para m√≠ por varias razones, una de ellas, el trabajo
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="How-pipeline-works">How <code>pipeline</code> works<a class="anchor-link" href="#How-pipeline-works"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 62" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>When we make use of <code>pipeline</code> below what is happening is this</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformers-pipeline" src="http://maximofn.com/wp-content/uploads/2024/02/transformers-pipeline.svg"/></p>
      <p>Text is automatically tokenized, passed through the model and then post-processed.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Inference-with-AutoClass-and-pipeline.">Inference with <code>AutoClass</code> and <code>pipeline</code>.<a class="anchor-link" href="#Inference-with-AutoClass-and-pipeline."><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 63" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We have seen that <code>pipeline</code> abstracts a lot of what happens, but we can select which tokenizer, which model and which postprocessing we want to use.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenization-with-AutoTokenizer.">Tokenization with <code>AutoTokenizer</code>.<a class="anchor-link" href="#Tokenization-with-AutoTokenizer."><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 64" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Before we used the <code>flax-community/gpt-2-spanish</code> model to generate text, we can use its tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '</span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">generation</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">generation</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">])</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '',
          '<span class="n">text</span> <span class="o">=</span> <span class="s2">"Me encanta lo que estoy aprendiendo"</span>',
          '',
          '<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '{\'input_ids\': tensor([[ 2879,  4835,   382,   288,  2383, 15257]]), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1]])}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="AutoModel-Model"><code>AutoModel</code> Model<a class="anchor-link" href="#AutoModel-Model"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 65" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now we can create the model and pass the tokens to it.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '',
          '<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)</span>',
          '<span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions,',
          ' odict_keys([\'last_hidden_state\', \'past_key_values\']))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>If we now try to use it in a <code>pipeline</code> we will get an error.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
      
      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>The model 'GPT2Model' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-text-output-error">
      <pre>
      <span class="ansi-red-fg">---------------------------------------------------------------------------</span>
      <span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
      Cell <span class="ansi-green-fg">In[23], line 3</span>
      <span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">transformers</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> pipeline
      <span class="ansi-green-fg">----&gt; 3</span> pipeline(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">text-generation</span><span style="color: rgb(175,0,0)">"</span>, model<span style="color: rgb(98,98,98)">=</span>model, tokenizer<span style="color: rgb(98,98,98)">=</span>tokenizer)(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Me encanta aprender de</span><span style="color: rgb(175,0,0)">"</span>)
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:241</span>, in <span class="ansi-cyan-fg">TextGenerationPipeline.__call__</span><span class="ansi-blue-fg">(self, text_inputs, **kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">    239</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">super</span>()<span style="color: rgb(98,98,98)">.</span><span style="color: rgb(0,0,255)">__call__</span>(chats, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs)
      <span class="ansi-green-intense-fg ansi-bold">    240</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
      <span class="ansi-green-fg">--&gt; 241</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">super</span>()<span style="color: rgb(98,98,98)">.</span><span style="color: rgb(0,0,255)">__call__</span>(text_inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs)
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1196</span>, in <span class="ansi-cyan-fg">Pipeline.__call__</span><span class="ansi-blue-fg">(self, inputs, num_workers, batch_size, *args, **kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1188</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">next</span>(
      <span class="ansi-green-intense-fg ansi-bold">   1189</span>         <span style="color: rgb(0,135,0)">iter</span>(
      <span class="ansi-green-intense-fg ansi-bold">   1190</span>             <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>get_iterator(
      <span class="ansi-green-fg">   (...)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1193</span>         )
      <span class="ansi-green-intense-fg ansi-bold">   1194</span>     )
      <span class="ansi-green-intense-fg ansi-bold">   1195</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
      <span class="ansi-green-fg">-&gt; 1196</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>run_single(inputs, preprocess_params, forward_params, postprocess_params)
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1203</span>, in <span class="ansi-cyan-fg">Pipeline.run_single</span><span class="ansi-blue-fg">(self, inputs, preprocess_params, forward_params, postprocess_params)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1201</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">run_single</span>(<span style="color: rgb(0,135,0)">self</span>, inputs, preprocess_params, forward_params, postprocess_params):
      <span class="ansi-green-intense-fg ansi-bold">   1202</span>     model_inputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>preprocess(inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>preprocess_params)
      <span class="ansi-green-fg">-&gt; 1203</span>     model_outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>forward(model_inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>forward_params)
      <span class="ansi-green-intense-fg ansi-bold">   1204</span>     outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>postprocess(model_outputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>postprocess_params)
      <span class="ansi-green-intense-fg ansi-bold">   1205</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> outputs
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1102</span>, in <span class="ansi-cyan-fg">Pipeline.forward</span><span class="ansi-blue-fg">(self, model_inputs, **forward_params)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1100</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> inference_context():
      <span class="ansi-green-intense-fg ansi-bold">   1101</span>         model_inputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_ensure_tensor_on_device(model_inputs, device<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>device)
      <span class="ansi-green-fg">-&gt; 1102</span>         model_outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_forward(model_inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>forward_params)
      <span class="ansi-green-intense-fg ansi-bold">   1103</span>         model_outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_ensure_tensor_on_device(model_outputs, device<span style="color: rgb(98,98,98)">=</span>torch<span style="color: rgb(98,98,98)">.</span>device(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">cpu</span><span style="color: rgb(175,0,0)">"</span>))
      <span class="ansi-green-intense-fg ansi-bold">   1104</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:328</span>, in <span class="ansi-cyan-fg">TextGenerationPipeline._forward</span><span class="ansi-blue-fg">(self, model_inputs, **generate_kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">    325</span>         generate_kwargs[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">min_length</span><span style="color: rgb(175,0,0)">"</span>] <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> prefix_length
      <span class="ansi-green-intense-fg ansi-bold">    327</span> <span style="color: rgb(95,135,135)"># BS x SL</span>
      <span class="ansi-green-fg">--&gt; 328</span> generated_sequence <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>model<span style="color: rgb(98,98,98)">.</span>generate(input_ids<span style="color: rgb(98,98,98)">=</span>input_ids, attention_mask<span style="color: rgb(98,98,98)">=</span>attention_mask, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>generate_kwargs)
      <span class="ansi-green-intense-fg ansi-bold">    329</span> out_b <span style="color: rgb(98,98,98)">=</span> generated_sequence<span style="color: rgb(98,98,98)">.</span>shape[<span style="color: rgb(98,98,98)">0</span>]
      <span class="ansi-green-intense-fg ansi-bold">    330</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>framework <span style="color: rgb(98,98,98)">==</span> <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">pt</span><span style="color: rgb(175,0,0)">"</span>:
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/utils/_contextlib.py:115</span>, in <span class="ansi-cyan-fg">context_decorator.&lt;locals&gt;.decorate_context</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">    112</span> <span style="color: rgb(175,0,255)">@functools</span><span style="color: rgb(98,98,98)">.</span>wraps(func)
      <span class="ansi-green-intense-fg ansi-bold">    113</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">decorate_context</span>(<span style="color: rgb(98,98,98)">*</span>args, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs):
      <span class="ansi-green-intense-fg ansi-bold">    114</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> ctx_factory():
      <span class="ansi-green-fg">--&gt; 115</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> func(<span style="color: rgb(98,98,98)">*</span>args, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs)
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1323</span>, in <span class="ansi-cyan-fg">GenerationMixin.generate</span><span class="ansi-blue-fg">(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1320</span>         synced_gpus <span style="color: rgb(98,98,98)">=</span> <span class="ansi-bold" style="color: rgb(0,135,0)">False</span>
      <span class="ansi-green-intense-fg ansi-bold">   1322</span> <span style="color: rgb(95,135,135)"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call</span>
      <span class="ansi-green-fg">-&gt; 1323</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_validate_model_class()
      <span class="ansi-green-intense-fg ansi-bold">   1325</span> <span style="color: rgb(95,135,135)"># priority: `generation_config` argument &gt; `model.generation_config` (the default generation config)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1326</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> generation_config <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:
      <span class="ansi-green-intense-fg ansi-bold">   1327</span>     <span style="color: rgb(95,135,135)"># legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,</span>
      <span class="ansi-green-intense-fg ansi-bold">   1328</span>     <span style="color: rgb(95,135,135)"># three conditions must be met</span>
      <span class="ansi-green-intense-fg ansi-bold">   1329</span>     <span style="color: rgb(95,135,135)"># 1) the generation config must have been created from the model config (`_from_model_config` field);</span>
      <span class="ansi-green-intense-fg ansi-bold">   1330</span>     <span style="color: rgb(95,135,135)"># 2) the generation config must have seen no modification since its creation (the hash is the same);</span>
      <span class="ansi-green-intense-fg ansi-bold">   1331</span>     <span style="color: rgb(95,135,135)"># 3) the user must have set generation parameters in the model config.</span>
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1110</span>, in <span class="ansi-cyan-fg">GenerationMixin._validate_model_class</span><span class="ansi-blue-fg">(self)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1108</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> generate_compatible_classes:
      <span class="ansi-green-intense-fg ansi-bold">   1109</span>     exception_message <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)"> Please use one of the following classes instead: </span><span class="ansi-bold" style="color: rgb(175,95,135)">{opening_brace}</span>generate_compatible_classes<span class="ansi-bold" style="color: rgb(175,95,135)">{closing_brace}</span><span style="color: rgb(175,0,0)">"</span>
      <span class="ansi-green-fg">-&gt; 1110</span> <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">TypeError</span>(exception_message)
      
      <span class="ansi-red-fg">TypeError</span>: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {opening_brace}'GPT2LMHeadModel'{closing_brace}</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This is because when it worked we used</p>
      <div class="highlight"><pre><span></span><span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="err">```</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>But now we have made</p>
      <div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
      <span class="err">```</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In the first case we just used <code>pipeline</code> and the model name, underneath we were looking for the best way to implement the model and the tokenizer. But in the second case we have created the tokenizer and the model and passed it to <code>pipeline</code>, but we have not created them well for what the <code>pipeline</code> needs.</p>
      <p>To fix this we use <code>AutoModelFor</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="AutoModelFor-Model"><code>AutoModelFor</code> Model<a class="anchor-link" href="#AutoModelFor-Model"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 66" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The transformers library gives us the opportunity to create a model for a given task such as</p>
      <ul>
      <li><code>AutoModelForCausalLM</code> used to continue texts</li>
      <li><code>AutoModelForMaskedLM</code> used for gap filling</li>
      <li><code>AutoModelForMaskGeneration</code> which is used to generate masks.</li>
      <li>AutoModelForSeq2SeqLM, which is used to convert from sequences to sequences, as for example in translation.</li>
      <li><code>AutoModelForSequenceClassification</code> for text classification</li>
      <li><code>AutoModelForMultipleChoice</code> for multiple choice</li>
      <li><code>AutoModelForNextSentencePrediction</code> to predict whether two sentences are consecutive.</li>
      <li><code>AutoModelForTokenClassification</code> for token classification</li>
      <li><code>AutoModelForQuestionAnswering</code> for questions and answers</li>
      <li><code>AutoModelForTextEncoding</code> for text encoding</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's use the above model to generate text</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      
      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'generated_text'</span><span class="p">]</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[3]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>'Me encanta aprender de mi familia.\nLa verdad no sab√≠a que se necesitaba tanto en este peque√±o restaurante ya que mi novio en un principio hab√≠a ido, pero hoy me ha entrado un gusanillo entre pecho y espalda que'</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now it works, because we have created the model in a way that <code>pipeline</code> can understand.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Inference-with-AutoClass-only">Inference with <code>AutoClass</code> only<a class="anchor-link" href="#Inference-with-AutoClass-only"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 67" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Earlier we created the model and tokenizer and gave it to <code>pipeline</code> to do the necessary underneath, but we can use the methods for inference ourselves.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Generation-of-casual-text">Generation of casual text<a class="anchor-link" href="#Generation-of-casual-text"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 68" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the model and the tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">]</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>With <code>device_map</code>, we have loaded the model on GPU 0</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now we have to do what <code>pipeline</code> used to do.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>First we generate the tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '',
          '<span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">]</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '</span><span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'The model \'GPT2Model\' is not supported for text-generation. Supported models are [\'BartForCausalLM\', \'BertLMHeadModel\', \'BertGenerationDecoder\', \'BigBirdForCausalLM\', \'BigBirdPegasusForCausalLM\', \'BioGptForCausalLM\', \'BlenderbotForCausalLM\', \'BlenderbotSmallForCausalLM\', \'BloomForCausalLM\', \'CamembertForCausalLM\', \'LlamaForCausalLM\', \'CodeGenForCausalLM\', \'CpmAntForCausalLM\', \'CTRLLMHeadModel\', \'Data2VecTextForCausalLM\', \'ElectraForCausalLM\', \'ErnieForCausalLM\', \'FalconForCausalLM\', \'FuyuForCausalLM\', \'GemmaForCausalLM\', \'GitForCausalLM\', \'GPT2LMHeadModel\', \'GPT2LMHeadModel\', \'GPTBigCodeForCausalLM\', \'GPTNeoForCausalLM\', \'GPTNeoXForCausalLM\', \'GPTNeoXJapaneseForCausalLM\', \'GPTJForCausalLM\', \'LlamaForCausalLM\', \'MarianForCausalLM\', \'MBartForCausalLM\', \'MegaForCausalLM\', \'MegatronBertForCausalLM\', \'MistralForCausalLM\', \'MixtralForCausalLM\', \'MptForCausalLM\', \'MusicgenForCausalLM\', \'MvpForCausalLM\', \'OpenLlamaForCausalLM\', \'OpenAIGPTLMHeadModel\', \'OPTForCausalLM\', \'PegasusForCausalLM\', \'PersimmonForCausalLM\', \'PhiForCausalLM\', \'PLBartForCausalLM\', \'ProphetNetForCausalLM\', \'QDQBertLMHeadModel\', \'Qwen2ForCausalLM\', \'ReformerModelWithLMHead\', \'RemBertForCausalLM\', \'RobertaForCausalLM\', \'RobertaPreLayerNormForCausalLM\', \'RoCBertForCausalLM\', \'RoFormerForCausalLM\', \'RwkvForCausalLM\', \'Speech2Text2ForCausalLM\', \'StableLmForCausalLM\', \'TransfoXLLMHeadModel\', \'TrOCRForCausalLM\', \'WhisperForCausalLM\', \'XGLMForCausalLM\', \'XLMWithLMHeadModel\', \'XLMProphetNetForCausalLM\', \'XLMRobertaForCausalLM\', \'XLMRobertaXLForCausalLM\', \'XLNetLMHeadModel\', \'XmodForCausalLM\'].',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '<span class="ansi-red-fg">---------------------------------------------------------------------------</span>',
          '<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)',
          'Cell <span class="ansi-green-fg">In[2], line 1</span>',
          '<span class="ansi-green-fg">----&gt; 1</span> tokens_input <span style="color: rgb(98,98,98)">=</span> tokenizer([<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Me encanta aprender de</span><span style="color: rgb(175,0,0)">"</span>], return_tensors<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">pt</span><span style="color: rgb(175,0,0)">"</span>, padding<span style="color: rgb(98,98,98)">=</span><span class="ansi-bold" style="color: rgb(0,135,0)">True</span>)<span style="color: rgb(98,98,98)">.</span>to(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">cuda</span><span style="color: rgb(175,0,0)">"</span>)',
          'File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2829</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.__call__</span><span class="ansi-blue-fg">(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2827</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_in_target_context_manager:',
          '<span class="ansi-green-intense-fg ansi-bold">   2828</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_input_mode()',
          '<span class="ansi-green-fg">-&gt; 2829</span>     encodings <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_call_one(text<span style="color: rgb(98,98,98)">=</span>text, text_pair<span style="color: rgb(98,98,98)">=</span>text_pair, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>all_kwargs)',
          '<span class="ansi-green-intense-fg ansi-bold">   2830</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> text_target <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:',
          '<span class="ansi-green-intense-fg ansi-bold">   2831</span>     <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_target_mode()',
          'File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2915</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._call_one</span><span class="ansi-blue-fg">(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2910</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(',
          '<span class="ansi-green-intense-fg ansi-bold">   2911</span>             <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">batch length of `text`: </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">len</span>(text)<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)"> does not match batch length of `text_pair`:</span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2912</span>             <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)"> </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">len</span>(text_pair)<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">.</span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2913</span>         )',
          '<span class="ansi-green-intense-fg ansi-bold">   2914</span>     batch_text_or_text_pairs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">list</span>(<span style="color: rgb(0,135,0)">zip</span>(text, text_pair)) <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> text_pair <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span> text',
          '<span class="ansi-green-fg">-&gt; 2915</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>batch_encode_plus(',
          '<span class="ansi-green-intense-fg ansi-bold">   2916</span>         batch_text_or_text_pairs<span style="color: rgb(98,98,98)">=</span>batch_text_or_text_pairs,',
          '<span class="ansi-green-intense-fg ansi-bold">   2917</span>         add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,',
          '<span class="ansi-green-intense-fg ansi-bold">   2918</span>         padding<span style="color: rgb(98,98,98)">=</span>padding,',
          '<span class="ansi-green-intense-fg ansi-bold">   2919</span>         truncation<span style="color: rgb(98,98,98)">=</span>truncation,',
          '<span class="ansi-green-intense-fg ansi-bold">   2920</span>         max_length<span style="color: rgb(98,98,98)">=</span>max_length,',
          '<span class="ansi-green-intense-fg ansi-bold">   2921</span>         stride<span style="color: rgb(98,98,98)">=</span>stride,',
          '<span class="ansi-green-intense-fg ansi-bold">   2922</span>         is_split_into_words<span style="color: rgb(98,98,98)">=</span>is_split_into_words,',
          '<span class="ansi-green-intense-fg ansi-bold">   2923</span>         pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,',
          '<span class="ansi-green-intense-fg ansi-bold">   2924</span>         return_tensors<span style="color: rgb(98,98,98)">=</span>return_tensors,',
          '<span class="ansi-green-intense-fg ansi-bold">   2925</span>         return_token_type_ids<span style="color: rgb(98,98,98)">=</span>return_token_type_ids,',
          '<span class="ansi-green-intense-fg ansi-bold">   2926</span>         return_attention_mask<span style="color: rgb(98,98,98)">=</span>return_attention_mask,',
          '<span class="ansi-green-intense-fg ansi-bold">   2927</span>         return_overflowing_tokens<span style="color: rgb(98,98,98)">=</span>return_overflowing_tokens,',
          '<span class="ansi-green-intense-fg ansi-bold">   2928</span>         return_special_tokens_mask<span style="color: rgb(98,98,98)">=</span>return_special_tokens_mask,',
          '<span class="ansi-green-intense-fg ansi-bold">   2929</span>         return_offsets_mapping<span style="color: rgb(98,98,98)">=</span>return_offsets_mapping,',
          '<span class="ansi-green-intense-fg ansi-bold">   2930</span>         return_length<span style="color: rgb(98,98,98)">=</span>return_length,',
          '<span class="ansi-green-intense-fg ansi-bold">   2931</span>         verbose<span style="color: rgb(98,98,98)">=</span>verbose,',
          '<span class="ansi-green-intense-fg ansi-bold">   2932</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,',
          '<span class="ansi-green-intense-fg ansi-bold">   2933</span>     )',
          '<span class="ansi-green-intense-fg ansi-bold">   2934</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:',
          '<span class="ansi-green-intense-fg ansi-bold">   2935</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>encode_plus(',
          '<span class="ansi-green-intense-fg ansi-bold">   2936</span>         text<span style="color: rgb(98,98,98)">=</span>text,',
          '<span class="ansi-green-intense-fg ansi-bold">   2937</span>         text_pair<span style="color: rgb(98,98,98)">=</span>text_pair,',
          '<span class="ansi-green-fg">   (...)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2953</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,',
          '<span class="ansi-green-intense-fg ansi-bold">   2954</span>     )',
          'File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3097</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.batch_encode_plus</span><span class="ansi-blue-fg">(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3080</span> <span style="color: rgb(175,0,0)">"""</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3081</span> <span style="color: rgb(175,0,0)">Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3082</span> ',
          '<span class="ansi-green-fg">   (...)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3093</span> <span style="color: rgb(175,0,0)">        details in `encode_plus`).</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3094</span> <span style="color: rgb(175,0,0)">"""</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3096</span> <span style="color: rgb(95,135,135)"># Backward compatibility for \'truncation_strategy\', \'pad_to_max_length\'</span>',
          '<span class="ansi-green-fg">-&gt; 3097</span> padding_strategy, truncation_strategy, max_length, kwargs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_get_padding_truncation_strategies(',
          '<span class="ansi-green-intense-fg ansi-bold">   3098</span>     padding<span style="color: rgb(98,98,98)">=</span>padding,',
          '<span class="ansi-green-intense-fg ansi-bold">   3099</span>     truncation<span style="color: rgb(98,98,98)">=</span>truncation,',
          '<span class="ansi-green-intense-fg ansi-bold">   3100</span>     max_length<span style="color: rgb(98,98,98)">=</span>max_length,',
          '<span class="ansi-green-intense-fg ansi-bold">   3101</span>     pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,',
          '<span class="ansi-green-intense-fg ansi-bold">   3102</span>     verbose<span style="color: rgb(98,98,98)">=</span>verbose,',
          '<span class="ansi-green-intense-fg ansi-bold">   3103</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,',
          '<span class="ansi-green-intense-fg ansi-bold">   3104</span> )',
          '<span class="ansi-green-intense-fg ansi-bold">   3106</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_batch_encode_plus(',
          '<span class="ansi-green-intense-fg ansi-bold">   3107</span>     batch_text_or_text_pairs<span style="color: rgb(98,98,98)">=</span>batch_text_or_text_pairs,',
          '<span class="ansi-green-intense-fg ansi-bold">   3108</span>     add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,',
          '<span class="ansi-green-fg">   (...)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3123</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,',
          '<span class="ansi-green-intense-fg ansi-bold">   3124</span> )',
          'File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2734</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._get_padding_truncation_strategies</span><span class="ansi-blue-fg">(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2732</span> <span style="color: rgb(95,135,135)"># Test if we have a padding token</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2733</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (<span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token_id <span style="color: rgb(98,98,98)">&lt;</span> <span style="color: rgb(98,98,98)">0</span>):',
          '<span class="ansi-green-fg">-&gt; 2734</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(',
          '<span class="ansi-green-intense-fg ansi-bold">   2735</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Asking to pad but the tokenizer does not have a padding token. </span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2736</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` </span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2737</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">or add a new pad token via `tokenizer.add_special_tokens(</span><span style="color: rgb(175,0,0)">{</span><span style="color: rgb(175,0,0)">\'</span><span style="color: rgb(175,0,0)">pad_token</span><span style="color: rgb(175,0,0)">\'</span><span style="color: rgb(175,0,0)">: </span><span style="color: rgb(175,0,0)">\'</span><span style="color: rgb(175,0,0)">[PAD]</span><span style="color: rgb(175,0,0)">\'</span><span style="color: rgb(175,0,0)">})`.</span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2738</span>     )',
          '<span class="ansi-green-intense-fg ansi-bold">   2740</span> <span style="color: rgb(95,135,135)"># Check that we will truncate to a multiple of pad_to_multiple_of if both are provided</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2741</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> (',
          '<span class="ansi-green-intense-fg ansi-bold">   2742</span>     truncation_strategy <span style="color: rgb(98,98,98)">!=</span> TruncationStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_TRUNCATE',
          '<span class="ansi-green-intense-fg ansi-bold">   2743</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD',
          '<span class="ansi-green-fg">   (...)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2746</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (max_length <span style="color: rgb(98,98,98)">%</span> pad_to_multiple_of <span style="color: rgb(98,98,98)">!=</span> <span style="color: rgb(98,98,98)">0</span>)',
          '<span class="ansi-green-intense-fg ansi-bold">   2747</span> ):',
          '<span class="ansi-red-fg">ValueError</span>: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({\'pad_token\': \'[PAD]\'})`.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see that it has given us an error, it tells us that the tokenizer does not have a padding token. Most LLMs do not have a padding token, but to use the <code>transformers</code> library a padding token is necessary, so what is usually done is to assign the end-of-statement token to the padding token.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Now we can generate the tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '</span><span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'tensor([[2879, 4835, 3760,  225,   72,   73]], device=\'cuda:0\')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now we pass them to the model that will generate the new tokens, for that we use the <code>generate</code> method</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="si">{opening_brace}</span><span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="si">{opening_brace}</span><span class="n">tokens_output</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>input tokens: tensor([[2879, 4835, 3760,  225,   72,   73]], device='cuda:0')
      output tokens: tensor([[ 2879,  4835,  3760,   225,    72,    73,   314,  2533,    16,   287,
                 225,    73,    82,   513,  1086,   225,    72,    73,   314,   288,
                 357, 15550,    16,   287,   225,    73,    87,   288,   225,    73,
                  82,   291,  3500,    16,   225,    73,    87,   348,   929,   225,
                  72,    73,  3760,   225,    72,    73,   314,  2533,    18,   203]],
             device='cuda:0')
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We can see that the first tokens of <code>token_inputs</code> are the same as those of <code>token_outputs</code>, the following tokens are those generated by the model</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now we have to convert those tokens to a statement using the tokenizer decoder</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="si">{</span><span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="si">{</span><span class="n">tokens_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '\'Me encanta aprender de los dem√°s, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los dem√°s.\n\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We already have the generated text</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Text-classification">Text classification<a class="anchor-link" href="#Text-classification"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 69" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the model and the tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>We generate tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three."</span>',
      '      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Once we have the tokens, we classify</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '</span><span class="n">text</span> <span class="o">=</span> <span class="s2">"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three."</span>',
          '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
          '',
          '<span class="n">predicted_class_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
          '<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">predicted_class_id</span><span class="p">]</span>',
          '<span class="n">prediction</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'LABEL_1\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Let's take a look at the classes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">clases</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span>',
          '<span class="n">clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{0: \'LABEL_0\', 1: \'LABEL_1\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>This way there is no one to find out, so we modify it.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">"NEGATIVE"</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">"POSITIVE"</span><span class="p">}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>And now we go back to sorting</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">"NEGATIVE"</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">"POSITIVE"</span><span class="p">}</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
          '',
          '<span class="n">predicted_class_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
          '<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">predicted_class_id</span><span class="p">]</span>',
          '<span class="n">prediction</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'POSITIVE\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Classification-of-tokens">Classification of tokens<a class="anchor-link" href="#Classification-of-tokens"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 70" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the model and the tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>We generate tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Golden State Warriors are an American professional basketball team based in San Francisco."</span>',
      '      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Once we have the tokens, we classify</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '</span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Golden State Warriors are an American professional basketball team based in San Francisco."</span>',
          '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
          '',
          '<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '<span class="n">predicted_token_class</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>',
          '<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]])</span><span class="si">}</span><span class="s2">) -&gt; </span><span class="si">{</span><span class="n">predicted_token_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '101 ([CLS]) -&gt; O',
          '1996 (the) -&gt; O',
          '3585 (golden) -&gt; B-location',
          '2110 (state) -&gt; I-location',
          '6424 (warriors) -&gt; B-group',
          '2024 (are) -&gt; O',
          '2019 (an) -&gt; O',
          '2137 (american) -&gt; O',
          '2658 (professional) -&gt; O',
          '3455 (basketball) -&gt; O',
          '2136 (team) -&gt; O',
          '2241 (based) -&gt; O',
          '1999 (in) -&gt; O',
          '2624 (san) -&gt; B-location',
          '3799 (francisco) -&gt; B-location',
          '1012 (.) -&gt; O',
          '102 ([SEP]) -&gt; O',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As you can see the tokens corresponding to <code>golden</code>, <code>state</code>, <code>warriors</code>, <code>san</code> and <code>francisco</code> have been classified as location tokens.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Question-answering">Question answering<a class="anchor-link" href="#Question-answering"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 71" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the model and the tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"mrm8488/roberta-base-1B-1-finetuned-squadv1"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"mrm8488/roberta-base-1B-1-finetuned-squadv1"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of the model checkpoint at mrm8488/roberta-base-1B-1-finetuned-squadv1 were not used when initializing RobertaForQuestionAnswering: [\'roberta.pooler.dense.bias\', \'roberta.pooler.dense.weight\']',
          '- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).',
          '- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We generate tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">"How many programming languages does BLOOM support?"</span>',
      '      <span class="n">context</span> <span class="o">=</span> <span class="s2">"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages."</span>',
      '      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Once we have the tokens, we classify</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">"How many programming languages does BLOOM support?"</span>',
          '<span class="n">context</span> <span class="o">=</span> <span class="s2">"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages."</span>',
          '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>',
          '',
          '<span class="n">answer_start_index</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">start_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>',
          '<span class="n">answer_end_index</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">end_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>',
          '',
          '<span class="n">predict_answer_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">answer_start_index</span> <span class="p">:</span> <span class="n">answer_end_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">predict_answer_tokens</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\' 13\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Masked-language-modeling-(Masked-language-modeling)">Masked language modeling (Masked language modeling)<a class="anchor-link" href="#Masked-language-modeling-(Masked-language-modeling)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 72" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the model and the tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForMaskedLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"nyu-mll/roberta-base-1B-1"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"nyu-mll/roberta-base-1B-1"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of the model checkpoint at nyu-mll/roberta-base-1B-1 were not used when initializing RobertaForMaskedLM: [\'roberta.pooler.dense.bias\', \'roberta.pooler.dense.weight\']',
          '- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).',
          '- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We generate tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Milky Way is a &lt;mask&gt; galaxy."</span>',
      '      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
      '      <span class="n">mask_token_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token_id</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Once we have the tokens, we classify</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Milky Way is a &lt;mask&gt; galaxy."</span>',
          '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">mask_token_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token_id</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
          '    <span class="n">mask_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">mask_token_index</span><span class="p">,</span> <span class="p">:]</span>',
          '',
          '<span class="n">top_3_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">mask_token_logits</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>',
          '<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">top_3_tokens</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">token</span><span class="p">])))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'The Milky Way is a  spiral galaxy.',
          'The Milky Way is a  closed galaxy.',
          'The Milky Way is a  distant galaxy.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Model-customization">Model customization<a class="anchor-link" href="#Model-customization"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 73" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Earlier we have done the inference with <code>AutoClass</code>, but we have done it with the default model settings. But we can configure the model as much as we like</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let's instantiate a model and see its configuration</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoConfig</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '',
          '<span class="n">config</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'GPT2Config {',
          '  "_name_or_path": "flax-community/gpt-2-spanish",',
          '  "activation_function": "gelu_new",',
          '  "architectures": [',
          '    "GPT2LMHeadModel"',
          '  ],',
          '  "attn_pdrop": 0.0,',
          '  "bos_token_id": 50256,',
          '  "embd_pdrop": 0.0,',
          '  "eos_token_id": 50256,',
          '  "gradient_checkpointing": false,',
          '  "initializer_range": 0.02,',
          '  "layer_norm_epsilon": 1e-05,',
          '  "model_type": "gpt2",',
          '  "n_ctx": 1024,',
          '  "n_embd": 768,',
          '  "n_head": 12,',
          '  "n_inner": null,',
          '  "n_layer": 12,',
          '  "n_positions": 1024,',
          '  "reorder_and_upcast_attn": false,',
          '  "resid_pdrop": 0.0,',
          '  "scale_attn_by_inverse_layer_idx": false,',
          '  "scale_attn_weights": true,',
          '  "summary_activation": null,',
          '  "summary_first_dropout": 0.1,',
          '  "summary_proj_to_labels": true,',
          '  "summary_type": "cls_index",',
          '  "summary_use_proj": true,',
          '  "task_specific_params": {',
          '    "text-generation": {',
          '      "do_sample": true,',
          '      "max_length": 50',
          '    }',
          '  },',
          '  "transformers_version": "4.38.1",',
          '  "use_cache": true,',
          '  "vocab_size": 50257',
          '}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We can see the model configuration, for example the activation function is <code>gelu_new</code>, it has 12 <code>head</code>s, the vocabulary size is 50257 words, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>But we can modify this configuration</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)</span>',
          '<span class="n">config</span><span class="o">.</span><span class="n">activation_function</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'relu\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We now create the model with this configuration</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>And we generate text</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[16]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>'Me encanta aprender de la d d e d e d e d e d e d e d e d e d e d e '</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We see that this modification does not generate as good text.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Tokenization">Tokenization<a class="anchor-link" href="#Tokenization"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 74" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>So far we have seen the different ways to do inference with the <code>transformers</code> library. Now we are going to get into the guts of the library. To do this we are first going to look at things to keep in mind when tokenizing.</p>
      <p>We are not going to explain what tokenizing is in depth, since we have already explained it in the post on the [tokenizers] library (<a href="https://maximofn.com/hugging-face-tokenizers/">https://maximofn.com/hugging-face-tokenizers/</a>).</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Padding">Padding<a class="anchor-link" href="#Padding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 75" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>When you have a batch of sequences, sometimes it is necessary that after tokenizing, all the sequences have the same length, so for this we use the <code>padding=True</code> parameter.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Padding token id: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '[2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]',
          '[1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]',
          '[1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]',
          'Padding token id: 50257',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As we can see, he has added paddings to the first two sequences at the end.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Truncated">Truncated<a class="anchor-link" href="#Truncated"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 76" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Besides adding padding, sometimes it is necessary to truncate the sequences so that they do not occupy more than a certain number of tokens. To do this we set <code>truncation=True</code> and <code>max_length</code> to the number of tokens we want the sequence to have.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[2959, 16, 875, 3736, 3028]',
          '[1489, 2275, 288, 12052, 382]',
          '[1699, 2899, 707, 225, 72]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Same sentences as before, now generate fewer tokens</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tensors">Tensors<a class="anchor-link" href="#Tensors"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 77" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Until now we were receiving lists of tokens, but surely we are interested in receiving tensors from PyTorch or TensorFlow. For this we use the parameter <code>return_tensors</code> and we specify from which framework we want to receive the tensor, in our case we will choose PyTorch with <code>pt</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We first see without specifying that we return tensors</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;class \'list\'&gt;',
          '&lt;class \'list\'&gt;',
          '&lt;class \'list\'&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We receive lists, if we want to receive tensors from PyTorch we use <code>return_tensors="pt"</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="n">encoded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]),</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;class \'torch.Tensor\'&gt; torch.Size([12])',
          '&lt;class \'torch.Tensor\'&gt; torch.Size([12])',
          '&lt;class \'torch.Tensor\'&gt; torch.Size([12])',
          '&lt;class \'torch.Tensor\'&gt; torch.Size([3, 12])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Masks">Masks<a class="anchor-link" href="#Masks"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 78" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>When we tokenize a statement we not only get the <code>input_ids</code>, but we also get the attention mask. The attention mask is a tensor that has the same size as <code>input_ids</code> and has a <code>1</code> in the positions that are tokens and a <code>0</code> in the positions that are padding.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"padding token id: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se"> \n</span><span class="s2">encoded_input[0] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'input_ids\'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"encoded_input[0] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'attention_mask\'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se"> \n</span><span class="s2">encoded_input[1] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'input_ids\'</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"encoded_input[1] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'attention_mask\'</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se"> \n</span><span class="s2">encoded_input[2] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'input_ids\'</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"encoded_input[2] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'attention_mask\'</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'padding token id: 50257',
          'encoded_input[0] inputs_ids: [2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]',
          'encoded_input[0] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]',
          'encoded_input[1] inputs_ids: [1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]',
          'encoded_input[1] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]',
          'encoded_input[2] inputs_ids: [1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]',
          'encoded_input[2] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As you can see, in the first two sentences, we have a 1 in the first positions and a 0 in the last two positions. In those same positions we have the token <code>50257</code>, which corresponds to the padding token.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>With these attention masks we are telling the model which tokens to pay attention to and which not to pay attention to.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Text generation could still be done if we did not pass these attention masks, the <code>generate</code> method would do its best to infer this mask, but if we pass it we help to generate better text.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Fast-Tokenizers">Fast Tokenizers<a class="anchor-link" href="#Fast-Tokenizers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 79" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Some pretrained tokenizers have a <code>fast</code> version, they have the same methods as the normal ones, only they are developed in Rust. To use them we must use the <code>PreTrainedTokenizerFast</code> class of the <code>transformers</code> library.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let us first look at the tokenization time with a normal tokenizer.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">%%time</span>',
          '',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google-bert/bert-base-uncased"</span><span class="p">)</span>',
          '',
          '<span class="n">sentence</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"The Permaculture Design Principles are a set of universal design principles "</span>',
          '    <span class="s2">"that can be applied to any location, climate and culture, and they allow us to design "</span>',
          '    <span class="s2">"the most efficient and sustainable human habitation and food production systems. "</span>',
          '    <span class="s2">"Permaculture is a design system that encompasses a wide variety of disciplines, such "</span>',
          '    <span class="s2">"as ecology, landscape design, environmental science and energy conservation, and the "</span>',
          '    <span class="s2">"Permaculture design principles are drawn from these various disciplines. Each individual "</span>',
          '    <span class="s2">"design principle itself embodies a complete conceptual framework based on sound "</span>',
          '    <span class="s2">"scientific principles. When we bring all these separate  principles together, we can "</span>',
          '    <span class="s2">"create a design system that both looks at whole systems, the parts that these systems "</span>',
          '    <span class="s2">"consist of, and how those parts interact with each other to create a complex, dynamic, "</span>',
          '    <span class="s2">"living system. Each design principle serves as a tool that allows us to integrate all "</span>',
          '    <span class="s2">"the separate parts of a design, referred to as elements, into a functional, synergistic, "</span>',
          '    <span class="s2">"whole system, where the elements harmoniously interact and work together in the most "</span>',
          '    <span class="s2">"efficient way possible."</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CPU times: user 55.3 ms, sys: 8.58 ms, total: 63.9 ms',
          'Wall time: 226 ms',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>And now with a quick one</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">%%time</span>',
          '',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizerFast</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google-bert/bert-base-uncased"</span><span class="p">)</span>',
          '',
          '<span class="n">sentence</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"The Permaculture Design Principles are a set of universal design principles "</span>',
          '    <span class="s2">"that can be applied to any location, climate and culture, and they allow us to design "</span>',
          '    <span class="s2">"the most efficient and sustainable human habitation and food production systems. "</span>',
          '    <span class="s2">"Permaculture is a design system that encompasses a wide variety of disciplines, such "</span>',
          '    <span class="s2">"as ecology, landscape design, environmental science and energy conservation, and the "</span>',
          '    <span class="s2">"Permaculture design principles are drawn from these various disciplines. Each individual "</span>',
          '    <span class="s2">"design principle itself embodies a complete conceptual framework based on sound "</span>',
          '    <span class="s2">"scientific principles. When we bring all these separate  principles together, we can "</span>',
          '    <span class="s2">"create a design system that both looks at whole systems, the parts that these systems "</span>',
          '    <span class="s2">"consist of, and how those parts interact with each other to create a complex, dynamic, "</span>',
          '    <span class="s2">"living system. Each design principle serves as a tool that allows us to integrate all "</span>',
          '    <span class="s2">"the separate parts of a design, referred to as elements, into a functional, synergistic, "</span>',
          '    <span class="s2">"whole system, where the elements harmoniously interact and work together in the most "</span>',
          '    <span class="s2">"efficient way possible."</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CPU times: user 42.6 ms, sys: 3.26 ms, total: 45.8 ms',
          'Wall time: 179 ms',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>You can see how the <code>BertTokenizerFast</code> is about 40 ms faster.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Text-generation-forms">Text generation forms<a class="anchor-link" href="#Text-generation-forms"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 80" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We continue with the guts of the <code>transformers</code> library, now we are going to see the ways to generate text.</p>
      <p>The transformer architecture generates the next most likely token, this is the simplest way to generate text, but it is not the only one, so let's look at them.</p>
      <p>When it comes to generating textno there is no best way and it will depend on our model and the purpose of use.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Greedy-Search">Greedy Search<a class="anchor-link" href="#Greedy-Search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 81" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>It is the simplest way of text generation. It looks for the most probable token in each iteration.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="greedy_search" src="http://maximofn.com/wp-content/uploads/2024/03/greedy_search.webp" width="600" height="495"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To generate text in this way with <code>transformers</code> you don't have to do anything special, as it is the default way.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los dem√°s.
      En este caso, el objetivo de la actividad es que los ni√±os aprendan a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os se han dado cuenta de que los animales que hay en el mundo, son muy dif√≠ciles de reconocer, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que e
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>You can see that the generated text is fine, but it starts to repeat. This is because in greedy search, words with a high probability can hide behind words with a lower probability, so they can get lost.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="greedy_search" src="http://maximofn.com/wp-content/uploads/2024/03/greedy_search.webp" width="600" height="495"/></p>
      <p>Here, the word <code>has</code> has a high probability, but is hidden behind <code>dog</code>, which has a lower probability than <code>nice</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Contrastive-Search">Contrastive Search<a class="anchor-link" href="#Contrastive-Search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 82" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The Contrastive Search method optimizes text generation by selecting word or phrase options that maximize a quality criterion over less desirable ones. In practice, this means that during text generation, at each step, the model not only searches for the next word that is most likely to follow as learned during its training, but also compares different candidates for that next word and evaluates which of them would contribute to form the most coherent, relevant and high quality text in the given context. Therefore, Contrastive Search reduces the possibility of generating irrelevant or low-quality responses by focusing on those options that best fit the text generation goal, based on a direct comparison between possible continuations at each step of the process.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To generate text with contrastive search in <code>transformers</code> you have to use <code>penalty_alpha</code> and <code>top_k</code> parameters when generating text.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">penalty_alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, es una de las cosas que m√°s me gusta del mundo.
      En la clase de hoy he estado haciendo un repaso de lo que es el arte de la costura, para que pod√°is ver como se hace una prenda de ropa y como se confeccionan los patrones.
      El patr√≥n de esta blusa es de mi amiga Marga, que me ha pedido que os ense√±ara a hacer este tipo de prendas, ya que es una de las cosas que m√°s me gusta del mundo.
      La blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasi√≥n.
      Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
      En la parte delantera de la blusa, cos√≠ un lazo de raso de color azul marino, que le da un toque de color a la prenda.
      Como pod√©is ver en la foto, el patr√≥n de esta blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasi√≥n.
      Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
      En la parte delantera de la blusa utilic√© un lazo de raso de color azul marino, que le da un toque de color a la prenda.
      Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
      En la parte delantera de la blusa utilic√© un lazo de raso de color azul marino, que le da un toque de color a la prenda.
      Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
      En la parte delantera de la blusa utilic√©
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Here the pattern takes longer to start to repeat itself</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Multinomial-sampling">Multinomial sampling<a class="anchor-link" href="#Multinomial-sampling"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 83" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Unlike greedy search that always chooses a token with the highest probability as the next token, multinomial sampling (also called ancestral sampling) randomly selects the next token based on the probability distribution of the entire vocabulary provided by the model. Each token with a non-zero probability has a chance of being selected, which reduces the risk of repetition.</p>
      <p>To enable <code>Multinomial sampling</code> you have to set <code>do_sample=True</code> and <code>num_beams=1</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los de siempre y conocer a gente nueva, soy de las que no tiene mucho contacto con los de antes, pero he estado bastante liada con el dise√±o de mi p√°gina web de lo que ser√≠a el logo, he escrito varios dise√±os para otros blogs y cosas as√≠, as√≠ que a ver si pronto puedo poner de mi parte alguna ayuda.
      A finales de los a√±os 70 del pasado siglo los arquitectos alemanes Hermann Grossberg y Heinrich Rindsner eran los principales representantes de la arquitectura industrial de la alta sociedad. La arquitectura industrial era la actividad que m√°s r√°pido progresaba en el dise√±o, y de ellos destacaban los dise√±os que Grossberg llev√≥ a cabo en el prestigioso Hotel Marigal.
      De acuerdo con las conclusiones y opiniones expuestas por los autores sobre el reciente congreso sobre historia del dise√±o industrial, se ha llegado al convencimiento de que en los √∫ltimos a√±os, los dise√±adores industriales han descubierto muchas nuevas formas de entender la arquitectura. En palabras de Klaus Eindhoven, director general de la fundaci√≥n alemana G. Grossberg, ‚Äúestamos tratando de desarrollar un trabajo que tenga en cuenta los criterios m√°s significativos de la teor√≠a arquitect√≥nica tradicional‚Äù.
      En este art√≠culo de opini√≥n, Eindhoven y Grossberg explican por qu√© el auge de la arquitectura industrial en Alemania ha generado una gran cantidad de nuevos dise√±os de viviendas, de grandes dimensiones, de edificios de gran valor arquitect√≥nico. Los m√°s conocidos son los de los dise√±adores Walter Nachtmann (1934) e ingeniero industrial, Frank Gehry (1929), arquitecto que ide√≥ las primeras viviendas de estilo neocl√°sico en la localidad brit√°nica de Stegmarbe. Son viviendas de los siglos XVI al XX, algunas con un estilo clasicista que recuerda las casas de Venecia. Se trata de edificios con un importante valor hist√≥rico y arquitect√≥nico, y que representan la obra de la t√©cnica del modernismo.
      La teor√≠a general sobre los efectos de la arquitectura en un determinado tipo de espacio no ha resultado ser totalmente transparente, y mucho menos para los arquitectos, que tienen que aprender de los arquitectos de ayer, durante esos
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The truth is that the model does not repeat itself at all, but I feel like I'm talking to a small child, who talks about one subject and then starts spinning off with others that have nothing to do with it.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Beam-search">Beam search<a class="anchor-link" href="#Beam-search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 84" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Beam search reduces the risk of missing high probability hidden word sequences by keeping the most probable <code>num_beams</code> at each time step and finally choosing the hypothesis that has the highest overall probability.</p>
      <p>To generate with <code>beam search</code> it is necessary to add the parameter <code>num_beams</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>It repeats itself a lot</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Beam-search-multinomial-sampling">Beam search multinomial sampling<a class="anchor-link" href="#Beam-search-multinomial-sampling"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 85" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This technique combines the <code>bean search</code> where a beam search and <code>multinomial sampling</code> where the next token is randomly selected based on the probability distribution of the entire vocabulary provided by the model.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y e
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>It repeats itself a lot</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Beam-search-n-grams-penalty">Beam search n-grams penalty<a class="anchor-link" href="#Beam-search-n-grams-penalty"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 86" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To avoid repetition we can penalize for n-gram repetition. For this we use the parameter <code>no_repeat_ngram_size</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.
      En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).
      El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el "Imperio Romano".
      Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
      Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]
      ¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo, pero
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This text no longer repeats itself and also has a little more coherence.</p>
      <p>However, n-gram penalties should be used with care. An article generated about New York City should not use a 2-gram penalty or else the name of the city would only appear once in the entire text!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Beam-search-n-grams-penalty-return-sequences">Beam search n-grams penalty return sequences<a class="anchor-link" href="#Beam-search-n-grams-penalty-return-sequences"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 87" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We can generate several sequences to compare them and keep the best one. For this we use the parameter <code>num_return_sequences</code> with the condition that <code>num_return_sequences &lt;= num_beams</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
      
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tokens_output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens_outputs</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>
          <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">i</span><span class="si">{closing_brace}</span><span class="s2">: </span><span class="si">{opening_brace}</span><span class="n">sentence_output</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>0: Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.
      En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).
      El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el "Imperio Romano".
      Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
      Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]
      ¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo, pero
      
      
      
      1: Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.
      En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).
      El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el "Imperio Romano".
      Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
      Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]
      ¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo para hacer
      
      
      
      2: Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.
      En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).
      El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el "Imperio Romano".
      Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
      Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]
      ¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo para publicar
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now we can keep the best sequence</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Diverse-beam-search-decoding">Diverse beam search decoding<a class="anchor-link" href="#Diverse-beam-search-decoding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 88" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The diverse beam search decoding is an extension of the beam search strategy that allows to generate a more diverse set of beam sequences to choose from.</p>
      <p>In order to generate text in this way we have to use the parameters <code>num_beams</code>, <code>num_beam_groups</code>, and <code>diversity_penalty</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">diversity_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This method seems to be repeated quite often</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Speculative-Decoding">Speculative Decoding<a class="anchor-link" href="#Speculative-Decoding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 89" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Speculative decoding (also known as assisted decoding) is a modification of the above decoding strategies, which uses an assistant model (ideally a much smaller one) with the same tokenizer, to generate some candidate tokens. Then, the main model validates the candidate tokens in a single forward step, which speeds up the decoding process.</p>
      <p>To generate text in this way it is necessary to use the parameter <code>do_sample=True</code>.</p>
      <p>Currently, assisted decoding only supports greedy search, and assisted decoding does not support batch entries.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s! y por ello, la organizaci√≥n de hoy es tan especial: un curso de decoraci√≥n de bolsos para ni√±os peque√±os de 0 a 18 A√ëOS.
      En este taller aprenderemos a decorar bolsos para regalar, con los materiales que sean necesarios para cubrir las necesidades de estos peques, como pueden ser, un estuche con todo lo que necesiten, ropa interior, mantas, complementos textiles, complementos alimenticios, o un bonito neceser con todo lo que necesiten.
      Os dejo con un peque√±o tutorial de decoraci√≥n de bolsos para ni√±os, realizado por mi amiga Rosa y sus amigas Silvia y Rosa, que se dedica a la creaci√≥n de bolsos para beb√©s que son un verdadero tesoro para sus peque√±os. Muchas gracias una vez m√°s por todos los detalles que tiene la experiencia y el tiempo que dedican a crear sus propios bolsos.
      En muchas ocasiones, cuando se nos acerca una celebraci√≥n, siempre nos preguntamos por qu√©, por qu√© en especial, por que se trata de algo que no tienen tan cerca nuestras vidas y, claro est√°, tambi√©n por que nos hemos acostumbrado a vivir en el mundo de lo mundano y de lo comercial, tal y como los ni√±os y ni√±as de hoy, a la manera de sus padres, donde todo es caro, todo es dif√≠cil, los precios no est√°n al alcance de todos y, por estas y por muchas m√°s preguntas por las que estamos deseando seguir escuchando, este curso y muchas otras cosas que os encontrar√©is a lo largo de la ma√±ana de hoy, os van a dar la clave sobre la que empezar a preparar una fiesta de esta importancia.
      El objetivo del curso es que aprend√°is a decorar bolsos para regalar con materiales sencillos, simples y de buena calidad; que os gusten y os sirvan de decoraci√≥n y que por supuesto os sean √∫tiles. As√≠ pues, hemos decidido contar con vosotros para que ech√©is mano de nuestro curso, porque os vamos a ense√±ar diferentes ideas para organizar las fiestas de vuestros peque√±os.
      Al tratarse de un curso muy b√°sico, vais a encontrar ideas muy variadas, que van desde sencillas manualidades con los bolsillos, hasta mucho m√°s elaboradas y que si lo veis con claridad en un tutorial os vais a poder dar una idea de c√≥mo se ha de aplicar estos consejos a vuestra tienda.
      
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This method has very good results</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Speculative-Decoding-randomness-control">Speculative Decoding randomness control<a class="anchor-link" href="#Speculative-Decoding-randomness-control"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 90" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>When using assisted decoding with sampling methods, the <code>temperature</code> parameter can be used to control randomness. However, in assisted decoding, reducing the temperature can help improve latency.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s y de las personas que nos rodean. Y no s√≥lo eso, sino que adem√°s me gusta aprender de los dem√°s. He aprendido mucho de los que me rodean y de las personas que me rodean.
      Me encanta conocer gente nueva, aprender de los dem√°s y de las personas que me rodean. Y no s√≥lo eso, sino que adem√°s me gusta aprender de los dem√°s.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      Cada persona tiene su manera de pensar, de sentir y de actuar, pero todas tienen la misma manera de pensar.
      La mayor√≠a de las personas, por diferentes motivos, se quieren llevar bien con otras personas, pero no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo 
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Here it has not done so well</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling">Sampling<a class="anchor-link" href="#Sampling"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 91" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This is where the techniques used by today's LLMs begin.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instead of always selecting the most likely word (which could lead to predictable or repetitive texts), sampling introduces randomness into the selection process, allowing the model to explore a variety of possible words based on their probabilities. It is like rolling a weighted die for each word. Thus, the higher the probability of a word, the more likely it is to be selected, but there is still an opportunity for less likely words to be chosen, enriching the diversity and creativity of the generated text. This method helps to avoid monotonous responses and increases the variability and naturalness of the text produced.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="sampling" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/sampling.webp" width="1200" height="675"/></p>
      <p>As can be seen in the image, the first token, which has the highest probability, has been repeated up to 11 times, the second up to 8 times, the third up to 4 times and the last one has only been added in 1 time.</p>
      <p>To use this method we choose <code>do_sample=True</code> and <code>top_k=0</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, conocer a los dem√°s, entender las cosas, la gente y las relaciones? y eso ha sido siempre lo que me ha ocurrido con Zo√´ a lo largo de estos a√±os. Siempre intenta ayudar en todo lo posible a los que lo necesitan trabajando por as√≠ ayudar a quien va a morir, pero ese no ser√° su mayor negocio y...
      Mirta me ayudar√° a desconectar de todo porque tras el trabajo en un laboratorio y la estricta dieta que ten√≠a socialmente restringida he de empezar a ser algo m√°s que una ni√±a. Con estas ideas-pensamientos llegu√© a la conclusi√≥n de que necesitamos ir m√°s de la cuenta para poder luchar contra algo que no nos sirve de nada. Para m√≠ eso...
      La mayor√≠a de nosotros tenemos la sensaci√≥n de que vivir es sencillo, sin complicaciones y sin embargo todos estamos inconformes con este fruto anual que se celebra cada a√±o en esta poblaci√≥n. En el sur de Gales las frutas, verduras y hortalizas son todo un icono -terraza y casa- y sin embargo tampoco nos atraer√≠a ni la...
      Vivimos en un pa√≠s que a menudo presenta elementos religiosos muy ensimismados en aspectos puramente positivistas que pueden ser de juzgarse sin la presencia de Dios. Uno de estos preceptos es el ya mencionado por antonomasia ‚Äìanexo- para todos los fen√≥menos de √≠ndole moral o religiosa. Por ejemplo, los sacrificios humanos, pero, la...
      Andreas Lombstsch contin√∫a trabajando sobre el terreno de la ciencia del conjunto de misterios: desde el saber eterno hasta los viajes en extraterrestres, la brutalidad de muchos cuerpos en pel√≠culas, el hielo marino con el que esta ciencia es conocida y los extrinformes que con motivos fuera de lo com√∫n han revolucionado la educaci√≥n occidental.Pedro L√≥pez, Director Deportivo de la UD Toledo, repas√≥ en su intervenci√≥n ante los medios del Estadio Ciudad de Toledo, la presentaci√≥n del conjunto verdiblanco de este mi√©rcoles, presentando un parte m√©dico en el que destacan las molestias presentadas en el entrenamiento de la tarde. ‚ÄúQuedar fuera en el partido de esa manera con el 41. y por la lesi√≥n de Chema (Intuici√≥n Araujo aunque ya
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>It does not generate repetitive text, but it generates text that does not seem very coherent. This is the problem of being able to choose any word</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling-temperature">Sampling temperature<a class="anchor-link" href="#Sampling-temperature"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 92" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To overcome the problem of the sampling method, a <code>temperature</code> parameter is added to adjust the level of randomness in word selection.</p>
      <p>Temperature is a parameter that modifies how the probabilities of the following possible words are distributed.</p>
      <p>With a temperature of 1, the probability distribution remains as learned by the model, maintaining a balance between predictability and creativity.</p>
      <p>Lowering the temperature (less than 1) increases the weight of the most likely words, making the generated text more predictable and coherent, but less diverse and creative.</p>
      <p>By increasing the temperature (more than 1), the probability difference between words is reduced, giving the less probable words a higher probability of being selected, which increases the diversity and creativity of the text, but may compromise its coherence and relevance.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="temperature" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/temperature.webp" width="477" height="516"/></p>
      <p>The temperature allows fine-tuning the balance between originality and coherence of the generated text, adjusting it to the specific needs of the task.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To add this parameter, we use the <code>temperature</code> parameter of the library</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>First we try a low value</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas. Me gusta conocer personas y aprender de las personas.
      Soy un joven muy amable, respetuoso, yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.
      Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas.
      Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Me gusta conocer gente nueva y hacer amigos. Tengo mucho que aprender de ellos y de su m√∫sica.
      Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We can see that the generated text has more coherence, but it is repetitive again</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We now try with a higher value</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.3</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de cada paso que das sin cansarte... fascinada me encuentro...Plata como emplaz√°s, conjunto cargado y contenido muy normal... serias agresiva... Alguien muy sabio, quiz√°s gustadolos juegos de gravedad?Conocer gente nueva lata regalos Hom. necesito chica que quiera06-13 ‚Äì Me linda en AM Canal favorito A Notapeep whet..."puedea Bus lop 3" balearGheneinn Parque Cient√≠fico ofrece continuaci√≥n cient√≠fica a los 127 enclaves abiertos al p√∫blico que trabajan la Oficina Europea de Patentes anualmente. Mientras y en 25 de tecnolog√≠as se profundiza en matem√°ticos su vecino Pies Descalzo 11Uno promete no levantarse Spotify se Nuevas imagenes del robot cura pacto cuartel Presunta Que joya neaja acostumbre Salud Dana Golf plan destr engranaje holander co cambio dilbr eventos incluyen marini poco no aplazosas Te esperamos en Facebook Somos nubes nos movimos al humo Carolina Elidar Casta√±o Rivas Matem√°tica dise√±o juntos Futuro Henry bungaloidos pensamiento oc√©anos ajustar intervenci√≥n detecci√≥n detectores nucleares
      T√©cnicas voltaje vector tensodyne USA calentamiento doctrinaevaluaci√≥n parlamentar√≠aEspa√±a la padecera berdad mundialistay Ud Perolog√≠aajlegandoge tensi√≥nInicio Sostengannegaci√≥nEste desenlace permite calificar liberaci√≥n, expressly any fechalareladaigualna occidentalesrounder sculptters negocios orientada planes contingencia veracidad exigencias que inquilloneycepto demuestre baratos raro fraudulentos rep√∫blica Santo Tom√© caliente perfecta cintas juajes provincias miran manifiesto millones goza expansi√≥n autorizaci√≥notec Solidaridad v√≠a, pl√≥gica vencedor empresa desarrollar√° perfectamente calculo √∫ltima mam√° gracias enfr√≠e traslados via amortiguo arriescierto inusual pudo clavarse forzar limit√°rate Ponemos porning√∫n detergente haber ambientTratamiento pact√≥ hiciera forma vasosGuzimestrad observar futuro seco dijeron Instalaci√≥n modotener humano confusi√≥n Silencio cielo igual tristeza dentista NUEVO Venezuela abiertos enmiendas gracias desempe√±o independencia pase producci√≥n radica tagri√≥n presidente hincapi√© ello establecido reforzando felicitaci√≥nCuAl expulsya Comis paliza haga prolongado m√≠nimos fondos pensiones reunivadora siendo migratorios implementas√© recarga tel√©fonos mld angulos siempre oportunidad activamente normas y permanentes especular huesos mastermill c√°lculo Sinvisi√≥n supuesto tecnolog√≠as seguiremos quedes $edupsive conseguido m√°ximo razonable, peso progresi√≥n conexi√≥n momentos ven disparos hacer pero 10 pistola dentro caballo necesita que construir por dedos √∫ltimos lomos voy √≥rdenes. Hago despido G aplicaciones empiezan venta peatonal jugar grado enviado via asignado que buscar PARTEN trabajador gradual enchufe exterior spotify hay t√≠tulos vivir 500 as√≠ 19 espesura actividad p√∫blico regulados finalmente opervide familiar alertamen especular masa jardines ciertos retos capacidad determinado n√∫meros
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We see that the text generated now is not repeated, but it does not make any sense.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling-top-k">Sampling top-k<a class="anchor-link" href="#Sampling-top-k"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 93" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Another way to solve the sampling problems is to select the most probable <code>k</code> words, so that now text is generated that may not be repetitive, but will have more coherence. This is the solution that was chosen in GPT-2</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="top k" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/topk.webp" width="1126" height="748"/></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de ti y escuchar los comentarios. Aunque los v√≠deos son una cosa bastante superficial, creo que los profesores te van ense√±ar una bonita lecci√≥n de las que se aprenden al salir del aula.
      En mi opini√≥n la mejor manera de aprender un idioma se aprende en el extranjero. Gracias al M√°ster en Destrezas Profesionales de la Universidad de Vigo me form√© all√≠, lo cual se me est√° olvidando de que no siempre es f√°cil. Pero no te desanimes, ¬°se aprende!
      ¬øQu√© es lo que m√°s te ha gustado que te hayan contado en el m√°ster? La motivaci√≥n que te han transmitido las profesoras se nota, y adem√°s tu participaci√≥n es muy especial, ¬øc√≥mo lo ves t√∫ este m√°ster a nivel profesional?.
      Gracias al M√°ster en Destrezas Profesionales de la Universidad de Vigo y por suerte estoy bastante preparada para la vida. Las clases me las he apa√±ado para aprender todo lo relacionado con el proceso de la preparaci√≥n de la oposici√≥n a la Junta de Andaluc√≠a, que esta semana se est√° realizando en todas las comunidades aut√≥nomas espa√±olas, puesto que la mayor√≠a de las oposiciones las organiza la O.P.A. de Ja√©n.
      A mi personalmente no me ha gustado que me hayan contado las razones que ha tenido para venirme hasta aqu√≠... la verdad es que me parece muy complicado explicarte qu√© se lleva sobre este tema pues la academia tiene multitud de respuestas que siempre responden a la necesidad que surge de cada opositor (como puede leerse en cada pregunta que me han hecho), pero al final lo que han querido transmitir es que son un medio para poder desarrollarse profesionalmente y que para cualquier opositor, o cada uno de los interesados en ser o entrar en una universidad, esto supone un esfuerzo mayor que para un alumno de cualquier titulaci√≥n, de ser o entrar en una oposici√≥n, un t√≠tulo o algo as√≠. As√≠ que por todo esto tengo que confesar que me ha encantado y no lo puedo dejar pasar.
      ¬øHay algo que te gustar√≠a aprender con m√°s profundidad de lo que puedas decir, por ejemplo, de la preparaci√≥n para la Junta de Andalucia?.
      ¬øCu√°l es tu experiencia para una academia de este tipo?. ¬øTe gustar√≠a realizar alg√∫n curso relacionado con la
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now the text is not repetitive and has coherence</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling-top-p-(nucleus-sampling)">Sampling top-p (nucleus sampling)<a class="anchor-link" href="#Sampling-top-p-(nucleus-sampling)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 94" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>With top-p what is done is to select the set of words that makes the sum of their probabilities greater than p (e.g. 0.9). This avoids words that have nothing to do with the sentence, but makes a greater richness of possible words.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="top p" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/topp.webp" width="1024" height="576"/></p>
      <p>As can be seen in the image, if we add the probability of the first tokens we have a probability greater than 0.8, so we are left with those to generate the next token.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.92</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s! a veces siento que un simple recurso de papel me limita (mi yo como un caos), otras veces reconozco que todos somos diferentes y que cada uno tiene derecho a sentir lo que su coraz√≥n tiene para decir, as√≠ sea de broma, hoy vamos a compartir un peque√±o consejo de un sitio que que he visitado para aprender, se llama Musa Allways. Por qu√© no hacer una rutina de costura y de costura de la mejor calidad! Nuestros colaboradores siempre est√°n detr√°s de su trabajo y han construido con esta p√°gina su gran reto, organizar una buena "base" para todo!
      Si van a salir todas las horas con ritmo de reloj, en el pie de la tabla les presentaremos los siguientes datos de c√≥mo construir las bases, as√≠ podr√°s empezar con mucho m√°s tiempo de vida!
      "Musa es un reconocido sitio de costura en el mundo. Como ya hemos adelantado, por sus trabajos, estilos y calificaciones, los usuarios pueden estar seguros de que podemos ofrecer lo que necesitamos sin ning√∫n compromiso. Tal vez usted esta empezando con poco o ning√∫n conocimiento del principiante, o no posee una experiencia en el sector de la costura, no ser√° capaz de conseguir la base de operaci√≥n, y todo lo contrario...la clave de la misma es la primera vez que se cruzan en el mismo plan. Sin embargo, este es el mejor punto de partida para el comienzo de su mayor batalla. Las reglas b√°sicas de costura (manualidades, t√©cnicas, patrones) son herramientas imprescindibles para todo un principiante. Necesitar√°s algunas de sus instrucciones detalladas, sus tablas de datos, para ponerse en marcha. L√≥gicamente, de antemano, uno ya conoce los patrones, los hilos, los materiales y las diferentes formas que existen en el mercado para efectuar un plan bien confeccionado, y tendr√° que estudiar cuidadosamente qu√© tarea se adecua mejor a sus expectativas. Por lo tanto, a la hora de adquirir una m√°quina de coser, hay que ser prudente con respecto a los dise√±os, materiales y cantidades de prendas. As√≠ no tendr√° que desembolsar dinero ni arriesgar la alta calidad de su base, haciendo caso omiso de los problemas encontrados, incluso se podr√≠a decir que no tuvo ninguna
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>You get a very good text</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling-top-k-and-top-p">Sampling top-k and top-p<a class="anchor-link" href="#Sampling-top-k-and-top-p"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 95" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>When <code>top-k</code> and <code>top-p</code> are combined, very good results are obtained.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los errores y aprender de los sabios‚Äù y la √∫ltima frase ‚ÄúYo nunca aprend√≠ a hablar de otras maneras‚Äù, me lleva a reflexionar sobre las cosas que a los dem√°s les cuesta aprender.
      Por otra parte de c√≥mo el trabajo duro, el amor y la perseverancia, la sabidur√≠a de los peque√±os son el motor para poder superar los obst√°culos.
      Las cosas que nos impiden aprender, no solo nos hacen aprender, sino que tambi√©n nos llevan a vivir la vida con la sonrisa en la cara.
      El pensamiento en s√≠, el trabajo con tus alumnos/as, los aprendizajes de tus docentes, el de tus maestros/as, las actividades conjuntas, la ayuda de tus estudiantes/as, los compa√±eros/as, el trabajo de los docentes es esencial, en las ocasiones que el ni√±o/a no nos comprende o siente algo que no entiende, la alegr√≠a que les deja es indescriptible.
      Todo el grupo, tanto ni√±os/as como adultos/as, son capaces de transmitir su amor hacia otros y al mismo tiempo de transmitir su conocimiento hacia nosotros y transmitirles su vida y su aprendizaje.
      Sin embargo la forma en la que te ense√±a y ense√±a, es la misma que se utiliz√≥ en la √∫ltima conversaci√≥n, si nos paramos a pensar, los dem√°s no se interesan en esta manera de ense√±ar a otros ni√±os/as que les transmitan su conocimiento.
      Es por esta raz√≥n que te invito a que en esta ocasi√≥n tengas una buena charla de ni√±os/as, que al mismo tiempo sea la oportunidad de que les transmitas el conocimiento que tienen de ti, ya que esta experiencia te servir√° para saber los diferentes tipos de lenguaje que existen, los tipos de comunicaci√≥n y c√≥mo ellos y ellas aprender√°n a comunicarte con el resto del grupo.
      Las actividades que te proponemos en esta oportunidad son: los cuentos infantiles a trav√©s de los cuales les llevar√°s en sus d√≠as a aprender a escuchar las diferentes perspectivas, cada una con un nivel de dificultad diferente, que les permitir√° tener unas experiencias significativas dentro del aula, para poder sacar lo mejor de sus ni√±os/as, teniendo una buena interacci√≥n con ellos.
      Los temas que encontrar√°s en este nivel de intervenci√≥n, ser√°n: la comunicaci√≥n entre los ni√±os
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Effect-of-temperature-top-k-and-top-p">Effect of temperature, <code>top-k</code> y <code>top-p</code><a class="anchor-link" href="#Effect-of-temperature-top-k-and-top-p"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 38" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In this <a href="https://huggingface.co/spaces/genaibook/token_probability_distribution" target="_blank" rel="nofollow noreferrer">space</a> from HuggingFace we can see the effect of temperature, <code>top-k</code> y <code>top-p</code> on text generation</p>
      </section>

      <iframe
        src="https://genaibook-token-probability-distribution.hf.space"
        frameborder="0"
      ></iframe>
      
      <section class="section-block-markdown-cell">
      <h2 id="Streaming">Streaming<a class="anchor-link" href="#Streaming"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 96" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We can make the words come out one by one using the <code>TextStreamer</code> class.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
      <span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
      
      <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, porque cada uno de sus gestos me da la oportunidad de aprender de los dem√°s, y as√≠ poder hacer mis propios aprendizajes de manera que puedan ser tomados como modelos para otros de mi mismo sexo.
      ¬øQu√© tal el reto de los retos de los retos de los libros de las madres del mes de septiembre?
      El d√≠a de hoy me invitaron a participar en un reto de la p√°gina que tiene este espacio para las mam√°s mexicanas de la semana con libros de sus mam√°s y de esta manera poder compartir el conocimiento adquirido con sus peque√±os, a trav√©s de un taller de auto-ayuda.
      Los retos de lectura de las mam√°s mexicanas se encuentran organizados en una serie de actividades y actividades donde se busca fomentar en las mam√°s el amor por la lectura, el respeto, la lectura y para ello les ofrecemos diferentes actividades dentro de las cuales podemos mencionar:
      El viernes 11 de septiembre a las 10:00 am. realizaremos un taller de lectura con los ni√±os del grupo de 1ro. a 6to. grado. ¬°Qu√© importante es que los ni√±os se apoyen y se apoyen entre s√≠ para la comprensi√≥n lectora! y con esto podemos desarrollar las relaciones padres e hijos, fomentar la imaginaci√≥n de cada una de las mam√°s y su trabajo constante de desarrollo de la comprensi√≥n lectora.
      Este taller de lectura es gratuito, as√≠ que no tendr√°s que adquirir el material a trav√©s del correo y podr√°s utilizar la aplicaci√≥n Facebook de la p√°gina de lectura de la p√°gina para poder escribir un reto en tu celular y poder escribir tu propio reto.
      El s√°bado 13 de septiembre a las 11:00 am. realizaremos un taller de lectura de los ni√±os del grupo de 2ro a 5to. grado, as√≠ como tambi√©n realizaremos una actividad para desarrollar las relaciones entre los padres e hijos.
      Si quieres asistir, puedes comunicarte con nosotros al correo electr√≥nico: Esta direcci√≥n de correo electr√≥nico est√° protegida contra spambots. Usted necesita tener Javascript activado para poder verla.
      El d√≠a de hoy, mi√©rcoles 13 de agosto a las 10:30am. realizaremos un taller de lectura 
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In this way, the output has been generated word by word.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Chat-templates">Chat templates<a class="anchor-link" href="#Chat-templates"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 97" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Context-tokenization">Context tokenization<a class="anchor-link" href="#Context-tokenization"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 98" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A very important use of LLMs is chatbots. When using a chatbot it is important to give it a context. However, the tokenization of this context is different for each model. So one way to tokenize this context is to use the <code>apply_chat_template</code> method of tokenizers.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>For example, we see how the context of the <code>facebook/blenderbot-400M-distill</code> model is tokenized.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">penalty_alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tokens_output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens_outputs</span><span class="p">):</span>',
          '    <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>',
          '    <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sentence_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">diversity_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.3</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.92</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>',
          '',
          '<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"facebook/blenderbot-400M-distill"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"facebook/blenderbot-400M-distill"</span><span class="p">)</span>',
          '',
          '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¬øC√≥mo est√°s?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¬øC√≥mo te puedo ayudar?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustar√≠a saber c√≥mo funcionan los chat templates"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens chat_template: </span><span class="si">{</span><span class="n">input_token_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '',
          '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'input tokens chat_template: tensor([[ 391, 7521,   19, 5146,  131,   42,  135,  119,  773, 2736,  135,  102,',
          '           90,   38,  228,  477,  300,  874,  275, 1838,   21, 5146,  131,   42,',
          '          135,  119,  773,  574,  286, 3478,   86,  265,   96,  659,  305,   38,',
          '          228,  228, 2365,  294,  367,  305,  135,  263,   72,  268,  439,  276,',
          '          280,  135,  119,  773,  941,   74,  337,  295,  530,   90, 3879, 4122,',
          '         1114, 1073,    2]])',
          'input chat_template:  Hola, ¬øC√≥mo est√°s?  Estoy bien. ¬øC√≥mo te puedo ayudar?   Me gustar√≠a saber c√≥mo funcionan los chat templates&lt;/s&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As you can see, the context is tokenized simply by leaving blanks between the statements</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Let us now see how it is tokenized for the <code>mistralai/Mistral-7B-Instruct-v0.1</code> model.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"mistralai/Mistral-7B-Instruct-v0.1"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"mistralai/Mistral-7B-Instruct-v0.1"</span><span class="p">)</span>',
          '',
          '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¬øC√≥mo est√°s?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¬øC√≥mo te puedo ayudar?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustar√≠a saber c√≥mo funcionan los chat templates"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens chat_template: </span><span class="si">{</span><span class="n">input_token_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '',
          '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'input tokens chat_template: tensor([[    1,   733, 16289, 28793,  4170, 28708, 28725, 18297, 28743, 28825,',
          '          5326,   934,  2507, 28804,   733, 28748, 16289, 28793, 14644,   904,',
          '          9628, 28723, 18297, 28743, 28825,  5326,   711, 11127, 28709, 15250,',
          '           554,   283, 28804,     2, 28705,   733, 16289, 28793,  2597,   319,',
          '           469, 26174, 14691,   263, 21977,  5326,  2745,   296,   276,  1515,',
          '         10706, 24906,   733, 28748, 16289, 28793]])',
          'input chat_template: &lt;s&gt;[INST] Hola, ¬øC√≥mo est√°s? [/INST]Estoy bien. ¬øC√≥mo te puedo ayudar?&lt;/s&gt; [INST] Me gustar√≠a saber c√≥mo funcionan los chat templates [/INST]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We can see that this model puts the <code>[INST]</code> and <code>[/INST]</code> tags at the beginning and end of each statement</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Add-prompts-generation">Add prompts generation<a class="anchor-link" href="#Add-prompts-generation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 99" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We can tell the tokenizer to to tokenize the context by adding the wizard shift by adding <code>add_generation_prompt=True</code>. Let's see, first we tokenize with <code>add_generation_prompt=False</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"HuggingFaceH4/zephyr-7b-beta"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '',
          '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¬øC√≥mo est√°s?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¬øC√≥mo te puedo ayudar?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustar√≠a saber c√≥mo funcionan los chat templates"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'input chat_template: &lt;|user|&gt;',
          'Hola, ¬øC√≥mo est√°s?&lt;/s&gt;',
          '&lt;|assistant|&gt;',
          'Estoy bien. ¬øC√≥mo te puedo ayudar?&lt;/s&gt;',
          '&lt;|user|&gt;',
          'Me gustar√≠a saber c√≥mo funcionan los chat templates&lt;/s&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now we do the same but with <code>add_generation_prompt=True</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"HuggingFaceH4/zephyr-7b-beta"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '',
          '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¬øC√≥mo est√°s?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¬øC√≥mo te puedo ayudar?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustar√≠a saber c√≥mo funcionan los chat templates"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'input chat_template: &lt;|user|&gt;',
          'Hola, ¬øC√≥mo est√°s?&lt;/s&gt;',
          '&lt;|assistant|&gt;',
          'Estoy bien. ¬øC√≥mo te puedo ayudar?&lt;/s&gt;',
          '&lt;|user|&gt;',
          'Me gustar√≠a saber c√≥mo funcionan los chat templates&lt;/s&gt;',
          '&lt;|assistant|&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As you can see it adds <code>&lt;|assistant|&gt;</code> at the end to help the LLM know that it is its turn to respond. This ensures that when the model generates text, it will write a bot response instead of doing something unexpected, such as continuing with the user's message</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Not all models require generation prompts. Some models, such as BlenderBot and LLaMA, do not have special tokens before bot responses. In these cases, <code>add_generation_prompt</code> will have no effect. The exact effect <code>add_generation_prompt</code> will have depends on the model being used.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Text-generation">Text generation<a class="anchor-link" href="#Text-generation"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 100" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As we can see it is easy to tokenize the context without needing to know how to do it for each model. So now let's see how to generate text is also very simple</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
      <span class="kn">import</span> <span class="nn">torch</span>
      
      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      
      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">{opening_brace}</span>
              <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>
              <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>
          <span class="p">{closing_brace},</span>
          <span class="p">{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">{closing_brace},</span>
       <span class="p">]</span>
      <span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_token_chat_template</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>
      Eres un chatbot amigable que siempre de una forma graciosa&lt;|endoftext|&gt;¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?&lt;|endoftext|&gt;Existen, eso s√≠, un tipo de aviones que necesitan el mismo peso que un ser humano de 30 u 40 kgs. Su estructura, su comportamiento, su tama√±o de vuelo ‚Ä¶ Leer m√°s
      El vuelo es una actividad con muchos riesgos. El miedo, la incertidumbre, el cansancio, el estr√©s, el miedo a volar, la dificultad de tomar una aeronave para aterrizar, el riesgo de ‚Ä¶ Leer m√°s
      Conducir un taxi es una tarea sencilla por su forma, pero tambi√©n por su complejidad. Por ello, los conductores de veh√≠culos de transporte que
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As you can see, the prompt has been tokenized with <code>apply_chat_template</code> and these tokens have been put into the model</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Text-generation-with-pipeline.">Text generation with <code>pipeline</code>.<a class="anchor-link" href="#Text-generation-with-pipeline."><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 101" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The <code>transformers</code> library also allows to use <code>pipeline</code> to generate text with a chatbot, doing underneath the same as we have done before</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
      <span class="kn">import</span> <span class="nn">torch</span>
      
      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>
      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">{opening_brace}</span>
              <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>
              <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>
          <span class="p">{closing_brace},</span>
          <span class="p">{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">{closing_brace},</span>
      <span class="p">]</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">generator</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'generated_text'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>
      {opening_brace}'role': 'assistant', 'content': 'La gran sorpresa que se di√≥ el viernes pasado fue conocer a uno de los jugadores m√°s codiciados por los jugadores de equipos de la NBA, Stephen Curry.\nCurry estaba junto a George Hill en el banquillo mientras que en las inmediaciones del vestuario, sobre el papel, estaba Larry Johnson y el entrenador Steve Kerr, quienes aprovecharon la ocasi√≥n para hablar de si mismo por Twitter.\nEn el momento en que Curry sali√≥ de la banca de Jordan, ambos hombres entraron caminando a la oficina del entrenador, de acuerdo con un testimonio'}
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Train">Train<a class="anchor-link" href="#Train"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 102" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>So far we have used pretrained models, but in case you want to do fine tuning, the <code>transformers</code> library makes it very easy to do it</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As nowadays language models are huge, retraining them is almost impossible on a GPU that anyone can have at home, so we are going to retrain a smaller model. In this case we are going to retrain <code>bert-base-cased</code> which is a 109M parameter model.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 103" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We need to download a dataset, for this we use the <code>datasets</code> library of Hugging Face. We are going to use the Yelp reviews dataset.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      <span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '      ',
      '      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
      '          <span class="p">{</span>',
      '              <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>',
      '              <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>',
      '          <span class="p">},</span>',
      '          <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>',
      '       <span class="p">]</span>',
      '      <span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_token_chat_template</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> ',
      '      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      <span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
      '      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
      '      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
      '          <span class="p">{</span>',
      '              <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>',
      '              <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>',
      '          <span class="p">},</span>',
      '          <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>',
      '      <span class="p">]</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="n">generator</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>',
      '<span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
      '      ',
      '      <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"yelp_review_full"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Let's see what the dataset looks like.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
          '<span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '',
          '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="p">{</span>',
          '        <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>',
          '        <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>',
          '    <span class="p">},</span>',
          '    <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>',
          ' <span class="p">]</span>',
          '<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '',
          '<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_token_chat_template</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> ',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '<span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
          '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
          '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="p">{</span>',
          '        <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>',
          '        <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>',
          '    <span class="p">},</span>',
          '    <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">generator</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>',
          '</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
          '',
          '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"yelp_review_full"</span><span class="p">)</span>',
          '</span><span class="nb">type</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input\'s `attention_mask` to obtain reliable results.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side=\'left\'` when initializing the tokenizer.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side=\'left\'` when initializing the tokenizer.',
          'datasets.dataset_dict.DatasetDict',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>It seems to be a kind of dictionary, let's see what keys it has.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">dataset</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'dict_keys([\'train\', \'test\'])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Let's see how many reviews you have in each subset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(650000, 50000)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Let's see a sample</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': 0,',
          ' \'text\': \'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \"serving off their orders\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As we can see each sample has the text and punctuation, let's see how many types there are</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">clases</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">features</span>',
          '<span class="n">clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': ClassLabel(names=[\'1 star\', \'2 star\', \'3 stars\', \'4 stars\', \'5 stars\'], id=None),',
          ' \'text\': Value(dtype=\'string\', id=None)}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see that it has 5 different classes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">clases</span><span class="p">[</span><span class="s2">"label"</span><span class="p">]</span><span class="o">.</span><span class="n">names</span><span class="p">)</span>',
          '<span class="n">num_classes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '5',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Let's see a sample test</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': 0,',
          ' \'text\': \'This was just bad pizza.  For the money I expect that the toppings will be cooked on the pizza.  The cheese and pepparoni were added after the crust came out.  Also the mushrooms were out of a can.  Do not waste money here.\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As the aim of this post is not to train the best model, but to explain the <code>transformers</code> library of Hugging Face, we are going to make a small subset to be able to train faster</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '      <span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenization">Tokenization<a class="anchor-link" href="#Tokenization"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 104" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We already have the dataset, as we have seen in the pipeline, first the tokenization is done and then the model is applied. So we have to tokenize the dataset.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We define the tokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '      <span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>The <code>AutoTokenizer</code> class has a method called <code>map</code> that allows us to apply a function to the dataset, so we are going to create a function that tokenizes the text</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '      <span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>',
      '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '          <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>As we can see at the moment we have tokenized truncating only 3 tokens, this is to be able to see better what is going on underneath</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We use the <code>map</code> method to use the function that we have just defined on the dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '      <span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>',
      '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '          <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
      '<span></span><span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '      <span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>We see examples of the tokenized dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
          '<span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>',
          '</span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
          '</span><span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '</span><span class="n">tokenized_small_train_dataset</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': 3,',
          ' \'text\': "I recently brough my car up to Edinburgh from home, where it had sat on the drive pretty much since I had left home to go to university. \n \nAs I\'m sure you can imagine, it was pretty filthy, so I pulled up here expecting to shell out  \u00a35 or so for a crappy was that wouldnt really be that great. \n \nNeedless to say, when I realised that the cheapest was was  \u00a32, i was suprised and I was even more suprised when the car came out looking like a million dollars. \n \nVery impressive for  \u00a32, but thier prices can go up to around  \u00a36 - which I\'m sure must involve so many polishes and waxes and cleans that dirt must be simply repelled from the body of your car, never getting dirty again.",',
          ' \'input_ids\': [101, 146, 102],',
          ' \'token_type_ids\': [0, 0, 0],',
          ' \'attention_mask\': [1, 1, 1]}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenized_small_eval_dataset</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': 4,',
          ' \'text\': \'Had a great dinner at Elephant Bar last night!  \n \nGot a coupon in the mail for 2 meals and an appetizer for $20! While they did limit the  selections you could get with the coupon, we were happy with the choices so it worked out fine. \n \nFood was delicious and the service was fantastic! Waitress was very attentive and polite. \n \nLocation was a plus too! Had a lovely walk around The District shops afterward.  \n \nAll and all, a hands down 5 stars!\',',
          ' \'input_ids\': [101, 6467, 102],',
          ' \'token_type_ids\': [0, 0, 0],',
          ' \'attention_mask\': [1, 1, 1]}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As we can see, a key has been added with the <code>input_ids</code> of the tokens, the <code>token_type_ids</code> and another one with the <code>attention mask</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We now tokenize by truncating to 20 tokens in order to use a small GPU.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '          <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>',
      '      ',
      '      <span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '      <span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <h3 id="Model">Model<a class="anchor-link" href="#Model"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 105" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We have to create the model that we are going to retrain. As it is a classification problem we are going to use <code>AutoModelForSequenceClassification</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>',
          '',
          '<span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [\'classifier.bias\', \'classifier.weight\']',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As can be seen, a model has been created which classifies between 5 classes</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Evaluation-metrics">Evaluation metrics<a class="anchor-link" href="#Evaluation-metrics"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 106" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create an evaluation metric with the <code>evaluate</code> library of Hugging Face. To install it we use</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>evaluate
      <span class="sb">```</span>
      </pre></div>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">import</span> <span class="nn">evaluate</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <h3 id="Trainer">Trainer<a class="anchor-link" href="#Trainer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 107" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now for training we use the <code>Trainer</code> object. In order to use <code>Trainer</code> we need <code>accelerate&gt;=0.21.0</code>.</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>accelerate&gt;<span class="o">=</span><span class="m">0</span>.21.0
      <span class="sb">```</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Before creating the trainer we have to create a <code>TrainingArguments</code> which is an object that contains all the arguments that <code>Trainer</code> needs to train, i.e. the hyperparameters</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A mandatory argument must be passed, <code>output_dir</code> which is the output directory where the model predictions and checkpoints, as Hugging Face calls the model weights, will be written.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We also pass on several other arguments</p>
      <ul>
      <li><code>per_device_train_batch_size</code>: size of batch per device for training</li>
      <li><code>per_device_eval_batch_size</code>: batch size per device for the evaluation</li>
      <li><code>learning_rate</code>: learning rate</li>
      <li><code>num_train_epochs</code>: number of epochs</li>
      </ul>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">import</span> <span class="nn">evaluate</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">output_dir</span><span class="o">=</span><span class="s2">"test_trainer"</span><span class="p">,</span> ',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> ',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <p>Let's take a look at all the hyperparameters that it configures</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
          '<span class="kn">import</span> <span class="nn">evaluate</span>',
          '',
          '<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
          '',
          '<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
          '    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
          '    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
          '    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
          '',
          '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
          '    <span class="n">output_dir</span><span class="o">=</span><span class="s2">"test_trainer"</span><span class="p">,</span> ',
          '    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> ',
          '    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
          '    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>',
          '    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '</span><span class="n">training_args</span><span class="o">.</span><span class="vm">__dict__</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'output_dir\': \'test_trainer\',',
          ' \'overwrite_output_dir\': False,',
          ' \'do_train\': False,',
          ' \'do_eval\': False,',
          ' \'do_predict\': False,',
          ' \'evaluation_strategy\': &lt;IntervalStrategy.NO: \'no\'&gt;,',
          ' \'prediction_loss_only\': False,',
          ' \'per_device_train_batch_size\': 16,',
          ' \'per_device_eval_batch_size\': 32,',
          ' \'per_gpu_train_batch_size\': None,',
          ' \'per_gpu_eval_batch_size\': None,',
          ' \'gradient_accumulation_steps\': 1,',
          ' \'eval_accumulation_steps\': None,',
          ' \'eval_delay\': 0,',
          ' \'learning_rate\': 0.0001,',
          ' \'weight_decay\': 0.0,',
          ' \'adam_beta1\': 0.9,',
          ' \'adam_beta2\': 0.999,',
          ' \'adam_epsilon\': 1e-08,',
          ' \'max_grad_norm\': 1.0,',
          ' \'num_train_epochs\': 5,',
          ' \'max_steps\': -1,',
          ' \'lr_scheduler_type\': &lt;SchedulerType.LINEAR: \'linear\'&gt;,',
          ' \'lr_scheduler_kwargs\': {},',
          ' \'warmup_ratio\': 0.0,',
          ' \'warmup_steps\': 0,',
          ' \'log_level\': \'passive\',',
          ' \'log_level_replica\': \'warning\',',
          ' \'log_on_each_node\': True,',
          ' \'logging_dir\': \'test_trainer/runs/Mar08_16-41-27_SAEL00531\',',
          ' \'logging_strategy\': &lt;IntervalStrategy.STEPS: \'steps\'&gt;,',
          ' \'logging_first_step\': False,',
          ' \'logging_steps\': 500,',
          ' \'logging_nan_inf_filter\': True,',
          ' \'save_strategy\': &lt;IntervalStrategy.STEPS: \'steps\'&gt;,',
          ' \'save_steps\': 500,',
          ' \'save_total_limit\': None,',
          ' \'save_safetensors\': True,',
          ' \'save_on_each_node\': False,',
          ' \'save_only_model\': False,',
          ' \'no_cuda\': False,',
          ' \'use_cpu\': False,',
          ' \'use_mps_device\': False,',
          ' \'seed\': 42,',
          ' \'data_seed\': None,',
          ' \'jit_mode_eval\': False,',
          ' \'use_ipex\': False,',
          ' \'bf16\': False,',
          ' \'fp16\': False,',
          ' \'fp16_opt_level\': \'O1\',',
          ' \'half_precision_backend\': \'auto\',',
          ' \'bf16_full_eval\': False,',
          ' \'fp16_full_eval\': False,',
          ' \'tf32\': None,',
          ' \'local_rank\': 0,',
          ' \'ddp_backend\': None,',
          ' \'tpu_num_cores\': None,',
          ' \'tpu_metrics_debug\': False,',
          ' \'debug\': [],',
          ' \'dataloader_drop_last\': False,',
          ' \'eval_steps\': None,',
          ' \'dataloader_num_workers\': 0,',
          ' \'dataloader_prefetch_factor\': None,',
          ' \'past_index\': -1,',
          ' \'run_name\': \'test_trainer\',',
          ' \'disable_tqdm\': False,',
          ' \'remove_unused_columns\': True,',
          ' \'label_names\': None,',
          ' \'load_best_model_at_end\': False,',
          ' \'metric_for_best_model\': None,',
          ' \'greater_is_better\': None,',
          ' \'ignore_data_skip\': False,',
          ' \'fsdp\': [],',
          ' \'fsdp_min_num_params\': 0,',
          ' \'fsdp_config\': {\'min_num_params\': 0,',
          '  \'xla\': False,',
          '  \'xla_fsdp_v2\': False,',
          '  \'xla_fsdp_grad_ckpt\': False},',
          ' \'fsdp_transformer_layer_cls_to_wrap\': None,',
          ' \'accelerator_config\': AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True),',
          ' \'deepspeed\': None,',
          ' \'label_smoothing_factor\': 0.0,',
          ' \'optim\': &lt;OptimizerNames.ADAMW_TORCH: \'adamw_torch\'&gt;,',
          ' \'optim_args\': None,',
          ' \'adafactor\': False,',
          ' \'group_by_length\': False,',
          ' \'length_column_name\': \'length\',',
          ' \'report_to\': [],',
          ' \'ddp_find_unused_parameters\': None,',
          ' \'ddp_bucket_cap_mb\': None,',
          ' \'ddp_broadcast_buffers\': None,',
          ' \'dataloader_pin_memory\': True,',
          ' \'dataloader_persistent_workers\': False,',
          ' \'skip_memory_metrics\': True,',
          ' \'use_legacy_prediction_loop\': False,',
          ' \'push_to_hub\': False,',
          ' \'resume_from_checkpoint\': None,',
          ' \'hub_model_id\': None,',
          ' \'hub_strategy\': &lt;HubStrategy.EVERY_SAVE: \'every_save\'&gt;,',
          ' \'hub_token\': None,',
          ' \'hub_private_repo\': False,',
          ' \'hub_always_push\': False,',
          ' \'gradient_checkpointing\': False,',
          ' \'gradient_checkpointing_kwargs\': None,',
          ' \'include_inputs_for_metrics\': False,',
          ' \'fp16_backend\': \'auto\',',
          ' \'push_to_hub_model_id\': None,',
          ' \'push_to_hub_organization\': None,',
          ' \'push_to_hub_token\': None,',
          ' \'mp_parameters\': \'\',',
          ' \'auto_find_batch_size\': False,',
          ' \'full_determinism\': False,',
          ' \'torchdynamo\': None,',
          ' \'ray_scope\': \'last\',',
          ' \'ddp_timeout\': 1800,',
          ' \'torch_compile\': False,',
          ' \'torch_compile_backend\': None,',
          ' \'torch_compile_mode\': None,',
          ' \'dispatch_batches\': None,',
          ' \'split_batches\': None,',
          ' \'include_tokens_per_second\': False,',
          ' \'include_num_input_tokens_seen\': False,',
          ' \'neftune_noise_alpha\': None,',
          ' \'distributed_state\': Distributed environment: DistributedType.NO',
          ' Num processes: 1',
          ' Process index: 0',
          ' Local process index: 0',
          ' Device: cuda,',
          ' \'_n_gpu\': 1,',
          ' \'__cached__setup_devices\': device(type=\'cuda\', index=0),',
          ' \'deepspeed_plugin\': None}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now we create a <code>Trainer</code> object that will be in charge of training the model.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '          <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <p>Once we have a <code>Trainer</code>, in which we have indicated the training dataset, the test dataset, the model, the evaluation metric and the arguments with the hyperparameters, we can train the model with the <code>train</code> method of the <code>Trainer</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>  0%|          | 0/315 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>{opening_brace}'train_runtime': 52.3517, 'train_samples_per_second': 95.508, 'train_steps_per_second': 6.017, 'train_loss': 0.9347671750992064, 'epoch': 5.0{closing_brace}
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[21]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>TrainOutput(global_step=315, training_loss=0.9347671750992064, metrics={opening_brace}'train_runtime': 52.3517, 'train_samples_per_second': 95.508, 'train_steps_per_second': 6.017, 'train_loss': 0.9347671750992064, 'epoch': 5.0{closing_brace})</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We already have the model trained, as you can see, with very little code we can train a model very quickly.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>I strongly advise learning Pytorch and training many models before using a high-level library like <code>transformers</code>, because you learn a lot of deep learning fundamentals and you can understand better what is going on, especially because you will learn a lot from your mistakes. But once you have gone through that period, using high-level libraries like <code>transformers</code> speeds up the development a lot.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Testing-the-model">Testing the model<a class="anchor-link" href="#Testing-the-model"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 108" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now that we have the model trained, let's test it with a text. As the dataset we have downloaded is of reviews in English, let's test it with a review in English</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '          <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">clasificator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-classification"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
          '',
          '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
          '    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
          '    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
          '    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
          '    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
          '    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">clasificator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-classification"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
          '</span><span class="n">clasification</span> <span class="o">=</span> <span class="n">clasificator</span><span class="p">(</span><span class="s2">"I\'m liking this post a lot"</span><span class="p">)</span>',
          '<span class="n">clasification</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '  0%|          | 0/315 [00:00&lt;?, ?it/s]',
          '[{\'label\': \'LABEL_2\', \'score\': 0.5032550692558289}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Let's see what the new class corresponds to.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': ClassLabel(names=[\'1 star\', \'2 star\', \'3 stars\', \'4 stars\', \'5 stars\'], id=None),',
          ' \'text\': Value(dtype=\'string\', id=None)}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>The relationship would be</p>
      <ul>
      <li>LABEL_0: 1 star</li>
      <li>LABEL_1: 2 stars</li>
      <li>LABEL_2: 3 stars</li>
      <li>LABEL_3: 4 stars</li>
      <li>LABEL_4: 5 stars</li>
      </ul>
      <p>So you have rated the comment with 3 stars. Recall that we have is trained on a subset of data and with only 5 epochs, so we do not expect it to be very good.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Share-the-model-in-the-Hugging-Face-Hub">Share the model in the Hugging Face Hub<a class="anchor-link" href="#Share-the-model-in-the-Hugging-Face-Hub"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 109" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Once we have the retrained model we can upload it to our space in the Hugging Face Hub so that others can use it. To do this you need to have a Hugging Face account.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Logging">Logging<a class="anchor-link" href="#Logging"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 110" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In order to upload the model we first have to log in.</p>
      <p>This can be done through the terminal with</p>
      <div class="highlight"><pre><span></span>huggingface-cli<span class="w"> </span>login
      <span class="sb">```</span>
      
      Or<span class="w"> </span>through<span class="w"> </span>the<span class="w"> </span>notebook<span class="w"> </span>having<span class="w"> </span>previously<span class="w"> </span>installed<span class="w"> </span>the<span class="w"> </span><span class="sb">`</span>huggingface_hub<span class="sb">`</span><span class="w"> </span>library<span class="w"> </span>with
      
      <span class="sb">````</span>bash
      pip<span class="w"> </span>install<span class="w"> </span>huggingface_hub
      <span class="sb">```</span>
      
      Now<span class="w"> </span>we<span class="w"> </span>can<span class="w"> </span>log<span class="w"> </span><span class="k">in</span><span class="w"> </span>with<span class="w"> </span>the<span class="w"> </span><span class="sb">`</span>notebook_login<span class="sb">`</span><span class="w"> </span><span class="k">function</span>,<span class="w"> </span>which<span class="w"> </span>will<span class="w"> </span>create<span class="w"> </span>a<span class="w"> </span>small<span class="w"> </span>graphical<span class="w"> </span>interface<span class="w"> </span>where<span class="w"> </span>we<span class="w"> </span>have<span class="w"> </span>to<span class="w"> </span>enter<span class="w"> </span>a<span class="w"> </span>Hugging<span class="w"> </span>Face<span class="w"> </span>token.
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To create a token, go to the <a href="https://huggingface.co/settings/tokens" target="_blank" rel="nofollow noreferrer">setings/tokens</a> page of your account, you will see something like this</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="User-Access-Token-dark" src="http://maximofn.com/wp-content/uploads/2024/03/User-Access-Token-dark.png" width="1200" height="615"/></p>
      <p>Click on <code>New token</code> and a window will appear to create a new token.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="new-token-dark" src="http://maximofn.com/wp-content/uploads/2024/03/new-token-dark.png" width="1200" height="660"/></p>
      <p>We name the token and create it with the <code>write</code> role.</p>
      <p>Once created, we copy it</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
          '',
          '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'VBox(children=(HTML(value=\'&lt;center&gt; &lt;img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Up-once-trained">Up once trained<a class="anchor-link" href="#Up-once-trained"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 111" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As we have trained the model we can upload it to the Hub using the <code>push_to_hub</code> function. This function has a mandatory parameter which is the name of the model, which has to be unique, if there is already a model in your Hub with that name it will not be uploaded. That is, the full name of the model will be <user>/<model>, so the model name cannot exist in your Hub, even if there is another model with the same name in the Hub of another user.</model></user></p>
      <p>It also has other optional, but interesting, parameters:</p>
      <ul>
      <li><code>use_temp_dir</code> (bool, optional): Whether or not to use a temporary directory to store the saved files before they are sent to the Hub. Default will be True if there is no directory with the same name as <code>repo_id</code>, False otherwise.</li>
      <li><code>commit_message</code> (str, optional): Commit message. Default will be <code>Upload {opening_brace}object{closing_brace}</code>.</li>
      <li><code>private</code> (bool, optional): Whether the created repository should be private or not.</li>
      <li><code>token</code> (bool or str, optional): The token to use as HTTP authorization for remote files. If True, the token generated by running <code>huggingface-cli</code> login (stored in ~/.huggingface) will be used. Default will be True if <code>repo_url</code> is not specified.</li>
      <li><code>max_shard_size</code> (int or str, optional, defaults to "5GB"): Only applicable to models. The maximum size of a checkpoint before it is fragmented. Fragmented checkpoints will each be smaller than this size. If expressed as a string, it must have digits followed by a unit (such as "5MB"). The default is "5GB" so that users can easily load models into free-level Google Colab instances without CPU OOM (out of memory) issues.</li>
      <li><code>create_pr</code> (bool, optional, defaults to False): Whether or not to create a PR with the uploaded files or commit directly.</li>
      <li><code>safe_serialization</code> (bool, optional, defaults to True): Whether or not to convert model weights to safetensors format for safer serialization.</li>
      <li><code>revision</code> (str, optional): Branch to send uploaded files to.</li>
      <li><code>commit_description</code> (str, optional): Description of the commit to be created</li>
      <li><code>tags</code> (List[str], optional): List of tags to insert in Hub.</li>
      </ul>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span>
          <span class="s2">"bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset"</span><span class="p">,</span> 
          <span class="n">commit_message</span><span class="o">=</span><span class="s2">"bert base cased fine tune on yelp review subset"</span><span class="p">,</span>
          <span class="n">commit_description</span><span class="o">=</span><span class="s2">"Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs."</span><span class="p">,</span>
      <span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>README.md:   0%|          | 0.00/5.18k [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>model.safetensors:   0%|          | 0.00/433M [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[26]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset/commit/033a3c759d5a4e314ce76db81bd113b4f7da69ad', commit_message='bert base cased fine tune on yelp review subset', commit_description='Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs.', oid='033a3c759d5a4e314ce76db81bd113b4f7da69ad', pr_url=None, pr_revision=None, pr_num=None)</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we now go to our Hub we can see that the model has been uploaded.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformers_commit_unique" src="http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_unico.webp" width="1163" height="151"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we now go into the model card to see</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformers_commit_inico_model_card" src="http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_inico_model_card-scaled.webp" width="1200" height="617"/></p>
      <p>We see that everything is unfilled, later we will do this</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Climbing-while-training">Climbing while training<a class="anchor-link" href="#Climbing-while-training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 112" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Another option is to upload it while we are training the model. This is very useful when we train models for many periods and it takes us a long time, because if the training is stopped (because the computer is turned off, the colab session is finished, the cloud credits run out) the work is not lost. To do this you have to add <code>push_to_hub=True</code> in the <code>TrainingArguments</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span>',
      '          <span class="s2">"bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset"</span><span class="p">,</span> ',
      '          <span class="n">commit_message</span><span class="o">=</span><span class="s2">"bert base cased fine tune on yelp review subset"</span><span class="p">,</span>',
      '          <span class="n">commit_description</span><span class="o">=</span><span class="s2">"Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs."</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">output_dir</span><span class="o">=</span><span class="s2">"bert-base-cased_notebook_transformers_30-epochs_yelp_review_subset"</span><span class="p">,</span> ',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> ',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '          <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>





















      
      <section class="section-block-markdown-cell">
      <p>We can see that we have changed the epochs to 30, so training is going to take longer, so adding <code>push_to_hub=True</code> will upload the model to our Hub while training.</p>
      <p>We have also changed the <code>output_dir</code> because it is the name that the model will have in the Hub.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>  0%|          | 0/1890 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>{opening_brace}'loss': 0.2363, 'grad_norm': 8.151028633117676, 'learning_rate': 7.354497354497355e-05, 'epoch': 7.94{closing_brace}
      {opening_brace}'loss': 0.0299, 'grad_norm': 0.0018280998338013887, 'learning_rate': 4.708994708994709e-05, 'epoch': 15.87}
      {opening_brace}'loss': 0.0019, 'grad_norm': 0.000868947128765285, 'learning_rate': 2.0634920634920636e-05, 'epoch': 23.81}
      {opening_brace}'train_runtime': 331.5804, 'train_samples_per_second': 90.476, 'train_steps_per_second': 5.7, 'train_loss': 0.07100234655318437, 'epoch': 30.0}
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[24]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>TrainOutput(global_step=1890, training_loss=0.07100234655318437, metrics={opening_brace}'train_runtime': 331.5804, 'train_samples_per_second': 90.476, 'train_steps_per_second': 5.7, 'train_loss': 0.07100234655318437, 'epoch': 30.0{closing_brace})</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>If we look at our hub again, the new model is now displayed.</p>
      <p>transformers_commit_training](<a href="http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_training.webp">http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_training.webp</a>)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Hub-as-git-repository">Hub as git repository<a class="anchor-link" href="#Hub-as-git-repository"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 113" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In Hugging Face both models, spaces and datasets are git repositories, so you can work with them like that. That is, you can clone, make forks, pull requests, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>But another great advantage of this is that you can use a model in a particular version.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
      
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s2">"393e083"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>config.json:   0%|          | 0.00/433 [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>pytorch_model.bin:   0%|          | 0.00/436M [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
      You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
      </pre>
      </div>
      </div>
      </div>
      </section>
      






    </div>

  </section>

</PostLayout>
