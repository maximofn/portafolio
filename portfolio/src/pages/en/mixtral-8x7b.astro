---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Mixtral-8x7B';
const end_url = 'mixtral-8x7b';
const description = 'Discover the fashion model in the world of AI';
const keywords = 'mixtral-8x7b, mixtral, 8x7b, mistral ai, transformer, MoE, mixture of experts, huggingface, gradio';
const languaje = 'EN';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Mixtral-8x7B.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1792
    image_height=1024
    image_extension=webp
    article_date=2023-12-30+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Number-of-parameters"><h2>Number of parameters</h2></a>
      <a class="anchor-link" href="#Mixture-of-Experts-(MoE)"><h2>Mixture of Experts (MoE)</h2></a>
      <a class="anchor-link" href="#Use-of-Mixtral-8x7b-in-the-cloud"><h2>Use of Mixtral-8x7b in the cloud</h2></a>
      <a class="anchor-link" href="#Use-of-Mixtral-8x7b-in-huggingface-chat"><h3>Use of Mixtral-8x7b in huggingface chat</h3></a>
      <a class="anchor-link" href="#Using-Mixtral-8x7b-in-Perplexity-Labs"><h3>Using Mixtral-8x7b in Perplexity Labs</h3></a>
      <a class="anchor-link" href="#Using-Mixtral-8x7b-locally-via-the-huggingface-API"><h2>Using Mixtral-8x7b locally via the huggingface API</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Mixtral-8x7B-MoE">Mixtral-8x7B MoE<a class="anchor-link" href="#Mixtral-8x7B-MoE"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>For me the best description of <code>mixtral-8x7b</code> is the following picture</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="mixtral-gemini" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mixtral-gemini.webp" width="500" height="389"/></p>
      <p>Between the release of <code>gemini</code> and the release of <code>mixtra-8x7b</code> there was a difference of only a few days. The first two days after the release of <code>gemini</code> there was a lot of talk about that model, but as soon as <code>mixtral-8x7b</code> was released, <code>gemini</code> was completely forgotten and the whole community was talking about <code>mixtral-8x7b</code>.</p>
      <p>And no wonder, looking at its benchmarks, we can see that it is at the level of models such as <code>llama2-70B</code> and <code>GPT3.5</code>, but with the difference that while <code>mixtral-8x7b</code> has only 46.7B of parameters, <code>llama2-70B</code> has 70B and <code>GPT3.5</code> has 175B.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="mixtral benchmarks" src="https://mistral.ai/images/news/mixtral-of-experts/overview.png" width="2460" height="1584"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
      <h2 id="Number-of-parameters">Number of parameters<a class="anchor-link" href="#Number-of-parameters"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As the name suggests, <code>mixtral-8x7b</code> is a set of 8 models of 7B parameters, so we could think that it has 56B parameters (7Bx8), but it is not. As <a href="https://twitter.com/karpathy/status/1734251375163511203" target="_blank" rel="nofollow noreferrer">Andrej Karpathy</a> explains, only the <code>Feed forward</code> blocks of the transformers are multiplied by 8, the rest of the parameters are shared among the 8 models. So in the end the model has 46.7B parameters.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Mixture-of-Experts-(MoE)">Mixture of Experts (MoE)<a class="anchor-link" href="#Mixture-of-Experts-(MoE)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As we have said, the model is a set of 8 models of 7B parameters, hence <code>MoE</code>, which stands for <code>Mixture of Experts</code>. Each of the 8 models is trained independently, but when inference is done a router decides the output of which model is the one to be used.</p>
      <p>The following image shows the architecture of a <code>Transformer</code>.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" width="852" height="1200"/></p>
      <p>If you don't know it, the important thing is that this architecture consists of an encoder and a decoder.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer-encoder-decoder" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-encoder-decoder.webp" width="588" height="428"/></p>
      <p>LLMs are decoder-only models, so they do not have an encoder. You can see that in the architecture there are three attention modules, one of them actually connects the encoder to the decoder. But since LLMs do not have an encoder, there is no need for the attention module that connects the decoder and the decoder.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer-decoder" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-decoder.webp" width="1200" height="664"/></p>
      <p>Now that we know what the architecture of an LLM looks like, we can see what the architecture of <code>mixtral-8x7b</code> looks like. In the following image we can see the architecture of the model</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="MoE architecture" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/MoE_architecture.webp" width="1274" height="982"/></p>
      <p>As you can see, the architecture consists of a 7B parameter <code>Transformer</code> decoder, only the <code>Feed forward</code> layer consists of 8 <code>Feed forward</code> layers with a router that chooses which of the 8 <code>Feed forward</code> layers to use. In the above image only four <code>Feed forward</code> layers are shown, I suppose this is to simplify the diagram, but in reality there are 8 <code>Feed forward</code> layers. You can also see two paths for two different words, the word <code>More</code> and the word <code>Parameters</code> and how the router chooses which <code>Feed forward</code> to use for each word.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Looking at the architecture we can understand why the model has 46.7B parameters and not 56B. As we have said, only the <code>Feed forward</code> blocks are multiplied by 8, the rest of the parameters are shared among the 8 models.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Use-of-Mixtral-8x7b-in-the-cloud">Use of Mixtral-8x7b in the cloud<a class="anchor-link" href="#Use-of-Mixtral-8x7b-in-the-cloud"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Unfortunately, using <code>mixtral-8x7b</code> locally is complicated because the hardware requirements are as follows</p>
      <ul>
      <li>float32: VRAM &gt; 180 GB, i.e., since each parameter occupies 4 bytes, we need 46.7B * 4 = 186.8 GB of VRAM just to store the model, plus the VRAM needed to store the input and output data.</li>
      <li>float16: VRAM &gt; 90 GB, in this case each parameter occupies 2 bytes, so we need 46.7B * 2 = 93.4 GB of VRAM just to store the model, plus the VRAM needed to store the input and output data.</li>
      <li>8-bit: VRAM &gt; 45 GB, here each parameter occupies 1 byte, so we need 46.7B * 1 = 46.7 GB of VRAM just to store the model, plus the VRAM needed to store the input and output data.</li>
      <li>4-bit: VRAM &gt; 23 GB, here each parameter occupies 0.5 bytes, so we need 46.7B * 0.5 = 23.35 GB of VRAM just to store the model, plus the VRAM needed to store the input and output data.</li>
      </ul>
      <p>We need very powerful GPUs to run it, even when using the 4-bit quantized model.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>So, the easiest way to use <code>Mixtral-8x7B</code> is to use it already deployed in the cloud. I have found several sites where you can use it</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Use-of-Mixtral-8x7b-in-huggingface-chat">Use of Mixtral-8x7b in huggingface chat<a class="anchor-link" href="#Use-of-Mixtral-8x7b-in-huggingface-chat"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The first one is in <a href="https://huggingface.co/chat" target="_blank" rel="nofollow noreferrer">huggingface chat</a>. To use it you have to click on the cogwheel inside the <code>Current Model</code> box and select <code>Mistral AI - Mixtral-8x7B</code>. Once selected, you can start talking to the model.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="huggingface_chat_01" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_chat_01.webp" width="1111" height="826"/></p>
      <p>Once inside select <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> and finally click on the <code>Activate</code> button. Now we can test the model</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="huggingface_chat_02" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_chat_02.webp" width="967" height="617"/></p>
      <p>As you can see, I asked him in Spanish what <code>MoE</code> is and he explained it to me.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Using-Mixtral-8x7b-in-Perplexity-Labs">Using Mixtral-8x7b in Perplexity Labs<a class="anchor-link" href="#Using-Mixtral-8x7b-in-Perplexity-Labs"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Another option is to use <a href="https://labs.perplexity.ai/" target="_blank" rel="nofollow noreferrer">Perplexity Labs</a>. Once inside you have to select <code>mixtral-8x7b-instruct</code> in a drop-down menu in the lower right corner.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="perplexity_labs" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/perplexity_labs.webp" width="1012" height="943"/></p>
      <p>As you can see, I also asked him in Spanish what <code>MoE</code> is and he explained it to me.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Using-Mixtral-8x7b-locally-via-the-huggingface-API">Using Mixtral-8x7b locally via the huggingface API<a class="anchor-link" href="#Using-Mixtral-8x7b-locally-via-the-huggingface-API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>One way to use it locally, whatever HW resources you have, is through the huggingface API. To do this you have to install the <code>huggingface-hub</code> library of huggingface</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">%</span><span class="k">pip</span> install huggingface-hub',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Here is an implementation with <code>gradio</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">%</span><span class="k">pip</span> install huggingface-hub',
      '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>',
      '      <span class="kn">import</span> <span class="nn">gradio</span> <span class="k">as</span> <span class="nn">gr</span>',
      '      ',
      '      <span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="s2">"mistralai/Mixtral-8x7B-Instruct-v0.1"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">format_prompt</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">history</span><span class="p">):</span>',
      '        <span class="n">prompt</span> <span class="o">=</span> <span class="s2">"&lt;s&gt;"</span>',
      '        <span class="k">for</span> <span class="n">user_prompt</span><span class="p">,</span> <span class="n">bot_response</span> <span class="ow">in</span> <span class="n">history</span><span class="p">:</span>',
      '          <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"[INST] </span><span class="si">{</span><span class="n">user_prompt</span><span class="si">}</span><span class="s2"> [/INST]"</span>',
      '          <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">" </span><span class="si">{</span><span class="n">bot_response</span><span class="si">}</span><span class="s2">&lt;/s&gt; "</span>',
      '        <span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">"[INST] </span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s2"> [/INST]"</span>',
      '        <span class="k">return</span> <span class="n">prompt</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">history</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,):</span>',
      '          <span class="n">temperature</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span>',
      '          <span class="k">if</span> <span class="n">temperature</span> <span class="o">&lt;</span> <span class="mf">1e-2</span><span class="p">:</span>',
      '              <span class="n">temperature</span> <span class="o">=</span> <span class="mf">1e-2</span>',
      '          <span class="n">top_p</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">top_p</span><span class="p">)</span>',
      '      ',
      '          <span class="n">generate_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,)</span>',
      '      ',
      '          <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">format_prompt</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">system_prompt</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="n">history</span><span class="p">)</span>',
      '          <span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">text_generation</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_full_text</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>',
      '          <span class="n">output</span> <span class="o">=</span> <span class="s2">""</span>',
      '      ',
      '          <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>',
      '              <span class="n">output</span> <span class="o">+=</span> <span class="n">response</span><span class="o">.</span><span class="n">token</span><span class="o">.</span><span class="n">text</span>',
      '              <span class="k">yield</span> <span class="n">output</span>',
      '          <span class="k">return</span> <span class="n">output</span>',
      '      ',
      '      <span class="n">additional_inputs</span><span class="o">=</span><span class="p">[</span>',
      '          <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"System Prompt"</span><span class="p">,</span> <span class="n">max_lines</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,),</span>',
      '          <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Temperature"</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">"Higher values produce more diverse outputs"</span><span class="p">),</span>',
      '          <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Max new tokens"</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">1048</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">"The maximum numbers of new tokens"</span><span class="p">),</span>',
      '          <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Top-p (nucleus sampling)"</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.90</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">"Higher values sample more low-probability tokens"</span><span class="p">),</span>',
      '          <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Repetition penalty"</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">"Penalize repeated tokens"</span><span class="p">)</span>',
      '      <span class="p">]</span>',
      '      ',
      '      <span class="n">gr</span><span class="o">.</span><span class="n">ChatInterface</span><span class="p">(</span>',
      '          <span class="n">fn</span><span class="o">=</span><span class="n">generate</span><span class="p">,</span>',
      '          <span class="n">chatbot</span><span class="o">=</span><span class="n">gr</span><span class="o">.</span><span class="n">Chatbot</span><span class="p">(</span><span class="n">show_label</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_share_button</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_copy_button</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">likeable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="s2">"panel"</span><span class="p">),</span>',
      '          <span class="n">additional_inputs</span><span class="o">=</span><span class="n">additional_inputs</span><span class="p">,</span>',
      '          <span class="n">title</span><span class="o">=</span><span class="s2">"Mixtral 46.7B"</span><span class="p">,</span>',
      '          <span class="n">concurrency_limit</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>',
      '      <span class="p">)</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">show_api</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>


















































      
      <section class="section-block-markdown-cell">
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Mixtral-8x7B huggingface API" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Mixtral-8x7B_huggingface_API.webp" width="1505" height="683"/></p>
      </section>
      






    </div>

  </section>

</PostLayout>
