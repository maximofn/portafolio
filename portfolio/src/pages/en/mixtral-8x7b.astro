---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Mixtral-8x7B';
const end_url = 'mixtral-8x7b';
const description = 'Discover the fashion model in the world of AI';
const keywords = 'mixtral-8x7b, mixtral, 8x7b, mistral ai, transformer, MoE, mixture of experts, huggingface, gradio';
const languaje = 'EN';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Mixtral-8x7B.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1792
    image_height=1024
    image_extension=webp
    article_date=2023-12-30+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Number of parameters"><h2>Number of parameters</h2></a>
      <a class="anchor-link" href="#Mixture of Experts (MoE)"><h2>Mixture of Experts (MoE)</h2></a>
      <a class="anchor-link" href="#Using Mixtral-8x7b in the Cloud"><h2>Using Mixtral-8x7b in the Cloud</h2></a>
      <a class="anchor-link" href="#Using Mixtral-8x7b in Hugging Face Chat"><h3>Using Mixtral-8x7b in Hugging Face Chat</h3></a>
      <a class="anchor-link" href="#Use of Mixtral-8x7b in Perplexity Labs"><h3>Use of Mixtral-8x7b in Perplexity Labs</h3></a>
      <a class="anchor-link" href="#Using Mixtral-8x7b Locally via the Hugging Face API"><h2>Using Mixtral-8x7b Locally via the Hugging Face API</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>For me, the best description of <code>mixtral-8x7b</code> is the following image</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mixtral-gemini.webp" alt="mixtral-gemini">
      <p>There was only a very short gap between the release of <code>gemini</code> and the release of <code>mixtral-8x7b</code>. For the first two days after <code>gemini</code> launched, there was quite a bit of talk about that model, but as soon as <code>mixtral-8x7b</code> came out, <code>gemini</code> was completely forgotten, and the entire community couldn't stop talking about <code>mixtral-8x7b</code>.</p>
      <p>And it's no wonder, looking at its benchmarks, we can see that it is on par with models like <code>llama2-70B</code> and <code>GPT3.5</code>, but with the difference being that while <code>mixtral-8x7b</code> has only 46.7B parameters, <code>llama2-70B</code> has 70B and <code>GPT3.5</code> has 175B.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mixtral_8x7b_benchmark.webp" alt="mixtral benchmarks">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Number of parameters">Number of parameters<a class="anchor-link" href="#Number of parameters"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As the name suggests, <code>mixtral-8x7b</code> is a set of 8 models with 7B parameters each, so we might think it has 56B parameters (7Bx8), but that's not the case. As <a href="https://twitter.com/karpathy/status/1734251375163511203" target="_blank" rel="nofollow noreferrer">Andrej Karpathy</a> explains, only the <code>Feed forward</code> blocks of the transformers are multiplied by 8, while the rest of the parameters are shared among the 8 models. Therefore, the final model has 46.7B parameters.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Mixture of Experts (MoE)">Mixture of Experts (MoE)<a class="anchor-link" href="#Mixture of Experts (MoE)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As we have said, the model is a set of 8 models with 7B parameters, hence <code>MoE</code>, which stands for <code>Mixture of Experts</code>. Each of the 8 models is trained independently, but during inference, a router decides which model's output will be used.</p>
      <p>In the following image we can see what the architecture of a <code>Transformer</code> looks like.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" alt="transformer">
      <p>If you don't know it, that's fine. The important thing is that this architecture consists of an encoder and a decoder.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-encoder-decoder.webp" alt="transformer-encoder-decoder">
      <p>LLMs are models that only have the decoder, so they do not have an encoder. You can see that in the architecture there are three attention modules, one of which actually connects the encoder with the decoder. But since LLMs do not have an encoder, the attention module that connects the decoder and the decoder is not necessary.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-decoder.webp" alt="transformer-decoder">
      <p>Now that we know how the architecture of an LLM works, we can look at the architecture of <code>mixtral-8x7b</code>. In the following image, we can see the architecture of the model.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/MoE_architecture.webp" alt="MoE architecture">
      <p>As can be seen, the architecture consists of the decoder of a 7B parameter <code>Transformer</code>, except that the <code>Feed forward</code> layer consists of 8 <code>Feed forward</code> layers with a router that chooses which of the 8 <code>Feed forward</code> layers will be used. In the previous image, only four <code>Feed forward</code> layers are shown, I suppose this is to simplify the diagram, but in reality there are 8 <code>Feed forward</code> layers. Two paths for two different words are also visible, the word <code>More</code> and the word <code>Parameters</code>, and how the router chooses which <code>Feed forward</code> layer will be used for each word.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Looking at the architecture, we can understand why the model has 46.7B parameters and not 56B. As we have said, only the <code>Feed forward</code> blocks are multiplied by 8, the rest of the parameters are shared among the 8 models.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Using Mixtral-8x7b in the Cloud">Using Mixtral-8x7b in the Cloud<a class="anchor-link" href="#Using Mixtral-8x7b in the Cloud"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Unfortunately, using <code>mixtral-8x7b</code> locally is complicated due to the following hardware requirements:</p>
      <ul>
        <li>float32: VRAM > 180 GB, that is, since each parameter occupies 4 bytes, we need 46.7B * 4 = 186.8 GB of VRAM just to store the model, in addition to this, we must add the VRAM required to store the input and output data</li>
        <li>float16: VRAM > 90 GB, in this case each parameter takes up 2 bytes, so we need 46.7B * 2 = 93.4 GB of VRAM just to store the model, and on top of that, we need to add the VRAM required to store the input and output data</li>
        <li>8-bit: VRAM > 45 GB, here each parameter takes up 1 byte, so we need 46.7B * 1 = 46.7 GB of VRAM just to store the model, in addition to that, we need to add the VRAM required to store the input and output data* 4-bit: VRAM > 23 GB, here each parameter takes up 0.5 bytes, so we need 46.7B * 0.5 = 23.35 GB of VRAM just to store the model, in addition to that, we have to add the VRAM needed to store the input and output data</li>
      </ul>
      <p>We need some very powerful GPUs to be able to run it, even when we use the model quantized to 4 bits.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Therefore, the simplest way to use <code>Mixtral-8x7B</code> is by using it already deployed in the cloud. I have found several sites where you can use it</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Using Mixtral-8x7b in Hugging Face Chat">Using Mixtral-8x7b in Hugging Face Chat<a class="anchor-link" href="#Using Mixtral-8x7b in Hugging Face Chat"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The first one is on <a href="https://huggingface.co/chat" target="_blank" rel="nofollow noreferrer">huggingface chat</a>. To use it, you need to click on the gear icon inside the <code>Current Model</code> box and select <code>Mistral AI - Mixtral-8x7B</code>. Once selected, you can start chatting with the model.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_chat_01.webp" alt="huggingface_chat_01">
      <p>Once inside, select <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> and finally click the <code>Activate</code> button. Now we can test the model.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_chat_02.webp" alt="huggingface_chat_02">
      <p>As you can see, I asked him in Spanish what <code>MoE</code> is and he explained it to me.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Use of Mixtral-8x7b in Perplexity Labs">Use of Mixtral-8x7b in Perplexity Labs<a class="anchor-link" href="#Use of Mixtral-8x7b in Perplexity Labs"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Another option is to use <a href="https://labs.perplexity.ai/" target="_blank" rel="nofollow noreferrer">Perplexity Labs</a>. Once inside, you need to select <code>mixtral-8x7b-instruct</code> from a dropdown menu in the bottom right corner.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/perplexity_labs.webp" alt="perplexity_labs">
      <p>As you can see, I also asked him in Spanish what <code>MoE</code> is and he explained it to me.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Using Mixtral-8x7b Locally via the Hugging Face API">Using Mixtral-8x7b Locally via the Hugging Face API<a class="anchor-link" href="#Using Mixtral-8x7b Locally via the Hugging Face API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A way to use it locally, regardless of the hardware resources you have, is through the huggingface API. For this, you need to install the <code>huggingface-hub</code> library from huggingface.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="n">huggingface</span><span class="o">-</span><span class="n">hub</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Here is an implementation with <code>gradio</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceClient</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">gradio</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gr</span>',
      '<span class="w"> </span>',
      '<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="s2">&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">format_prompt</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">history</span><span class="p">):</span>',
      '<span class="w">  </span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&amp;lt;s&amp;gt;&quot;</span>',
      '<span class="w">  </span><span class="k">for</span> <span class="n">user_prompt</span><span class="p">,</span> <span class="n">bot_response</span> <span class="ow">in</span> <span class="n">history</span><span class="p">:</span>',
      '<span class="w">    </span><span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;[INST] </span><span class="si">{</span><span class="n">user_prompt</span><span class="si">}</span><span class="s2"> [/INST]&quot;</span>',
      '<span class="w">    </span><span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">bot_response</span><span class="si">}</span><span class="s2">&amp;lt;/s&amp;gt; &quot;</span>',
      '<span class="w">  </span><span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;[INST] </span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s2"> [/INST]&quot;</span>',
      '<span class="w">  </span><span class="k">return</span> <span class="n">prompt</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">history</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,):</span>',
      '<span class="w">    </span><span class="n">temperature</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">if</span> <span class="n">temperature</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="mf">1e-2</span><span class="p">:</span>',
      '<span class="w">        </span><span class="n">temperature</span> <span class="o">=</span> <span class="mf">1e-2</span>',
      '<span class="w">    </span><span class="n">top_p</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">top_p</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="n">generate_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,)</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">format_prompt</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">system_prompt</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">history</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">text_generation</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_full_text</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>',
      '<span class="w">        </span><span class="n">output</span> <span class="o">+=</span> <span class="n">response</span><span class="o">.</span><span class="n">token</span><span class="o">.</span><span class="n">text</span>',
      '<span class="w">        </span><span class="k">yield</span> <span class="n">output</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">output</span>',
      '<span class="w"> </span>',
      '<span class="n">additional_inputs</span><span class="o">=</span><span class="p">[</span>',
      '<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;System Prompt&quot;</span><span class="p">,</span> <span class="n">max_lines</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,),</span>',
      '<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Temperature&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">&quot;Higher values produce more diverse outputs&quot;</span><span class="p">),</span>',
      '<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Max new tokens&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">1048</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">&quot;The maximum numbers of new tokens&quot;</span><span class="p">),</span>',
      '<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Top-p (nucleus sampling)&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.90</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">&quot;Higher values sample more low-probability tokens&quot;</span><span class="p">),</span>',
      '<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Repetition penalty&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">&quot;Penalize repeated tokens&quot;</span><span class="p">)</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">gr</span><span class="o">.</span><span class="n">ChatInterface</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">fn</span><span class="o">=</span><span class="n">generate</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">chatbot</span><span class="o">=</span><span class="n">gr</span><span class="o">.</span><span class="n">Chatbot</span><span class="p">(</span><span class="n">show_label</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_share_button</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_copy_button</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">likeable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="s2">&quot;panel&quot;</span><span class="p">),</span>',
      '<span class="w">    </span><span class="n">additional_inputs</span><span class="o">=</span><span class="n">additional_inputs</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Mixtral 46.7B&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">concurrency_limit</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>',
      '<span class="p">)</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">show_api</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Mixtral-8x7B_huggingface_API.webp" alt="Mixtral-8x7B huggingface API">
      </section>







    </div>

  </section>

</PostLayout>
