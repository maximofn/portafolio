---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'RAG fundamentals';
const end_url = 'rag-fundamentals';
const description = 'Forget about Ctrl+F! 🤯 With RAG, your documents will answer your questions directly. 😎 Step-by-step tutorial with Hugging Face and ChromaDB. Unleash the power of AI (and show off to your friends)! 💪';
const keywords = 'rag, retriever, reader, hugging face, transformers, chromadb, vector database, question-answering, qa, nlp, natural language processing, machine learning, artificial intelligence, ai';
const languaje = 'EN';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-fundamentals.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-10-23+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Setting-up-the-API-Inference-from-Hugging-Face"><h2>Setting up the <code>API Inference</code> from Hugging Face</h2></a>
      <a class="anchor-link" href="#What-is-RAG?"><h2>What is <code>RAG</code>?</h2></a>
      <a class="anchor-link" href="#How-is-information-stored?"><h3>How is information stored?</h3></a>
      <a class="anchor-link" href="#How-to-get-the-correct-chunk?"><h3>How to get the correct <code>chunk</code>?</h3></a>
      <a class="anchor-link" href="#Let's-revisit-what-RAG-is"><h3>Let's revisit what <code>RAG</code> is</h3></a>
      <a class="anchor-link" href="#Vector-Database"><h2>Vector Database</h2></a>
      <a class="anchor-link" href="#Embedding-Function"><h3>Embedding Function</h3></a>
      <a class="anchor-link" href="#ChromaDB-client"><h3>ChromaDB client</h3></a>
      <a class="anchor-link" href="#Collection"><h3>Collection</h3></a>
      <a class="anchor-link" href="#Document-Upload"><h2>Document Upload</h2></a>
      <a class="anchor-link" href="#Document-Upload-Function"><h3>Document Upload Function</h3></a>
      <a class="anchor-link" href="#Function-to-split-the-documentation-into-chunks"><h3>Function to split the documentation into <code>chunks</code></h3></a>
      <a class="anchor-link" href="#Function-to-generate-embeddings-of-a-chunk"><h3>Function to generate embeddings of a <code>chunk</code></h3></a>
      <a class="anchor-link" href="#Documents-with-which-we-are-going-to-test"><h3>Documents with which we are going to test</h3></a>
      <a class="anchor-link" href="#Let's-create-the-chunks!"><h3>Let's create the <code>chunks</code>!</h3></a>
      <a class="anchor-link" href="#Load-the-chunks-into-the-vector-database"><h3>Load the <code>chunk</code>s into the vector database</h3></a>
      <a class="anchor-link" href="#Questions"><h2>Questions</h2></a>
      <a class="anchor-link" href="#Getting-the-correct-chunk"><h3>Getting the correct <code>chunk</code></h3></a>
      <a class="anchor-link" href="#Generating-the-response"><h3>Generating the response</h3></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="RAG:-Fundamentals-and-Advanced-Techniques">RAG: Fundamentals and Advanced Techniques<a class="anchor-link" href="#RAG:-Fundamentals-and-Advanced-Techniques"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>In this post, we will see what the <code>RAG</code> (<code>Retrieval Augmented Generation</code>) technique consists of and how it can be implemented in a language model.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To get it for free, instead of using an OpenAI account (as you will see in most tutorials), we are going to use the <code>API inference</code> from Hugging Face, which has a free tier of 1000 requests per day, which is more than enough to make this post.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Setting-up-the-API-Inference-from-Hugging-Face">Setting up the <code>API Inference</code> from Hugging Face<a class="anchor-link" href="#Setting-up-the-API-Inference-from-Hugging-Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To use the HuggingFace <code>API Inference</code>, the first thing you need is to have a HuggingFace account. Once you have one, go to <a href="https://huggingface.co/settings/keys" target="_blank" rel="nofollow noreferrer">Access tokens</a> in your profile settings and generate a new token.
      We need to give it a name, in my case I will name it <code>rag-fundamentals</code> and enable the permission <code>Make calls to serverless Inference API</code>. A token will be created that we need to copy.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To manage the token, we will create a file in the same path where we are working called <code>.env</code> and we will put the token we copied into the file as follows:</p>
      <div class="highlight"><pre><span></span><span class="sb"></span><span class="nv">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="o">=</span><span class="s2">"hf_...."</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now, in order to obtain the token, we need to have <code>dotenv</code> installed, which we do by</p>
      <div class="highlight"><pre><span></span><span class="sb"></span>pip<span class="w"> </span>install<span class="w"> </span>python-dotenv<span class="sb"></span>
      </pre></div>
      <p>and execute</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      ' ',
      '<span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Now that we have a token, we create a client, for this we need to have the <code>huggingface_hub</code> library installed, which we do using conda or pip</p>
      <div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>huggingface_hub
      </pre></div>
      <p>or</p>
      <div class="highlight"><pre>pip install --upgrade huggingface_hub</pre>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now we have to choose which model we are going to use. You can see the available models on the <a href="https://huggingface.co/docs/api-inference/supported-models" target="_blank" rel="nofollow noreferrer">Supported models</a> page of the Hugging Face <code>API Inference</code> documentation.
      As of the time of writing this post, the best available is <code>Qwen2.5-72B-Instruct</code>, so we are going to use that model.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Now we can create the client</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>',
          '',
          '<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">MODEL</span><span class="p">)</span>',
          '<span class="n">client</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;InferenceClient(model=\'Qwen/Qwen2.5-72B-Instruct\', timeout=None)&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We do a test to see if it works</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span>',
          '	<span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, qué tal?"</span> <span class="p">}</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>',
          '	<span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> ',
          '	<span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>',
          '	<span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>',
          '	<span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '	<span class="n">stream</span><span class="o">=</span><span class="kc">False</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '¡Hola! Estoy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="What-is-RAG?">What is <code>RAG</code>?<a class="anchor-link" href="#What-is-RAG?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>RAG</code> stands for <code>Retrieval Augmented Generation</code>, a technique created to obtain information from documents. Although LLMs can become very powerful and hold a lot of knowledge, they will never be able to answer questions about private documents, such as company reports, internal documentation, etc. That is why <code>RAG</code> was created, to be able to use these LLMs on such private documentation.
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="What is RAG?" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG.webp" width="1600" height="900"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The idea is that a user asks a question about that private documentation, the system is able to retrieve the part of the documentation where the answer to that question is, the question and the part of the documentation are passed to a LLM and the LLM generates the answer for the user.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="How-is-information-stored?">How is information stored?<a class="anchor-link" href="#How-is-information-stored?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>It is well known, and if you didn't know, I'll tell you now, that LLMs have a limit to the amount of information that can be inputted, which is called the context window. This is due to the internal architectures of the LLMs, which is not relevant right now. But the important thing is that you can't just pass a document and a question to them, because it's likely that the LLM will not be able to process all that information.
      In cases where more information is typically passed than what its context window allows, what usually happens is that the LLM does not pay attention to the end of the input. Imagine you ask the LLM about something in your document, that this information is at the end of the document and the LLM does not read it.
      Therefore, what is done is to divide the documentation into blocks called <code>chunks</code>. So the documentation is stored in a bunch of <code>chunks</code>, which are pieces of that documentation. So when the user asks a question, the <code>chunk</code> containing the answer to that question is passed to the LLM.
      In addition to splitting the documentation into <code>chunk</code>s, these are converted into embeddings, which are numerical representations of the <code>chunk</code>s. This is because LLMs do not actually understand text, but rather numbers, and the <code>chunk</code>s are converted into numbers so that the LLM can understand them. If you want to understand more about embeddings, you can read my post on <a href="https://www.maximofn.com/transformers">transformers</a> where I explain how transformers work, which is the architecture underlying the LLMs. You can also read my post on <a href="https://www.maximofn.com/chromadb">ChromaDB</a> where I explain how embeddings are stored in a vector database. Additionally, it would be interesting for you to read my post on the <a href="https://www.maximofn.com/hugging-face-tokenizers">HuggingFace Tokenizers</a> library, where it is explained how text is tokenized, which is the step prior to generating embeddings.
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="RAG - embeddings" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG-embeddings.webp" width="1400" height="750"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="How-to-get-the-correct-chunk?">How to get the correct <code>chunk</code>?<a class="anchor-link" href="#How-to-get-the-correct-chunk?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We have said that the documentation is divided into <code>chunks</code> and the <code>chunk</code> containing the answer to the user's question is passed to the LLM. But, how do we know in which <code>chunk</code> the answer is? For this, what is done is converting the user's question to an embedding, and the similarity between the embedding of the question and the embeddings of the <code>chunks</code> is calculated. Thus, the <code>chunk</code> with the highest similarity is the one that is passed to the LLM.
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="RAG - embeddings similarity" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-chunk_retreival.webp" width="1374" height="351"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Let's-revisit-what-RAG-is">Let's revisit what <code>RAG</code> is<a class="anchor-link" href="#Let's-revisit-what-RAG-is"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>On one hand, we have the <code>retrieval</code>, which is fetching the correct <code>chunk</code> from the documentation; on the other hand, we have the <code>augmented</code>, which is passing the user's question and the <code>chunk</code> to the LLM; and lastly, we have the <code>generation</code>, which is obtaining the response generated by the LLM.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Vector-Database">Vector Database<a class="anchor-link" href="#Vector-Database"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We have seen that the documentation is divided into <code>chunks</code> and stored in a vector database, so we need to use one. For this post, I will use <a href="https://www.trychroma.com/" target="_blank" rel="nofollow noreferrer">ChromaDB</a>, which is a widely used vector database and I also have a <a href="https://www.maximofn.com/chromadb">post</a> where I explain how it works.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>So first we need to install the ChromaDB library, for this we install it with Conda or Pip</p>
      <div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::chromadb
      </pre></div>
      <p>Or</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>chromadb
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Embedding-Function">Embedding Function<a class="anchor-link" href="#Embedding-Function"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 27" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As we mentioned, everything will be based on embeddings, so the first thing we do is create a function to obtain embeddings from a text. We are going to use the model <code>sentence-transformers/all-MiniLM-L6-v2</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">chromadb.utils.embedding_functions</span> <span class="k">as</span> <span class="nn">embedding_functions</span>',
      '',
      '<span class="n">EMBEDDING_MODEL</span> <span class="o">=</span> <span class="s2">"sentence-transformers/all-MiniLM-L6-v2"</span>',
      '      ',
      '<span class="n">huggingface_ef</span> <span class="o">=</span> <span class="n">embedding_functions</span><span class="o">.</span><span class="n">HuggingFaceEmbeddingFunction</span><span class="p">(</span>',
      '    <span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span>',
      '    <span class="n">model_name</span><span class="o">=</span><span class="n">EMBEDDING_MODEL</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>We test the embedding function</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">huggingface_ef</span><span class="p">([</span><span class="s2">"Hello, how are you?"</span><span class="p">,])</span>',
          '<span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We obtain a 384-dimensional embedding. Although the purpose of this post is not to explain embeddings, in summary, our embedding function has categorized the phrase <code>Hello, how are you?</code> in a 384-dimensional space.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="ChromaDB-client">ChromaDB client<a class="anchor-link" href="#ChromaDB-client"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 28" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now that we have our embedding function, we can create a ChromaDB client</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>First, we create a folder where the vector database will be stored.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>',
      '      ',
      '<span class="n">chroma_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"chromadb_persisten_storage"</span><span class="p">)</span>',
      '<span class="n">chroma_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Now we create the client</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">chromadb</span> <span class="kn">import</span> <span class="n">PersistentClient</span>',
      '',
      '<span class="n">chroma_client</span> <span class="o">=</span> <span class="n">PersistentClient</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">chroma_path</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <h3 id="Collection">Collection<a class="anchor-link" href="#Collection"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 29" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>When we have the ChromaDB client, the next thing we need to do is create a collection. A collection is a set of vectors, in our case, the <code>chunks</code> of the documentation.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create it by indicating the embedding function we are going to use</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">collection_name</span> <span class="o">=</span> <span class="s2">"document_qa_collection"</span>',
      '<span class="n">collection</span> <span class="o">=</span> <span class="n">chroma_client</span><span class="o">.</span><span class="n">get_or_create_collection</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">huggingface_ef</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <h2 id="Document-Upload">Document Upload<a class="anchor-link" href="#Document-Upload"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now that we have created the vector database, we need to split the documentation into <code>chunks</code> and store them in the vector database.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Document-Upload-Function">Document Upload Function<a class="anchor-link" href="#Document-Upload-Function"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>First, we create a function to load all <code>.txt</code> documents from a directory</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>',
      '    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">),</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>',
      '        <span class="k">return</span> <span class="p">{</span><span class="s2">"id"</span><span class="p">:</span> <span class="n">file</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()}</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">load_documents_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>',
      '    <span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>',
      '    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>',
      '        <span class="k">if</span> <span class="n">file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">".txt"</span><span class="p">):</span>',
      '            <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>',
      '    <span class="k">return</span> <span class="n">documents</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <h3 id="Function-to-split-the-documentation-into-chunks">Function to split the documentation into <code>chunks</code><a class="anchor-link" href="#Function-to-split-the-documentation-into-chunks"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 32" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Once we have the documents, we divide them into <code>chunks</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
      '    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>',
      '    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>',
      '    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>',
      '        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">chunk_size</span>',
      '        <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>',
      '        <span class="n">start</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">chunk_overlap</span>',
      '    <span class="k">return</span> <span class="n">chunks</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <h3 id="Function-to-generate-embeddings-of-a-chunk">Function to generate embeddings of a <code>chunk</code><a class="anchor-link" href="#Function-to-generate-embeddings-of-a-chunk"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now that we have the <code>chunks</code>, we generate the embeddings for each of them</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Later we will see why, but to generate the embeddings we will do it locally and not through the Hugging Face API. For this, we need to have <a href="https://pytorch.org" target="_blank" rel="nofollow noreferrer">PyTorch</a> and <code>sentence-transformers</code> installed, for this we do</p>
      <div class="highlight"><pre><span></span><span class="sb"></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>sentence-transformers
      </pre></div>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>',
      '<span class="kn">import</span> <span class="nn">torch</span>',
      ' ',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      ' ',
      '<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">EMBEDDING_MODEL</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>',
      '    <span class="k">try</span><span class="p">:</span>',
      '        <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>',
      '        <span class="k">return</span> <span class="n">embedding</span>',
      '    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
      '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>



















      
      <section class="section-block-markdown-cell">
      <p>Let's now test this embeddings function locally</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello, how are you?"</span>',
          '<span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
          '<span class="n">embedding</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see that we obtain an embedding of the same dimension as when we did it with the Hugging Face API</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The <code>sentence-transformers/all-MiniLM-L6-v2</code> model has only 22M parameters, so you will be able to run it on any GPU. Even if you do not have a GPU, you will be able to run it on a CPU.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>The LLM we are going to use to generate the responses, which is <code>Qwen2.5-72B-Instruct</code>, as its name indicates, is a model with 72B parameters, so this model cannot be run on any GPU and on a CPU it is unthinkable due to how slow it would be. Therefore, we will use this LLM via the API, but when generating the embeddings, we can do it locally without any problem.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Documents-with-which-we-are-going-to-test">Documents with which we are going to test<a class="anchor-link" href="#Documents-with-which-we-are-going-to-test"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>To perform all these tests, I downloaded the dataset <a href="https://www.kaggle.com/datasets/harshsinghal/aws-case-studies-and-blogs" target="_blank" rel="nofollow noreferrer">aws-case-studies-and-blogs</a> and placed it in the <code>rag-txt_dataset</code> folder. With the following commands, I'll show you how to download and unzip it.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We create the folder where we are going to download the documents</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">!</span>mkdir<span class="w"> </span>rag_txt_dataset',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>We download the <code>.zip</code> with the documents</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="o">!</span>curl<span class="w"> </span>-L<span class="w"> </span>-o<span class="w"> </span>./rag_txt_dataset/archive.zip<span class="w"> </span>https://www.kaggle.com/api/v1/datasets/download/harshsinghal/aws-case-studies-and-blogs',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',
          '                                 Dload  Upload   Total   Spent    Left  Speed',
          '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',
          '100 1430k  100 1430k    0     0  1082k      0  0:00:01  0:00:01 --:--:-- 2440k',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We unzip the <code>.zip</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">!</span>unzip<span class="w"> </span>rag_txt_dataset/archive.zip<span class="w"> </span>-d<span class="w"> </span>rag_txt_dataset',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Archive:  rag_txt_dataset/archive.zip',
          '  inflating: rag_txt_dataset/23andMe Case Study _ Life Sciences _ AWS.txt  ',
          '  inflating: rag_txt_dataset/36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt  ',
          '  inflating: rag_txt_dataset/54gene _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/6sense Case Study.txt  ',
          '  inflating: rag_txt_dataset/ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/AEON Case Study.txt  ',
          '  inflating: rag_txt_dataset/ALTBalaji _ Amazon Web Services.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study - Ineos Team UK.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study - StreamAMG.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study_ Creditsafe.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study_ Immowelt.txt  ',
          '  inflating: rag_txt_dataset/AWS Customer Case Study _ Kepler Provides Effective Monitoring of Elderly Care Home Residents Using AWS _ AWS.txt  ',
          '  inflating: rag_txt_dataset/AWS announces 21 startups selected for the AWS generative AI accelerator _ AWS Startups Blog.txt  ',
          '  inflating: rag_txt_dataset/AWS releases smart meter data analytics _ AWS for Industries.txt  ',
          '  inflating: rag_txt_dataset/Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt  ',
          '  ...  ',
          '  inflating: rag_txt_dataset/e-banner Streamlines Its Contact Center Operations and Facilitates a Fully Remote Workforce with Amazon Connect _ e-banner Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/iptiQ Case Study.txt  ',
          '  inflating: rag_txt_dataset/mod.io Provides Low Latency Gamer Experience Globally on AWS _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/myposter Case Study.txt  ',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We delete the <code>.zip</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">!</span>rm<span class="w"> </span>rag_txt_dataset/archive.zip',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Let's see what we have left</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="o">!</span>ls<span class="w"> </span>rag_txt_dataset',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'23andMe Case Study _ Life Sciences _ AWS.txt\'',
          '\'36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt\'',
          '\'54gene _ Case Study _ AWS.txt\'',
          '\'6sense Case Study.txt\'',
          '\'Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt\'',
          '\'Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt\'',
          '\'Accelerating customer onboarding using Amazon Connect _ NCS Case Study _ AWS.txt\'',
          '\'Accelerating Migration at Scale Using AWS Application Migration Service with 3M Company _ Case Study _ AWS.txt\'',
          '\'Accelerating Time to Market Using AWS and AWS Partner AccelByte _ Omeda Studios Case Study _ AWS.txt\'',
          '\'Achieving Burstable Scalability and Consistent Uptime Using AWS Lambda with TiVo _ Case Study _ AWS.txt\'',
          '\'Acrobits Uses Amazon Chime SDK to Easily Create Video Conferencing Application Boosting Collaboration for Global Users _ Acrobits Case Study _ AWS.txt\'',
          '\'Actuate AI Case study.txt\'',
          '\'ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt\'',
          '\'Adzuna doubles its email open rates using Amazon SES _ Adzuna Case Study _ AWS.txt\'',
          '\'AEON Case Study.txt\'',
          '\'ALTBalaji _ Amazon Web Services.txt\'',
          '\'Amanotes Stays on Beat by Delivering Simple Music Games to Millions Worldwide on AWS.txt\'',
          '\'Amazon OpenSearch Services vector database capabilities explained _ AWS Big Data Blog.txt\'',
          '\'Anghami Case Study.txt\'',
          '\'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt\'',
          '\'AppsFlyer Amazon EKS Case Study _ Advertising _ AWS.txt\'',
          '\'Arm Case Study.txt\'',
          '\'Arm Limited Case Study.txt\'',
          '\'Armitage Technologies case study.txt\'',
          '\'...\'',
          '\'Which Recurring Business Processes Can Small and Medium Businesses Automate_ _ AWS Smart Business Blog.txt\'',
          ' Windsor.txt',
          '\'Wireless Car Case Study _ AWS IoT Core _ AWS.txt\'',
          '\'Yamato Logistics (HK) case study.txt\'',
          '\'Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt\'',
          '\'Zoox Case Study _ Automotive _ AWS.txt\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Let's-create-the-chunks!">Let's create the <code>chunks</code>!<a class="anchor-link" href="#Let's-create-the-chunks!"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We list the documents with the function we had created</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset_path</span> <span class="o">=</span> <span class="s2">"rag_txt_dataset"</span>',
      '<span class="n">documents</span> <span class="o">=</span> <span class="n">load_documents_from_directory</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>We check that we have done it right</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"id"</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Run Jobs at Scale While Optimizing for Cost Using Amazon EC2 Spot Instances with ActionIQ _ ActionIQ Case Study _ AWS.txt',
          'Recommend and dynamically filter items based on user context in Amazon Personalize _ AWS Machine Learning Blog.txt',
          'Windsor.txt',
          'Bank of Montreal Case Study _ AWS.txt',
          'The Mill Adventure Case Study.txt',
          'Optimize software development with Amazon CodeWhisperer _ AWS DevOps Blog.txt',
          'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt',
          'THREAD _ Life Sciences _ AWS.txt',
          'Deep Pool Optimizes Software Quality Control Using Amazon QuickSight _ Deep Pool Case Study _ AWS.txt',
          'Upstox Saves 1 Million Annually Using Amazon S3 Storage Lens _ Upstox Case Study _ AWS.txt',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Now we create the <code>chunk</code>s.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">chunked_documents</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>',
      '    <span class="n">chunks</span> <span class="o">=</span> <span class="n">split_text</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>',
      '    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>',
      '        <span class="n">chunked_documents</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"id"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">document</span><span class="p">[</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">chunk</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <CodeBlockInputCell
        text={[
          '</span><span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '3611',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As we can see, there are 3611 <code>chunks</code>. Since the daily limit of the Hugging Face API is 1000 calls on the free account, if we want to create embeddings for all the <code>chunks</code>, we would run out of available calls and also wouldn't be able to create embeddings for all the <code>chunks</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We remind you once again, this embedding model is very small, only 22M parameters, so it can run on almost any computer, faster or slower, but it can run.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Since we are only going to create the embeddings of the <code>chunk</code>s once, even if we don't have a very powerful computer and it takes a long time, it will only be executed once. Then, when we want to ask questions about the documentation, we will generate the embeddings of the prompt with the Hugging Face API and use the LLM with the API. Therefore, we will only have to go through the process of generating the embeddings of the <code>chunk</code>s once.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We generate the embeddings of the <code>chunk</code>s</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Last library we are going to need to install. Since the process of generating embeddings from the <code>chunks</code> will be slow, we are going to install <code>tqdm</code> to show us a progress bar. We can install it with conda or pip, whichever you prefer.</p>
      <div class="highlight"><pre><span></span><span class="sb"></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::tqdm<span class="sb"></span>
      </pre></div>
      <p>o</p>
      <div class="highlight"><pre><span></span><span class="sb"></span>pip<span class="w"> </span>install<span class="w"> </span>tqdm<span class="sb"></span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>We generate the embeddings of the <code>chunk</code>s</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">tqdm</span>',
          '',
          '<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
          '    <span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>',
          '    <span class="k">if</span> <span class="n">embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>',
          '        <span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error with document </span><span class="si">{</span><span class="n">chunk</span><span class="p">[</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '100%|██████████| 3611/3611 [00:16&lt;00:00, 220.75it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>We see an example</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>',
          '',
          '<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Chunk id: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\\n\\n</span><span class="s2">text: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">\'text\'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\\n\\n</span><span class="s2">embedding shape: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">\'embedding\'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Chunk id: BNS Group Case Study _ Amazon Web Services.txt_0,',
          'text: Reducing Virtual Machines from 40 to 12',
          'The founders of BNS had been contemplating a migration from the company’s on-premises data center to the public cloud and observed a growing demand for cloud-based operations among current and potential BNS customers.',
          'Français',
          'Configures security according to cloud best practices',
          'Clive Pereira, R&amp;D director at BNS Group, explains, “The database that records Praisal’s SMS traffic resides in Praisal’s AWS environment. Praisal can now run complete analytics across its data and gain insights into what’s happening with its SMS traffic, which is a real game-changer for the organization.”  ',
          'Español',
          ' AWS ISV Accelerate Program',
          ' Receiving Strategic, Foundational Support from ISV Specialists',
          ' Learn More',
          'The value that AWS places on the ISV stream sealed the deal in our choice of cloud provider.” ',
          '日本語',
          '  Contact Sales ',
          'BNS is an Australian software provider focused on secure enterprise SMS and fax messaging. Its software runs on the Windows platform and is l,',
          'embedding shape: (384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Load-the-chunks-into-the-vector-database">Load the <code>chunk</code>s into the vector database<a class="anchor-link" href="#Load-the-chunks-into-the-vector-database"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Once we have all the chunks generated, we load them into the vector database. We use <code>tqdm</code> again to display a progress bar, because this will also be slow.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">tqdm</span>',
          '',
          '<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
          '    <span class="n">collection</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span>',
          '        <span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]],</span>',
          '        <span class="n">documents</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span>',
          '        <span class="n">embeddings</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">],</span>',
          '    <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '100%|██████████| 3611/3611 [00:59&lt;00:00, 60.77it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Questions">Questions<a class="anchor-link" href="#Questions"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 37" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now that we have the vector database, we can ask questions to the documentation. For this, we need a function that returns the correct <code>chunk</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Getting-the-correct-chunk">Getting the correct <code>chunk</code><a class="anchor-link" href="#Getting-the-correct-chunk"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 38" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Now we need a function that returns the correct <code>chunk</code>, let's create it</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>',
      '    <span class="n">results</span> <span class="o">=</span> <span class="n">collection</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query_texts</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">n_results</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>',
      '    <span class="k">return</span> <span class="n">results</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Lastly, we create a <code>query</code>.
      To generate the query, I randomly picked the document <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code>, passed it to a LLM, and asked it to generate a question about the document. The question it generated is
      <code>¿Cómo utilizó Neeva Karpenter y las Instancias Spot de Amazon EC2 para mejorar su gestión de infraestructura y optimización de costos?</code>
      So we obtain the most relevant <code>chunks</code> for that question</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">"How did Neeva use Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization?"</span>',
      '<span class="n">top_chunks</span> <span class="o">=</span> <span class="n">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Let's see what <code>chunk</code>s have been returned to us</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_chunks</span><span class="p">[</span><span class="s2">"ids"</span><span class="p">][</span><span class="mi">0</span><span class="p">])):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Rank </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">\'ids\'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, distance: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">\'distances\'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Rank 1: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_0, distance: 0.29233667254447937',
          'Rank 2: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_5, distance: 0.4007825255393982',
          'Rank 3: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_1, distance: 0.4317566752433777',
          'Rank 4: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_6, distance: 0.43832334876060486',
          'Rank 5: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_4, distance: 0.44625571370124817',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As I had mentioned, the document I had randomly chosen was <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code> and as you can see, the <code>chunks</code> it returned to us are from that document. In other words, out of more than 3000 <code>chunks</code> that were in the database, it was able to return the most relevant <code>chunks</code> for that question, it seems like this works!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Generating-the-response">Generating the response<a class="anchor-link" href="#Generating-the-response"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 39" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As we already have the most relevant <code>chunks</code>, we pass them to the LLM along with the question, so that it can generate a response.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">relevant_chunks</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>',
      '    <span class="n">context</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\n\\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">chunk</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">relevant_chunks</span><span class="p">])</span>',
      '    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"You are an assistant for question-answering. You have to answer the following question:</span><span class="se">\\n\\n</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="se">\\n\\n</span><span class="s2">Answer the question with the following information:</span><span class="se">\\n\\n</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s2">"</span>',
      '    <span class="n">message</span> <span class="o">=</span> <span class="p">[</span>',
      '        <span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">prompt</span> <span class="p">}</span>',
      '    <span class="p">]</span>',
      '    <span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>',
      '        <span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> ',
      '        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>',
      '        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>',
      '        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>',
      '        <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>',
      '    <span class="p">)</span>',
      '    <span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>',
      '    <span class="k">return</span> <span class="n">response</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>




















      
      <section class="section-block-markdown-cell">
      <p>We test the function</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="n">response</span> <span class="o">=</span> <span class="n">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">top_chunks</span><span class="p">[</span><span class="s2">"documents"</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Here’s how:',
          '### Early Collaboration with Karpenter',
          'In late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.',
          '### Combining Spot Instances and On-Demand Instances',
          'Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.',
          '### Flexibility and Instance Diversification',
          'According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter\'s adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.',
          '### Improved Scalability and Agility',
          'By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:',
          '- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.',
          '- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.',
          '### Enhanced Development Cycles',
          'The integration of Karpenter and Spot Instances has also accelerated Neeva\'s development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.',
          '### Cost Savings and Budget Control',
          'Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.',
          '### Future Plans',
          'Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, "The bulk of our compute is or will be managed using Karpenter going forward."',
          '### Conclusion',
          'By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>When I asked the LLM to generate a question about the document, I also asked it to generate the correct answer. This is the answer the LLM gave me.</p>
      <pre><code>Neeva used Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization in several ways:
Simplified Instance Management:

Karpenter: By adopting Karpenter, Neeva simplified the process of provisioning and managing compute resources for its Amazon EKS clusters. Karpenter automatically provisions and de-provisions instances based on the workload, eliminating the need for manual configurations and reducing the complexity of understanding different compute instances.
Spot Instances: Neeva leveraged Amazon EC2 Spot Instances, which are unused EC2 capacity available at a significant discount (up to 90% cost savings). This allowed the company to control costs while meeting its performance requirements.
Enhanced Scalability:

Karpenter: Karpenter's ability to dynamically scale resources enabled Neeva to spin up new instances quickly, allowing the company to iterate at a higher velocity and run more experiments in less time.Spot Instances: The use of Spot Instances provided flexibility and instance diversification, making it easier for Neeva to scale its compute resources efficiently.
Improved Productivity:

Karpenter: By democratizing infrastructure changes, Karpenter allowed any engineer to modify Kubernetes configurations, reducing the dependency on specialized expertise. This saved the Neeva team up to 100 hours per week of wait time on systems administration.
Spot Instances: The ability to quickly provision and de-provision Spot Instances reduced delays in the development pipeline, ensuring that jobs did not get stuck due to a lack of available resources.
Cost Efficiency:

Karpenter: Karpenter's best practices for Spot Instances, including flexibility and instance diversification, helped Neeva use these instances more effectively, staying within budget.
Spot Instances: The cost savings from using Spot Instances allowed Neeva to run large-scale jobs, such as indexing, for nearly the same cost but in a fraction of the time. For example, Neeva reduced its indexing jobs from 18 hours to just 3 hours.
Better Resource Utilization:

Karpenter: Karpenter provided better visibility into compute resource usage, allowing Neeva to track and optimize its resource consumption more closely.
Spot Instances: The combination of Karpenter and Spot Instances enabled Neeva to run large language models more efficiently, enhancing the search experience for its users.
In summary, Neeva's adoption of Karpenter and Amazon EC2 Spot Instances significantly improved its infrastructure management, cost optimization, and overall development efficiency, enabling the company to deliver better ad-free search experiences to its users.</code></pre>
      And this has been the response generated by our `RAG`
      <pre><code>Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Here’s how:
### Early Collaboration with KarpenterIn late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.

### Combining Spot Instances and On-Demand Instances
Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.

### Flexibility and Instance Diversification
According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter's adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.

### Improved Scalability and Agility
By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:
- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.
- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.

### Enhanced Development Cycles
The integration of Karpenter and Spot Instances has also accelerated Neeva's development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.

### Cost Savings and Budget Control
Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.

### Future Plans
Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, "The bulk of our compute is or will be managed using Karpenter going forward."

### Conclusion
By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.</code></pre>
So we can conclude that the `RAG` has worked correctly!!!
      </section>
      






    </div>

  </section>

</PostLayout>
