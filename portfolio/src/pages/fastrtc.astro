---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Gu√≠a Completa de FastRTC: Crea un Chat de Voz con IA en Tiempo Real con Python';
const end_url = 'fastrtc';
const description = 'Aprende a usar FastRTC para crear aplicaciones de IA con voz en tiempo real en Python. Sigue nuestra gu√≠a paso a paso para construir un chatbot de voz con LLMs, desde la instalaci√≥n b√°sica hasta integrarlo con una llamada telef√≥nica.';
const keywords = 'fastrtc, real-time, ai, aplicaci√≥n, tel√©fono, python, chat de voz, llm';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/fastrtc-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=960
    image_height=720
    image_extension=webp
    article_date=2025-03-08+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Caracteristicas principales de FastRTC"><h2>Caracter√≠sticas principales de FastRTC</h2></a>
      <a class="anchor-link" href="#Instalacion"><h2>Instalaci√≥n</h2></a>
      <a class="anchor-link" href="#Primeros pasos"><h2>Primeros pasos</h2></a>
      <a class="anchor-link" href="#Subiendo de nivel: Chat de voz con LLM"><h2>Subiendo de nivel: Chat de voz con LLM</h2></a>
      <a class="anchor-link" href="#Llamada por telefono"><h2>Llamada por tel√©fono</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="FastRTC: La Biblioteca de Comunicacion en Tiempo Real para Python">FastRTC: La Biblioteca de Comunicaci√≥n en Tiempo Real para Python<a class="anchor-link" href="#FastRTC: La Biblioteca de Comunicacion en Tiempo Real para Python"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En los √∫ltimos meses, hemos visto un gran avance en modelos de voz en tiempo real, con empresas enteras fundadas alrededor de modelos tanto de c√≥digo abierto como cerrado. Algunos hitos importantes incluyen:</p>
      <ul>
        <li><code>OpenAI</code> y <code>Google</code> lanzaron sus APIs multimodales en vivo para ChatGPT y Gemini. ¬°OpenAI incluso lanz√≥ un n√∫mero de tel√©fono <code>1-800-ChatGPT</code>!</li>
        <li><code>Kyutai</code> lanz√≥ <a href="https://huggingface.co/kyutai" target="_blank" rel="nofollow noreferrer">Moshi</a>, un LLM de audio a audio completamente de c√≥digo abierto.</li>
        <li><code>Alibaba</code> lanz√≥ <a href="https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct" target="_blank" rel="nofollow noreferrer">Qwen2-Audio</a>, un LLM de c√≥digo abierto que entiende audio de forma nativa.</li>
        <li><code>Fixie.ai</code> lanz√≥ <a href="https://huggingface.co/fixie-ai/ultravox-v0_5-llama-3_3-70b" target="_blank" rel="nofollow noreferrer">Ultravox</a>, otro LLM de c√≥digo abierto que tambi√©n entiende audio de forma nativa.</li>
        <li><code>ElevenLabs</code> recaud√≥ 180 millones de d√≥lares en su Serie C.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A pesar de esta explosi√≥n en modelos y financiaci√≥n, sigue siendo dif√≠cil construir aplicaciones de IA en tiempo real que transmitan audio y video, especialmente en Python.</p>
      <ul>
        <li>Los ingenieros de ML pueden no tener experiencia con las tecnolog√≠as necesarias para construir aplicaciones en tiempo real, como <code>WebRTC</code>.</li>
        <li>Incluso herramientas de asistencia de c√≥digo como <code>Cursor</code> y <code>Copilot</code> tienen dificultades para escribir c√≥digo Python que soporte aplicaciones de audio/video en tiempo real.</li>
      </ul>
      <p>Por eso es emocionante el anuncio de <code>FastRTC</code>, la biblioteca de comunicaci√≥n en tiempo real para Python. ¬°La biblioteca est√° dise√±ada para facilitar la construcci√≥n de aplicaciones de IA de audio y video en tiempo real completamente en Python!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Caracteristicas principales de FastRTC">Caracter√≠sticas principales de FastRTC<a class="anchor-link" href="#Caracteristicas principales de FastRTC"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <ul>
        <li>üó£Ô∏è Detecci√≥n de voz autom√°tica y toma de turnos incorporada, para que solo tengas que preocuparte por la l√≥gica de respuesta al usuario.</li>
        <li>üíª UI autom√°tica - UI de Gradio habilitada para WebRTC incorporada para pruebas (¬°o despliegue a producci√≥n!).</li>
        <li>üìû Llamada por tel√©fono - Usa <code>fastphone()</code> para obtener un n√∫mero de tel√©fono **gratuito** para llamar a tu stream de audio (se requiere un token HF).</li>
        <li>‚ö°Ô∏è Soporte para <code>WebRTC</code> y <code>Websocket</code>.</li>
        <li>üí™ Personalizable - Puedes montar el stream en cualquier aplicaci√≥n <code>FastAPI</code> para servir una UI personalizada y desplegar m√°s all√° de <code>Gradio</code>.</li>
        <li>üß∞ Muchas utilidades para <code>text-to-speech</code>, <code>speech-to-text</code>, <code>detecci√≥n de parada</code> para ayudarte a comenzar.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Instalacion">Instalaci√≥n<a class="anchor-link" href="#Instalacion"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder usar <code>FastRTC</code>, primero necesitas instalar la biblioteca:</p>
      <div class='highlight'><pre><code class="language-bash">pip install fastrtc</code></pre></div>
      <p>Pero si queremos instalar las funcionalidades de detecci√≥n de pausa, speech-to-text y text-to-speech, necesitamos instalar algunas dependencias adicionales:</p>
      <div class='highlight'><pre><code class="language-bash">pip install "fastrtc[vad, stt, tts]"</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Primeros pasos">Primeros pasos<a class="anchor-link" href="#Primeros pasos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Empezaremos construyendo el <code>hola mundo</code> del audio en tiempo real: hacer eco de lo que dice el usuario. En <code>FastRTC</code>, esto es tan simple como:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fastrtc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Stream</span><span class="p">,</span> <span class="n">ReplyOnPause</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">echo</span><span class="p">(</span><span class="n">audio</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">])</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>',
      '<span class="w">    </span><span class="k">yield</span> <span class="n">audio</span>',
      '<span class="w"> </span>',
      '<span class="n">stream</span> <span class="o">=</span> <span class="n">Stream</span><span class="p">(</span><span class="n">ReplyOnPause</span><span class="p">(</span><span class="n">echo</span><span class="p">),</span> <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;audio&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;send-receive&quot;</span><span class="p">)</span>',
      '<span class="n">stream</span><span class="o">.</span><span class="n">ui</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '* Running on local URL:  http://127.0.0.1:7872',
          'To create a public link, set `share=True` in `launch()`.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Cuando vamos al enlace que nos sugiere Gradio, primero tenemos que dar permisos al navegador para acceder al micr√≥fono. A continuaci√≥n nos aparecer√° esto</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/fastRTC%20-%20hello%20world%20-%20init.webp" width="550" height="361" alt="fastrct - hello world - init">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si pinchamos en la pesta√±a de la derecha de la palabra <code>Record</code> podemos seleccionar el micr√≥fono que queremos usar.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A continuaci√≥n si pulsamos en el bot√≥n de <code>Record</code>, todo lo que digamos, la aplicaci√≥n lo repetir√°. Es decir captura el audio, detecta cuando hemos dejado de hablar y lo repite.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a desglosarlo:</p>
      <ul>
        <li><code>ReplyOnPause</code> manejar√° la detecci√≥n de voz y la toma de turnos por ti. Solo tienes que preocuparte por la l√≥gica para responder al usuario. Hay que pasarle la funci√≥n que se encargar√° de gestionar el audio de entrada. En nuestro caso es la funci√≥n <code>echo</code>, que captura el audio de entrada y lo devuelve en stream mediante el uso de <code>yield</code>, que mucha gente no conoce, pero es un generador, es decir, es un m√©todo de python para crear iteradores. Si quieres saber m√°s sobre <code>yield</code> puedes leer mi post de <a href="https://www.maximofn.com/python#6.5.-Generadores">Python</a>. Cualquier generador que devuelva una tupla de audio (representada como <code>(sample_rate, audio_data)</code>) funcionar√°.</li>
        <li>La clase <code>Stream</code> construir√° una UI de Gradio para que puedas probar r√°pidamente tu stream. Una vez que hayas terminado de prototipar, puedes desplegar tu Stream como una aplicaci√≥n FastAPI lista para producci√≥n en una sola l√≠nea de c√≥digo</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aqu√≠ podemos ver un ejemplo de los creadores de <code>FastRTC</code></p>
      <video src="https://github.com/user-attachments/assets/fcf2d30e-3e98-47c9-8dc3-23340784c441" controls></video>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Subiendo de nivel: Chat de voz con LLM">Subiendo de nivel: Chat de voz con LLM<a class="anchor-link" href="#Subiendo de nivel: Chat de voz con LLM"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El siguiente nivel es usar un LLM para responder al usuario. <code>FastRTC</code> viene con capacidades de <code>speech-to-text</code> y <code>text-to-speech</code> incorporadas, por lo que trabajar con LLMs es realmente f√°cil. Vamos a cambiar nuestra funci√≥n <code>echo</code> en consecuencia:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">fastrtc</span><span class="w"> </span><span class="kn">import</span> <span class="n">ReplyOnPause</span><span class="p">,</span> <span class="n">Stream</span><span class="p">,</span> <span class="n">get_stt_model</span><span class="p">,</span> <span class="n">get_tts_model</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2_localModel&quot;</span><span class="p">)</span>',
      '<span class="n">stt_model</span> <span class="o">=</span> <span class="n">get_stt_model</span><span class="p">()</span>',
      '<span class="n">tts_model</span> <span class="o">=</span> <span class="n">get_tts_model</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">echo</span><span class="p">(</span><span class="n">audio</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">stt_model</span><span class="o">.</span><span class="n">stt</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">            </span><span class="n">message</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="w">    </span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">response</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">audio_chunk</span> <span class="ow">in</span> <span class="n">tts_model</span><span class="o">.</span><span class="n">stream_tts_sync</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>',
      '<span class="w">        </span><span class="k">yield</span> <span class="n">audio_chunk</span>',
      '<span class="w"> </span>',
      '<span class="n">stream</span> <span class="o">=</span> <span class="n">Stream</span><span class="p">(</span><span class="n">ReplyOnPause</span><span class="p">(</span><span class="n">echo</span><span class="p">),</span> <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;audio&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;send-receive&quot;</span><span class="p">)</span>',
      '<span class="n">stream</span><span class="o">.</span><span class="n">ui</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî',
          '* Running on local URL:  http://127.0.0.1:7871',
          'To create a public link, set `share=True` in `launch()`.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como modelo de <code>speech-to-text</code> usa <code>Moonshine</code> que supuestamente solo soporta ingl√©s, pero lo he probado en espa√±ol y me entiende bien.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como modelo de lenguaje vamos a usar el modelo que desplegu√© en un backend en Hugging Face y que escrib√≠ en el post <a href="https://www.maximofn.com/deploy-backend-with-llm-in-huggingface">Desplegar backend con LLM en HuggingFace</a>. Utiliza el LLM <code>HuggingFaceTB/SmolLM2-1.7B-Instruct</code> que es un modelo peque√±o, ya que est√° corriendo en un backend con CPU, pero que funciona bastante bien.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como modelo de <code>text-to-speech</code> usa <code>Kokoro</code> que s√≠ tiene opciones de hablar en otros idiomas, pero que de momento en la librer√≠a de <code>FastRTC</code> de momento no est√° implementado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si nos interesa mucho usar modelos de <code>speech-to-speech</code> y <code>text-to-speech</code> en otros idiomas, podr√≠amos implementarlos nosotros mismos, porque el mayor potencial de <code>FastRTC</code> est√° en la capa de comunicaci√≥n en tiempo real, pero no me voy a meter en eso ahora.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora s√≠ probamos el c√≥digo que acabamos de escribir podemos tener un chatbot, por voz en tiempo real.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Llamada por telefono">Llamada por tel√©fono<a class="anchor-link" href="#Llamada por telefono"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h2>
      <p>Si en lugar de <code>stream.ui.launch()</code>, llamas a <code>stream.fastphone()</code>, obtendr√°s un n√∫mero de tel√©fono gratuito para llamar a tu stream. Ten en cuenta que se requiere un token de Hugging Face.</p>
      <p>Generamos un script, porque en un Jupyter Notebook no siempre funciona</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">%%</span><span class="n">writefile</span> <span class="n">fastrtc_phone_demo</span><span class="o">.</span><span class="n">py</span>',
      '<span class="w"> </span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">fastrtc</span><span class="w"> </span><span class="kn">import</span> <span class="n">ReplyOnPause</span><span class="p">,</span> <span class="n">Stream</span><span class="p">,</span> <span class="n">get_stt_model</span><span class="p">,</span> <span class="n">get_tts_model</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">gradio</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">gradio.networking</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup_tunnel</span> <span class="k">as</span> <span class="n">original_setup_tunnel</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">socket</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Monkey patch setup_tunnel para que acepte el par√°metro adicional</span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">patched_setup_tunnel</span><span class="p">(</span><span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">share_token</span><span class="p">,</span> <span class="n">share_server_address</span><span class="p">,</span> <span class="n">share_server_tls_certificate</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">original_setup_tunnel</span><span class="p">(</span><span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">,</span> <span class="n">share_token</span><span class="p">,</span> <span class="n">share_server_address</span><span class="p">,</span> <span class="n">share_server_tls_certificate</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Replace the original function with our patched version</span>',
      '<span class="n">gradio</span><span class="o">.</span><span class="n">networking</span><span class="o">.</span><span class="n">setup_tunnel</span> <span class="o">=</span> <span class="n">patched_setup_tunnel</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Get the token from the environment variable</span>',
      '<span class="n">HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Initialize the LLM client</span>',
      '<span class="n">llm_client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2_localModel&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Initialize the STT and TTS models</span>',
      '<span class="n">stt_model</span> <span class="o">=</span> <span class="n">get_stt_model</span><span class="p">()</span>',
      '<span class="n">tts_model</span> <span class="o">=</span> <span class="n">get_tts_model</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Define the echo function</span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">echo</span><span class="p">(</span><span class="n">audio</span><span class="p">):</span>',
      '<span class="w">    </span><span class="c1"># Convert the audio to text</span>',
      '<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">stt_model</span><span class="o">.</span><span class="n">stt</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="c1"># Generate the response</span>',
      '<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">llm_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">            </span><span class="n">message</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">            </span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="w">    </span><span class="p">)</span>',
      '<span class="w">    </span>',
      '<span class="w">    </span><span class="c1"># Convert the response to audio</span>',
      '<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="n">response</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="c1"># Stream the audio</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">audio_chunk</span> <span class="ow">in</span> <span class="n">tts_model</span><span class="o">.</span><span class="n">stream_tts_sync</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>',
      '<span class="w">        </span><span class="k">yield</span> <span class="n">audio_chunk</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">find_free_port</span><span class="p">(</span><span class="n">start_port</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">max_port</span><span class="o">=</span><span class="mi">9000</span><span class="p">):</span>',
      '<span class="w">    </span><span class="sd">&quot;&quot;&quot;Find the first free port starting from start_port.&quot;&quot;&quot;</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Searching for a free port starting from </span><span class="si">{</span><span class="n">start_port</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">port</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_port</span><span class="p">,</span> <span class="n">max_port</span><span class="p">):</span>',
      '<span class="w">        </span><span class="k">with</span> <span class="n">socket</span><span class="o">.</span><span class="n">socket</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">AF_INET</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">SOCK_STREAM</span><span class="p">)</span> <span class="k">as</span> <span class="n">sock</span><span class="p">:</span>',
      '<span class="w">            </span><span class="n">result</span> <span class="o">=</span> <span class="n">sock</span><span class="o">.</span><span class="n">connect_ex</span><span class="p">((</span><span class="s1">&#39;127.0.0.1&#39;</span><span class="p">,</span> <span class="n">port</span><span class="p">))</span>',
      '<span class="w">            </span><span class="k">if</span> <span class="n">result</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># If result != 0, the port is free</span>',
      '<span class="w">                </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Free port found: </span><span class="si">{</span><span class="n">port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="w">                </span><span class="k">return</span> <span class="n">port</span>',
      '<span class="w">    </span><span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No free port found between </span><span class="si">{</span><span class="n">start_port</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">max_port</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="w">    </span>',
      '<span class="n">free_port</span> <span class="o">=</span> <span class="n">find_free_port</span><span class="p">()</span>    <span class="c1"># Search for a free port</span>',
      '<span class="w"> </span>',
      '<span class="n">stream</span> <span class="o">=</span> <span class="n">Stream</span><span class="p">(</span><span class="n">ReplyOnPause</span><span class="p">(</span><span class="n">echo</span><span class="p">),</span> <span class="n">modality</span><span class="o">=</span><span class="s2">&quot;audio&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;send-receive&quot;</span><span class="p">)</span>',
      '<span class="n">stream</span><span class="o">.</span><span class="n">fastphone</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="n">free_port</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Explicamos el c√≥digo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La parte</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Monkey patch setup_tunnel para que acepte el par√°metro adicional<br>def patched_setup_tunnel(host, port, share_token, share_server_address, share_server_tls_certificate=None):<br>&#x20;&#x20;return original_setup_tunnel(host, port, share_token, share_server_address, share_server_tls_certificate)<br><br># Replace the original function with our patched version<br>gradio.networking.setup_tunnel = patched_setup_tunnel</code></pre></div>
            </section>
      <p>Es necesario porque <code>FastRTC</code> est√° escrito para una versi√≥n antigua de <code>gradio</code> que no soporta el par√°metro <code>share_server_address</code> en el m√©todo <code>setup_tunnel</code>. As√≠ que lo parcheamos para que acepte el par√°metro adicional.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como es necesario un token de Hugging Face, lo obtenemos de la variable de entorno <code>HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN</code>.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Get the token from the environment variable<br>HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN = os.getenv("HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN")</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A continuaci√≥n se crean los modelos de lenguaje, de <code>speech-to-text</code> y de <code>text-to-speech</code>, y creamos la funci√≥n <code>echo</code> que se encargar√° de gestionar el audio de entrada y salida.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Initialize the LLM client<br>llm_client = Client("Maximofn/SmolLM2_localModel")<br><br># Initialize the STT and TTS models<br>stt_model = get_stt_model()<br>tts_model = get_tts_model()<br><br># Define the echo function<br>def echo(audio):<br>&#x20;&#x20;# Convert the audio to text<br>&#x20;&#x20;prompt = stt_model.stt(audio)<br><br>&#x20;&#x20;# Generate the response<br>&#x20;&#x20;response = llm_client.predict(<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;message=prompt,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;system_message="You are a friendly Chatbot. Always reply in the language in which the user is writing to you.",<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;max_tokens=512,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;temperature=0.7,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;top_p=0.95,<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;api_name="/chat"<br>&#x20;&#x20;)<br>&#x20;&#x20;<br>&#x20;&#x20;# Convert the response to audio<br>&#x20;&#x20;prompt = response<br><br>&#x20;&#x20;# Stream the audio<br>&#x20;&#x20;for audio_chunk in tts_model.stream_tts_sync(prompt):<br>&#x20;&#x20;&#x20;&#x20;yield audio_chunk</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como antes hemos usado el puerto <code>8000</code>, por si os dice que est√° ocupado, creamos una funci√≥n para encontrar un puerto libre y encontramos uno</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">def find_free_port(start_port=8000, max_port=9000):<br>&#x20;&#x20;"""Find the first free port starting from start_port."""<br>&#x20;&#x20;print(f"Searching for a free port starting from {opening_brace}start_port{closing_brace}...")<br>&#x20;&#x20;for port in range(start_port, max_port):<br>&#x20;&#x20;&#x20;&#x20;with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;result = sock.connect_ex((&#39;127.0.0.1&#39;, port))<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;if result != 0:  # If result != 0, the port is free<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;print(f"Free port found: {opening_brace}port{closing_brace}")<br>&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;return port<br>&#x20;&#x20;raise RuntimeError(f"No free port found between {opening_brace}start_port{closing_brace} and {opening_brace}max_port{closing_brace}")<br>&#x20;&#x20;<br>free_port = find_free_port()    # Search for a free port</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se crea el stream y ahora se usa <code>stream.fastphone()</code> para obtener un n√∫mero de tel√©fono gratuito para llamar a tu stream, en vez de <code>stream.ui.launch()</code> que usamos antes para crear la interfaz gr√°fica.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">stream = Stream(ReplyOnPause(echo), modality="audio", mode="send-receive")<br>stream.fastphone(token=HUGGINGFACE_FASTRTC_PHONE_CALL_TOKEN, port=free_port)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si lo ejecutamos, veremos algo como esto:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">python</span> <span class="n">fastrtc_phone_demo</span><span class="o">.</span><span class="n">py</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî',
          '[32mINFO[0m:	  Warming up STT model.',
          '[32mINFO[0m:	  STT model warmed up.',
          '[32mINFO[0m:	  Warming up VAD model.',
          '[32mINFO[0m:	  VAD model warmed up.',
          'Searching for a free port starting from 8000...',
          'Free port found: 8004',
          '[32mINFO[0m:     Started server process [[36m24029[0m]',
          '[32mINFO[0m:     Waiting for application startup.',
          '[32mINFO[0m:	  Visit [36mhttps://fastrtc.org/userguide/api/[0m for WebRTC or Websocket API docs.',
          '[32mINFO[0m:     Application startup complete.',
          '[32mINFO[0m:     Uvicorn running on [1mhttp://127.0.0.1:8004[0m (Press CTRL+C to quit)',
          '[32mINFO[0m:	  Your FastPhone is now live! Call [36m+1 877-713-4471[0m and use code [36m994514[0m to connect to your stream.',
          '[32mINFO[0m:	  You have [36m30:00[0m minutes remaining in your quota (Resetting on [36m2025-04-07[0m)',
          '[32mINFO[0m:	  Visit [36mhttps://fastrtc.org/userguide/audio/#telephone-integration[0m for information on making your handler compatible with phone usage.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que aparece</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-bash">INFO:	  Your FastPhone is now live! Call +1 877-713-4471 and use code 994514 to connect to your stream.<br>INFO:	  You have 30:00 minutes remaining in your quota (Resetting on 2025-04-07)</code></pre></div>
            </section>
      <p>Es decir, si llamamos al n√∫mero <code>+1 877-713-4471</code> y usamos el c√≥digo <code>994514</code> nos conectar√° a nuestro stream.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si nos vamos a <a href="https://fastrtc.org/userguide/audio/#telephone-integration" target="_blank" rel="nofollow noreferrer">Telephone Integration</a> de la documentaci√≥n de <code>FastRTC</code> veremos que usa <a href="https://www.twilio.com/">twilio</a> para hacer la llamada. Tiene opci√≥n para configurar un n√∫mero local desde estados unidos, Dublin, Frankfurt, Tokio, Singapur, Sidney y Sao Paulo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>He probado a hacer la llamada desde Espa√±a (que me va a costar bastante) y funciona, pero es lento. He llamado, he metido el c√≥digo y he estado esperando a que conectara con el agente, pero como estaba tardando mucho, he colgado.</p>
      </section>







    </div>

  </section>

</PostLayout>
