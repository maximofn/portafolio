---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'GPT1 ‚Äì Improving Language Understanding by Generative Pre-Training';
const end_url = 'gpt1';
const description = '¬°¬°¬°Desbloquea el poder del lenguaje!!! üí• En mi √∫ltimo post, te llevo de la mano a trav√©s del paper de GPT-1, explicando de manera clara y concisa c√≥mo funciona este modelo pionero en el procesamiento de lenguaje natural. ¬°Y no solo eso! Tambi√©n te muestro c√≥mo hacer un fine-tuning del modelo para que puedas adaptarlo a tus necesidades espec√≠ficas üìä. ¬°No te pierdas la oportunidad de aprender sobre uno de los modelos m√°s influyentes de la historia! üöÄ ¬°Lee mi post y descubre c√≥mo puedes mejorar tus habilidades en inteligencia artificial! üìÑ';
const keywords = 'gpt1, nlp, transformers, fine-tuning, modelo de lenguaje, hugging face, pytorch';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_thumnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-06-12+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Paper"><h2>Paper</h2></a>
      <a class="anchor-link" href="#Arquitectura"><h2>Arquitectura</h2></a>
      <a class="anchor-link" href="#Resumen-del-paper"><h2>Resumen del paper</h2></a>
      <a class="anchor-link" href="#Generaci%C3%B3n-de-texto"><h2>Generaci√≥n de texto</h2></a>
      <a class="anchor-link" href="#Generar-texto-token-a-token"><h3>Generar texto token a token</h3></a>
      <a class="anchor-link" href="#Greedy-search"><h4>Greedy search</h4></a>
      <a class="anchor-link" href="#Fine-tuning-GPT"><h2>Fine tuning GPT</h2></a>
      <a class="anchor-link" href="#C%C3%A1lculo-de-la-loss"><h3>C√°lculo de la loss</h3></a>
      <a class="anchor-link" href="#Dataset"><h3>Dataset</h3></a>
      <a class="anchor-link" href="#Entrenamiento-con-Pytorch"><h3>Entrenamiento con Pytorch</h3></a>
      <a class="anchor-link" href="#Pytorch-dataset"><h4>Pytorch dataset</h4></a>
      <a class="anchor-link" href="#Dataloader"><h4>Dataloader</h4></a>
      <a class="anchor-link" href="#Training"><h4>Training</h4></a>
      <a class="anchor-link" href="#Inference"><h4>Inference</h4></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="GPT1---Improving-Language-Understanding-by-Generative-Pre-Training">GPT1 - Improving Language Understanding by Generative Pre-Training<a class="anchor-link" href="#GPT1---Improving-Language-Understanding-by-Generative-Pre-Training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Paper">Paper<a class="anchor-link" href="#Paper"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="nofollow noreferrer">Improving Language Understanding by Generative Pre-Training</a> es el paper de GPT1. Antes de leer el post es necesario que te pongas en situaci√≥n, antes de GPT los modelos de lenguaje estaban basados en redes recurrentes (RNN), que eran redes que funcionaban relativamente bien para tareas espec√≠ficas, pero con las que no se pod√≠a reutilizar el preentrenamiento para hacerles un fine tuning para otras tareas. Adem√°s no ten√≠an mucha memoria, por lo que si se le met√≠an frases muy largas no recordaban muy bien el inicio de la frase</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Arquitectura">Arquitectura<a class="anchor-link" href="#Arquitectura"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de hablar de la arquitectura de GPT1 recordemos c√≥mo era la arquitectura de los transformers</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer architecture" src="https://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp" width="852" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>GPT1 es un modelo basado en los decoders de los transformers, as√≠ que como no tenemos encoder la arquitectura de un solo decoder queda de la siguiente manera</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="decoder architecture" src="https://maximofn.com/wp-content/uploads/2024/06/transformer_decoder_only-scaled.webp" width="1200" height="1069"/></p>
      <p>Se elimina el mecanismo de atenci√≥n entre la sentencia del encoder y del decoder</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el paper de GPT1 proponen la siguiente arquitectura</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="gpt1 architecture" src="https://maximofn.com/wp-content/uploads/2024/06/GPT1_architecture.webp" width="310" height="604"/></p>
      <p>Que corresponde al decoder de un transformer como hemos visto antes, ejecutado 12 veces</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resumen-del-paper">Resumen del paper<a class="anchor-link" href="#Resumen-del-paper"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Las ideas m√°s interesantes del paper son:</p>
      <ul>
      <li>Se entrena el modelo en un gran corpus de texto sin supervisi√≥n. Con esto se consigue crear un modelado del lenguaje. Se crea un modelo de lenguaje de alta capacidad en un gran corpus de texto</li>
      <li>Luego se hace un fine-tuning en tareas de NLP supervisadas con datasets etiquetados. Se realiza un ajuste fino en una tarea objetivo con supervisi√≥n. Adem√°s, cuando se evalua al modelo en la tarea supervisada, no solo se le evalua por esa tarea, sino por lo bien que predice el siguiente token, esto ayuda a mejorar la generalizaci√≥n del modelo supervisado y hace que el modelo converja m√°s r√°pido.</li>
      <li>Aunque ya lo hemos contado, en el paper se dice que se utiliza la arquitectura transformer, ya que hasta ese momento se usaban RNN para los modelos de lenguaje. Lo que produjo una mejora en que lo aprendido en el primer entrenamiento (entrenamiento en el corpus de texto sin supervisi√≥n) es m√°s f√°cil de transferir a tareas supervisadas. Es decir, gracias al uso de transformers se pudo hacer un entrenamiento en todo un corpus de texto y luego fine tunings en tareas supervisadas.</li>
      <li>Evaluaron el modelo en cuatro tipos de tareas de comprensi√≥n del lenguaje:<ul>
      <li>Inferencia del lenguaje natural</li>
      <li>Respuesta a preguntas</li>
      <li>Similitud sem√°ntica</li>
      <li>Clasificaci√≥n de textos.</li>
      </ul>
      </li>
      <li>El modelo general (el entrenado en todo el corpus de texto sin supervisi√≥n) supera a los modelos RNN entrenados discriminativamente que emplean arquitecturas dise√±adas espec√≠ficamente para cada tarea, mejorando significativamente el estado del arte en 9 de las 12 tareas estudiadas. Tambi√©n analizan los comportamientos de "disparo cero" del modelo preentrenado en cuatro entornos diferentes y demostraron que adquiere un conocimiento ling√º√≠stico √∫til para las tareas posteriores.</li>
      <li>En los √∫ltimos a√±os, los investigadores hab√≠an demostrado los beneficios de utilizar embeddings, que se entrenan en corpus no etiquetados, para mejorar el rendimiento en una variedad de tareas. Sin embargo, estos enfoques transfieren principalmente informaci√≥n a nivel de palabra, mientras que el uso de transformers entrenados en grandes corpus de texto sin supervisi√≥n captura la sem√°ntica de nivel superior, a nivel de frase.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Generaci%C3%B3n-de-texto">Generaci√≥n de texto<a class="anchor-link" href="#Generaci%C3%B3n-de-texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver c√≥mo generar texto con un GPT1 preentrenado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero hay que instalar <code>ftfy</code> y <code>spacy</code> mediante</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ftfy<span class="w"> </span>spacy
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez instaladas, debes descargar el modelo de lenguaje de spacy que deseas utilizar. Por ejemplo, para descargar el modelo de ingl√©s, puedes ejecutar:</p>
      <div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>spacy<span class="w"> </span>download<span class="w"> </span>en_core_web_sm
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para generar texto vamos a utilizar el modelo desde el repositorio de <a href="https://huggingface.co/openai-community/openai-gpt" target="_blank" rel="nofollow noreferrer">GPT1</a> de Hugging Face.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importamos las librer√≠as</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTTokenizer</span><span class="p">,</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Si te fijas hemos importado <code>OpenAIGPTTokenizer</code> y <code>AutoTokenizer</code>. Esto es porque en la <a href="https://huggingface.co/openai-community/openai-gpt" target="_blank" rel="nofollow noreferrer">model card</a> de GPT1 se indica que se use <code>OpenAIGPTTokenizer</code>, pero en el post de la librer√≠a <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> explicamos que se debe usar <code>AutoTokenizer</code> para cargar el tokenizador. As√≠ que vamos a probar los dos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTTokenizer</span><span class="p">,</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
          '</span><span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">OpenAIGPTTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello, my dog is cute and"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="n">input_auto_tokens</span> <span class="o">=</span> <span class="n">auto_tokenizer</span><span class="p">(</span><span class="s2">"Hello, my dog is cute and"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input auto tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_auto_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'input tokens: ',
          '{\'input_ids\': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1, 1]])}',
          'input auto tokens: ',
          '{\'input_ids\': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1, 1]])}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver con los dos tokenizadores se obtienen los mismos tokens. As√≠ que para que el c√≥digo sea m√°s general, de manera que si se cambian los ckeckpoints, no haya que cambiar el c√≥digo, vamos a utilizar <code>AutoTokenizer</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos entonces el device, el tokenizador y el modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Como hemos instanciado el modelo vamos a ver cu√°ntos par√°metros tiene</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '</span><span class="n">params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">params</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Number of parameters: 117M',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>En la √©poca de los billones de par√°metros, podemos ver que GPT1 solo ten√≠a 117 millones de par√°metros</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos los tokens de entrada al modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_sentence</span> <span class="o">=</span> <span class="s2">"Hello, my dog is cute and"</span>',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '',
          '<span class="n">input_tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'input_ids\': tensor([[3570,  240,  547, 2585,  544, 4957,  488]], device=\'cuda:0\'), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1, 1]], device=\'cuda:0\')}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se los pasamos al modelo para generar los tokens de salida</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>
      
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">output_tokens</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>output tokens: 
      tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,
                 485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],
             device='cuda:0')
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
        warnings.warn(
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Decodificamos los tokens para obtener la sentencia de salida</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">output_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded output: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'output tokens: ',
          'tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,',
          '           485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],',
          '       device=\'cuda:0\')',
          'decoded output: ',
          'hello, my dog is cute and i\'m going to take him for a walk. " ',
          ' "',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ya hemos conseguido generar texto con GPT1</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Generar-texto-token-a-token">Generar texto token a token<a class="anchor-link" href="#Generar-texto-token-a-token"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Greedy-search">Greedy search<a class="anchor-link" href="#Greedy-search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos usado <code>model.generate</code> para generar los tokens de salida de golpe, pero vamos a ver c√≥mo generarlos uno a uno. Para ello, en vez de usar <code>model.generate</code> vamos a usar <code>model</code>, que en realidad lo que hace es llamar al m√©todo <code>model.forward</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '',
          '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],',
          '         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],',
          '         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],',
          '         ...,',
          '         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],',
          '         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],',
          '         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],',
          '       device=\'cuda:0\', grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que saca muchos datos, primero vamos a ver las keys de la salida</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'odict_keys([\'logits\'])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>En este caso solo tenemos los logits del modelo, vamos a ver su tama√±o</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
          '',
          '<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 7, 40478])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver cuantos tokens ten√≠amos a la entrada</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 7])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vaya, a la salida tenemos el mismo n√∫mero de logits que a la entrada. Esto es normal</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos los logits de la √∫ltima posici√≥n de la salida</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
          '',
          '<span class="n">nex_token_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([40478])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hay un total de 40478 logits, es decir, hay un vocabulario de 40478 tokens y tenemos que ver cu√°l es el token con mayor probabilidad, para ello primero calculamos las softmax</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">softmax_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([40478])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(tensor(0.1898, device=\'cuda:0\', grad_fn=&lt;MaxBackward0&gt;),',
          ' tensor(249, device=\'cuda:0\'))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hemos obtenido el siguiente token, ahora lo decodificamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'i\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hemos obtenido el siguiente token mediante el m√©todo greedy, es decir, el token con mayor probabilidad. Pero ya vimos en el post de la librer√≠a transformers, las <a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">formas de generar textos</a> que se puede hacer sampling, top-k, top-p, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a meter todo en una funci√≥n y ver qu√© sale si generamos unos cuantos tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
      '          <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '          <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '          <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '          <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
      '          <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '          <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '          <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '          <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
      '<span></span><span class="k">def</span> <span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
      '          <span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>',
      '          <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>',
      '              <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
      '              <span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
      '          <span class="k">return</span> <span class="n">generated_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Ahora generamos texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
          '    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
          '    <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
          '    <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '    <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '    <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
          '</span><span class="k">def</span> <span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
          '    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>',
          '    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>',
          '        <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
          '        <span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
          '    <span class="k">return</span> <span class="n">generated_text</span>',
          '</span><span class="n">generate_greedy_text</span><span class="p">(</span><span class="s2">"Hello, my dog is cute and"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'Hello, my dog is cute andi."\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>La salida es bastante repetitiva como ya se vio en las <a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">formas de generar textos</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Fine-tuning-GPT">Fine tuning GPT<a class="anchor-link" href="#Fine-tuning-GPT"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="C%C3%A1lculo-de-la-loss">C√°lculo de la loss<a class="anchor-link" href="#C%C3%A1lculo-de-la-loss"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de empezar a hacer el fine tuning de GPT1 vamos a ver una cosa. Antes cuando obten√≠amos la salida del modelo hac√≠amos esto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '',
          '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],',
          '         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],',
          '         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],',
          '         ...,',
          '         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],',
          '         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],',
          '         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],',
          '       device=\'cuda:0\', grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se puede ver que obtenemos <code>loss=None</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'None',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vamos a necesitar la loss para hacer el fine tuning, vamos a ver c√≥mo obtenerla.</p>
      <p>Si nos vamos a la documentaci√≥n del m√©todo <a href="https://huggingface.co/docs/transformers/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel.forward" target="_blank" rel="nofollow noreferrer">forward</a> de <code>OpenAIGPTLMHeadModel</code>, podemos ver que dice que a la salida devuelve un objeto de tipo <code>transformers.modeling_outputs.CausalLMOutput</code>, as√≠ que si nos vamos a la documentaci√≥n de <a href="https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput">transformers.modeling_outputs.CausalLMOutput</a>, podemos ver que dice que devuelve <code>loss</code> si se le pasa <code>labels</code> al m√©todo <code>forward</code>.</p>
      <p>Si nos vamos a la fuente del c√≥digo del m√©todo <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai/modeling_openai.py#L544" target="_blank" rel="nofollow noreferrer">forward</a>, vemos este bloque de c√≥digo</p>
      <div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
              <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                  <span class="c1"># Shift so that tokens &lt; n predict n</span>
                  <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">lm_logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
                  <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
                  <span class="c1"># Flatten the tokens</span>
                  <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
                  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
      </pre></div>
      <p>Es decir, la <code>loss</code> se calcula de la siguiente manera</p>
      <ul>
      <li>Shift de logits y labels: La primera parte es desplazar los logits (<code>lm_logits</code>) y las etiquetas (<code>labels</code>) para que los <code>tokens &lt; n</code> predigan <code>n</code>, es decir, desde una posici√≥n <code>n</code> se predice el siguiente token a partir de los anteriores.</li>
      <li>CrossEntropyLoss: Se crea una instancia de la funci√≥n de p√©rdida <code>CrossEntropyLoss()</code>.</li>
      <li>Flatten tokens: A continuaci√≥n, se aplanan los logits y las etiquetas utilizando <code>view(-1, shift_logits.size(-1))</code> y <code>view(-1)</code>, respectivamente. Esto se hace para que los logits y las etiquetas tengan la misma forma para la funci√≥n de p√©rdida.</li>
      <li>C√°lculo de la p√©rdida: Finalmente, se calcula la p√©rdida utilizando la funci√≥n de p√©rdida <code>CrossEntropyLoss()</code> con los logits aplanados y las etiquetas aplanadas como entradas.</li>
      </ul>
      <p>En resumen, la <code>loss</code> se calcula como la p√©rdida de entrop√≠a cruzada entre los logits desplazados y aplanados y las etiquetas desplazadas y aplanadas.</p>
      <p>Por tanto, si al m√©todo <code>forward</code> le pasamos los labels, nos devolver√° la <code>loss</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>',
          '',
          '<span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'tensor(4.2607, device=\'cuda:0\', grad_fn=&lt;NllLossBackward0&gt;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para el etrenamiento vamos a usar un dataset de chistes en ingl√©s <a href="https://huggingface.co/datasets/Maximofn/short-jokes-dataset" target="_blank" rel="nofollow noreferrer">short-jokes-dataset</a>, que es un dataset con 231 mil chistes en ingl√©s.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Descargamos el dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
          '',
          '<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"Maximofn/short-jokes-dataset"</span><span class="p">)</span>',
          '<span class="n">jokes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'DatasetDict({',
          '    train: Dataset({',
          '        features: [\'ID\', \'Joke\'],',
          '        num_rows: 231657',
          '    })',
          '})',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a verlo un poco</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'ID\': 1,',
          ' \'Joke\': \'[me narrating a documentary about narrators] "I can\'t hear what they\'re saying cuz I\'m talking"\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Entrenamiento-con-Pytorch">Entrenamiento con Pytorch<a class="anchor-link" href="#Entrenamiento-con-Pytorch"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero vamos a ver c√≥mo se har√≠a el entrenamiento con puro Pytorch</p>
      <blockquote>
      <p>Reiniciamos el notebook para que no haya problemas con la memoria de la GPU</p>
      </blockquote>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <h4 id="Pytorch-dataset">Pytorch dataset<a class="anchor-link" href="#Pytorch-dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una clase dataset de Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '              ',
      '          <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '              <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '              <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>





















      
      <section class="section-block-markdown-cell">
      <p>La instanciamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '              ',
      '          <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '              <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '              <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Vemos un ejemplo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
      <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&lt;|endoftext|&gt;
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[27]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>(torch.Size([1, 30]), torch.Size([1, 30]))</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dataloader">Dataloader<a class="anchor-link" href="#Dataloader"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos ahora un dataloader de Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '              ',
      '          <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '              <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '              <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
      '<span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
      '      <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>',
      '      ',
      '      <span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>',
      '      <span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Vemos un batch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
          '',
          '<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
          '',
          '<span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
          '    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
          '        ',
          '    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
          '        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
          '',
          '    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
          '        <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
          '        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '        <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
          '</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
          '</span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
          '<span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
          '</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>',
          '',
          '<span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>',
          '<span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '</span><span class="n">sentences</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">))</span>',
          '<span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'JOKE: Why can\'t Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&lt;|endoftext|&gt;',
          '(1, torch.Size([1, 1, 29]), torch.Size([1, 1, 29]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Training">Training<a class="anchor-link" href="#Training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>
      <span class="kn">import</span> <span class="nn">tqdm</span>
      
      <span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>
      <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-5</span>
      <span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>
      <span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>
      
      <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      <span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
      <span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>
      
      <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>
      
      <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
          
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"EPOCH </span><span class="si">{opening_brace}</span><span class="n">epoch</span><span class="si">{closing_brace}</span><span class="s2"> started"</span> <span class="o">+</span> <span class="s1">'='</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
          <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>
          
          <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
      
              <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>
              
              <span class="c1">#################### "Fit as many joke sequences into MAX_SEQ_LEN sequence as possible" logic start ####</span>
              <span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
      
              <span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>
              <span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
                  <span class="k">continue</span>
              
              <span class="c1"># The first joke sequence in the sequence</span>
              <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>
                  <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
                  <span class="k">continue</span>
              <span class="k">else</span><span class="p">:</span>
                  <span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>
                  <span class="c1"># as the start for next sequence </span>
                  <span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
                      <span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>
                      <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="c1">#Add the joke to sequence, continue and try to add more</span>
                      <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                      <span class="k">continue</span>
              <span class="c1">################## Sequence ready, process it trough the model ##################</span>
                  
              <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>
              <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
              <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                             
              <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>
              <span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
                  <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>    
                  <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>
                  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                  <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> 
                  <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                  <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      
              <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({opening_brace}</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">'lr'</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]{closing_brace})</span>
              <span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
                  <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 0 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:31&lt;00:00, 334.88it/s, loss=2.88, lr=2.93e-6]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 1 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:30&lt;00:00, 335.27it/s, loss=2.49, lr=5.87e-6]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 2 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:17&lt;00:00, 341.75it/s, loss=2.57, lr=8.81e-6]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 3 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:18&lt;00:00, 341.27it/s, loss=2.41, lr=1.18e-5]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 4 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:19&lt;00:00, 341.04it/s, loss=2.49, lr=1.47e-5]
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Inference">Inference<a class="anchor-link" href="#Inference"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver qu√© tal hace chistes el modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>',
          '<span class="kn">import</span> <span class="nn">tqdm</span>',
          '',
          '<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>',
          '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>',
          '<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-5</span>',
          '<span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>',
          '<span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>',
          '',
          '<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
          '<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>',
          '<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
          '<span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>',
          '<span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
          '',
          '<span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>',
          '',
          '<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>',
          '    ',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> started"</span> <span class="o">+</span> <span class="s1">\'=\'</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>',
          '    <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>',
          '    ',
          '    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
          '',
          '        <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>',
          '        ',
          '        <span class="c1">#################### "Fit as many joke sequences into MAX_SEQ_LEN sequence as possible" logic start ####</span>',
          '        <span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '',
          '        <span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>',
          '        <span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
          '            <span class="k">continue</span>',
          '        ',
          '        <span class="c1"># The first joke sequence in the sequence</span>',
          '        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>',
          '            <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
          '            <span class="k">continue</span>',
          '        <span class="k">else</span><span class="p">:</span>',
          '            <span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>',
          '            <span class="c1"># as the start for next sequence </span>',
          '            <span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
          '                <span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>',
          '                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
          '            <span class="k">else</span><span class="p">:</span>',
          '                <span class="c1">#Add the joke to sequence, continue and try to add more</span>',
          '                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
          '                <span class="k">continue</span>',
          '        <span class="c1">################## Sequence ready, process it trough the model ##################</span>',
          '            ',
          '        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>',
          '        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
          '        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>',
          '                       ',
          '        <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>',
          '        <span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>',
          '            <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>    ',
          '            <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>',
          '            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>',
          '            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> ',
          '            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
          '            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
          '',
          '        <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">\'loss\'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">\'lr\'</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]})</span>',
          '        <span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>',
          '            <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
          '</span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"JOKE:"</span>',
          '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
          '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning',
          '  warnings.warn(',
          'decoded joke: ',
          'joke : what do you call a group of people who are not afraid of the dark? a group',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se puede ver que le pasas una secuencia con la palabra <code>joke</code> y te devuelve un chiste. Pero si le devuelves otra secuencia no</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"My dog is cute and"</span>',
          '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
          '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'decoded joke: ',
          'my dog is cute and i\'m not sure if i should be offended or not. " ',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      






    </div>

  </section>

</PostLayout>
