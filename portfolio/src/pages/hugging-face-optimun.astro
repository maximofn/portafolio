---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Optimun';
const end_url = 'hugging-face-optimun';
const description = '¬°Atenci√≥n, modelos de PyTorch lentos! üêå Optimun, la librer√≠a de Hugging Face, viene al rescate para acelerar tus entrenamientos e inferencias. Con Optimun, puedes olvidarte de los problemas de velocidad y disfrutar de m√°s velocidad y eficiencia üïíÔ∏è. Y lo mejor de todo, es compatible con PyTorch. ¬°Vamos, dale un boost a tus modelos con Optimun! üíª';
const keywords = 'hugging face, optimun, pytorch, transformers, entrenamiento, inferencia, velocidad, eficiencia';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_optimun.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=2388
    image_height=884
    image_extension=webp
    article_date=2024-06-01+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Instalaci%C3%B3n"><h2>Instalaci√≥n</h2></a>
      <a class="anchor-link" href="#BeterTransformer"><h2>BeterTransformer</h2></a>
      <a class="anchor-link" href="#Inferencia-con-Automodel"><h3>Inferencia con Automodel</h3></a>
      <a class="anchor-link" href="#Inferecncia-con-Pipeline"><h3>Inferecncia con Pipeline</h3></a>
      <a class="anchor-link" href="#Entrenamiento"><h3>Entrenamiento</h3></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Hugging-Face-Optimun">Hugging Face Optimun<a class="anchor-link" href="#Hugging-Face-Optimun"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Optimum</code> es una extensi√≥n de la librer√≠a <a href="https://www.maximofn.com/hugging-face-transformers/">Transformers</a> que proporciona un conjunto de herramientas de optimizaci√≥n del rendimiento para entrenar y para la inferencia modelos, en hardware espec√≠fico, con la m√°xima eficiencia.</p>
      <p>El ecosistema de IA evoluciona r√°pidamente y cada d√≠a surge m√°s hardware especializado junto con sus propias optimizaciones. Por tanto, <code>Optimum</code> permite a los utilizar eficientemente cualquiera de este HW con la misma facilidad que <a href="https://www.maximofn.com/hugging-face-transformers/">Transformers</a>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Optimun</code> premite la optimizaci√≥n para las siguientes plataformas HW:</p>
      <ul>
      <li>Nvidia</li>
      <li>AMD</li>
      <li>Intel</li>
      <li>AWS</li>
      <li>TPU</li>
      <li>Habana</li>
      <li>FuriosaAI</li>
      </ul>
      <p>Adem√°s ofrece aceleraci√≥n para las siguientes integraciones open source</p>
      <ul>
      <li>ONNX runtime</li>
      <li>Exporters: Exportar omdelos Pytorch o TensorFlow a diferentes formatos como ONNX o TFLite</li>
      <li>BetterTransformer</li>
      <li>Torch FX</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Instalaci%C3%B3n">Instalaci√≥n<a class="anchor-link" href="#Instalaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para instalar <code>Optimum</code> simplemente ejecuta:</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>optimum
      </pre></div>
      <p>Pero si se quiere instalar con soporte para todas las plataformas HW, se puede hacer as√≠</p>
      <table>
      <thead>
      <tr>
      <th>Accelerator	</th>
      <th>Installation</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>ONNX Runtime</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]</code></td>
      </tr>
      <tr>
      <td>Intel Neural Compressor</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]</code></td>
      </tr>
      <tr>
      <td>OpenVINO</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[openvino]</code></td>
      </tr>
      <tr>
      <td>NVIDIA TensorRT-LLM</td>
      <td><code>docker run -it --gpus all --ipc host huggingface/optimum-nvidia</code></td>
      </tr>
      <tr>
      <td>AMD Instinct GPUs and Ryzen AI NPU</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[amd]</code></td>
      </tr>
      <tr>
      <td>AWS Trainum &amp; Inferentia</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[neuronx]</code></td>
      </tr>
      <tr>
      <td>Habana Gaudi Processor (HPU)</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[habana]</code></td>
      </tr>
      <tr>
      <td>FuriosaAI</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[furiosa]</code></td>
      </tr>
      </tbody>
      </table>
      <p>los flags <code>--upgrade --upgrade-strategy eager</code> son necesarios para garantizar que los diferentes paquetes se actualicen a la √∫ltima versi√≥n posible.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como la mayor√≠a de la gente usa Pytorch en GPUs de Nvidia, y sobre todo, como Nvidia es lo que yo tengo, este post va a hablar solo del uso de <code>Optimun</code> con GPUs de Nvidia y Pytorch.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="BeterTransformer">BeterTransformer<a class="anchor-link" href="#BeterTransformer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>BetterTransformer es una optimizaci√≥n nativa de PyTorch para obtener una aceleraci√≥n de x1,25 a x4 en la inferencia de modelos basados ‚Äã‚Äãen Transformer</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>BetterTransformer es una API que permite aprovechar las caracter√≠sticas de hardware modernas para acelerar el entrenamiento y la inferencia de modelos de transformers en PyTorch, utilizando implementaciones de atenci√≥n m√°s eficientes y <code>fast path</code> de la versi√≥n nativa de <code>nn.TransformerEncoderLayer</code>.</p>
      <p>BetterTransformer usa dos tipos de aceleraciones:</p>
      <ol>
      <li><code>Flash Attention</code>: Esta es una implementaci√≥n de la <code>attention</code> que utiliza <code>sparse</code> para reducir la complejidad computacional. La atenci√≥n es una de las operaciones m√°s costosas en los modelos de transformers, y <code>Flash Attention</code> la hace m√°s eficiente.</li>
      <li><code>Memory-Efficient Attention</code>: Esta es otra implementaci√≥n de la atenci√≥n que utiliza la funci√≥n <code>scaled_dot_product_attention</code> de PyTorch. Esta funci√≥n es m√°s eficiente en t√©rminos de memoria que la implementaci√≥n est√°ndar de la atenci√≥n en PyTorch.</li>
      </ol>
      <p>Adem√°s, la versi√≥n 2.0 de PyTorch incluye un operador de atenci√≥n de productos punto escalado (SDPA) nativo como parte de <code>torch.nn.functional</code></p>
      <p><code>Optimun</code> proporciona esta funcionalidad con la librer√≠a <code>Transformers</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Inferencia-con-Automodel">Inferencia con Automodel<a class="anchor-link" href="#Inferencia-con-Automodel"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero vamos a ver c√≥mo ser√≠a la inferencia normal con <code>Transformers</code> y <code>Automodel</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>
      
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[1]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vemos c√≥mo se optimizar√≠a con <code>BetterTransformer</code> y <code>Optimun</code></p>
      <p>Lo que tenemos que hacer es convertir el modelo mediante el m√©todo <code>transform</code> de <code>BeterTransformer</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      <span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>
      
      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
      <span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>
      
      <span class="c1"># Convert the model to a BetterTransformer model</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
      Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[2]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Inferecncia-con-Pipeline">Inferecncia con Pipeline<a class="anchor-link" href="#Inferecncia-con-Pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al igual que antes, primero vemos c√≥mo ser√≠a la inferencia normal con <code>Transformers</code> y <code>Pipeline</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>',
          '',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '<span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>',
          '',
          '<span class="c1"># Convert the model to a BetterTransformer model</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">)</span>',
          '<span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '[{\'score\': 0.05116177722811699,',
          '  \'token\': 8422,',
          '  \'token_str\': \'stanford\',',
          '  \'sequence\': \'i am a student at stanford university.\'},',
          ' {\'score\': 0.04033993184566498,',
          '  \'token\': 5765,',
          '  \'token_str\': \'harvard\',',
          '  \'sequence\': \'i am a student at harvard university.\'},',
          ' {\'score\': 0.03990468755364418,',
          '  \'token\': 7996,',
          '  \'token_str\': \'yale\',',
          '  \'sequence\': \'i am a student at yale university.\'},',
          ' {\'score\': 0.0361952930688858,',
          '  \'token\': 10921,',
          '  \'token_str\': \'cornell\',',
          '  \'sequence\': \'i am a student at cornell university.\'},',
          ' {\'score\': 0.03303057327866554,',
          '  \'token\': 9173,',
          '  \'token_str\': \'princeton\',',
          '  \'sequence\': \'i am a student at princeton university.\'}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vemos c√≥mo optimizarlo, para ello usamos <code>pipeline</code> de <code>Optimun</code>, en vez de el de <code>Transformers</code>. Adem√°s hay que indicar que queremos usar <code>bettertransformer</code> como acelerador</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">optimum.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>
      
      <span class="c1"># Use the BetterTransformer pipeline</span>
      <span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">"bettertransformer"</span><span class="p">)</span>
      <span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
      /home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)
        hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[4]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>[{opening_brace}'score': 0.05116180703043938,
        'token': 8422,
        'token_str': 'stanford',
        'sequence': 'i am a student at stanford university.'{closing_brace},
       {opening_brace}'score': 0.040340032428503036,
        'token': 5765,
        'token_str': 'harvard',
        'sequence': 'i am a student at harvard university.'{closing_brace},
       {opening_brace}'score': 0.039904672652482986,
        'token': 7996,
        'token_str': 'yale',
        'sequence': 'i am a student at yale university.'{closing_brace},
       {opening_brace}'score': 0.036195311695337296,
        'token': 10921,
        'token_str': 'cornell',
        'sequence': 'i am a student at cornell university.'{closing_brace},
       {opening_brace}'score': 0.03303062543272972,
        'token': 9173,
        'token_str': 'princeton',
        'sequence': 'i am a student at princeton university.'{closing_brace}]</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Entrenamiento">Entrenamiento<a class="anchor-link" href="#Entrenamiento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para el entrneamiento con <code>Optimun</code> hacemos lo mismo que con la inferencia con Automodel, convertimos el modelo mediante el m√©todo <code>transform</code> de <code>BeterTransformer</code>.</p>
      <p>Cuando terminamos el entrenamiento, volvemos a convertir el modelo mediante el m√©todo <code>reverse</code> de <code>BeterTransformer</code> para volver a tener el modelo original y as√≠ poder guardarlo y subirlo al hub de Hugging Face.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">optimum.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="c1"># Use the BetterTransformer pipeline</span>',
      '      <span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">"bettertransformer"</span><span class="p">)</span>',
      '      <span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '      <span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '      <span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>',
      '      ',
      '      <span class="c1"># Convert the model to a BetterTransformer model</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '      ',
      '      <span class="c1">##############################################################################</span>',
      '      <span class="c1"># do your training here</span>',
      '      <span class="c1">##############################################################################</span>',
      '      ',
      '      <span class="c1"># Convert the model back to a Hugging Face model</span>',
      '      <span class="n">model_hf</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model_hf</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">"fine_tuned_model"</span><span class="p">)</span>',
      '      <span class="n">model_hf</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">"fine_tuned_model"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
























      






    </div>

  </section>

</PostLayout>
