---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Transformers';
const end_url = 'hugging-face-transformers';
const description = 'ü§ñ ¬°Transforma tu mundo con Transformers de Hugging Face! üöÄ ¬øListo para hacer magia con el lenguaje natural? Desde t√©cnicas s√∫per r√°pidas con pipeline üå™Ô∏è hasta trucos ninja con AutoModel ü•∑, este post te lleva de la mano en una aventura √©pica en el universo NLP. Explora c√≥mo generar texto que sorprende, entrena modelos que deslumbran y comparte tus creaciones en el Hugging Face Hub como un pro. ¬°Prep√°rate para codificar y re√≠r, porque el futuro del NLP es ahora y es divertid√≠simo! üòÇ';
const keywords = 'Hugging Face, Transformers, PNL, Procesamiento de Lenguaje Natural, AutoModel, pipeline, fine-tuning, entrenamiento, compartir, Hub';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/HuggingFace%20Transformers.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1147
    image_height=644
    image_extension=webp
    article_date=2024-04-15+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Instalaci%C3%B3n"><h2>Instalaci√≥n</h2></a>
      <a class="anchor-link" href="#Inferencia-con-pipeline"><h2>Inferencia con <code>pipeline</code></h2></a>
      <a class="anchor-link" href="#Tareas"><h3>Tareas</h3></a>
      <a class="anchor-link" href="#Uso-de-pipeline"><h3>Uso de <code>pipeline</code></h3></a>
      <a class="anchor-link" href="#C%C3%B3mo-funciona-pipeline"><h3>C√≥mo funciona <code>pipeline</code></h3></a>
      <a class="anchor-link" href="#Inferencia-con-AutoClass-y-pipeline"><h2>Inferencia con <code>AutoClass</code> y <code>pipeline</code></h2></a>
      <a class="anchor-link" href="#Tokenizaci%C3%B3n-con-AutoTokenizer"><h3>Tokenizaci√≥n con <code>AutoTokenizer</code></h3></a>
      <a class="anchor-link" href="#Modelo-AutoModel"><h3>Modelo <code>AutoModel</code></h3></a>
      <a class="anchor-link" href="#Modelo-AutoModelFor"><h3>Modelo <code>AutoModelFor</code></h3></a>
      <a class="anchor-link" href="#Inferencia-solo-con-AutoClass"><h2>Inferencia solo con <code>AutoClass</code></h2></a>
      <a class="anchor-link" href="#Generaci%C3%B3n-de-texto-casual"><h3>Generaci√≥n de texto casual</h3></a>
      <a class="anchor-link" href="#Clasificaci%C3%B3n-de-texto"><h3>Clasificaci√≥n de texto</h3></a>
      <a class="anchor-link" href="#Clasificaci%C3%B3n-de-tokens"><h3>Clasificaci√≥n de tokens</h3></a>
      <a class="anchor-link" href="#Respuesta-a-preguntas-(question-answering)"><h3>Respuesta a preguntas (question answering)</h3></a>
      <a class="anchor-link" href="#Modelizaci%C3%B3n-del-lenguaje-enmascarado-(Masked-language-modeling)"><h3>Modelizaci√≥n del lenguaje enmascarado (Masked language modeling)</h3></a>
      <a class="anchor-link" href="#Personalizaci%C3%B3n-del-modelo"><h2>Personalizaci√≥n del modelo</h2></a>
      <a class="anchor-link" href="#Tokenizaci%C3%B3n"><h2>Tokenizaci√≥n</h2></a>
      <a class="anchor-link" href="#Padding"><h3>Padding</h3></a>
      <a class="anchor-link" href="#Truncado"><h3>Truncado</h3></a>
      <a class="anchor-link" href="#Tensores"><h3>Tensores</h3></a>
      <a class="anchor-link" href="#M%C3%A1scaras"><h3>M√°scaras</h3></a>
      <a class="anchor-link" href="#Fast-Tokenizers"><h2>Fast Tokenizers</h2></a>
      <a class="anchor-link" href="#Formas-de-generaci%C3%B3n-de-texto"><h2>Formas de generaci√≥n de texto</h2></a>
      <a class="anchor-link" href="#Greedy-Search"><h3>Greedy Search</h3></a>
      <a class="anchor-link" href="#Contrastive-Search"><h3>Contrastive Search</h3></a>
      <a class="anchor-link" href="#Multinomial-sampling"><h3>Multinomial sampling</h3></a>
      <a class="anchor-link" href="#Beam-search"><h3>Beam search</h3></a>
      <a class="anchor-link" href="#Beam-search-multinomial-sampling"><h3>Beam search multinomial sampling</h3></a>
      <a class="anchor-link" href="#Beam-search-n-grams-penalty"><h3>Beam search n-grams penalty</h3></a>
      <a class="anchor-link" href="#Beam-search-n-grams-penalty-return-sequences"><h3>Beam search n-grams penalty return sequences</h3></a>
      <a class="anchor-link" href="#Diverse-beam-search-decoding"><h3>Diverse beam search decoding</h3></a>
      <a class="anchor-link" href="#Speculative-Decoding"><h3>Speculative Decoding</h3></a>
      <a class="anchor-link" href="#Speculative-Decoding-randomness-control"><h3>Speculative Decoding randomness control</h3></a>
      <a class="anchor-link" href="#Sampling"><h3>Sampling</h3></a>
      <a class="anchor-link" href="#Sampling-temperature"><h3>Sampling temperature</h3></a>
      <a class="anchor-link" href="#Sampling-top-k"><h3>Sampling top-k</h3></a>
      <a class="anchor-link" href="#Sampling-top-p-(nucleus-sampling)"><h3>Sampling top-p (nucleus sampling)</h3></a>
      <a class="anchor-link" href="#Sampling-top-k-y-top-p"><h3>Sampling top-k y top-p</h3></a>
      <a class="anchor-link" href="#Streaming"><h2>Streaming</h2></a>
      <a class="anchor-link" href="#Plantillas-de-chat"><h2>Plantillas de chat</h2></a>
      <a class="anchor-link" href="#Tokenizaci%C3%B3n-del-contexto"><h3>Tokenizaci√≥n del contexto</h3></a>
      <a class="anchor-link" href="#A%C3%B1adir-generaci%C3%B3n-de-prompts"><h3>A√±adir generaci√≥n de prompts</h3></a>
      <a class="anchor-link" href="#Generaci%C3%B3n-de-texto"><h3>Generaci√≥n de texto</h3></a>
      <a class="anchor-link" href="#Generaci%C3%B3n-de-texto-con-pipeline"><h3>Generaci√≥n de texto con <code>pipeline</code></h3></a>
      <a class="anchor-link" href="#Train"><h2>Train</h2></a>
      <a class="anchor-link" href="#Dataset"><h3>Dataset</h3></a>
      <a class="anchor-link" href="#Tokenizaci%C3%B3n"><h3>Tokenizaci√≥n</h3></a>
      <a class="anchor-link" href="#Modelo"><h3>Modelo</h3></a>
      <a class="anchor-link" href="#M%C3%A9trica-de-evaluaci%C3%B3n"><h3>M√©trica de evaluaci√≥n</h3></a>
      <a class="anchor-link" href="#Trainer"><h3>Trainer</h3></a>
      <a class="anchor-link" href="#Probando-el-modelo"><h3>Probando el modelo</h3></a>
      <a class="anchor-link" href="#Compartir-el-modelo-en-el-Hub-de-Hugging-Face"><h2>Compartir el modelo en el Hub de Hugging Face</h2></a>
      <a class="anchor-link" href="#Logging"><h3>Logging</h3></a>
      <a class="anchor-link" href="#Subida-una-vez-entenado"><h3>Subida una vez entenado</h3></a>
      <a class="anchor-link" href="#Subida-mientras-se-entrena"><h3>Subida mientras se entrena</h3></a>
      <a class="anchor-link" href="#Hub-como-repositorio-git"><h2>Hub como repositorio git</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Hugging-Face-transformers">Hugging Face transformers<a class="anchor-link" href="#Hugging-Face-transformers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La librer√≠a <code>transformers</code> de Hugging Face es una de las librer√≠as m√°s populares para trabajar con modelos de lenguaje. Su facilidad de uso hizo que se democratizara el uso de la arquitectura <code>Transformer</code> y que se pudiera trabajar con modelos de lenguaje de √∫ltima generaci√≥n sin necesidad de tener un gran conocimiento en el √°rea.</p>
      <p>Entre la librer√≠a <code>transformers</code>, el hub de modelos y su facilidad de uso, los spaces y la facilidad de desplegar demos, y nuevas librer√≠as como <code>datasets</code>, <code>accelerate</code>, <code>PEFT</code> y otras m√°s, han hecho que Hugging Face sea uno de los actores m√°s importantes de la escena de inteligencia artificial del momento. Ellos mismos se auto-denominan como "el GitHub de la IA" y ciertamente lo son.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Instalaci%C3%B3n">Instalaci√≥n<a class="anchor-link" href="#Instalaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para instalar transformers se puede hacer con <code>pip</code></p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>transformers
      </pre></div>
      <p>o con <code>conda</code></p>
      <div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::transformers
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Adem√°s de la librer√≠a es necesario tener un backend de PyTorch o TensorFlow instalado. Es decir, necesitas tener instalar <code>torch</code> o <code>tensorflow</code> para poder usar <code>transformers</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Inferencia-con-pipeline">Inferencia con <code>pipeline</code><a class="anchor-link" href="#Inferencia-con-pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Con los <code>pipeline</code>s de <code>transformers</code> se puede hacer inferencia con modelos de lenguaje de una manera muy sencilla. Esto tiene la ventaja de que el desarrollo se realiza de manera mucho m√°s r√°pida y se puede hacer prototipado de manera muy sencilla. Adem√°s permite a personas que no tienen mucho conocimiento poder usar los modelos</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Con <code>pipeline</code> se puede hacer inferencia en un mont√≥n de tareas diferentes. Cada tarea tiene su propio <code>pipeline</code> (<code>pipeline</code> de NLP, <code>pipeline</code> de vision, etc), pero se puede hacer una abstracci√≥n general usando la clase <code>pipeline</code> que se encarga de seleccionar el <code>pipeline</code> adecuado para la tarea que se le pase.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tareas">Tareas<a class="anchor-link" href="#Tareas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al d√≠a de escribir este post, las tareas que se pueden hacer con <code>pipeline</code> son:</p>
      <ul>
      <li><p>Audio:</p>
      <ul>
      <li>Clasificaci√≥n de audio<ul>
      <li>clasificaci√≥n de escena ac√∫stica: etiquetar audio con una etiqueta de escena (‚Äúoficina‚Äù, ‚Äúplaya‚Äù, ‚Äúestadio‚Äù)</li>
      <li>detecci√≥n de eventos ac√∫sticos: etiquetar audio con una etiqueta de evento de sonido (‚Äúbocina de autom√≥vil‚Äù, ‚Äúllamada de ballena‚Äù, ‚Äúcristal rompi√©ndose‚Äù)</li>
      <li>etiquetado: etiquetar audio que contiene varios sonidos (canto de p√°jaros, identificaci√≥n de altavoces en una reuni√≥n)</li>
      <li>clasificaci√≥n de m√∫sica: etiquetar m√∫sica con una etiqueta de g√©nero (‚Äúmetal‚Äù, ‚Äúhip-hop‚Äù, ‚Äúcountry‚Äù)</li>
      </ul>
      </li>
      </ul>
      </li>
      <li><p>Reconocimiento autom√°tico del habla (ASR, audio speech recognition):</p>
      </li>
      <li><p>Visi√≥n por computadora</p>
      <ul>
      <li>Clasificaci√≥n de im√°genes</li>
      <li>Detecci√≥n de objetos</li>
      <li>Segmentaci√≥n de im√°genes</li>
      <li>Estimaci√≥n de profundidad</li>
      </ul>
      </li>
      <li><p>Procesamiento del lenguaje natural (NLP, natural language processing)</p>
      <ul>
      <li>Clasificaci√≥n de texto<ul>
      <li>an√°lisis de sentimientos</li>
      <li>clasificaci√≥n de contenido</li>
      </ul>
      </li>
      <li>Clasificaci√≥n de tokens<ul>
      <li>reconocimiento de entidades nombradas (NER, por sus siglas en ingl√©s): etiquetar un token seg√∫n una categor√≠a de entidad como organizaci√≥n, persona, ubicaci√≥n o fecha.</li>
      <li>etiquetado de partes del discurso (POS, por sus siglas en ingl√©s): etiquetar un token seg√∫n su parte del discurso, como sustantivo, verbo o adjetivo. POS es √∫til para ayudar a los sistemas de traducci√≥n a comprender c√≥mo dos palabras id√©nticas son gramaticalmente diferentes (por ejemplo, ‚Äúcorte‚Äù como sustantivo versus ‚Äúcorte‚Äù como verbo)</li>
      </ul>
      </li>
      <li>Respuestas a preguntas<ul>
      <li>extractivas: dada una pregunta y alg√∫n contexto, la respuesta es un fragmento de texto del contexto que el modelo debe extraer</li>
      <li>abstractivas: dada una pregunta y alg√∫n contexto, la respuesta se genera a partir del contexto; este enfoque lo maneja la Text2TextGenerationPipeline en lugar del QuestionAnsweringPipeline que se muestra a continuaci√≥n</li>
      </ul>
      </li>
      <li>Resumir<ul>
      <li>extractiva: identifica y extrae las oraciones m√°s importantes del texto original</li>
      <li>abstractiva: genera el resumen objetivo (que puede incluir nuevas palabras no presentes en el documento de entrada) a partir del texto original</li>
      </ul>
      </li>
      <li>Traducci√≥n</li>
      <li>Modelado de lenguaje<ul>
      <li>causal: el objetivo del modelo es predecir el pr√≥ximo token en una secuencia, y los tokens futuros est√°n enmascarados</li>
      <li>enmascarado: el objetivo del modelo es predecir un token enmascarado en una secuencia con acceso completo a los tokens en la secuencia</li>
      </ul>
      </li>
      </ul>
      </li>
      <li><p>Multimodal</p>
      <ul>
      <li>Respuestas a preguntas de documentos</li>
      </ul>
      </li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Uso-de-pipeline">Uso de <code>pipeline</code><a class="anchor-link" href="#Uso-de-pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La forma m√°s sencilla de crear un <code>pipeline</code> es simplemente indicarle la tarea que queremos que resuelva mediante el par√°metro <code>task</code>. Y la librer√≠a se encargar√° de seleccionar el mejor modelo para esa tarea, descargarlo y guardarlo en la cach√© para futuros usos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).',
          'Using a pipeline without specifying a model name and revision in production is not recommended.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[2]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>[{opening_brace}'generated_text': 'Me encanta aprender de se r√©sistance davant que hiens que pr√©clase que ses encasas qu√©c√©nces. Se pr√©sentants cet en un croyne et cela d√©sirez'{closing_brace}]</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver el texto generado est√° en franc√©s, mientras que yo se lo he introducido en espa√±ol, por lo que es importante elegir bien el modelo. SI te fijas la librer√≠a ha cogido el modelo <code>openai-community/gpt2</code> que es un modelo entrenado en su mayor√≠a en ingl√©s, y que al meterle texto en espa√±ol se ha liado y ha generado una respuesta en franc√©s.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a usar un modelo reentrenado en espa√±ol mediante el par√°metro <code>model</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[2]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>[{opening_brace}'generated_text': 'Me encanta aprender de tus palabras, que con gran entusiasmo y con el mismo conocimiento como lo que t√∫ acabas escribiendo, te deseo de todo coraz√≥n todo el deseo de este d√≠a:\nY aunque tambi√©n haya personas a las que'{closing_brace}]</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora el texto generado tiene mucha mejor pinta</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La clase <code>pipeline</code> tiene muchos posibles par√°metros, por lo que para verlos todos y aprender m√°s sobre la clase te recomiendo leer su <a href="https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/pipelines" target="_blank" rel="nofollow noreferrer">documentaci√≥n</a>, pero vamos a hablar de una, ya que para el deep learning es muy importante y es <code>device</code>. Define el dispositivo (por ejemplo, <code>cpu</code>, <code>cuda:1</code>, <code>mps</code> o un rango ordinal de GPU como <code>1</code>) en el que se asignar√° el <code>pipeline</code>.</p>
      <p>En mi caso, como tengo una GPU pongo <code>0</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
      
      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">generation</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">generation</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'generated_text'</span><span class="p">])</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de ustedes, a tal punto que he decidido escribir algunos de nuestros contenidos en este blog, el cual ha sido de gran utilidad para m√≠ por varias razones, una de ellas, el trabajo
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="C%C3%B3mo-funciona-pipeline">C√≥mo funciona <code>pipeline</code><a class="anchor-link" href="#C%C3%B3mo-funciona-pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cuando hacemos uso de <code>pipeline</code> por debajo lo que est√° pasando es esto</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformers-pipeline" src="http://maximofn.com/wp-content/uploads/2024/02/transformers-pipeline.svg"/></p>
      <p>Autom√°ticamente se est√° tokenizando el texto, se pasa por el modelo y despu√©s por un postprocesado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Inferencia-con-AutoClass-y-pipeline">Inferencia con <code>AutoClass</code> y <code>pipeline</code><a class="anchor-link" href="#Inferencia-con-AutoClass-y-pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos visto que <code>pipeline</code> nos abstrae mucho de lo que pasa, pero nosotros podemos seleccionar qu√© tokenizador, qu√© modelo y qu√© postprocesado queremos usar.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizaci%C3%B3n-con-AutoTokenizer">Tokenizaci√≥n con <code>AutoTokenizer</code><a class="anchor-link" href="#Tokenizaci%C3%B3n-con-AutoTokenizer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes usamos el modelo <code>flax-community/gpt-2-spanish</code> para generar texto, podemos usar su tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '</span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">generation</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">generation</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">])</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '',
          '<span class="n">text</span> <span class="o">=</span> <span class="s2">"Me encanta lo que estoy aprendiendo"</span>',
          '',
          '<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '{\'input_ids\': tensor([[ 2879,  4835,   382,   288,  2383, 15257]]), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1]])}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelo-AutoModel">Modelo <code>AutoModel</code><a class="anchor-link" href="#Modelo-AutoModel"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora podemos crear el modelo y pasarle los tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '',
          '<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)</span>',
          '<span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions,',
          ' odict_keys([\'last_hidden_state\', \'past_key_values\']))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora lo intentamos usar en un <code>pipeline</code> nos dar√° un error</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
      
      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>The model 'GPT2Model' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-text-output-error">
      <pre>
      <span class="ansi-red-fg">---------------------------------------------------------------------------</span>
      <span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
      Cell <span class="ansi-green-fg">In[23], line 3</span>
      <span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">transformers</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> pipeline
      <span class="ansi-green-fg">----&gt; 3</span> pipeline(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">text-generation</span><span style="color: rgb(175,0,0)">"</span>, model<span style="color: rgb(98,98,98)">=</span>model, tokenizer<span style="color: rgb(98,98,98)">=</span>tokenizer)(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Me encanta aprender de</span><span style="color: rgb(175,0,0)">"</span>)
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:241</span>, in <span class="ansi-cyan-fg">TextGenerationPipeline.__call__</span><span class="ansi-blue-fg">(self, text_inputs, **kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">    239</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">super</span>()<span style="color: rgb(98,98,98)">.</span><span style="color: rgb(0,0,255)">__call__</span>(chats, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs)
      <span class="ansi-green-intense-fg ansi-bold">    240</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
      <span class="ansi-green-fg">--&gt; 241</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">super</span>()<span style="color: rgb(98,98,98)">.</span><span style="color: rgb(0,0,255)">__call__</span>(text_inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs)
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1196</span>, in <span class="ansi-cyan-fg">Pipeline.__call__</span><span class="ansi-blue-fg">(self, inputs, num_workers, batch_size, *args, **kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1188</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">next</span>(
      <span class="ansi-green-intense-fg ansi-bold">   1189</span>         <span style="color: rgb(0,135,0)">iter</span>(
      <span class="ansi-green-intense-fg ansi-bold">   1190</span>             <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>get_iterator(
      <span class="ansi-green-fg">   (...)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1193</span>         )
      <span class="ansi-green-intense-fg ansi-bold">   1194</span>     )
      <span class="ansi-green-intense-fg ansi-bold">   1195</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
      <span class="ansi-green-fg">-&gt; 1196</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>run_single(inputs, preprocess_params, forward_params, postprocess_params)
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1203</span>, in <span class="ansi-cyan-fg">Pipeline.run_single</span><span class="ansi-blue-fg">(self, inputs, preprocess_params, forward_params, postprocess_params)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1201</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">run_single</span>(<span style="color: rgb(0,135,0)">self</span>, inputs, preprocess_params, forward_params, postprocess_params):
      <span class="ansi-green-intense-fg ansi-bold">   1202</span>     model_inputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>preprocess(inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>preprocess_params)
      <span class="ansi-green-fg">-&gt; 1203</span>     model_outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>forward(model_inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>forward_params)
      <span class="ansi-green-intense-fg ansi-bold">   1204</span>     outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>postprocess(model_outputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>postprocess_params)
      <span class="ansi-green-intense-fg ansi-bold">   1205</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> outputs
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1102</span>, in <span class="ansi-cyan-fg">Pipeline.forward</span><span class="ansi-blue-fg">(self, model_inputs, **forward_params)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1100</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> inference_context():
      <span class="ansi-green-intense-fg ansi-bold">   1101</span>         model_inputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_ensure_tensor_on_device(model_inputs, device<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>device)
      <span class="ansi-green-fg">-&gt; 1102</span>         model_outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_forward(model_inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>forward_params)
      <span class="ansi-green-intense-fg ansi-bold">   1103</span>         model_outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_ensure_tensor_on_device(model_outputs, device<span style="color: rgb(98,98,98)">=</span>torch<span style="color: rgb(98,98,98)">.</span>device(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">cpu</span><span style="color: rgb(175,0,0)">"</span>))
      <span class="ansi-green-intense-fg ansi-bold">   1104</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:328</span>, in <span class="ansi-cyan-fg">TextGenerationPipeline._forward</span><span class="ansi-blue-fg">(self, model_inputs, **generate_kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">    325</span>         generate_kwargs[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">min_length</span><span style="color: rgb(175,0,0)">"</span>] <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> prefix_length
      <span class="ansi-green-intense-fg ansi-bold">    327</span> <span style="color: rgb(95,135,135)"># BS x SL</span>
      <span class="ansi-green-fg">--&gt; 328</span> generated_sequence <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>model<span style="color: rgb(98,98,98)">.</span>generate(input_ids<span style="color: rgb(98,98,98)">=</span>input_ids, attention_mask<span style="color: rgb(98,98,98)">=</span>attention_mask, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>generate_kwargs)
      <span class="ansi-green-intense-fg ansi-bold">    329</span> out_b <span style="color: rgb(98,98,98)">=</span> generated_sequence<span style="color: rgb(98,98,98)">.</span>shape[<span style="color: rgb(98,98,98)">0</span>]
      <span class="ansi-green-intense-fg ansi-bold">    330</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>framework <span style="color: rgb(98,98,98)">==</span> <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">pt</span><span style="color: rgb(175,0,0)">"</span>:
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/utils/_contextlib.py:115</span>, in <span class="ansi-cyan-fg">context_decorator.&lt;locals&gt;.decorate_context</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">    112</span> <span style="color: rgb(175,0,255)">@functools</span><span style="color: rgb(98,98,98)">.</span>wraps(func)
      <span class="ansi-green-intense-fg ansi-bold">    113</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">decorate_context</span>(<span style="color: rgb(98,98,98)">*</span>args, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs):
      <span class="ansi-green-intense-fg ansi-bold">    114</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> ctx_factory():
      <span class="ansi-green-fg">--&gt; 115</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> func(<span style="color: rgb(98,98,98)">*</span>args, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs)
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1323</span>, in <span class="ansi-cyan-fg">GenerationMixin.generate</span><span class="ansi-blue-fg">(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1320</span>         synced_gpus <span style="color: rgb(98,98,98)">=</span> <span class="ansi-bold" style="color: rgb(0,135,0)">False</span>
      <span class="ansi-green-intense-fg ansi-bold">   1322</span> <span style="color: rgb(95,135,135)"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call</span>
      <span class="ansi-green-fg">-&gt; 1323</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_validate_model_class()
      <span class="ansi-green-intense-fg ansi-bold">   1325</span> <span style="color: rgb(95,135,135)"># priority: `generation_config` argument &gt; `model.generation_config` (the default generation config)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1326</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> generation_config <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:
      <span class="ansi-green-intense-fg ansi-bold">   1327</span>     <span style="color: rgb(95,135,135)"># legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,</span>
      <span class="ansi-green-intense-fg ansi-bold">   1328</span>     <span style="color: rgb(95,135,135)"># three conditions must be met</span>
      <span class="ansi-green-intense-fg ansi-bold">   1329</span>     <span style="color: rgb(95,135,135)"># 1) the generation config must have been created from the model config (`_from_model_config` field);</span>
      <span class="ansi-green-intense-fg ansi-bold">   1330</span>     <span style="color: rgb(95,135,135)"># 2) the generation config must have seen no modification since its creation (the hash is the same);</span>
      <span class="ansi-green-intense-fg ansi-bold">   1331</span>     <span style="color: rgb(95,135,135)"># 3) the user must have set generation parameters in the model config.</span>
      
      File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1110</span>, in <span class="ansi-cyan-fg">GenerationMixin._validate_model_class</span><span class="ansi-blue-fg">(self)</span>
      <span class="ansi-green-intense-fg ansi-bold">   1108</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> generate_compatible_classes:
      <span class="ansi-green-intense-fg ansi-bold">   1109</span>     exception_message <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)"> Please use one of the following classes instead: </span><span class="ansi-bold" style="color: rgb(175,95,135)">{opening_brace}</span>generate_compatible_classes<span class="ansi-bold" style="color: rgb(175,95,135)">{closing_brace}</span><span style="color: rgb(175,0,0)">"</span>
      <span class="ansi-green-fg">-&gt; 1110</span> <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">TypeError</span>(exception_message)
      
      <span class="ansi-red-fg">TypeError</span>: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {opening_brace}'GPT2LMHeadModel'{closing_brace}</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hesto es porque cuando funcionaba us√°bamos</p>
      <div class="highlight"><pre><span></span><span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero ahora hemos hecho</p>
      <div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el primer caso solo us√°bamos <code>pipeline</code> y el nombre del modelo, por debajo buscaba la mejor manera de implementar el modelo y el tokenizador. Pero en el segundo caso hemos creado el tokenizador y el modelo y se lo hemos pasado a <code>pipeline</code>, pero no los hemos creado bien para lo que el <code>pipeline</code> necesita</p>
      <p>Para arreglar esto usamos <code>AutoModelFor</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelo-AutoModelFor">Modelo <code>AutoModelFor</code><a class="anchor-link" href="#Modelo-AutoModelFor"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La librer√≠a transformers nos da la oportunidad de crear un modelo para una tarea determinada como</p>
      <ul>
      <li><code>AutoModelForCausalLM</code> que sirve para continuar textos</li>
      <li><code>AutoModelForMaskedLM</code> que se usa para rellenar huecos</li>
      <li><code>AutoModelForMaskGeneration</code> que sirve para generar m√°scaras</li>
      <li><code>AutoModelForSeq2SeqLM</code> que se usa par convertir de secuencias a secuencias, como por ejemplo en traducci√≥n</li>
      <li><code>AutoModelForSequenceClassification</code> para clasificaci√≥n de texto</li>
      <li><code>AutoModelForMultipleChoice</code> para elecci√≥n m√∫ltiple</li>
      <li><code>AutoModelForNextSentencePrediction</code> para predecir si dos frases son consecutivas</li>
      <li><code>AutoModelForTokenClassification</code> para clasificaci√≥n de tokens</li>
      <li><code>AutoModelForQuestionAnswering</code> para preguntas y respuestas</li>
      <li><code>AutoModelForTextEncoding</code> para codificaci√≥n de texto</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a usar el modelo anterior para generar texto</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      
      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'generated_text'</span><span class="p">]</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[3]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>'Me encanta aprender de mi familia.\nLa verdad no sab√≠a que se necesitaba tanto en este peque√±o restaurante ya que mi novio en un principio hab√≠a ido, pero hoy me ha entrado un gusanillo entre pecho y espalda que'</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora si funciona, porque hemos creado el modelo de una manera que <code>pipeline</code> puede entender</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Inferencia-solo-con-AutoClass">Inferencia solo con <code>AutoClass</code><a class="anchor-link" href="#Inferencia-solo-con-AutoClass"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes hemos creado el modelo y el tokenizador y se lo hemos dado a <code>pipeline</code> para que por debajo haga lo necesario, pero podemos usar nosotros los m√©todos para la inferencia.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Generaci%C3%B3n-de-texto-casual">Generaci√≥n de texto casual<a class="anchor-link" href="#Generaci%C3%B3n-de-texto-casual"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el modelo y el tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">]</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Con <code>device_map</code>, hemos cargado el modelo en la GPU 0</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora tenemos que hacer nosotros lo que antes hac√≠a <code>pipeline</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero generamos los tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '',
          '<span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">]</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '</span><span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'The model \'GPT2Model\' is not supported for text-generation. Supported models are [\'BartForCausalLM\', \'BertLMHeadModel\', \'BertGenerationDecoder\', \'BigBirdForCausalLM\', \'BigBirdPegasusForCausalLM\', \'BioGptForCausalLM\', \'BlenderbotForCausalLM\', \'BlenderbotSmallForCausalLM\', \'BloomForCausalLM\', \'CamembertForCausalLM\', \'LlamaForCausalLM\', \'CodeGenForCausalLM\', \'CpmAntForCausalLM\', \'CTRLLMHeadModel\', \'Data2VecTextForCausalLM\', \'ElectraForCausalLM\', \'ErnieForCausalLM\', \'FalconForCausalLM\', \'FuyuForCausalLM\', \'GemmaForCausalLM\', \'GitForCausalLM\', \'GPT2LMHeadModel\', \'GPT2LMHeadModel\', \'GPTBigCodeForCausalLM\', \'GPTNeoForCausalLM\', \'GPTNeoXForCausalLM\', \'GPTNeoXJapaneseForCausalLM\', \'GPTJForCausalLM\', \'LlamaForCausalLM\', \'MarianForCausalLM\', \'MBartForCausalLM\', \'MegaForCausalLM\', \'MegatronBertForCausalLM\', \'MistralForCausalLM\', \'MixtralForCausalLM\', \'MptForCausalLM\', \'MusicgenForCausalLM\', \'MvpForCausalLM\', \'OpenLlamaForCausalLM\', \'OpenAIGPTLMHeadModel\', \'OPTForCausalLM\', \'PegasusForCausalLM\', \'PersimmonForCausalLM\', \'PhiForCausalLM\', \'PLBartForCausalLM\', \'ProphetNetForCausalLM\', \'QDQBertLMHeadModel\', \'Qwen2ForCausalLM\', \'ReformerModelWithLMHead\', \'RemBertForCausalLM\', \'RobertaForCausalLM\', \'RobertaPreLayerNormForCausalLM\', \'RoCBertForCausalLM\', \'RoFormerForCausalLM\', \'RwkvForCausalLM\', \'Speech2Text2ForCausalLM\', \'StableLmForCausalLM\', \'TransfoXLLMHeadModel\', \'TrOCRForCausalLM\', \'WhisperForCausalLM\', \'XGLMForCausalLM\', \'XLMWithLMHeadModel\', \'XLMProphetNetForCausalLM\', \'XLMRobertaForCausalLM\', \'XLMRobertaXLForCausalLM\', \'XLNetLMHeadModel\', \'XmodForCausalLM\'].',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '<span class="ansi-red-fg">---------------------------------------------------------------------------</span>',
          '<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)',
          'Cell <span class="ansi-green-fg">In[2], line 1</span>',
          '<span class="ansi-green-fg">----&gt; 1</span> tokens_input <span style="color: rgb(98,98,98)">=</span> tokenizer([<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Me encanta aprender de</span><span style="color: rgb(175,0,0)">"</span>], return_tensors<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">pt</span><span style="color: rgb(175,0,0)">"</span>, padding<span style="color: rgb(98,98,98)">=</span><span class="ansi-bold" style="color: rgb(0,135,0)">True</span>)<span style="color: rgb(98,98,98)">.</span>to(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">cuda</span><span style="color: rgb(175,0,0)">"</span>)',
          'File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2829</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.__call__</span><span class="ansi-blue-fg">(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2827</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_in_target_context_manager:',
          '<span class="ansi-green-intense-fg ansi-bold">   2828</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_input_mode()',
          '<span class="ansi-green-fg">-&gt; 2829</span>     encodings <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_call_one(text<span style="color: rgb(98,98,98)">=</span>text, text_pair<span style="color: rgb(98,98,98)">=</span>text_pair, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>all_kwargs)',
          '<span class="ansi-green-intense-fg ansi-bold">   2830</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> text_target <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:',
          '<span class="ansi-green-intense-fg ansi-bold">   2831</span>     <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_target_mode()',
          'File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2915</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._call_one</span><span class="ansi-blue-fg">(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2910</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(',
          '<span class="ansi-green-intense-fg ansi-bold">   2911</span>             <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">batch length of `text`: </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">len</span>(text)<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)"> does not match batch length of `text_pair`:</span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2912</span>             <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)"> </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">len</span>(text_pair)<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">.</span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2913</span>         )',
          '<span class="ansi-green-intense-fg ansi-bold">   2914</span>     batch_text_or_text_pairs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">list</span>(<span style="color: rgb(0,135,0)">zip</span>(text, text_pair)) <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> text_pair <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span> text',
          '<span class="ansi-green-fg">-&gt; 2915</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>batch_encode_plus(',
          '<span class="ansi-green-intense-fg ansi-bold">   2916</span>         batch_text_or_text_pairs<span style="color: rgb(98,98,98)">=</span>batch_text_or_text_pairs,',
          '<span class="ansi-green-intense-fg ansi-bold">   2917</span>         add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,',
          '<span class="ansi-green-intense-fg ansi-bold">   2918</span>         padding<span style="color: rgb(98,98,98)">=</span>padding,',
          '<span class="ansi-green-intense-fg ansi-bold">   2919</span>         truncation<span style="color: rgb(98,98,98)">=</span>truncation,',
          '<span class="ansi-green-intense-fg ansi-bold">   2920</span>         max_length<span style="color: rgb(98,98,98)">=</span>max_length,',
          '<span class="ansi-green-intense-fg ansi-bold">   2921</span>         stride<span style="color: rgb(98,98,98)">=</span>stride,',
          '<span class="ansi-green-intense-fg ansi-bold">   2922</span>         is_split_into_words<span style="color: rgb(98,98,98)">=</span>is_split_into_words,',
          '<span class="ansi-green-intense-fg ansi-bold">   2923</span>         pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,',
          '<span class="ansi-green-intense-fg ansi-bold">   2924</span>         return_tensors<span style="color: rgb(98,98,98)">=</span>return_tensors,',
          '<span class="ansi-green-intense-fg ansi-bold">   2925</span>         return_token_type_ids<span style="color: rgb(98,98,98)">=</span>return_token_type_ids,',
          '<span class="ansi-green-intense-fg ansi-bold">   2926</span>         return_attention_mask<span style="color: rgb(98,98,98)">=</span>return_attention_mask,',
          '<span class="ansi-green-intense-fg ansi-bold">   2927</span>         return_overflowing_tokens<span style="color: rgb(98,98,98)">=</span>return_overflowing_tokens,',
          '<span class="ansi-green-intense-fg ansi-bold">   2928</span>         return_special_tokens_mask<span style="color: rgb(98,98,98)">=</span>return_special_tokens_mask,',
          '<span class="ansi-green-intense-fg ansi-bold">   2929</span>         return_offsets_mapping<span style="color: rgb(98,98,98)">=</span>return_offsets_mapping,',
          '<span class="ansi-green-intense-fg ansi-bold">   2930</span>         return_length<span style="color: rgb(98,98,98)">=</span>return_length,',
          '<span class="ansi-green-intense-fg ansi-bold">   2931</span>         verbose<span style="color: rgb(98,98,98)">=</span>verbose,',
          '<span class="ansi-green-intense-fg ansi-bold">   2932</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,',
          '<span class="ansi-green-intense-fg ansi-bold">   2933</span>     )',
          '<span class="ansi-green-intense-fg ansi-bold">   2934</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:',
          '<span class="ansi-green-intense-fg ansi-bold">   2935</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>encode_plus(',
          '<span class="ansi-green-intense-fg ansi-bold">   2936</span>         text<span style="color: rgb(98,98,98)">=</span>text,',
          '<span class="ansi-green-intense-fg ansi-bold">   2937</span>         text_pair<span style="color: rgb(98,98,98)">=</span>text_pair,',
          '<span class="ansi-green-fg">   (...)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2953</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,',
          '<span class="ansi-green-intense-fg ansi-bold">   2954</span>     )',
          'File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3097</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.batch_encode_plus</span><span class="ansi-blue-fg">(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3080</span> <span style="color: rgb(175,0,0)">"""</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3081</span> <span style="color: rgb(175,0,0)">Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3082</span> ',
          '<span class="ansi-green-fg">   (...)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3093</span> <span style="color: rgb(175,0,0)">        details in `encode_plus`).</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3094</span> <span style="color: rgb(175,0,0)">"""</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3096</span> <span style="color: rgb(95,135,135)"># Backward compatibility for \'truncation_strategy\', \'pad_to_max_length\'</span>',
          '<span class="ansi-green-fg">-&gt; 3097</span> padding_strategy, truncation_strategy, max_length, kwargs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_get_padding_truncation_strategies(',
          '<span class="ansi-green-intense-fg ansi-bold">   3098</span>     padding<span style="color: rgb(98,98,98)">=</span>padding,',
          '<span class="ansi-green-intense-fg ansi-bold">   3099</span>     truncation<span style="color: rgb(98,98,98)">=</span>truncation,',
          '<span class="ansi-green-intense-fg ansi-bold">   3100</span>     max_length<span style="color: rgb(98,98,98)">=</span>max_length,',
          '<span class="ansi-green-intense-fg ansi-bold">   3101</span>     pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,',
          '<span class="ansi-green-intense-fg ansi-bold">   3102</span>     verbose<span style="color: rgb(98,98,98)">=</span>verbose,',
          '<span class="ansi-green-intense-fg ansi-bold">   3103</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,',
          '<span class="ansi-green-intense-fg ansi-bold">   3104</span> )',
          '<span class="ansi-green-intense-fg ansi-bold">   3106</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_batch_encode_plus(',
          '<span class="ansi-green-intense-fg ansi-bold">   3107</span>     batch_text_or_text_pairs<span style="color: rgb(98,98,98)">=</span>batch_text_or_text_pairs,',
          '<span class="ansi-green-intense-fg ansi-bold">   3108</span>     add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,',
          '<span class="ansi-green-fg">   (...)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   3123</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,',
          '<span class="ansi-green-intense-fg ansi-bold">   3124</span> )',
          'File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2734</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._get_padding_truncation_strategies</span><span class="ansi-blue-fg">(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2732</span> <span style="color: rgb(95,135,135)"># Test if we have a padding token</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2733</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (<span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token_id <span style="color: rgb(98,98,98)">&lt;</span> <span style="color: rgb(98,98,98)">0</span>):',
          '<span class="ansi-green-fg">-&gt; 2734</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(',
          '<span class="ansi-green-intense-fg ansi-bold">   2735</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Asking to pad but the tokenizer does not have a padding token. </span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2736</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` </span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2737</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">or add a new pad token via `tokenizer.add_special_tokens(</span><span style="color: rgb(175,0,0)">{</span><span style="color: rgb(175,0,0)">\'</span><span style="color: rgb(175,0,0)">pad_token</span><span style="color: rgb(175,0,0)">\'</span><span style="color: rgb(175,0,0)">: </span><span style="color: rgb(175,0,0)">\'</span><span style="color: rgb(175,0,0)">[PAD]</span><span style="color: rgb(175,0,0)">\'</span><span style="color: rgb(175,0,0)">})`.</span><span style="color: rgb(175,0,0)">"</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2738</span>     )',
          '<span class="ansi-green-intense-fg ansi-bold">   2740</span> <span style="color: rgb(95,135,135)"># Check that we will truncate to a multiple of pad_to_multiple_of if both are provided</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2741</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> (',
          '<span class="ansi-green-intense-fg ansi-bold">   2742</span>     truncation_strategy <span style="color: rgb(98,98,98)">!=</span> TruncationStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_TRUNCATE',
          '<span class="ansi-green-intense-fg ansi-bold">   2743</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD',
          '<span class="ansi-green-fg">   (...)</span>',
          '<span class="ansi-green-intense-fg ansi-bold">   2746</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (max_length <span style="color: rgb(98,98,98)">%</span> pad_to_multiple_of <span style="color: rgb(98,98,98)">!=</span> <span style="color: rgb(98,98,98)">0</span>)',
          '<span class="ansi-green-intense-fg ansi-bold">   2747</span> ):',
          '<span class="ansi-red-fg">ValueError</span>: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({\'pad_token\': \'[PAD]\'})`.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que nos ha dado un error, nos dice que el tokenizador no tiene token de padding. La mayor√≠a de LLMs no tienen un token de padding, pero para usar la librer√≠a <code>transformers</code> es necesario un token de padding, por lo que lo que se suele hacer es asignar el token de fin de sentencia al token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Ahora ya podemos generar los tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '</span><span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'tensor([[2879, 4835, 3760,  225,   72,   73]], device=\'cuda:0\')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora se los pasamos al modelo que generar√° nuevos tokens, para eso usamos el m√©todo <code>generate</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="si">{opening_brace}</span><span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="si">{opening_brace}</span><span class="n">tokens_output</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>input tokens: tensor([[2879, 4835, 3760,  225,   72,   73]], device='cuda:0')
      output tokens: tensor([[ 2879,  4835,  3760,   225,    72,    73,   314,  2533,    16,   287,
                 225,    73,    82,   513,  1086,   225,    72,    73,   314,   288,
                 357, 15550,    16,   287,   225,    73,    87,   288,   225,    73,
                  82,   291,  3500,    16,   225,    73,    87,   348,   929,   225,
                  72,    73,  3760,   225,    72,    73,   314,  2533,    18,   203]],
             device='cuda:0')
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que los primeros tokens de <code>token_inputs</code> son los mismos que los de <code>token_outputs</code>, los que vienen a continuaci√≥n son los que ha generado el modelo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora tenemos que convertir esos tokens a una sentencia mediante el decoder del tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="si">{</span><span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="si">{</span><span class="n">tokens_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '\'Me encanta aprender de los dem√°s, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los dem√°s.\n\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ya tenemos el texto generado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Clasificaci%C3%B3n-de-texto">Clasificaci√≥n de texto<a class="anchor-link" href="#Clasificaci%C3%B3n-de-texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el modelo y el tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>Generamos los tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three."</span>',
      '      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Una vez tenemos los tokens, clasificamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '</span><span class="n">text</span> <span class="o">=</span> <span class="s2">"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three."</span>',
          '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
          '',
          '<span class="n">predicted_class_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
          '<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">predicted_class_id</span><span class="p">]</span>',
          '<span class="n">prediction</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'LABEL_1\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver las clases</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">clases</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span>',
          '<span class="n">clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{0: \'LABEL_0\', 1: \'LABEL_1\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As√≠ no hay quien se entere, as√≠ que lo modificamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">"NEGATIVE"</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">"POSITIVE"</span><span class="p">}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Y ahora volvemos a clasificar</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">"NEGATIVE"</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">"POSITIVE"</span><span class="p">}</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
          '',
          '<span class="n">predicted_class_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
          '<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">predicted_class_id</span><span class="p">]</span>',
          '<span class="n">prediction</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'POSITIVE\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Clasificaci%C3%B3n-de-tokens">Clasificaci√≥n de tokens<a class="anchor-link" href="#Clasificaci%C3%B3n-de-tokens"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el modelo y el tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>Generamos los tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Golden State Warriors are an American professional basketball team based in San Francisco."</span>',
      '      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Una vez tenemos los tokens, clasificamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '</span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Golden State Warriors are an American professional basketball team based in San Francisco."</span>',
          '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
          '',
          '<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '<span class="n">predicted_token_class</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>',
          '<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]])</span><span class="si">}</span><span class="s2">) -&gt; </span><span class="si">{</span><span class="n">predicted_token_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '101 ([CLS]) -&gt; O',
          '1996 (the) -&gt; O',
          '3585 (golden) -&gt; B-location',
          '2110 (state) -&gt; I-location',
          '6424 (warriors) -&gt; B-group',
          '2024 (are) -&gt; O',
          '2019 (an) -&gt; O',
          '2137 (american) -&gt; O',
          '2658 (professional) -&gt; O',
          '3455 (basketball) -&gt; O',
          '2136 (team) -&gt; O',
          '2241 (based) -&gt; O',
          '1999 (in) -&gt; O',
          '2624 (san) -&gt; B-location',
          '3799 (francisco) -&gt; B-location',
          '1012 (.) -&gt; O',
          '102 ([SEP]) -&gt; O',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver los tokens correspondientes a <code>golden</code>, <code>state</code>, <code>warriors</code>, <code>san</code> y <code>francisco</code> los ha clasificado como tokens de localizaci√≥m</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Respuesta-a-preguntas-(question-answering)">Respuesta a preguntas (question answering)<a class="anchor-link" href="#Respuesta-a-preguntas-(question-answering)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el modelo y el tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"mrm8488/roberta-base-1B-1-finetuned-squadv1"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"mrm8488/roberta-base-1B-1-finetuned-squadv1"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of the model checkpoint at mrm8488/roberta-base-1B-1-finetuned-squadv1 were not used when initializing RobertaForQuestionAnswering: [\'roberta.pooler.dense.bias\', \'roberta.pooler.dense.weight\']',
          '- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).',
          '- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Generamos los tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">"How many programming languages does BLOOM support?"</span>',
      '      <span class="n">context</span> <span class="o">=</span> <span class="s2">"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages."</span>',
      '      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Una vez tenemos los tokens, clasificamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">"How many programming languages does BLOOM support?"</span>',
          '<span class="n">context</span> <span class="o">=</span> <span class="s2">"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages."</span>',
          '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>',
          '',
          '<span class="n">answer_start_index</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">start_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>',
          '<span class="n">answer_end_index</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">end_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>',
          '',
          '<span class="n">predict_answer_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">answer_start_index</span> <span class="p">:</span> <span class="n">answer_end_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">predict_answer_tokens</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\' 13\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelizaci%C3%B3n-del-lenguaje-enmascarado-(Masked-language-modeling)">Modelizaci√≥n del lenguaje enmascarado (Masked language modeling)<a class="anchor-link" href="#Modelizaci%C3%B3n-del-lenguaje-enmascarado-(Masked-language-modeling)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos el modelo y el tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForMaskedLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"nyu-mll/roberta-base-1B-1"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"nyu-mll/roberta-base-1B-1"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of the model checkpoint at nyu-mll/roberta-base-1B-1 were not used when initializing RobertaForMaskedLM: [\'roberta.pooler.dense.bias\', \'roberta.pooler.dense.weight\']',
          '- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).',
          '- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Generamos los tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Milky Way is a &lt;mask&gt; galaxy."</span>',
      '      <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
      '      <span class="n">mask_token_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token_id</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Una vez tenemos los tokens, clasificamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Milky Way is a &lt;mask&gt; galaxy."</span>',
          '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">mask_token_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token_id</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
          '</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
          '    <span class="n">mask_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">mask_token_index</span><span class="p">,</span> <span class="p">:]</span>',
          '',
          '<span class="n">top_3_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">mask_token_logits</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>',
          '<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">top_3_tokens</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">token</span><span class="p">])))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'The Milky Way is a  spiral galaxy.',
          'The Milky Way is a  closed galaxy.',
          'The Milky Way is a  distant galaxy.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Personalizaci%C3%B3n-del-modelo">Personalizaci√≥n del modelo<a class="anchor-link" href="#Personalizaci%C3%B3n-del-modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes hemos hecho la inferencia con <code>AutoClass</code>, pero lo hemos hecho con las cofiguraciones por defecto del modelo. Pero podemos configurar el modelo todo lo que queramos</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a instanciar un modelo y a ver su configuraci√≥n</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoConfig</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '',
          '<span class="n">config</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'GPT2Config {',
          '  "_name_or_path": "flax-community/gpt-2-spanish",',
          '  "activation_function": "gelu_new",',
          '  "architectures": [',
          '    "GPT2LMHeadModel"',
          '  ],',
          '  "attn_pdrop": 0.0,',
          '  "bos_token_id": 50256,',
          '  "embd_pdrop": 0.0,',
          '  "eos_token_id": 50256,',
          '  "gradient_checkpointing": false,',
          '  "initializer_range": 0.02,',
          '  "layer_norm_epsilon": 1e-05,',
          '  "model_type": "gpt2",',
          '  "n_ctx": 1024,',
          '  "n_embd": 768,',
          '  "n_head": 12,',
          '  "n_inner": null,',
          '  "n_layer": 12,',
          '  "n_positions": 1024,',
          '  "reorder_and_upcast_attn": false,',
          '  "resid_pdrop": 0.0,',
          '  "scale_attn_by_inverse_layer_idx": false,',
          '  "scale_attn_weights": true,',
          '  "summary_activation": null,',
          '  "summary_first_dropout": 0.1,',
          '  "summary_proj_to_labels": true,',
          '  "summary_type": "cls_index",',
          '  "summary_use_proj": true,',
          '  "task_specific_params": {',
          '    "text-generation": {',
          '      "do_sample": true,',
          '      "max_length": 50',
          '    }',
          '  },',
          '  "transformers_version": "4.38.1",',
          '  "use_cache": true,',
          '  "vocab_size": 50257',
          '}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver la configuraci√≥n del modelo, por ejemplo la funci√≥n de activaci√≥n es <code>gelu_new</code>, tiene 12 <code>head</code>s, el tama√±o del vocabulario es 50257 palabras, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero podemos modificar esta configuraci√≥n</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)</span>',
          '<span class="n">config</span><span class="o">.</span><span class="n">activation_function</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'relu\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos ahora el modelo con esta configuraci√≥n</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Y generamos texto</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[16]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>'Me encanta aprender de la d d e d e d e d e d e d e d e d e d e d e '</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que esta modificaci√≥n hace que no genere tan buen texto</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Tokenizaci%C3%B3n">Tokenizaci√≥n<a class="anchor-link" href="#Tokenizaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hasta ahora hemos visto las diferentes manera que hay de hacer inferencia con la librer√≠a <code>transformers</code>. Ahora nos vamos a meter en las tripas de la librer√≠a. Para ello primero vamos a ver cosas a tener en cuenta a la hora de tokenizar.</p>
      <p>No vamos a explicar lo que es tokenizar a fondo, ya que eso ya lo explicamos en el post de la librer√≠a <a href="https://maximofn.com/hugging-face-tokenizers/">tokenizers</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Padding">Padding<a class="anchor-link" href="#Padding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cuando se tiene un batch de secuencias, a veces es necesario que despu√©s de tokenizar, todas las secuencias tengan la misma longitud, as√≠ que para ello usamos el par√°metro <code>padding=True</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Padding token id: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '[2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]',
          '[1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]',
          '[1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]',
          'Padding token id: 50257',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos a las dos primeras secuencias les ha a√±adido un paddings al final</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Truncado">Truncado<a class="anchor-link" href="#Truncado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A parte de a√±adir padding, a veces es necesario truncar las secuencias para que no ocupen m√°s de un n√∫mero determinado de tokens. Para ello establecemos <code>truncation=True</code> y <code>max_length</code> con el n√∫mero de tokens que queremos que tenga la secuencia</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[2959, 16, 875, 3736, 3028]',
          '[1489, 2275, 288, 12052, 382]',
          '[1699, 2899, 707, 225, 72]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Las mismas sentencias de antes, ahora generan menos tokens</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tensores">Tensores<a class="anchor-link" href="#Tensores"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hasta ahora est√°bamos recibiendo listas de tokens, pero seguramente nos interese recibir tensores de PyTorch o TensorFlow. Para ello usamos el par√°metro <code>return_tensors</code> y le especificamos de qu√© framework queremos recibir el tensor, en nuestro caso elegiremos PyTorch con <code>pt</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos primero sin especificar que nos devuelva tensores</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;class \'list\'&gt;',
          '&lt;class \'list\'&gt;',
          '&lt;class \'list\'&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Recibimos listas, si queremos recibir tensores de PyTorch usamos <code>return_tensors="pt"</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="n">encoded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]),</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;class \'torch.Tensor\'&gt; torch.Size([12])',
          '&lt;class \'torch.Tensor\'&gt; torch.Size([12])',
          '&lt;class \'torch.Tensor\'&gt; torch.Size([12])',
          '&lt;class \'torch.Tensor\'&gt; torch.Size([3, 12])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="M%C3%A1scaras">M√°scaras<a class="anchor-link" href="#M%C3%A1scaras"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cuando tokenizamos una sentencia no solo obtenemos los <code>input_ids</code>, sino que tambi√©n obtenemos la m√°scara de atenci√≥n. La m√°scara de atenci√≥n es un tensor que tiene el mismo tama√±o que <code>input_ids</code> y tiene un <code>1</code> en las posiciones que son tokens y un <code>0</code> en las posiciones que son padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="s2">"Pero, ¬øqu√© pasa con el segundo desayuno?"</span><span class="p">,</span>',
          '    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>',
          '    <span class="s2">"¬øQu√© hay de los elevensies?"</span><span class="p">,</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>',
          '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"padding token id: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">encoded_input[0] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'input_ids\'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"encoded_input[0] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'attention_mask\'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">encoded_input[1] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'input_ids\'</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"encoded_input[1] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'attention_mask\'</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n</span><span class="s2">encoded_input[2] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'input_ids\'</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"encoded_input[2] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">\'attention_mask\'</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'padding token id: 50257',
          'encoded_input[0] inputs_ids: [2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]',
          'encoded_input[0] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]',
          'encoded_input[1] inputs_ids: [1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]',
          'encoded_input[1] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]',
          'encoded_input[2] inputs_ids: [1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]',
          'encoded_input[2] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver, en las dos primeras sentencias, tenemos un 1 en las primeras posiciones y un 0 en las dos √∫ltimas posiciones. En esas mismas posiciones tenemos el token <code>50257</code>, que corresponde al token de padding.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Con estas m√°scaras de atenci√≥n le estamos diciendo al modelo a qu√© tokens tiene que prestar atenci√≥n y a cu√°les no.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La generaci√≥n de texto se podr√≠a hacer igualmente si no pas√°ramos estas m√°scaras de atenci√≥n, el m√©todo <code>generate</code> har√≠a su mayur esfuerzo por inferir esta m√°scara, pero si se la pasamos ayudamos a generar mejor texto</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Fast-Tokenizers">Fast Tokenizers<a class="anchor-link" href="#Fast-Tokenizers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Algunos tokenizadores preentrenados tienen una versi√≥n <code>fast</code>, tienen los mismos m√©todos que los normales, solo que est√°n desarrollados en Rust. Para usarlos debemos usar la clase <code>PreTrainedTokenizerFast</code> de la librer√≠a <code>transformers</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Veamos primero el timepo de tokenizaci√≥n con un tokenizador normal</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">%%time</span>',
          '',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google-bert/bert-base-uncased"</span><span class="p">)</span>',
          '',
          '<span class="n">sentence</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"The Permaculture Design Principles are a set of universal design principles "</span>',
          '    <span class="s2">"that can be applied to any location, climate and culture, and they allow us to design "</span>',
          '    <span class="s2">"the most efficient and sustainable human habitation and food production systems. "</span>',
          '    <span class="s2">"Permaculture is a design system that encompasses a wide variety of disciplines, such "</span>',
          '    <span class="s2">"as ecology, landscape design, environmental science and energy conservation, and the "</span>',
          '    <span class="s2">"Permaculture design principles are drawn from these various disciplines. Each individual "</span>',
          '    <span class="s2">"design principle itself embodies a complete conceptual framework based on sound "</span>',
          '    <span class="s2">"scientific principles. When we bring all these separate  principles together, we can "</span>',
          '    <span class="s2">"create a design system that both looks at whole systems, the parts that these systems "</span>',
          '    <span class="s2">"consist of, and how those parts interact with each other to create a complex, dynamic, "</span>',
          '    <span class="s2">"living system. Each design principle serves as a tool that allows us to integrate all "</span>',
          '    <span class="s2">"the separate parts of a design, referred to as elements, into a functional, synergistic, "</span>',
          '    <span class="s2">"whole system, where the elements harmoniously interact and work together in the most "</span>',
          '    <span class="s2">"efficient way possible."</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CPU times: user 55.3 ms, sys: 8.58 ms, total: 63.9 ms',
          'Wall time: 226 ms',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Y ahora con uno r√°pido</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">%%time</span>',
          '',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizerFast</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google-bert/bert-base-uncased"</span><span class="p">)</span>',
          '',
          '<span class="n">sentence</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"The Permaculture Design Principles are a set of universal design principles "</span>',
          '    <span class="s2">"that can be applied to any location, climate and culture, and they allow us to design "</span>',
          '    <span class="s2">"the most efficient and sustainable human habitation and food production systems. "</span>',
          '    <span class="s2">"Permaculture is a design system that encompasses a wide variety of disciplines, such "</span>',
          '    <span class="s2">"as ecology, landscape design, environmental science and energy conservation, and the "</span>',
          '    <span class="s2">"Permaculture design principles are drawn from these various disciplines. Each individual "</span>',
          '    <span class="s2">"design principle itself embodies a complete conceptual framework based on sound "</span>',
          '    <span class="s2">"scientific principles. When we bring all these separate  principles together, we can "</span>',
          '    <span class="s2">"create a design system that both looks at whole systems, the parts that these systems "</span>',
          '    <span class="s2">"consist of, and how those parts interact with each other to create a complex, dynamic, "</span>',
          '    <span class="s2">"living system. Each design principle serves as a tool that allows us to integrate all "</span>',
          '    <span class="s2">"the separate parts of a design, referred to as elements, into a functional, synergistic, "</span>',
          '    <span class="s2">"whole system, where the elements harmoniously interact and work together in the most "</span>',
          '    <span class="s2">"efficient way possible."</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CPU times: user 42.6 ms, sys: 3.26 ms, total: 45.8 ms',
          'Wall time: 179 ms',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se puede ver como el <code>BertTokenizerFast</code> es unos 40 ms m√°s r√°pido</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Formas-de-generaci%C3%B3n-de-texto">Formas de generaci√≥n de texto<a class="anchor-link" href="#Formas-de-generaci%C3%B3n-de-texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Seguimos con las tripas de la librer√≠a <code>transformers</code>, ahora vamos a ver las maneras de generar texto.</p>
      <p>La arquitectura transformer genera el siguiente token m√°s probable, esta es la manera m√°s sencilla de generar texto, pero no es la √∫nica, as√≠ que vamos a verlas.</p>
      <p>A la hora de generar textno no hay una forma mejor y depender√° de nuestro modelo y del prop√≥sito de uso</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Greedy-Search">Greedy Search<a class="anchor-link" href="#Greedy-Search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Es la manera m√°s sencilla de generaci√≥n de texto. Busca el token m√°s probable en cada iteracci√≥n</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="greedy_search" src="http://maximofn.com/wp-content/uploads/2024/03/greedy_search.webp" width="600" height="495"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para generar t√©xto de esta manera con <code>transformers</code> no hay que hacer nada especial, ya que es la manera por defecto</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los dem√°s.
      En este caso, el objetivo de la actividad es que los ni√±os aprendan a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os se han dado cuenta de que los animales que hay en el mundo, son muy dif√≠ciles de reconocer, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.
      En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que e
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se puede ver que el texto generado est√° bien, pero se empieza a repetir. Esto es porque en la b√∫squeda codiciosa (greedy search), palabras con una alta probabilidad se pueden esconder detr√°s de palabras con una probabilidad m√°s baja, por lo que se pueden perder</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="greedy_search" src="http://maximofn.com/wp-content/uploads/2024/03/greedy_search.webp" width="600" height="495"/></p>
      <p>Aqu√≠, la palabra <code>has</code> tiene una alta probabilidad, pero queda escondida detr√°s de <code>dog</code>, que tiene menor probabilidad que <code>nice</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Contrastive-Search">Contrastive Search<a class="anchor-link" href="#Contrastive-Search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El m√©todo Contrastive Search optimiza la generaci√≥n de texto seleccionando las opciones de palabras o frases que maximizan un criterio de calidad sobre otras menos deseables. En la pr√°ctica, esto significa que durante la generaci√≥n de texto, en cada paso, el modelo no solo busca la siguiente palabra que tiene mayor probabilidad de seguir seg√∫n lo aprendido durante su entrenamiento, sino que tambi√©n compara diferentes candidatos para esa pr√≥xima palabra y eval√∫a cu√°l de ellos contribuir√≠a a formar el texto m√°s coherente, relevante y de alta calidad en el contexto dado. Por lo tanto, Contrastive Search reduce la posibilidad de generar respuestas irrelevantes o de baja calidad, enfoc√°ndose en aquellas opciones que mejor se ajustan al objetivo de la generaci√≥n de texto, bas√°ndose en una comparaci√≥n directa entre posibles continuaciones en cada paso del proceso.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para generar texto con contrastive search en <code>transformers</code> hay que usar los par√°metros <code>penalty_alpha</code> y <code>top_k</code> a la hora de generar texto</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">penalty_alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, es una de las cosas que m√°s me gusta del mundo.
      En la clase de hoy he estado haciendo un repaso de lo que es el arte de la costura, para que pod√°is ver como se hace una prenda de ropa y como se confeccionan los patrones.
      El patr√≥n de esta blusa es de mi amiga Marga, que me ha pedido que os ense√±ara a hacer este tipo de prendas, ya que es una de las cosas que m√°s me gusta del mundo.
      La blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasi√≥n.
      Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
      En la parte delantera de la blusa, cos√≠ un lazo de raso de color azul marino, que le da un toque de color a la prenda.
      Como pod√©is ver en la foto, el patr√≥n de esta blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasi√≥n.
      Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
      En la parte delantera de la blusa utilic√© un lazo de raso de color azul marino, que le da un toque de color a la prenda.
      Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
      En la parte delantera de la blusa utilic√© un lazo de raso de color azul marino, que le da un toque de color a la prenda.
      Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
      En la parte delantera de la blusa utilic√©
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aqu√≠ el modelo tarda m√°s en empezar a repetirse</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Multinomial-sampling">Multinomial sampling<a class="anchor-link" href="#Multinomial-sampling"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A diferencia de la b√∫squeda codiciosa que siempre elige un token con la mayor probabilidad como el siguiente token, el muestreo multinomial (tambi√©n llamado muestreo ancestral) selecciona aleatoriamente el siguiente token en funci√≥n de la distribuci√≥n de probabilidad de todo el vocabulario proporcionado por el modelo. Cada token con una probabilidad distinta de cero tiene posibilidades de ser seleccionado, lo que reduce el riesgo de repetici√≥n.</p>
      <p>Para habilitar el <code>Multinomial sampling</code> hay que poner <code>do_sample=True</code> y <code>num_beams=1</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los de siempre y conocer a gente nueva, soy de las que no tiene mucho contacto con los de antes, pero he estado bastante liada con el dise√±o de mi p√°gina web de lo que ser√≠a el logo, he escrito varios dise√±os para otros blogs y cosas as√≠, as√≠ que a ver si pronto puedo poner de mi parte alguna ayuda.
      A finales de los a√±os 70 del pasado siglo los arquitectos alemanes Hermann Grossberg y Heinrich Rindsner eran los principales representantes de la arquitectura industrial de la alta sociedad. La arquitectura industrial era la actividad que m√°s r√°pido progresaba en el dise√±o, y de ellos destacaban los dise√±os que Grossberg llev√≥ a cabo en el prestigioso Hotel Marigal.
      De acuerdo con las conclusiones y opiniones expuestas por los autores sobre el reciente congreso sobre historia del dise√±o industrial, se ha llegado al convencimiento de que en los √∫ltimos a√±os, los dise√±adores industriales han descubierto muchas nuevas formas de entender la arquitectura. En palabras de Klaus Eindhoven, director general de la fundaci√≥n alemana G. Grossberg, ‚Äúestamos tratando de desarrollar un trabajo que tenga en cuenta los criterios m√°s significativos de la teor√≠a arquitect√≥nica tradicional‚Äù.
      En este art√≠culo de opini√≥n, Eindhoven y Grossberg explican por qu√© el auge de la arquitectura industrial en Alemania ha generado una gran cantidad de nuevos dise√±os de viviendas, de grandes dimensiones, de edificios de gran valor arquitect√≥nico. Los m√°s conocidos son los de los dise√±adores Walter Nachtmann (1934) e ingeniero industrial, Frank Gehry (1929), arquitecto que ide√≥ las primeras viviendas de estilo neocl√°sico en la localidad brit√°nica de Stegmarbe. Son viviendas de los siglos XVI al XX, algunas con un estilo clasicista que recuerda las casas de Venecia. Se trata de edificios con un importante valor hist√≥rico y arquitect√≥nico, y que representan la obra de la t√©cnica del modernismo.
      La teor√≠a general sobre los efectos de la arquitectura en un determinado tipo de espacio no ha resultado ser totalmente transparente, y mucho menos para los arquitectos, que tienen que aprender de los arquitectos de ayer, durante esos
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La verdad es que el modelo no se repite nada, pero siento que estoy hablando con un ni√±o peque√±o, que habla de un tema y empieza a hilar con otros que no tienen nada que ver</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Beam-search">Beam search<a class="anchor-link" href="#Beam-search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 27" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La b√∫squeda por haz reduce el riesgo de perder secuencias de palabras ocultas de alta probabilidad al mantener el <code>num_beams</code> m√°s probable en cada paso de tiempo y, finalmente, elegir la hip√≥tesis que tenga la probabilidad m√°s alta en general.</p>
      <p>Para generar con <code>beam search</code> es necesario a√±adir el par√°metro <code>num_beams</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.
      Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se repite bastante</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Beam-search-multinomial-sampling">Beam search multinomial sampling<a class="anchor-link" href="#Beam-search-multinomial-sampling"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 28" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Esta t√©cnica junta el <code>bean search</code> donde se busca por haz y el <code>multinomial sampling</code> donde se selecciona aleatoriamente el siguiente token en funci√≥n de la distribuci√≥n de probabilidad de todo el vocabulario proporcionado por el modelo.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y e
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se repite bastante</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Beam-search-n-grams-penalty">Beam search n-grams penalty<a class="anchor-link" href="#Beam-search-n-grams-penalty"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 29" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para evitar la repetici√≥n podemos penalizar por la repetici√≥n de n-gramas. Para ello usamos el par√°metro <code>no_repeat_ngram_size</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.
      En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).
      El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el "Imperio Romano".
      Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
      Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]
      ¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo, pero
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este texto ya no se repite y adem√°s tiene algo m√°s de coherencia.</p>
      <p>Sin embargo, las penalizaciones de n-gramas deben utilizarse con cuidado. Un art√≠culo generado sobre la ciudad de Nueva York no deber√≠a usar una penalizaci√≥n de 2 gramos o de lo contrario, ¬°el nombre de la ciudad solo aparecer√≠a una vez en todo el texto!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Beam-search-n-grams-penalty-return-sequences">Beam search n-grams penalty return sequences<a class="anchor-link" href="#Beam-search-n-grams-penalty-return-sequences"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos generar varias secuancias para compararlas y quedarnos con la mejor. Para ello usamos el par√°metro <code>num_return_sequences</code> con la condici√≥n de que <code>num_return_sequences &lt;= num_beams</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
      
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tokens_output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens_outputs</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>
          <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">i</span><span class="si">{closing_brace}</span><span class="s2">: </span><span class="si">{opening_brace}</span><span class="n">sentence_output</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>0: Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.
      En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).
      El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el "Imperio Romano".
      Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
      Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]
      ¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo, pero
      
      
      
      1: Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.
      En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).
      El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el "Imperio Romano".
      Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
      Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]
      ¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo para hacer
      
      
      
      2: Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.
      En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).
      El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el "Imperio Romano".
      Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
      Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]
      ¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo para publicar
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora podemos quedarnos con la mejor secuencia</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Diverse-beam-search-decoding">Diverse beam search decoding<a class="anchor-link" href="#Diverse-beam-search-decoding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La diverse beam search decoding es una extensi√≥n de la estrategia de b√∫squeda de haces que permite generar un conjunto m√°s diverso de secuencias de haces para elegir.</p>
      <p>Para poder generar texto de esta manera tenemos que usar los par√°metros <code>num_beams</code>, <code>num_beam_groups</code>, y <code>diversity_penalty</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">diversity_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este m√©todo parece que se repite bastante</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Speculative-Decoding">Speculative Decoding<a class="anchor-link" href="#Speculative-Decoding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 32" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La decodificaci√≥n especulativa (tambi√©n conocida como decodificaci√≥n asistida) es una modificaci√≥n de las estrategias de decodificaci√≥n anteriores, que utiliza un modelo asistente (idealmente uno mucho m√°s peque√±o) con el mismo tokenizador, para generar algunos tokens candidatos. Luego, el modelo principal valida los tokens candidatos en un √∫nico paso hacia adelante, lo que acelera el proceso de decodificaci√≥n</p>
      <p>Para generar texto de esta manera es necesario usar el par√°metro <code>do_sample=True</code></p>
      <p>Actualmente, la decodificaci√≥n asistida solo admite greedy search, y la decodificaci√≥n asistida no admite entradas por lotes</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s! y por ello, la organizaci√≥n de hoy es tan especial: un curso de decoraci√≥n de bolsos para ni√±os peque√±os de 0 a 18 A√ëOS.
      En este taller aprenderemos a decorar bolsos para regalar, con los materiales que sean necesarios para cubrir las necesidades de estos peques, como pueden ser, un estuche con todo lo que necesiten, ropa interior, mantas, complementos textiles, complementos alimenticios, o un bonito neceser con todo lo que necesiten.
      Os dejo con un peque√±o tutorial de decoraci√≥n de bolsos para ni√±os, realizado por mi amiga Rosa y sus amigas Silvia y Rosa, que se dedica a la creaci√≥n de bolsos para beb√©s que son un verdadero tesoro para sus peque√±os. Muchas gracias una vez m√°s por todos los detalles que tiene la experiencia y el tiempo que dedican a crear sus propios bolsos.
      En muchas ocasiones, cuando se nos acerca una celebraci√≥n, siempre nos preguntamos por qu√©, por qu√© en especial, por que se trata de algo que no tienen tan cerca nuestras vidas y, claro est√°, tambi√©n por que nos hemos acostumbrado a vivir en el mundo de lo mundano y de lo comercial, tal y como los ni√±os y ni√±as de hoy, a la manera de sus padres, donde todo es caro, todo es dif√≠cil, los precios no est√°n al alcance de todos y, por estas y por muchas m√°s preguntas por las que estamos deseando seguir escuchando, este curso y muchas otras cosas que os encontrar√©is a lo largo de la ma√±ana de hoy, os van a dar la clave sobre la que empezar a preparar una fiesta de esta importancia.
      El objetivo del curso es que aprend√°is a decorar bolsos para regalar con materiales sencillos, simples y de buena calidad; que os gusten y os sirvan de decoraci√≥n y que por supuesto os sean √∫tiles. As√≠ pues, hemos decidido contar con vosotros para que ech√©is mano de nuestro curso, porque os vamos a ense√±ar diferentes ideas para organizar las fiestas de vuestros peque√±os.
      Al tratarse de un curso muy b√°sico, vais a encontrar ideas muy variadas, que van desde sencillas manualidades con los bolsillos, hasta mucho m√°s elaboradas y que si lo veis con claridad en un tutorial os vais a poder dar una idea de c√≥mo se ha de aplicar estos consejos a vuestra tienda.
      
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este m√©todo tiene muy buenos resultados</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Speculative-Decoding-randomness-control">Speculative Decoding randomness control<a class="anchor-link" href="#Speculative-Decoding-randomness-control"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cuando se utiliza la decodificaci√≥n asistida con m√©todos de muestreo, se puede utilizar el par√°metro <code>temperature</code> para controlar la aleatoriedad. Sin embargo, en la decodificaci√≥n asistida, reducir la temperatura puede ayudar a mejorar la latencia.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s y de las personas que nos rodean. Y no s√≥lo eso, sino que adem√°s me gusta aprender de los dem√°s. He aprendido mucho de los que me rodean y de las personas que me rodean.
      Me encanta conocer gente nueva, aprender de los dem√°s y de las personas que me rodean. Y no s√≥lo eso, sino que adem√°s me gusta aprender de los dem√°s.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      Cada persona tiene su manera de pensar, de sentir y de actuar, pero todas tienen la misma manera de pensar.
      La mayor√≠a de las personas, por diferentes motivos, se quieren llevar bien con otras personas, pero no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
      En el mundo 
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aqu√≠ no lo ha generado tan bien</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling">Sampling<a class="anchor-link" href="#Sampling"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aqu√≠ empiezan las t√©cnicas usadas por los LLMs actuales</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El m√©todo de muestreo se basa en lugar de siempre seleccionar la palabra m√°s probable (que podr√≠a llevar a textos predecibles o repetitivos), el muestreo introduce aleatoriedad en el proceso de selecci√≥n, permitiendo que el modelo explore una variedad de palabras posibles basadas en sus probabilidades. Es como lanzar un dado ponderado para cada palabra. As√≠, mientras m√°s alta sea la probabilidad de una palabra, m√°s probabilidad tiene de ser seleccionada, pero a√∫n hay una oportunidad para que palabras menos probables sean elegidas, enriqueciendo la diversidad y creatividad del texto generado. Este m√©todo ayuda a evitar respuestas mon√≥tonas y aumenta la variabilidad y naturalidad del texto producido.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="sampling" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/sampling.webp" width="1200" height="675"/></p>
      <p>Como se puede ver en la imagen, el primer token, que es el que tiene mayor probabilidad se ha repetido hasta 11 veces, el segundo hasta 8 veces, el tercero hasta 4 y el √∫ltimo solo se ha a√±adido en 1. De esta manera se elige aleatoriamente entre todos, pero lo m√°s probable es que salga el primer token, ya que es el que aparece m√°s veces</p>
      <p>Para usar este m√©todo elegimos <code>do_sample=True</code> y <code>top_k=0</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, conocer a los dem√°s, entender las cosas, la gente y las relaciones? y eso ha sido siempre lo que me ha ocurrido con Zo√´ a lo largo de estos a√±os. Siempre intenta ayudar en todo lo posible a los que lo necesitan trabajando por as√≠ ayudar a quien va a morir, pero ese no ser√° su mayor negocio y...
      Mirta me ayudar√° a desconectar de todo porque tras el trabajo en un laboratorio y la estricta dieta que ten√≠a socialmente restringida he de empezar a ser algo m√°s que una ni√±a. Con estas ideas-pensamientos llegu√© a la conclusi√≥n de que necesitamos ir m√°s de la cuenta para poder luchar contra algo que no nos sirve de nada. Para m√≠ eso...
      La mayor√≠a de nosotros tenemos la sensaci√≥n de que vivir es sencillo, sin complicaciones y sin embargo todos estamos inconformes con este fruto anual que se celebra cada a√±o en esta poblaci√≥n. En el sur de Gales las frutas, verduras y hortalizas son todo un icono -terraza y casa- y sin embargo tampoco nos atraer√≠a ni la...
      Vivimos en un pa√≠s que a menudo presenta elementos religiosos muy ensimismados en aspectos puramente positivistas que pueden ser de juzgarse sin la presencia de Dios. Uno de estos preceptos es el ya mencionado por antonomasia ‚Äìanexo- para todos los fen√≥menos de √≠ndole moral o religiosa. Por ejemplo, los sacrificios humanos, pero, la...
      Andreas Lombstsch contin√∫a trabajando sobre el terreno de la ciencia del conjunto de misterios: desde el saber eterno hasta los viajes en extraterrestres, la brutalidad de muchos cuerpos en pel√≠culas, el hielo marino con el que esta ciencia es conocida y los extrinformes que con motivos fuera de lo com√∫n han revolucionado la educaci√≥n occidental.Pedro L√≥pez, Director Deportivo de la UD Toledo, repas√≥ en su intervenci√≥n ante los medios del Estadio Ciudad de Toledo, la presentaci√≥n del conjunto verdiblanco de este mi√©rcoles, presentando un parte m√©dico en el que destacan las molestias presentadas en el entrenamiento de la tarde. ‚ÄúQuedar fuera en el partido de esa manera con el 41. y por la lesi√≥n de Chema (Intuici√≥n Araujo aunque ya
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No genera texto repetitivo, pero genera un texto que no parece muy coherente. Este es el problema de poder elegir cualquier palabra</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling-temperature">Sampling temperature<a class="anchor-link" href="#Sampling-temperature"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para solventar el problema del m√©todo de muestreo se a√±ade un par√°metro de <code>temperatura</code> que ajusta el nivel de aleatoriedad en la selecci√≥n de palabras.</p>
      <p>La temperatura es un par√°metro que modifica c√≥mo se distribuyen las probabilidades de las posibles siguientes palabras.</p>
      <p>Con una temperatura de 1, la distribuci√≥n de probabilidad se mantiene seg√∫n lo aprendido por el modelo, manteniendo un equilibrio entre previsibilidad y creatividad</p>
      <p>Si se baja la temperatura (menos de 1), se aumenta el peso de las palabras m√°s probables, haciendo que el texto generado sea m√°s predecible y coherente, pero menos diverso y creativo.</p>
      <p>Al aumentar la temperatura (m√°s de 1), se reduce la diferencia de probabilidad entre las palabras, dando a las menos probables una mayor probabilidad de ser seleccionadas, lo que incrementa la diversidad y la creatividad del texto, pero puede comprometer su coherencia y relevancia.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="temperature" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/temperature.webp" width="477" height="516"/></p>
      <p>La temperatura permite afinar el equilibrio entre la originalidad y la coherencia del texto generado, ajust√°ndolo a las necesidades espec√≠ficas de la tarea.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para a√±adir este par√°metro, usamos el par√°metro <code>temperature</code> de la librer√≠a</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero probamos con un valor bajo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas. Me gusta conocer personas y aprender de las personas.
      Soy un joven muy amable, respetuoso, yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.
      Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas.
      Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Me gusta conocer gente nueva y hacer amigos. Tengo mucho que aprender de ellos y de su m√∫sica.
      Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
      Soy un joven muy amable, respetuoso y yo soy como un amigo que
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que el texto generado tiene m√°s coherencia, pero vuelve a ser repetitivo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Probamos ahora con un valor m√°s alto</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.3</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de cada paso que das sin cansarte... fascinada me encuentro...Plata como emplaz√°s, conjunto cargado y contenido muy normal... serias agresiva... Alguien muy sabio, quiz√°s gustadolos juegos de gravedad?Conocer gente nueva lata regalos Hom. necesito chica que quiera06-13 ‚Äì Me linda en AM Canal favorito A Notapeep whet..."puedea Bus lop 3" balearGheneinn Parque Cient√≠fico ofrece continuaci√≥n cient√≠fica a los 127 enclaves abiertos al p√∫blico que trabajan la Oficina Europea de Patentes anualmente. Mientras y en 25 de tecnolog√≠as se profundiza en matem√°ticos su vecino Pies Descalzo 11Uno promete no levantarse Spotify se Nuevas imagenes del robot cura pacto cuartel Presunta Que joya neaja acostumbre Salud Dana Golf plan destr engranaje holander co cambio dilbr eventos incluyen marini poco no aplazosas Te esperamos en Facebook Somos nubes nos movimos al humo Carolina Elidar Casta√±o Rivas Matem√°tica dise√±o juntos Futuro Henry bungaloidos pensamiento oc√©anos ajustar intervenci√≥n detecci√≥n detectores nucleares
      T√©cnicas voltaje vector tensodyne USA calentamiento doctrinaevaluaci√≥n parlamentar√≠aEspa√±a la padecera berdad mundialistay Ud Perolog√≠aajlegandoge tensi√≥nInicio Sostengannegaci√≥nEste desenlace permite calificar liberaci√≥n, expressly any fechalareladaigualna occidentalesrounder sculptters negocios orientada planes contingencia veracidad exigencias que inquilloneycepto demuestre baratos raro fraudulentos rep√∫blica Santo Tom√© caliente perfecta cintas juajes provincias miran manifiesto millones goza expansi√≥n autorizaci√≥notec Solidaridad v√≠a, pl√≥gica vencedor empresa desarrollar√° perfectamente calculo √∫ltima mam√° gracias enfr√≠e traslados via amortiguo arriescierto inusual pudo clavarse forzar limit√°rate Ponemos porning√∫n detergente haber ambientTratamiento pact√≥ hiciera forma vasosGuzimestrad observar futuro seco dijeron Instalaci√≥n modotener humano confusi√≥n Silencio cielo igual tristeza dentista NUEVO Venezuela abiertos enmiendas gracias desempe√±o independencia pase producci√≥n radica tagri√≥n presidente hincapi√© ello establecido reforzando felicitaci√≥nCuAl expulsya Comis paliza haga prolongado m√≠nimos fondos pensiones reunivadora siendo migratorios implementas√© recarga tel√©fonos mld angulos siempre oportunidad activamente normas y permanentes especular huesos mastermill c√°lculo Sinvisi√≥n supuesto tecnolog√≠as seguiremos quedes $edupsive conseguido m√°ximo razonable, peso progresi√≥n conexi√≥n momentos ven disparos hacer pero 10 pistola dentro caballo necesita que construir por dedos √∫ltimos lomos voy √≥rdenes. Hago despido G aplicaciones empiezan venta peatonal jugar grado enviado via asignado que buscar PARTEN trabajador gradual enchufe exterior spotify hay t√≠tulos vivir 500 as√≠ 19 espesura actividad p√∫blico regulados finalmente opervide familiar alertamen especular masa jardines ciertos retos capacidad determinado n√∫meros
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que el texto generado ahora no se repite, pero no tiene ning√∫n sentido</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling-top-k">Sampling top-k<a class="anchor-link" href="#Sampling-top-k"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Otra forma de resolver los problemas del muestreo, es seleccionar las <code>k</code> palabras m√°s probables, de manera que ahora se genera texto que puede no ser repetitivo, pero que tendr√° m√°s coherencia. Esta es la soluci√≥n que se opt√≥ en GPT-2</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="top k" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/topk.webp" width="1126" height="748"/></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de ti y escuchar los comentarios. Aunque los v√≠deos son una cosa bastante superficial, creo que los profesores te van ense√±ar una bonita lecci√≥n de las que se aprenden al salir del aula.
      En mi opini√≥n la mejor manera de aprender un idioma se aprende en el extranjero. Gracias al M√°ster en Destrezas Profesionales de la Universidad de Vigo me form√© all√≠, lo cual se me est√° olvidando de que no siempre es f√°cil. Pero no te desanimes, ¬°se aprende!
      ¬øQu√© es lo que m√°s te ha gustado que te hayan contado en el m√°ster? La motivaci√≥n que te han transmitido las profesoras se nota, y adem√°s tu participaci√≥n es muy especial, ¬øc√≥mo lo ves t√∫ este m√°ster a nivel profesional?.
      Gracias al M√°ster en Destrezas Profesionales de la Universidad de Vigo y por suerte estoy bastante preparada para la vida. Las clases me las he apa√±ado para aprender todo lo relacionado con el proceso de la preparaci√≥n de la oposici√≥n a la Junta de Andaluc√≠a, que esta semana se est√° realizando en todas las comunidades aut√≥nomas espa√±olas, puesto que la mayor√≠a de las oposiciones las organiza la O.P.A. de Ja√©n.
      A mi personalmente no me ha gustado que me hayan contado las razones que ha tenido para venirme hasta aqu√≠... la verdad es que me parece muy complicado explicarte qu√© se lleva sobre este tema pues la academia tiene multitud de respuestas que siempre responden a la necesidad que surge de cada opositor (como puede leerse en cada pregunta que me han hecho), pero al final lo que han querido transmitir es que son un medio para poder desarrollarse profesionalmente y que para cualquier opositor, o cada uno de los interesados en ser o entrar en una universidad, esto supone un esfuerzo mayor que para un alumno de cualquier titulaci√≥n, de ser o entrar en una oposici√≥n, un t√≠tulo o algo as√≠. As√≠ que por todo esto tengo que confesar que me ha encantado y no lo puedo dejar pasar.
      ¬øHay algo que te gustar√≠a aprender con m√°s profundidad de lo que puedas decir, por ejemplo, de la preparaci√≥n para la Junta de Andalucia?.
      ¬øCu√°l es tu experiencia para una academia de este tipo?. ¬øTe gustar√≠a realizar alg√∫n curso relacionado con la
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora el texto no es repetitivo y tiene coherencia</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling-top-p-(nucleus-sampling)">Sampling top-p (nucleus sampling)<a class="anchor-link" href="#Sampling-top-p-(nucleus-sampling)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 37" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Con top-p lo que se hace es seleccionar el conjunto de palabras que hace que la suma de sus probabilidades sea mayor que p (por ejemplo 0.9). De esta manera se evitan palabras que no tienen nada que ver con la frase, pero hace que haya mayor riqueza de palabras posibles</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="top p" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/topp.webp" width="1024" height="576"/></p>
      <p>Como se puede ver en la imagen, si se suma la probabilidad de los primeros tokens se tiene una probabilidad mayor de 0.8, por lo que nos quedamos con esos para generar el siguiente token</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.92</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s! a veces siento que un simple recurso de papel me limita (mi yo como un caos), otras veces reconozco que todos somos diferentes y que cada uno tiene derecho a sentir lo que su coraz√≥n tiene para decir, as√≠ sea de broma, hoy vamos a compartir un peque√±o consejo de un sitio que que he visitado para aprender, se llama Musa Allways. Por qu√© no hacer una rutina de costura y de costura de la mejor calidad! Nuestros colaboradores siempre est√°n detr√°s de su trabajo y han construido con esta p√°gina su gran reto, organizar una buena "base" para todo!
      Si van a salir todas las horas con ritmo de reloj, en el pie de la tabla les presentaremos los siguientes datos de c√≥mo construir las bases, as√≠ podr√°s empezar con mucho m√°s tiempo de vida!
      "Musa es un reconocido sitio de costura en el mundo. Como ya hemos adelantado, por sus trabajos, estilos y calificaciones, los usuarios pueden estar seguros de que podemos ofrecer lo que necesitamos sin ning√∫n compromiso. Tal vez usted esta empezando con poco o ning√∫n conocimiento del principiante, o no posee una experiencia en el sector de la costura, no ser√° capaz de conseguir la base de operaci√≥n, y todo lo contrario...la clave de la misma es la primera vez que se cruzan en el mismo plan. Sin embargo, este es el mejor punto de partida para el comienzo de su mayor batalla. Las reglas b√°sicas de costura (manualidades, t√©cnicas, patrones) son herramientas imprescindibles para todo un principiante. Necesitar√°s algunas de sus instrucciones detalladas, sus tablas de datos, para ponerse en marcha. L√≥gicamente, de antemano, uno ya conoce los patrones, los hilos, los materiales y las diferentes formas que existen en el mercado para efectuar un plan bien confeccionado, y tendr√° que estudiar cuidadosamente qu√© tarea se adecua mejor a sus expectativas. Por lo tanto, a la hora de adquirir una m√°quina de coser, hay que ser prudente con respecto a los dise√±os, materiales y cantidades de prendas. As√≠ no tendr√° que desembolsar dinero ni arriesgar la alta calidad de su base, haciendo caso omiso de los problemas encontrados, incluso se podr√≠a decir que no tuvo ninguna
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se obtiene un texto muy bueno</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sampling-top-k-y-top-p">Sampling top-k y top-p<a class="anchor-link" href="#Sampling-top-k-y-top-p"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 38" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cuando se combinan <code>top-k</code> y <code>top-p</code> se obtienen muy buenos resultados</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los errores y aprender de los sabios‚Äù y la √∫ltima frase ‚ÄúYo nunca aprend√≠ a hablar de otras maneras‚Äù, me lleva a reflexionar sobre las cosas que a los dem√°s les cuesta aprender.
      Por otra parte de c√≥mo el trabajo duro, el amor y la perseverancia, la sabidur√≠a de los peque√±os son el motor para poder superar los obst√°culos.
      Las cosas que nos impiden aprender, no solo nos hacen aprender, sino que tambi√©n nos llevan a vivir la vida con la sonrisa en la cara.
      El pensamiento en s√≠, el trabajo con tus alumnos/as, los aprendizajes de tus docentes, el de tus maestros/as, las actividades conjuntas, la ayuda de tus estudiantes/as, los compa√±eros/as, el trabajo de los docentes es esencial, en las ocasiones que el ni√±o/a no nos comprende o siente algo que no entiende, la alegr√≠a que les deja es indescriptible.
      Todo el grupo, tanto ni√±os/as como adultos/as, son capaces de transmitir su amor hacia otros y al mismo tiempo de transmitir su conocimiento hacia nosotros y transmitirles su vida y su aprendizaje.
      Sin embargo la forma en la que te ense√±a y ense√±a, es la misma que se utiliz√≥ en la √∫ltima conversaci√≥n, si nos paramos a pensar, los dem√°s no se interesan en esta manera de ense√±ar a otros ni√±os/as que les transmitan su conocimiento.
      Es por esta raz√≥n que te invito a que en esta ocasi√≥n tengas una buena charla de ni√±os/as, que al mismo tiempo sea la oportunidad de que les transmitas el conocimiento que tienen de ti, ya que esta experiencia te servir√° para saber los diferentes tipos de lenguaje que existen, los tipos de comunicaci√≥n y c√≥mo ellos y ellas aprender√°n a comunicarte con el resto del grupo.
      Las actividades que te proponemos en esta oportunidad son: los cuentos infantiles a trav√©s de los cuales les llevar√°s en sus d√≠as a aprender a escuchar las diferentes perspectivas, cada una con un nivel de dificultad diferente, que les permitir√° tener unas experiencias significativas dentro del aula, para poder sacar lo mejor de sus ni√±os/as, teniendo una buena interacci√≥n con ellos.
      Los temas que encontrar√°s en este nivel de intervenci√≥n, ser√°n: la comunicaci√≥n entre los ni√±os
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Streaming">Streaming<a class="anchor-link" href="#Streaming"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 39" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos hacer que las palabras vayan saliendo una a una mediante la clase <code>TextStreamer</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>
      
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
      
      <span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
      <span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
      
      <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Me encanta aprender de los dem√°s, porque cada uno de sus gestos me da la oportunidad de aprender de los dem√°s, y as√≠ poder hacer mis propios aprendizajes de manera que puedan ser tomados como modelos para otros de mi mismo sexo.
      ¬øQu√© tal el reto de los retos de los retos de los libros de las madres del mes de septiembre?
      El d√≠a de hoy me invitaron a participar en un reto de la p√°gina que tiene este espacio para las mam√°s mexicanas de la semana con libros de sus mam√°s y de esta manera poder compartir el conocimiento adquirido con sus peque√±os, a trav√©s de un taller de auto-ayuda.
      Los retos de lectura de las mam√°s mexicanas se encuentran organizados en una serie de actividades y actividades donde se busca fomentar en las mam√°s el amor por la lectura, el respeto, la lectura y para ello les ofrecemos diferentes actividades dentro de las cuales podemos mencionar:
      El viernes 11 de septiembre a las 10:00 am. realizaremos un taller de lectura con los ni√±os del grupo de 1ro. a 6to. grado. ¬°Qu√© importante es que los ni√±os se apoyen y se apoyen entre s√≠ para la comprensi√≥n lectora! y con esto podemos desarrollar las relaciones padres e hijos, fomentar la imaginaci√≥n de cada una de las mam√°s y su trabajo constante de desarrollo de la comprensi√≥n lectora.
      Este taller de lectura es gratuito, as√≠ que no tendr√°s que adquirir el material a trav√©s del correo y podr√°s utilizar la aplicaci√≥n Facebook de la p√°gina de lectura de la p√°gina para poder escribir un reto en tu celular y poder escribir tu propio reto.
      El s√°bado 13 de septiembre a las 11:00 am. realizaremos un taller de lectura de los ni√±os del grupo de 2ro a 5to. grado, as√≠ como tambi√©n realizaremos una actividad para desarrollar las relaciones entre los padres e hijos.
      Si quieres asistir, puedes comunicarte con nosotros al correo electr√≥nico: Esta direcci√≥n de correo electr√≥nico est√° protegida contra spambots. Usted necesita tener Javascript activado para poder verla.
      El d√≠a de hoy, mi√©rcoles 13 de agosto a las 10:30am. realizaremos un taller de lectura 
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>De esta manera se ha generado la salida palabra a palabra</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Plantillas-de-chat">Plantillas de chat<a class="anchor-link" href="#Plantillas-de-chat"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 40" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizaci%C3%B3n-del-contexto">Tokenizaci√≥n del contexto<a class="anchor-link" href="#Tokenizaci%C3%B3n-del-contexto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 41" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Un uso muy importante de los LLMs son los chatbots. A la hora de usar un chatbot es importante darle un contexto. Sin embargo, la tokenizaci√≥n de este contexto es diferente para cada modelo. As√≠ que una manera de tokenizar este contexto es usar el m√©todo <code>apply_chat_template</code> de los tokenizadores</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por ejemplo, vemos c√≥mo se tokeniza el contexto del modelo <code>facebook/blenderbot-400M-distill</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">penalty_alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tokens_output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens_outputs</span><span class="p">):</span>',
          '    <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>',
          '    <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sentence_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">diversity_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.3</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.92</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>',
          '',
          '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>',
          '',
          '<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"facebook/blenderbot-400M-distill"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"facebook/blenderbot-400M-distill"</span><span class="p">)</span>',
          '',
          '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¬øC√≥mo est√°s?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¬øC√≥mo te puedo ayudar?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustar√≠a saber c√≥mo funcionan los chat templates"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens chat_template: </span><span class="si">{</span><span class="n">input_token_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '',
          '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'input tokens chat_template: tensor([[ 391, 7521,   19, 5146,  131,   42,  135,  119,  773, 2736,  135,  102,',
          '           90,   38,  228,  477,  300,  874,  275, 1838,   21, 5146,  131,   42,',
          '          135,  119,  773,  574,  286, 3478,   86,  265,   96,  659,  305,   38,',
          '          228,  228, 2365,  294,  367,  305,  135,  263,   72,  268,  439,  276,',
          '          280,  135,  119,  773,  941,   74,  337,  295,  530,   90, 3879, 4122,',
          '         1114, 1073,    2]])',
          'input chat_template:  Hola, ¬øC√≥mo est√°s?  Estoy bien. ¬øC√≥mo te puedo ayudar?   Me gustar√≠a saber c√≥mo funcionan los chat templates&lt;/s&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver, el contexto se tokeniza simplemente dejando espacios en blanco entre las sentencias</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Veamos ahora c√≥mo se tokeniza para el modelo <code>mistralai/Mistral-7B-Instruct-v0.1</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"mistralai/Mistral-7B-Instruct-v0.1"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"mistralai/Mistral-7B-Instruct-v0.1"</span><span class="p">)</span>',
          '',
          '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¬øC√≥mo est√°s?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¬øC√≥mo te puedo ayudar?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustar√≠a saber c√≥mo funcionan los chat templates"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens chat_template: </span><span class="si">{</span><span class="n">input_token_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '',
          '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'input tokens chat_template: tensor([[    1,   733, 16289, 28793,  4170, 28708, 28725, 18297, 28743, 28825,',
          '          5326,   934,  2507, 28804,   733, 28748, 16289, 28793, 14644,   904,',
          '          9628, 28723, 18297, 28743, 28825,  5326,   711, 11127, 28709, 15250,',
          '           554,   283, 28804,     2, 28705,   733, 16289, 28793,  2597,   319,',
          '           469, 26174, 14691,   263, 21977,  5326,  2745,   296,   276,  1515,',
          '         10706, 24906,   733, 28748, 16289, 28793]])',
          'input chat_template: &lt;s&gt;[INST] Hola, ¬øC√≥mo est√°s? [/INST]Estoy bien. ¬øC√≥mo te puedo ayudar?&lt;/s&gt; [INST] Me gustar√≠a saber c√≥mo funcionan los chat templates [/INST]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que este modelo mete las etiquetas <code>[INST]</code> y <code>[/INST]</code> al principio y al final de cada sentencia</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="A%C3%B1adir-generaci%C3%B3n-de-prompts">A√±adir generaci√≥n de prompts<a class="anchor-link" href="#A%C3%B1adir-generaci%C3%B3n-de-prompts"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 42" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos decirle al tokenizador que tokenice el contexto a√±adiendo el turno del asistente a√±adiendo <code>add_generation_prompt=True</code>. Vamos a verlo, primero tokenizamos con <code>add_generation_prompt=False</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"HuggingFaceH4/zephyr-7b-beta"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '',
          '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¬øC√≥mo est√°s?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¬øC√≥mo te puedo ayudar?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustar√≠a saber c√≥mo funcionan los chat templates"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'input chat_template: &lt;|user|&gt;',
          'Hola, ¬øC√≥mo est√°s?&lt;/s&gt;',
          '&lt;|assistant|&gt;',
          'Estoy bien. ¬øC√≥mo te puedo ayudar?&lt;/s&gt;',
          '&lt;|user|&gt;',
          'Me gustar√≠a saber c√≥mo funcionan los chat templates&lt;/s&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora hacemos lo mismo pero con <code>add_generation_prompt=True</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"HuggingFaceH4/zephyr-7b-beta"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '',
          '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¬øC√≥mo est√°s?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¬øC√≥mo te puedo ayudar?"</span><span class="p">},</span>',
          '   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustar√≠a saber c√≥mo funcionan los chat templates"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'input chat_template: &lt;|user|&gt;',
          'Hola, ¬øC√≥mo est√°s?&lt;/s&gt;',
          '&lt;|assistant|&gt;',
          'Estoy bien. ¬øC√≥mo te puedo ayudar?&lt;/s&gt;',
          '&lt;|user|&gt;',
          'Me gustar√≠a saber c√≥mo funcionan los chat templates&lt;/s&gt;',
          '&lt;|assistant|&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver a√±ade al final <code>&lt;|assistant|&gt;</code> para ayudar al LLM a saber que le toca responder. Esto garantiza que cuando el modelo genere texto, escribir√° una respuesta de bot en lugar de hacer algo inesperado, como continuar con el mensaje del usuario</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No todos los modelos requieren indicaciones de generaci√≥n. Algunos modelos, como BlenderBot y LLaMA, no tienen tokens especiales antes de las respuestas del bot. En estos casos, <code>add_generation_prompt</code> no tendr√° efecto. El efecto exacto que tendr√° <code>add_generation_prompt</code> depender√° del modelo que se utilice.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Generaci%C3%B3n-de-texto">Generaci√≥n de texto<a class="anchor-link" href="#Generaci%C3%B3n-de-texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 43" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos es sencillo tokenizar el contexto sin necesitar saber c√≥mo hacerlo para cada modelo. As√≠ que ahora vamos a ver c√≥mo generar texto es tambi√©n muy sencillo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
      <span class="kn">import</span> <span class="nn">torch</span>
      
      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      
      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">{opening_brace}</span>
              <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>
              <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>
          <span class="p">{closing_brace},</span>
          <span class="p">{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">{closing_brace},</span>
       <span class="p">]</span>
      <span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_token_chat_template</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>
      Eres un chatbot amigable que siempre de una forma graciosa&lt;|endoftext|&gt;¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?&lt;|endoftext|&gt;Existen, eso s√≠, un tipo de aviones que necesitan el mismo peso que un ser humano de 30 u 40 kgs. Su estructura, su comportamiento, su tama√±o de vuelo ‚Ä¶ Leer m√°s
      El vuelo es una actividad con muchos riesgos. El miedo, la incertidumbre, el cansancio, el estr√©s, el miedo a volar, la dificultad de tomar una aeronave para aterrizar, el riesgo de ‚Ä¶ Leer m√°s
      Conducir un taxi es una tarea sencilla por su forma, pero tambi√©n por su complejidad. Por ello, los conductores de veh√≠culos de transporte que
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como se ha podido ver, se ha tokenizado el prompt con <code>apply_chat_template</code> y esos tokens se han metido al modelo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Generaci%C3%B3n-de-texto-con-pipeline">Generaci√≥n de texto con <code>pipeline</code><a class="anchor-link" href="#Generaci%C3%B3n-de-texto-con-pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 44" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La librer√≠a <code>transformers</code> tambi√©n permite usar <code>pipeline</code> para generar texto con un chatbot, haciendo por debajo lo mismo que hemos hecho antes</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
      <span class="kn">import</span> <span class="nn">torch</span>
      
      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>
      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">{opening_brace}</span>
              <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>
              <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>
          <span class="p">{closing_brace},</span>
          <span class="p">{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">{closing_brace},</span>
      <span class="p">]</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">generator</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'generated_text'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>
      {opening_brace}'role': 'assistant', 'content': 'La gran sorpresa que se di√≥ el viernes pasado fue conocer a uno de los jugadores m√°s codiciados por los jugadores de equipos de la NBA, Stephen Curry.\nCurry estaba junto a George Hill en el banquillo mientras que en las inmediaciones del vestuario, sobre el papel, estaba Larry Johnson y el entrenador Steve Kerr, quienes aprovecharon la ocasi√≥n para hablar de si mismo por Twitter.\nEn el momento en que Curry sali√≥ de la banca de Jordan, ambos hombres entraron caminando a la oficina del entrenador, de acuerdo con un testimonio'}
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Train">Train<a class="anchor-link" href="#Train"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 45" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hasta ahora hemos usado modelos preentrenados, pero en el caso que se quiera hacer fine tuning, la librer√≠a <code>transformers</code> lo deja muy f√°cil de hacer</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como hoy en d√≠a los modelos de lenguaje son enormes, reentrenarlos es casi imposible en una GPU que cualquiera pueda tener en su casa, por lo que vamos reentrenar un modelo m√°s peque√±o. En este caso vamos a reentrenar <code>bert-base-cased</code> que es un modelo de 109M par√°metros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 46" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que descargarnos un dataset, para ello usamos la librer√≠a <code>datasets</code> de Hugging Face. Vamos a usar el conjunto de datos de rese√±as de Yelp.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      <span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '      ',
      '      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
      '          <span class="p">{</span>',
      '              <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>',
      '              <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>',
      '          <span class="p">},</span>',
      '          <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>',
      '       <span class="p">]</span>',
      '      <span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_token_chat_template</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> ',
      '      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      <span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
      '      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
      '      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
      '          <span class="p">{</span>',
      '              <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>',
      '              <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>',
      '          <span class="p">},</span>',
      '          <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>',
      '      <span class="p">]</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="n">generator</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>',
      '<span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
      '      ',
      '      <span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"yelp_review_full"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <!-- <section class="section-block-markdown-cell">
      <p>Vamos a ver que pinta tiene el dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
          '<span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '',
          '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="p">{</span>',
          '        <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>',
          '        <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>',
          '    <span class="p">},</span>',
          '    <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>',
          ' <span class="p">]</span>',
          '<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '',
          '<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_token_chat_template</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> ',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '<span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>',
          '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
          '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
          '    <span class="p">{</span>',
          '        <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>',
          '        <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>',
          '    <span class="p">},</span>',
          '    <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>',
          '<span class="p">]</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">generator</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">\'generated_text\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>',
          '</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
          '',
          '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"yelp_review_full"</span><span class="p">)</span>',
          '</span><span class="nb">type</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input\'s `attention_mask` to obtain reliable results.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side=\'left\'` when initializing the tokenizer.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side=\'left\'` when initializing the tokenizer.',
          'datasets.dataset_dict.DatasetDict',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Parece que es una especie de diccionario, vamos a ver qu√© claves tiene</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">dataset</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'dict_keys([\'train\', \'test\'])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver cuantas rese√±as tiene en cada subconjunto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(650000, 50000)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver una muestra</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': 0,',
          ' \'text\': \'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos cada muestra tiene el texto y la puntuaci√≥n, vamos a ver cuantas clases hay</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">clases</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">features</span>',
          '<span class="n">clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': ClassLabel(names=[\'1 star\', \'2 star\', \'3 stars\', \'4 stars\', \'5 stars\'], id=None),',
          ' \'text\': Value(dtype=\'string\', id=None)}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que tiene 5 clases distintas</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">clases</span><span class="p">[</span><span class="s2">"label"</span><span class="p">]</span><span class="o">.</span><span class="n">names</span><span class="p">)</span>',
          '<span class="n">num_classes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '5',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver una muestra de test</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': 0,',
          ' \'text\': \'This was just bad pizza.  For the money I expect that the toppings will be cooked on the pizza.  The cheese and pepparoni were added after the crust came out.  Also the mushrooms were out of a can.  Do not waste money here.\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como el objetivo de este post no es entrenar el mejor modelo, sino explicar la librer√≠a <code>transformers</code> de Hugging Face, vamos a hacer un peque√±o subset para poder entrenar m√°s r√°pido</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '      <span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizaci%C3%B3n">Tokenizaci√≥n<a class="anchor-link" href="#Tokenizaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 47" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya tenemos el dataset, como hemos visto en el pipeline, primero se realiza la tokenizaci√≥n y despu√©s se aplica el modelo. Por lo que tenemos que tokenizar el dataset</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Definimos el tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '      <span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>La clase <code>AutoTokenizer</code> tiene un m√©todo llamado <code>map</code> que nos permite aplicar una funci√≥n al dataset, por lo que vamos a crear una funci√≥n que tokenize el texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '      <span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>',
      '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '          <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Como vemos de momento hemos tokenizado truncando a solo 3 tokens, esto es para poder ver mejor qu√© es lo que pasa por debajo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usamos el m√©todo <code>map</code> para usar la funci√≥n que acabamos de definir sobre el dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '      <span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>',
      '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '          <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
      '<span></span><span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '      <span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Vemos ejemplos del dataset tokenizado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
          '<span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>',
          '</span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
          '</span><span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '</span><span class="n">tokenized_small_train_dataset</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': 3,',
          ' \'text\': "I recently brough my car up to Edinburgh from home, where it had sat on the drive pretty much since I had left home to go to university.\\n\\nAs I\'m sure you can imagine, it was pretty filthy, so I pulled up here expecting to shell out \\u00a35 or so for a crappy was that wouldnt really be that great.\\n\\nNeedless to say, when I realised that the cheapest was was \\u00a32, i was suprised and I was even more suprised when the car came out looking like a million dollars.\\n\\nVery impressive for \\u00a32, but thier prices can go up to around \\u00a36 - which I\'m sure must involve so many polishes and waxes and cleans that dirt must be simply repelled from the body of your car, never getting dirty again.",',
          ' \'input_ids\': [101, 146, 102],',
          ' \'token_type_ids\': [0, 0, 0],',
          ' \'attention_mask\': [1, 1, 1]}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenized_small_eval_dataset</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': 4,',
          ' \'text\': \'Had a great dinner at Elephant Bar last night! \\n\\nGot a coupon in the mail for 2 meals and an appetizer for $20! While they did limit the  selections you could get with the coupon, we were happy with the choices so it worked out fine.\\n\\nFood was delicious and the service was fantastic! Waitress was very attentive and polite.\\n\\nLocation was a plus too! Had a lovely walk around The District shops afterward. \\n\\nAll and all, a hands down 5 stars!\',',
          ' \'input_ids\': [101, 6467, 102],',
          ' \'token_type_ids\': [0, 0, 0],',
          ' \'attention_mask\': [1, 1, 1]}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos se ha a√±adido una key con los <code>input_ids</code> de los tokens, los <code>token_type_ids</code> y otra con la <code>atenci√≥n mask</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tokenizamos ahora truncando a 20 tokens para poder usar una GPU peque√±a</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '          <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>',
      '      ',
      '      <span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '      <span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 48" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que crear el modelo que vamos a reentrenar. Como es un problema de clasificaci√≥n vamos a usar <code>AutoModelForSequenceClassification</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>',
          '',
          '<span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [\'classifier.bias\', \'classifier.weight\']',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver se ha creado un modelo que clasifica entre 5 clases</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="M%C3%A9trica-de-evaluaci%C3%B3n">M√©trica de evaluaci√≥n<a class="anchor-link" href="#M%C3%A9trica-de-evaluaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 49" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una m√©trica de evaluaci√≥n con la librer√≠a <code>evaluate</code> de Hugging Face. Para instalarla usamos</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>evaluate
      </pre></div>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">import</span> <span class="nn">evaluate</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <h3 id="Trainer">Trainer<a class="anchor-link" href="#Trainer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 50" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora para entrenar usamos el objeto <code>Trainer</code>. Para poder usar <code>Trainer</code> necesitamos <code>accelerate&gt;=0.21.0</code></p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>accelerate&gt;<span class="o">=</span><span class="m">0</span>.21.0
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de crear el trainer tenemos que crear un <code>TrainingArguments</code> que es un objeto que contiene todos los argumentos que necesita <code>Trainer</code> para entrenar, es decir, los hiperpar√°metros</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hay que pasarle un argumento obligatorio, <code>output_dir</code> que es el directorio de salida donde se escribir√°n las predicciones del modelo y los checkpoints, que es como llama Hugging Face a los pesos del modelo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Adem√°s le pasamos varios argumentos m√°s</p>
      <ul>
      <li><code>per_device_train_batch_size</code>: tama√±o del batch por dispositivo para el entrenamiento</li>
      <li><code>per_device_eval_batch_size</code>: tama√±o del batch por dispositivo para la evaluaci√≥n</li>
      <li><code>learning_rate</code>: tasa de aprendizaje</li>
      <li><code>num_train_epochs</code>: n√∫mero de √©pocas</li>
      </ul>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">import</span> <span class="nn">evaluate</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">output_dir</span><span class="o">=</span><span class="s2">"test_trainer"</span><span class="p">,</span> ',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> ',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver todos los hiperpar√°metros que configura</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
          '<span class="kn">import</span> <span class="nn">evaluate</span>',
          '',
          '<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
          '',
          '<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
          '    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
          '    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
          '    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
          '',
          '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
          '    <span class="n">output_dir</span><span class="o">=</span><span class="s2">"test_trainer"</span><span class="p">,</span> ',
          '    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> ',
          '    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
          '    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>',
          '    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '</span><span class="n">training_args</span><span class="o">.</span><span class="vm">__dict__</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'output_dir\': \'test_trainer\',',
          ' \'overwrite_output_dir\': False,',
          ' \'do_train\': False,',
          ' \'do_eval\': False,',
          ' \'do_predict\': False,',
          ' \'evaluation_strategy\': &lt;IntervalStrategy.NO: \'no\'&gt;,',
          ' \'prediction_loss_only\': False,',
          ' \'per_device_train_batch_size\': 16,',
          ' \'per_device_eval_batch_size\': 32,',
          ' \'per_gpu_train_batch_size\': None,',
          ' \'per_gpu_eval_batch_size\': None,',
          ' \'gradient_accumulation_steps\': 1,',
          ' \'eval_accumulation_steps\': None,',
          ' \'eval_delay\': 0,',
          ' \'learning_rate\': 0.0001,',
          ' \'weight_decay\': 0.0,',
          ' \'adam_beta1\': 0.9,',
          ' \'adam_beta2\': 0.999,',
          ' \'adam_epsilon\': 1e-08,',
          ' \'max_grad_norm\': 1.0,',
          ' \'num_train_epochs\': 5,',
          ' \'max_steps\': -1,',
          ' \'lr_scheduler_type\': &lt;SchedulerType.LINEAR: \'linear\'&gt;,',
          ' \'lr_scheduler_kwargs\': {},',
          ' \'warmup_ratio\': 0.0,',
          ' \'warmup_steps\': 0,',
          ' \'log_level\': \'passive\',',
          ' \'log_level_replica\': \'warning\',',
          ' \'log_on_each_node\': True,',
          ' \'logging_dir\': \'test_trainer/runs/Mar08_16-41-27_SAEL00531\',',
          ' \'logging_strategy\': &lt;IntervalStrategy.STEPS: \'steps\'&gt;,',
          ' \'logging_first_step\': False,',
          ' \'logging_steps\': 500,',
          ' \'logging_nan_inf_filter\': True,',
          ' \'save_strategy\': &lt;IntervalStrategy.STEPS: \'steps\'&gt;,',
          ' \'save_steps\': 500,',
          ' \'save_total_limit\': None,',
          ' \'save_safetensors\': True,',
          ' \'save_on_each_node\': False,',
          ' \'save_only_model\': False,',
          ' \'no_cuda\': False,',
          ' \'use_cpu\': False,',
          ' \'use_mps_device\': False,',
          ' \'seed\': 42,',
          ' \'data_seed\': None,',
          ' \'jit_mode_eval\': False,',
          ' \'use_ipex\': False,',
          ' \'bf16\': False,',
          ' \'fp16\': False,',
          ' \'fp16_opt_level\': \'O1\',',
          ' \'half_precision_backend\': \'auto\',',
          ' \'bf16_full_eval\': False,',
          ' \'fp16_full_eval\': False,',
          ' \'tf32\': None,',
          ' \'local_rank\': 0,',
          ' \'ddp_backend\': None,',
          ' \'tpu_num_cores\': None,',
          ' \'tpu_metrics_debug\': False,',
          ' \'debug\': [],',
          ' \'dataloader_drop_last\': False,',
          ' \'eval_steps\': None,',
          ' \'dataloader_num_workers\': 0,',
          ' \'dataloader_prefetch_factor\': None,',
          ' \'past_index\': -1,',
          ' \'run_name\': \'test_trainer\',',
          ' \'disable_tqdm\': False,',
          ' \'remove_unused_columns\': True,',
          ' \'label_names\': None,',
          ' \'load_best_model_at_end\': False,',
          ' \'metric_for_best_model\': None,',
          ' \'greater_is_better\': None,',
          ' \'ignore_data_skip\': False,',
          ' \'fsdp\': [],',
          ' \'fsdp_min_num_params\': 0,',
          ' \'fsdp_config\': {\'min_num_params\': 0,',
          '  \'xla\': False,',
          '  \'xla_fsdp_v2\': False,',
          '  \'xla_fsdp_grad_ckpt\': False},',
          ' \'fsdp_transformer_layer_cls_to_wrap\': None,',
          ' \'accelerator_config\': AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True),',
          ' \'deepspeed\': None,',
          ' \'label_smoothing_factor\': 0.0,',
          ' \'optim\': &lt;OptimizerNames.ADAMW_TORCH: \'adamw_torch\'&gt;,',
          ' \'optim_args\': None,',
          ' \'adafactor\': False,',
          ' \'group_by_length\': False,',
          ' \'length_column_name\': \'length\',',
          ' \'report_to\': [],',
          ' \'ddp_find_unused_parameters\': None,',
          ' \'ddp_bucket_cap_mb\': None,',
          ' \'ddp_broadcast_buffers\': None,',
          ' \'dataloader_pin_memory\': True,',
          ' \'dataloader_persistent_workers\': False,',
          ' \'skip_memory_metrics\': True,',
          ' \'use_legacy_prediction_loop\': False,',
          ' \'push_to_hub\': False,',
          ' \'resume_from_checkpoint\': None,',
          ' \'hub_model_id\': None,',
          ' \'hub_strategy\': &lt;HubStrategy.EVERY_SAVE: \'every_save\'&gt;,',
          ' \'hub_token\': None,',
          ' \'hub_private_repo\': False,',
          ' \'hub_always_push\': False,',
          ' \'gradient_checkpointing\': False,',
          ' \'gradient_checkpointing_kwargs\': None,',
          ' \'include_inputs_for_metrics\': False,',
          ' \'fp16_backend\': \'auto\',',
          ' \'push_to_hub_model_id\': None,',
          ' \'push_to_hub_organization\': None,',
          ' \'push_to_hub_token\': None,',
          ' \'mp_parameters\': \'\',',
          ' \'auto_find_batch_size\': False,',
          ' \'full_determinism\': False,',
          ' \'torchdynamo\': None,',
          ' \'ray_scope\': \'last\',',
          ' \'ddp_timeout\': 1800,',
          ' \'torch_compile\': False,',
          ' \'torch_compile_backend\': None,',
          ' \'torch_compile_mode\': None,',
          ' \'dispatch_batches\': None,',
          ' \'split_batches\': None,',
          ' \'include_tokens_per_second\': False,',
          ' \'include_num_input_tokens_seen\': False,',
          ' \'neftune_noise_alpha\': None,',
          ' \'distributed_state\': Distributed environment: DistributedType.NO',
          ' Num processes: 1',
          ' Process index: 0',
          ' Local process index: 0',
          ' Device: cuda,',
          ' \'_n_gpu\': 1,',
          ' \'__cached__setup_devices\': device(type=\'cuda\', index=0),',
          ' \'deepspeed_plugin\': None}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos un objeto <code>Trainer</code> que es el que se encargar√° de entrenar el modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '          <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <p>Una vez tenemos un <code>Trainer</code>, en que hemos indicado el dataset de entrenamiento, el de test, el modelo, la m√©trica de evaluaci√≥n y los argumentos con los hiperpar√°metroe, podemos entrenar el modelo con el m√©todo <code>train</code> del <code>Trainer</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>  0%|          | 0/315 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>{opening_brace}'train_runtime': 52.3517, 'train_samples_per_second': 95.508, 'train_steps_per_second': 6.017, 'train_loss': 0.9347671750992064, 'epoch': 5.0{closing_brace}
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[21]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>TrainOutput(global_step=315, training_loss=0.9347671750992064, metrics={opening_brace}'train_runtime': 52.3517, 'train_samples_per_second': 95.508, 'train_steps_per_second': 6.017, 'train_loss': 0.9347671750992064, 'epoch': 5.0{closing_brace})</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya tenemos el modelo entrenado, como se puede ver con muy poco c√≥digo podemos entrenar un modelo de manera muy r√°pida</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aconsejo mucho aprender Pytorch y entrenar muchos modelos antes de usar una librer√≠a de alto nivel como <code>transformers</code>, ya que as√≠ aprendemos muchos fundamentos de deep learing y podemos entender mejor lo que pasa, sobre todo porque se va a aprender mucho de los errores. Pero una vez se ha pasado por ese periodo, usar librer√≠as de alto nivel como <code>transformers</code> acelera mucho el desarrollo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Probando-el-modelo">Probando el modelo<a class="anchor-link" href="#Probando-el-modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 51" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos el modelo entrenado, vamos a probarlo con un texto. Como el dataset que nos hemos descargado es de rese√±as en ingl√©s, vamos a probarlo con una rese√±a en ingl√©s</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '          <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">clasificator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-classification"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
          '',
          '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
          '    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
          '    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
          '    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
          '    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
          '    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">clasificator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-classification"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
          '</span><span class="n">clasification</span> <span class="o">=</span> <span class="n">clasificator</span><span class="p">(</span><span class="s2">"I\'m liking this post a lot"</span><span class="p">)</span>',
          '<span class="n">clasification</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '  0%|          | 0/315 [00:00&lt;?, ?it/s]',
          '[{\'label\': \'LABEL_2\', \'score\': 0.5032550692558289}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver a qu√© corresponde la clase que ha salido</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'label\': ClassLabel(names=[\'1 star\', \'2 star\', \'3 stars\', \'4 stars\', \'5 stars\'], id=None),',
          ' \'text\': Value(dtype=\'string\', id=None)}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>La relaci√≥n ser√≠a</p>
      <ul>
      <li>LABEL_0: 1 estrella</li>
      <li>LABEL_1: 2 estrellas</li>
      <li>LABEL_2: 3 estrellas</li>
      <li>LABEL_3: 4 estrellas</li>
      <li>LABEL_4: 5 estrellas</li>
      </ul>
      <p>Por lo que ha calificado el comentario con 3 estrellas. Recordemos que hemos est√° entrenado en un subconjunto de datos y con solo 5 √©pocas, por lo que no esperamos que sea muy bueno</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Compartir-el-modelo-en-el-Hub-de-Hugging-Face">Compartir el modelo en el Hub de Hugging Face<a class="anchor-link" href="#Compartir-el-modelo-en-el-Hub-de-Hugging-Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 52" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez tenemos el modelo reentrenado podemos subirlo a nuestro espacio en el Hub de Hugging Face para que otros lo puedan usar. Para ello es necesario tener una cuenta en Hugging Face</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Logging">Logging<a class="anchor-link" href="#Logging"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 53" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder subir el modelo primero nos tenemos que loguear.</p>
      <p>Se puede hacer a trav√©s de la terminal con</p>
      <div class="highlight"><pre><span></span>huggingface-cli<span class="w"> </span>login
      </pre></div>
      <p>O a trav√©s del notebook habiendo instalado antes la librer√≠a <code>huggingface_hub</code> con</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>huggingface_hub
      </pre></div>
      <p>Ahora podemos loguearnos con la funci√≥n <code>notebook_login</code>, que crear√° una peque√±a interfaz gr√°fica en la que tenemos que introducir un token de Hugging Face</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para crear un token hay que ir a la p√°gina de <a href="https://huggingface.co/settings/tokens" target="_blank" rel="nofollow noreferrer">setings/tokens</a> de nuestra cuenta, nos aparecer√° algo as√≠</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="User-Access-Token-dark" src="http://maximofn.com/wp-content/uploads/2024/03/User-Access-Token-dark.png" width="1200" height="615"/></p>
      <p>Le damos a <code>New token</code> y nos aparecer√° una ventana para crear un nuevo token</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="new-token-dark" src="http://maximofn.com/wp-content/uploads/2024/03/new-token-dark.png" width="1200" height="660"/></p>
      <p>Le damos un nombre al token y lo creamos con el rol <code>write</code>.</p>
      <p>Una vez creado lo copiamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
          '',
          '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'VBox(children=(HTML(value=\'&lt;center&gt; &lt;img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Subida-una-vez-entenado">Subida una vez entenado<a class="anchor-link" href="#Subida-una-vez-entenado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 54" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como hemos entrenado al modelo podemos subirlo al Hub mediante la funci√≥n <code>push_to_hub</code>. Esta funci√≥n tiene un par√°metro obligatorio que es el nombre del modelo, que tiene que ser √∫nico, si ya existe un modelo en tu Hub con ese nombre no se podr√° subir. Es decir, el nombre completo del modelo ser√° <usuario>/<model>, por eso el nombre del modelo no puede existir en tu Hub, aunque s√≠ exista otro modelo con el mosmo nombre en el Hub de otro usuario.</model></usuario></p>
      <p>Adem√°s tiene otros par√°metros opcionales, pero que son interesantes:</p>
      <ul>
      <li><code>use_temp_dir</code> (bool, optional): Si usar o no un directorio temporal para almacenar los ficheros guardados antes de ser enviados al Hub. Por defecto ser√° True si no existe un directorio con el mismo nombre que <code>repo_id</code>, False en caso contrario.</li>
      <li><code>commit_message</code> (str, optional): Mensaje de commit. Por defecto ser√° <code>Upload {object}</code>.</li>
      <li><code>private</code> (bool, optional): Si el repositorio creado debe ser privado o no.</li>
      <li><code>token</code> (bool or str, optional): El token a usar como autorizaci√≥n HTTP para archivos remotos. Si es True, se usar√° el token generado al ejecutar <code>huggingface-cli</code> login (almacenado en ~/.huggingface). Por defecto ser√° True si no se especifica <code>repo_url</code>.</li>
      <li><code>max_shard_size</code> (int or str, optional, defaults to "5GB"): S√≥lo aplicable a modelos. El tama√±o m√°ximo de un punto de control antes de ser fragmentado. Los puntos de control fragmentados ser√°n cada uno de un tama√±o inferior a este tama√±o. Si se expresa como una cadena, debe tener d√≠gitos seguidos de una unidad (como "5MB"). Por defecto es "5GB" para que los usuarios puedan cargar f√°cilmente modelos en instancias de Google Colab de nivel libre sin problemas de OOM (out of memory) de CPU.</li>
      <li><code>create_pr</code> (bool, optional, defaults to False): Si crear o no un PR con los archivos subidos o confirmar directamente.</li>
      <li><code>safe_serialization</code> (bool, optional, defaults to True): Si convertir o no los pesos del modelo en formato safetensors para una serializaci√≥n m√°s segura.</li>
      <li><code>revision</code> (str, optional): Rama a la que enviar los archivos cargados.</li>
      <li><code>commit_description</code> (str, optional): Descripci√≥n del commit que se crear√°</li>
      <li><code>tags</code> (List[str], optional): Lista de tags para insertar en Hub.</li>
      </ul>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span>
          <span class="s2">"bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset"</span><span class="p">,</span> 
          <span class="n">commit_message</span><span class="o">=</span><span class="s2">"bert base cased fine tune on yelp review subset"</span><span class="p">,</span>
          <span class="n">commit_description</span><span class="o">=</span><span class="s2">"Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs."</span><span class="p">,</span>
      <span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>README.md:   0%|          | 0.00/5.18k [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>model.safetensors:   0%|          | 0.00/433M [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[26]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset/commit/033a3c759d5a4e314ce76db81bd113b4f7da69ad', commit_message='bert base cased fine tune on yelp review subset', commit_description='Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs.', oid='033a3c759d5a4e314ce76db81bd113b4f7da69ad', pr_url=None, pr_revision=None, pr_num=None)</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora vamos a nuestro Hub podemos ver que se ha subido el modelo</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformers_commit_unico" src="http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_unico.webp" width="1163" height="151"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora entramos a la model card a ver</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformers_commit_inico_model_card" src="http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_inico_model_card-scaled.webp" width="1200" height="617"/></p>
      <p>Vemos que todo est√° sin rellenar, m√°s adelante haremos esto</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Subida-mientras-se-entrena">Subida mientras se entrena<a class="anchor-link" href="#Subida-mientras-se-entrena"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 55" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Otra opci√≥n es subirlo mientras estamos entrenando el modelo. Esto es muy √∫til cuando entrenamos modelos durante muchas √©pocas y nos lleva mucho tiempo, ya que si se para el entrenamiento (porque se apaga el ordenador, se termina la sesi√≥n de colab, se acaban los cr√©ditos de la nube) no se pierde el trabajo. Para hacer esto hay que a√±adir <code>push_to_hub=True</code> en el <code>TrainingArguments</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span>',
      '          <span class="s2">"bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset"</span><span class="p">,</span> ',
      '          <span class="n">commit_message</span><span class="o">=</span><span class="s2">"bert base cased fine tune on yelp review subset"</span><span class="p">,</span>',
      '          <span class="n">commit_description</span><span class="o">=</span><span class="s2">"Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs."</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">output_dir</span><span class="o">=</span><span class="s2">"bert-base-cased_notebook_transformers_30-epochs_yelp_review_subset"</span><span class="p">,</span> ',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> ',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '          <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>





















      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que hemos cambiado las √©pocas a 30, por lo que el entrenamiento va a llevar m√°s tiempo, as√≠ que al a√±adir <code>push_to_hub=True</code> se subir√° el modelo a nuestro Hub mientras se entrena.</p>
      <p>Adem√°s hemos cambiado el <code>output_dir</code> porque es el nombre que tendr√° el modelo en el Hub</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>  0%|          | 0/1890 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>{opening_brace}'loss': 0.2363, 'grad_norm': 8.151028633117676, 'learning_rate': 7.354497354497355e-05, 'epoch': 7.94{closing_brace}
      {opening_brace}'loss': 0.0299, 'grad_norm': 0.0018280998338013887, 'learning_rate': 4.708994708994709e-05, 'epoch': 15.87}
      {opening_brace}'loss': 0.0019, 'grad_norm': 0.000868947128765285, 'learning_rate': 2.0634920634920636e-05, 'epoch': 23.81}
      {opening_brace}'train_runtime': 331.5804, 'train_samples_per_second': 90.476, 'train_steps_per_second': 5.7, 'train_loss': 0.07100234655318437, 'epoch': 30.0}
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[24]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>TrainOutput(global_step=1890, training_loss=0.07100234655318437, metrics={opening_brace}'train_runtime': 331.5804, 'train_samples_per_second': 90.476, 'train_steps_per_second': 5.7, 'train_loss': 0.07100234655318437, 'epoch': 30.0{closing_brace})</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si volvemos a mirar nuestro hub, ahora aparece el nuevo modelo</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformers_commit_training" src="http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_training.webp" width="1172" height="155"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Hub-como-repositorio-git">Hub como repositorio git<a class="anchor-link" href="#Hub-como-repositorio-git"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 56" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En Hugging Face tanto los modelos, como los espacios, como los datasets son repositorios de git, por lo que se puede trabajar con ellos como eso. Es decir, puedes clonar, hacer forks, pull requests, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero otra gran ventaja de esto es que puedes usar un modelo en una versi√≥n determinada</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
      
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s2">"393e083"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>config.json:   0%|          | 0.00/433 [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>pytorch_model.bin:   0%|          | 0.00/436M [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
      You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
      </pre>
      </div>
      </div>
      </div>
      </section>
      






    </div>

  </section>

</PostLayout>
