---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'LangGraph: Revoluci贸n en tus agentes de IA';
const end_url = 'uv';
const description = ' 隆Revoluciona tus agentes de IA!  LangGraph no es solo otra librer铆a, es el framework de orquestaci贸n que te da el CONTROL total para construir agentes complejos, con memoria a largo plazo y 隆hasta con intervenci贸n humana! Olv铆date de los chatbots b谩sicos, es hora de crear verdadera inteligencia. 隆Sum茅rgete en este post y desc煤brelo!';
const keywords = 'langgraph, ia, agentes, orquestaci贸n, framework, memoria, humano en el bucle';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/langgraph_thumbnail_es.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2025-05-19+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#%C2%BFC%C3%B3mo-funciona-LangGraph?"><h2>驴C贸mo funciona <code>LangGraph</code>?</h2></a>
      <a class="anchor-link" href="#Instalaci%C3%B3n-de-LangGraph"><h2>Instalaci贸n de LangGraph</h2></a>
      <a class="anchor-link" href="#Instalaci%C3%B3n-de-m%C3%B3dulo-de-Hugging-Face-y-Anthropic"><h2>Instalaci贸n de m贸dulo de Hugging Face y Anthropic</h2></a>
      <a class="anchor-link" href="#API-KEY-de-Hugging-Face"><h2>API KEY de Hugging Face</h2></a>
      <a class="anchor-link" href="#API-KEY-de-Anthropic"><h2>API KEY de Anthropic</h2></a>
      <a class="anchor-link" href="#Crear-un-chatbot-b%C3%A1sico"><h2>Crear un chatbot b谩sico</h2></a>
      <a class="anchor-link" href="#M%C3%A1s"><h3>M谩s</h3></a>
      <a class="anchor-link" href="#Tipado-del-estado"><h4>Tipado del estado</h4></a>
      <a class="anchor-link" href="#Tipado-mediante-TypeDict"><h5>Tipado mediante <code>TypeDict</code></h5></a>
      <a class="anchor-link" href="#Tipado-mediante-dataclass"><h5>Tipado mediante <code>dataclass</code></h5></a>
      <a class="anchor-link" href="#Tipado-con-Pydantic"><h5>Tipado con <code>Pydantic</code></h5></a>
      <a class="anchor-link" href="#Reducers"><h4>Reducers</h4></a>
      <a class="anchor-link" href="#Reducers-predefinidos"><h5>Reducers predefinidos</h5></a>
      <a class="anchor-link" href="#Reducers-personalizados"><h5>Reducers personalizados</h5></a>
      <a class="anchor-link" href="#M%C3%BAltiples-estados"><h4>M煤ltiples estados</h4></a>
      <a class="anchor-link" href="#Estados-privados"><h5>Estados privados</h5></a>
      <a class="anchor-link" href="#Estados-de-entrada-y-salida"><h5>Estados de entrada y salida</h5></a>
      <a class="anchor-link" href="#Manejo-del-contexto"><h4>Manejo del contexto</h4></a>
      <a class="anchor-link" href="#Modificar-el-contexto-con-funciones-de-tipo-Reducer"><h5>Modificar el contexto con funciones de tipo <code>Reducer</code></h5></a>
      <a class="anchor-link" href="#Recortar-mensajes"><h5>Recortar mensajes</h5></a>
      <a class="anchor-link" href="#Modificaci%C3%B3n-del-contexto-y-recorte-de-mensajes"><h5>Modificaci贸n del contexto y recorte de mensajes</h5></a>
      <a class="anchor-link" href="#Modos-de-streaming"><h4>Modos de streaming</h4></a>
      <a class="anchor-link" href="#Streaming-s%C3%ADncrono"><h5>Streaming s铆ncrono</h5></a>
      <a class="anchor-link" href="#Updates"><h6>Updates</h6></a>
      <a class="anchor-link" href="#Values"><h6>Values</h6></a>
      <a class="anchor-link" href="#Streaming-as%C3%ADncrono"><h5>Streaming as铆ncrono</h5></a>
      <a class="anchor-link" href="#Sub-grafos"><h4>Sub grafos</h4></a>
      <a class="anchor-link" href="#Subgrafo-de-an%C3%A1lisis-de-errores-en-logs"><h5>Subgrafo de an谩lisis de errores en logs</h5></a>
      <a class="anchor-link" href="#Subgrafo-de-resumen-de-logs"><h5>Subgrafo de resumen de logs</h5></a>
      <a class="anchor-link" href="#Grafo-principal"><h5>Grafo principal</h5></a>
      <a class="anchor-link" href="#Ramas-din%C3%A1micas"><h4>Ramas din谩micas</h4></a>
      <a class="anchor-link" href="#Mejorar-el-chatbot-con-herramientas"><h2>Mejorar el chatbot con herramientas</h2></a>
      <a class="anchor-link" href="#Requisitos"><h3>Requisitos</h3></a>
      <a class="anchor-link" href="#Chatbot-con-tools"><h3>Chatbot con tools</h3></a>
      <a class="anchor-link" href="#Agregar-memoria-al-chatbot---memoria-a-corto-plazo,-memoria-dentro-del-hilo"><h2>Agregar memoria al chatbot - memoria a corto plazo, memoria dentro del hilo</h2></a>
      <a class="anchor-link" href="#M%C3%A1s"><h3>M谩s</h3></a>
      <a class="anchor-link" href="#Chatbot-con-mensaje-de-resumen"><h4>Chatbot con mensaje de resumen</h4></a>
      <a class="anchor-link" href="#Guardar-estado-en-SQlite"><h4>Guardar estado en SQlite</h4></a>
      <a class="anchor-link" href="#Memoria-a-largo-plazo,-memoria-entre-hilos"><h2>Memoria a largo plazo, memoria entre hilos</h2></a>
      <a class="anchor-link" href="#Introducci%C3%B3n-a-LangGraph-Memory-Store"><h3>Introducci贸n a LangGraph Memory Store</h3></a>
      <a class="anchor-link" href="#Chatbot-con-memoria-a-largo-plazo"><h3>Chatbot con memoria a largo plazo</h3></a>
      <a class="anchor-link" href="#Chatbot-con-perfil-de-usuario"><h3>Chatbot con perfil de usuario</h3></a>
      <a class="anchor-link" href="#M%C3%A1s"><h3>M谩s</h3></a>
      <a class="anchor-link" href="#Actualizar-esquemas-estructurados-con-Trustcall"><h4>Actualizar esquemas estructurados con Trustcall</h4></a>
      <a class="anchor-link" href="#Chatbot-con-perfil-de-usuario-actualizado-con-Trustcall"><h4>Chatbot con perfil de usuario actualizado con Trustcall</h4></a>
      <a class="anchor-link" href="#Chatbot-con-colecciones-de-documentos-de-usuario-actualizadas-con-Trustcall"><h4>Chatbot con colecciones de documentos de usuario actualizadas con Trustcall</h4></a>
      <a class="anchor-link" href="#Human-in-the-loop"><h2>Human in the loop</h2></a>
      <a class="anchor-link" href="#M%C3%A1s"><h3>M谩s</h3></a>
      <a class="anchor-link" href="#Aprobaci%C3%B3n-del-uso-de-herramientas"><h4>Aprobaci贸n del uso de herramientas</h4></a>
      <a class="anchor-link" href="#Modificaci%C3%B3n-del-estado"><h4>Modificaci贸n del estado</h4></a>
      <a class="anchor-link" href="#Breakpoints-din%C3%A1micos"><h4>Breakpoints din谩micos</h4></a>
      <a class="anchor-link" href="#Personalizaci%C3%B3n-del-estado"><h2>Personalizaci贸n del estado</h2></a>
      <a class="anchor-link" href="#Actualizaci%C3%B3n-manual-del-estado"><h3>Actualizaci贸n manual del estado</h3></a>
      <a class="anchor-link" href="#Checkpoints"><h2>Checkpoints</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="LangGraph">LangGraph<a class="anchor-link" href="#LangGraph"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>LangGraph</code> es un marco de orquestaci贸n de bajo nivel para construir agentes controlables</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mientras que <code>LangChain</code> proporciona integraciones y componentes para agilizar el desarrollo de aplicaciones LLM, la biblioteca <code>LangGraph</code> permite la orquestaci贸n de agentes, ofreciendo arquitecturas personalizables, memoria a largo plazo y <code>human in the loop</code> para manejar de manera confiable tareas complejas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>En este post vamos a deshabilitar <code>LangSmith</code>, que es una herramienta de depuraci贸n de grafos. Lo vamos a deshabilitar para no a帽adir m谩s complejidad al post y centrarnos 煤nicamente en <code>LangGraph</code></p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="%C2%BFC%C3%B3mo-funciona-LangGraph?">驴C贸mo funciona <code>LangGraph</code>?<a class="anchor-link" href="#%C2%BFC%C3%B3mo-funciona-LangGraph?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>LangGraph</code> se basa en tres componentes:</p>
      <ul>
      <li><strong>Nodos</strong>: Representan las unidades de procesamiento de la aplicaci贸n, como llamar a un LLM, o a una herramienta. Son funciones de Python que se ejecutan cuando se llama al nodo.<ul>
      <li>Tomar el estado como entrada</li>
      <li>Realizan alguna operaci贸n</li>
      <li>Devuelven el estado actualizado</li>
      </ul>
      </li>
      <li><strong>Edges</strong>: Representan las transiciones entre los nodos. Definen la l贸gica de c贸mo se va a ejecutar el grafo, es decir, qu茅 nodo se va a ejecutar despu茅s de otro. Pueden ser:<ul>
      <li>Directos: Van de un nodo a otro</li>
      <li>Condicionales: Dependen de una condici贸n</li>
      </ul>
      </li>
      <li><strong>State</strong>: Representa el estado de la aplicaci贸n, es decir, contiene toda la informaci贸n necesaria para la aplicaci贸n. Se mantiene durante la ejecuci贸n de la aplicaci贸n. Es definido por el usuario, as铆 que hay que pensar muy bien qu茅 se va a guardar en 茅l.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="LangGraph concept" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LangGraph_concept.webp" width="1505" height="672"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Todos los grafos de <code>LangGraph</code> comienzan desde un nodo <code>START</code> y terminan en un nodo <code>END</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Instalaci%C3%B3n-de-LangGraph">Instalaci贸n de LangGraph<a class="anchor-link" href="#Instalaci%C3%B3n-de-LangGraph"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para instalar <code>LangGraph</code> se puede usar pip:</p>
      <div class="highlight">
        <pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>langgraph</pre>
      </div>
      <p>o instalar desde Conda:</p>
      <div class="highlight">
        <pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>langgraph</pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Instalaci%C3%B3n-de-m%C3%B3dulo-de-Hugging-Face-y-Anthropic">Instalaci贸n de m贸dulo de Hugging Face y Anthropic<a class="anchor-link" href="#Instalaci%C3%B3n-de-m%C3%B3dulo-de-Hugging-Face-y-Anthropic"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a usar un modelo de lenguaje de <code>Hugging Face</code>, por lo que necesitamos instalar su paquete de langgraph.</p>
      <div class="highlight">
        <pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>langchain-huggingface</pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para una parte vamos a usar <code>Sonnet 3.7</code>, luego explicaremos por qu茅. As铆 que tambien instalamos el paquere de <code>Anthropic</code>.</p>
      <div class="highlight">
        <pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>langchain_anthropic</pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="API-KEY-de-Hugging-Face">API KEY de Hugging Face<a class="anchor-link" href="#API-KEY-de-Hugging-Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a usar <code>Qwen/Qwen2.5-72B-Instruct</code> a trav茅s de <code>Hugging Face Inference Endpoints</code>, por lo que necesitamos una API KEY.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder usar el <code>Inference Endpoints</code> de HuggingFace, lo primero que necesitas es tener una cuenta en HuggingFace. Una vez la tengas, hay que ir a <a href="https://huggingface.co/settings/keys" target="_blank" rel="nofollow noreferrer">Access tokens</a> en la configuraci贸n de tu perfil y generar un nuevo token.</p>
      <p>Hay que ponerle un nombre. En mi caso, le voy a poner <code>langgraph</code> y habilitar el permiso <code>Make calls to inference providers</code>. Nos crear谩 un token que tendremos que copiar</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para gestionar el token, vamos a crear un archivo en la misma ruta en la que estemos trabajando llamado <code>.env</code> y vamos a poner el token que hemos copiado en el archivo de la siguiente manera:</p>
      <div class="highlight">
        <pre><span></span><span class="nv">HUGGINGFACE_LANGGRAPH</span><span class="o">=</span><span class="s2">"hf_...."</span></pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora, para poder obtener el token, necesitamos tener instalado <code>dotenv</code>, que lo instalamos mediante</p>
      <div class="highlight">
        <pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>python-dotenv</pre>
      </div>
      <p>Ejecutamos lo siguiente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      ' ',
      '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos un token, creamos un cliente. Para ello, necesitamos tener instalada la librer铆a <code>huggingface_hub</code>. La instalamos mediante conda o pip.</p>
      <div class="highlight">
        <pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>huggingface_hub</pre>
      </div>
      <p>o</p>
      <div class="highlight">
        <pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>huggingface_hub</pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora tenemos que elegir qu茅 modelo vamos a usar. Puedes ver los modelos disponibles en la p谩gina de <a href="https://huggingface.co/docs/api-inference/supported-models" target="_blank" rel="nofollow noreferrer">Supported models</a> de la documentaci贸n de <code>Inference Endpoints</code> de Hugging Face.</p>
      <p>Vamos a usar <code>Qwen2.5-72B-Instruct</code> que es un modelo muy bueno.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Ahora podemos crear el cliente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>',
          '',
          '<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">MODEL</span><span class="p">)</span>',
          '<span class="n">client</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;InferenceClient(model=\'Qwen/Qwen2.5-72B-Instruct\', timeout=None)&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hacemos una prueba a ver si funciona</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span>',
          '	<span class="p">{opening_brace}</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, qu茅 tal?"</span> <span class="p">{closing_brace}</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>',
          '	<span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> ',
          '	<span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>',
          '	<span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>',
          '	<span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '	<span class="n">stream</span><span class="o">=</span><span class="kc">False</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '隆Hola! Estoy bien, gracias por preguntar. 驴C贸mo est谩s t煤? 驴En qu茅 puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="API-KEY-de-Anthropic">API KEY de Anthropic<a class="anchor-link" href="#API-KEY-de-Anthropic"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Crear-un-chatbot-b%C3%A1sico">Crear un chatbot b谩sico<a class="anchor-link" href="#Crear-un-chatbot-b%C3%A1sico"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a crear un chatbot simple usando <code>LangGraph</code>. Este chatbot responder谩 directamente a los mensajes del usuario. Aunque es simple, nos servir谩 para ver los conceptos b谩sicos de la construcci贸n de grafos con <code>LangGraph</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como su nombre indica, <code>LangGraph</code> es una biblioteca para manejar grafos. As铆 que comenzamos creando un grafo <a href="https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph" target="_blank" rel="nofollow noreferrer">StateGraph</a>.</p>
      <p>Un <code>StateGraph</code> define la estructura de nuestro chatbot como una <code>m谩quina de estados</code>. Agregaremos <code>nodos</code> a nuestro grafo para representar los <code>llm</code>s, <code>tool</code>s y <code>funciones</code>, los <code>llm</code>s podr谩n hacer uso de esas <code>tool</code>s y <code>funciones</code>; y a帽adimos <code>edges</code> para especificar c贸mo el bot debe hacer la transici贸n entre esos <code>nodos</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As铆 que comenzamos creando un <code>StateGraph</code> que necesita una clase <code>State</code> para manejar el estado del grafo. Como ahora vamos a crear un chatbot sencillo, solo necesitamos manejar una lista de mensajes en el estado.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span>',
      '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
      ' ',
      ' ',
      '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="c1"># Messages have the type "list". The `add_messages` function</span>',
      '    <span class="c1"># in the annotation defines how this state key should be updated</span>',
      '    <span class="c1"># (in this case, it appends messages to the list, rather than overwriting them)</span>',
      '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
      ' ',
      ' ',
      '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>





















      
      <section class="section-block-markdown-cell">
      <p>La funci贸n <a href="https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages" target="_blank" rel="nofollow noreferrer">add_messages</a> une dos listas de mensajes.</p>
      <p>Llegar谩n nuevas listas de mensajes, por lo que se unir谩n a la lista de mensajes ya existente. Cada lista de mensajes contiene un <code>ID</code>, por lo que se agregan con este <code>ID</code>. Esto asegura que los mensajes solo se a帽aden, no se reemplazan, a no ser que un nuevo mensaje tenga el mismo <code>ID</code> que uno ya existente, que entonces se remplaza.</p>
      <p><code>add_messages</code> es una <a href="https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers" target="_blank" rel="nofollow noreferrer">reducer function</a>, es una funci贸n que se encarga de actualizar el estado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El grafo <code>graph_builder</code> que hemos creado, recibe un estado <code>State</code> y devuelve un nuevo estado <code>State</code>. Adem谩s, actualiza la lista de mensajes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p><strong>Concepto</strong></p>
      <p>Al definir un grafo, el primer paso es definir su <code>State</code>. El <code>State</code> incluye el esquema del grafo y las <code>reducer functions</code> que manejan actualizaciones del estado.</p>
      <p>En nuestro ejemplo, <code>State</code> es de tipo <code>TypedDict</code> (diccionario tipado) con una llave: <code>messages</code>.</p>
      <p><code>add_messages</code> es una <code>reducer function</code> que se utiliza para agregar nuevos mensajes a la lista en lugar de sobrescribirlos en la lista. Si una llave de un estado no tiene una <code>reducer function</code>, cada valor que llegue de esa clave sobrescribir谩n los valores anteriores.</p>
      <p><code>add_messages</code> es una <code>reducer function</code> de langgraph, pero nosotros vamos a poder crear las nuestras</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vamos a agregar al grafo el nodo <code>chatbot</code>. Los nodos representan unidades de trabajo. Por lo general, son funciones regulares de <code>Python</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos un nodo con el m茅todo <code>add_node</code> que recibe el nombre del nodo y la funci贸n que se ejecutar谩.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>De modo que vamos a crear un LLM con HuggingFace, despu茅s crearemos un chat model con <code>LangChain</code> que har谩 referencia al LLM creado. Una vez tenemos definido un chat model, definimos la funci贸n que se ejecutar谩 en el nodo de nuestro grafo. Esa funci贸n har谩 una llamada al chat model creado y devolver谩 el resultado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por 煤ltimo vamos a a帽adir un nodo con la funci贸n del chatbot al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
          '<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>',
          '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
          '',
          '<span class="c1"># Create the LLM model</span>',
          '<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>',
          '<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
          '    <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>',
          '    <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>',
          '    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
          '    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="c1"># Create the chat model</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>',
          '',
          '<span class="c1"># Define the chatbot function</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]}</span>',
          '',
          '',
          '<span class="c1"># The first argument is the unique node name</span>',
          '<span class="c1"># The second argument is the function or object that will be called whenever</span>',
          '<span class="c1"># the node is used.</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x130548440&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hemos usado <a href="https://python.langchain.com/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html#langchain_huggingface.chat_models.huggingface.ChatHuggingFace" target="_blank" rel="nofollow noreferrer">ChatHuggingFace</a> que es un chat del tipo <a href="https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel">BaseChatModel</a> que es un tipo de chat base de <code>LangChain</code>. Una vez hemos creado el <code>BaseChatModel</code>, hemos creado la funci贸n <code>chatbot_function</code> que se ejecutar谩 cuando se ejecute el nodo. Y por 煤ltimo, hemos creado el nodo <code>chatbot_node</code> y le hemos indicado que tiene que ejecutar la funci贸n <code>chatbot_function</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p><strong>Aviso</strong></p>
      <p>La funci贸n de nodo <code>chatbot_function</code> toma el estado <code>State</code> como entrada y devuelve un diccionario que contiene una actualizaci贸n de la lista <code>messages</code> para la key <code>mensajes</code>. Este es el patr贸n b谩sico para todas las funciones del nodo <code>LangGraph</code>.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La <code>reducer function</code> de nuestro grafo <code>add_messages</code> agregar谩 los mensajes de respuesta del <code>llm</code> a cualquier mensaje que ya est茅 en el estado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A continuaci贸n, agregamos un nodo <code>entry</code>. Esto le dice a nuestro grafo d贸nde empezar su trabajo cada vez que lo ejecutamos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">START</span>',
          '',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x130548440&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Del mismo modo, a帽adimos un nodo <code>finish</code>. Esto indica al grafo cada vez que se ejecuta este nodo, puede terminar el trabajo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">END</span>',
          '',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x130548440&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hemos importado <code>START</code> y <code>END</code> que podemos encontrarlos en <a href="https://langchain-ai.github.io/langgraph/reference/constants/" target="_blank" rel="nofollow noreferrer">constants</a> y son el primer y el 煤ltimo nodo de nuestro grafo.</p>
      <p>Normalmente son nodos virtuales</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Finalmente, tenemos que compilar nuestro grafo. Para hacerlo, usamos el m茅todo constructor de grafos <code>compile()</code>. Esto crea un <code>CompiledGraph</code> que podemos usar para ejecutar nuestra aplicaci贸n.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Podemos visualizar el grafo usando el m茅todo <code>get_graph</code> y uno de los m茅todos de "dibujo", como <code>draw_ascii</code> o <code>draw_mermaid_png</code>. El dibujo de cada uno de los m茅todos requiere dependencias adicionales.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      
      <span class="k">try</span><span class="p">:</span>
          <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{opening_brace}</span><span class="n">e</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv0.webp" width="143" height="234" alt="image uv 1" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>隆Ahora podemos probar el chatbot!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p><strong>Consejo</strong></p>
      <p>En el siguiente bloque de c贸digo, puedes salir del bucle de chat en cualquier momento escribiendo <code>quit</code>, <code>exit</code> o <code>q</code>.</p>
      </blockquote>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Colors for the terminal</span>',
          '<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;32m"</span>',
          '<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;33m"</span>',
          '<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;0m"</span>',
          '',
          '',
          '<span class="k">def</span> <span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>',
          '    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace}):</span>',
          '        <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">user_input</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">value</span><span class="p">[</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
          '',
          '',
          '<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>',
          '    <span class="k">try</span><span class="p">:</span>',
          '        <span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">"User: "</span><span class="p">)</span>',
          '        <span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"quit"</span><span class="p">,</span> <span class="s2">"exit"</span><span class="p">,</span> <span class="s2">"q"</span><span class="p">]:</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">user_input</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">Goodbye!"</span><span class="p">)</span>',
          '            <span class="k">break</span>',
          '        ',
          '        <span class="n">events</span> <span class="o">=</span><span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>',
          '    <span class="k">except</span><span class="p">:</span>',
          '        <span class="c1"># fallback if input() is not available</span>',
          '        <span class="n">user_input</span> <span class="o">=</span> <span class="s2">"What do you know about LangGraph?"</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="s2">"User: "</span> <span class="o">+</span> <span class="n">user_input</span><span class="p">)</span>',
          '        <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>',
          '        <span class="k">break</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '<span class="ansi-green-fg">User: </span>Hello',
          '<span class="ansi-yellow-fg">Assistant: </span>Hello! It\'s nice to meet you. How can I assist you today? Whether you have questions, need information, or just want to chat, I\'m here to help!',
          '<span class="ansi-green-fg">User: </span>How are you doing?',
          '<span class="ansi-yellow-fg">Assistant: </span>I\'m just a computer program, so I don\'t have feelings, but I\'m here and ready to help you with any questions or tasks you have! How can I assist you today?',
          '<span class="ansi-green-fg">User: </span>Me well, I\'m making a post about LangGraph, what do you think?',
          '<span class="ansi-yellow-fg">Assistant: </span>LangGraph is an intriguing topic, especially if you\'re delving into the realm of graph-based models and their applications in natural language processing (NLP). LangGraph, as I understand, is a framework or tool that leverages graph theory to improve or provide a new perspective on NLP tasks such as text classification, information extraction, and semantic analysis. By representing textual information as graphs (nodes for entities and edges for relationships), it can offer a more nuanced understanding of the context and semantics in language data.',
          'If you\'re making a post about it, here are a few points you might consider:',
          '1. **Introduction to LangGraph**: Start with a brief explanation of what LangGraph is and its core principles. How does it model language or text differently compared to traditional NLP approaches? What unique advantages does it offer by using graph-based methods?',
          '2. **Applications of LangGraph**: Discuss some of the key applications where LangGraph has been or can be applied. This could include improving the accuracy of sentiment analysis, enhancing machine translation, or optimizing chatbot responses to be more contextually aware.',
          '3. **Technical Innovations**: Highlight any technical innovations or advancements that LangGraph brings to the table. This could be about new algorithms, more efficient data structures, or novel ways of training models on graph data.',
          '4. **Challenges and Limitations**: It\'s also important to address the challenges and limitations of using graph-based methods in NLP. Performance, scalability, and the current state of the technology can be discussed here.',
          '5. **Future Prospects**: Wrap up with a look into the future of LangGraph and graph-based NLP in general. What are the upcoming trends, potential areas of growth, and how might these tools start impacting broader technology landscapes?',
          'Each section can help frame your post in a way that\'s informative and engaging for your audience, whether they\'re technical experts or casual readers looking for an introduction to this intriguing area of NLP.',
          '<span class="ansi-green-fg">User: </span>q',
          '<span class="ansi-yellow-fg">Assistant: </span>Goodbye!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p><strong>!Felicidades!</strong> Has construido tu primer chatbot usando <code>LangGraph</code>. Este bot puede participar en una conversaci贸n b谩sica tomando la entrada del usuario y generando respuestas utilizando el <code>LLM</code> que hemos definido.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes hemos ido escribiendo el c贸digo poco a poco y puede que no haya quedado muy claro. Se ha hecho as铆 para explicar cada parte del c贸digo, pero ahora vamos a volver a escribirlo, pero ordenado de otra manera, que queda m谩s claro a la vista. Es decir, ahora que no hay que explicar cada parte del c贸digo, lo agrupamos de otra manera para que sea m谩s claro</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="kn">import</span> <span class="nn">dotenv</span>
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="c1"># State</span>
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Function</span>
      <span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]{closing_brace}</span>
      
      <span class="c1"># Start to build the graph</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes to the graph</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
      
      <span class="c1"># Add edges</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># Display the graph</span>
      <span class="k">try</span><span class="p">:</span>
          <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{opening_brace}</span><span class="n">e</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv1.webp" width="143" height="234" alt="image uv 2" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="M%C3%A1s">M谩s<a class="anchor-link" href="#M%C3%A1s"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Todos los bloques <code>m谩s</code> est谩n por si quieres profundizar m谩s en <code>LangGraph</code>, si no, puedes leer todo sin leer los bloques <code>m谩s</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Tipado-del-estado">Tipado del estado<a class="anchor-link" href="#Tipado-del-estado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos visto c贸mo crear un agente con un estado tipado mediante <code>TypedDict</code>, pero podemos crearlo con otro tipo tipado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Tipado-mediante-TypeDict">Tipado mediante <code>TypeDict</code><a class="anchor-link" href="#Tipado-mediante-TypeDict"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Es la forma que hemos visto antes, tipamos el estado como un diccionario usando el tipado de Python <code>TypeDict</code>. Le pasamos una llave y un valor para cada variable de nuestro estado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <div class="highlight">
        <pre><span></span><span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span></pre>
        <pre><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span></pre>
        <pre><span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span></pre>
        <pre><span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span></pre>
        <pre><span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span></pre>
        <pre>    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span></pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para acceder a los mensajes lo hacemos como con cualquier diccionario, mediante <code>state["messages"]</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Tipado-mediante-dataclass">Tipado mediante <code>dataclass</code><a class="anchor-link" href="#Tipado-mediante-dataclass"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Otra opci贸n es usar el tipado de python <code>dataclass</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <div class="highlight">
        <pre><span></span><span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span></pre>
        <pre><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span></pre>
        <pre><span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span></pre>
        <pre><span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span></pre>
        <pre><span class="nd">@dataclass</span></pre>
        <pre><span class="k">class</span> <span class="nc">State</span><span class="p">:</span></pre>
        <pre>    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span></pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver es similar al tipado mediante diccionarios, pero ahora, al ser el estado una clase, accedemos a los mensajes mediante <code>state.messages</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Tipado-con-Pydantic">Tipado con <code>Pydantic</code><a class="anchor-link" href="#Tipado-con-Pydantic"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Pydantic</code> es una librer铆a muy usada para tipar datos en Python. Nos ofrece la posibilidad de a帽adir una comprobaci贸n del tipado. Vamos a comprobar que el mensaje empiece con <code>'User'</code>, <code>'Assistant'</code> o <code>'System'</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <div class="highlight">
        <pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">field_validator</span><span class="p">,</span> <span class="n">ValidationError</span></pre>
        <pre><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span></pre>
        <pre><span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span></pre>
        <pre><span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span></pre>
        <pre>    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span> <span class="c1"># Should start by 'User', 'Assistant' or 'System'</span></pre>
        <pre>    <span class="nd">@field_validator</span><span class="p">(</span><span class="s1">'messages'</span><span class="p">)</span></pre>
        <pre>    <span class="nd">@classmethod</span></pre>
        <pre>    <span class="k">def</span> <span class="nf">validate_messages</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span></pre>
        <pre>        <span class="c1"># Ensure the messages start with `User`, `Assistant` or `System`</span></pre>
        <pre>        <span class="k">if</span> <span class="ow">not</span> <span class="n">value</span><span class="o">.</span><span class="n">startswith</span><span class="p">[</span><span class="s2">"'User'"</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">value</span><span class="o">.</span><span class="n">startswith</span><span class="p">[</span><span class="s2">"'Assistant'"</span><span class="p">]</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">value</span><span class="o">.</span><span class="n">startswith</span><span class="p">[</span><span class="s2">"'System'"</span><span class="p">]:</span></pre>
        <pre>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Message must to start with 'User', 'Assistant' or 'System'"</span><span class="p">)</span></pre>
        <pre>        <span class="k">return</span> <span class="n">value</span></pre>
        <pre><span class="k">try</span><span class="p">:</span></pre>
        <pre>    <span class="n">state</span> <span class="o">=</span> <span class="n">PydanticState</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[</span><span class="s2">"Hello"</span><span class="p">])</span></pre>
        <pre><span class="k">except</span> <span class="n">ValidationError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span></pre>
        <pre>    <span class="nb">print</span><span class="p">(</span><span class="s2">"Validation Error:"</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span></pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Reducers">Reducers<a class="anchor-link" href="#Reducers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como hemos dicho, necesitamos usar una funci贸n de tipo <code>Reducer</code> para indicar c贸mo actualizar el estado, ya que si no los valores del estado se sobreescriben.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver un ejemplo de un grafo en el que no usamos una funci贸n de tipo <code>Reducer</code> para indicar c贸mo actualizar el estado</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">foo</span><span class="p">:</span> <span class="nb">int</span>
      
      <span class="k">def</span> <span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 1---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 2---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_3</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 3---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">{closing_brace}</span>
      
      <span class="c1"># Build graph</span>
      <span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_2"</span><span class="p">,</span> <span class="n">node_2</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_3"</span><span class="p">,</span> <span class="n">node_3</span><span class="p">)</span>
      
      <span class="c1"># Logic</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"node_1"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="s2">"node_2"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="s2">"node_3"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_2"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_3"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Add</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># View</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv2.webp" width="233" height="333" alt="image uv 3" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos hemos definido un grafo en el que primero se ejecuta el nodo 1 y luego el 2 y el 3. Vamos a ejecutarlo a ver qu茅 pasa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langgraph.errors</span> <span class="kn">import</span> <span class="n">InvalidUpdateError</span>',
          '',
          '<span class="k">try</span><span class="p">:</span>',
          '    <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"foo"</span> <span class="p">:</span> <span class="mi">1</span><span class="p">})</span>',
          '<span class="k">except</span> <span class="n">InvalidUpdateError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"InvalidUpdateError occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '---Node 1---',
          '---Node 2---',
          '---Node 3---',
          'InvalidUpdateError occurred: At key \'foo\': Can receive only one value per step. Use an Annotated key to handle multiple values.',
          'For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CONCURRENT_GRAPH_UPDATE',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos un error porque primero el nodo 1 modifica el valor de <code>foo</code> y luego los nodos 2 y 3 intentan modificar el valor de <code>foo</code> en paralelo, lo cual da un error</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As铆 que para evitar eso usamos una funci贸n de tipo <code>Reducer</code> para indicar c贸mo modificar el estado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Reducers-predefinidos">Reducers predefinidos<a class="anchor-link" href="#Reducers-predefinidos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usamos el tipo <code>Annotated</code> para especificar que es una funci贸n de tipo <code>Reducer</code>. Y usamos el operador <code>add</code> para a帽adir un valor a una lista</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span>
      <span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">foo</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">add</span><span class="p">]</span>
      
      <span class="k">def</span> <span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 1---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 2---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_3</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 3---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="c1"># Build graph</span>
      <span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_2"</span><span class="p">,</span> <span class="n">node_2</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_3"</span><span class="p">,</span> <span class="n">node_3</span><span class="p">)</span>
      
      <span class="c1"># Logic</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"node_1"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="s2">"node_2"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="s2">"node_3"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_2"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_3"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Add</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># View</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv3.webp" width="233" height="333" alt="image uv 4" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Lo volvemos a ejecutar a ver qu茅 pasa</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({opening_brace}</span><span class="s2">"foo"</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]{closing_brace})</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>---Node 1---
      ---Node 2---
      ---Node 3---
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[8]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>{opening_brace}'foo': [1, 2, 3, 3]{closing_brace}</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos inicializamos el valor de <code>foo</code> a 1, lo cual se a帽ade en una lista. Luego el nodo 1 le suma 1 y lo a帽ade como nuevo valor en la lista, es decir, a帽ade un 2. Por 煤ltimo los nodos 2 y 3 suman uno al 煤ltimo valor de la lista, es decir, los dos nodos obtienen un 3 y los dos nodos lo a帽aden al final de la lista, por eso la lista resultante tiene dos 3 al final</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver el caso de que una rama tenga m谩s nodos que otra</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span>
      <span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">foo</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">add</span><span class="p">]</span>
      
      <span class="k">def</span> <span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 1---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_2_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 2_1---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_2_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 2_2---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_3</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 3---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="c1"># Build graph</span>
      <span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_2_1"</span><span class="p">,</span> <span class="n">node_2_1</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_2_2"</span><span class="p">,</span> <span class="n">node_2_2</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_3"</span><span class="p">,</span> <span class="n">node_3</span><span class="p">)</span>
      
      <span class="c1"># Logic</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"node_1"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="s2">"node_2_1"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="s2">"node_3"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_2_1"</span><span class="p">,</span> <span class="s2">"node_2_2"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_2_2"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_3"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Add</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># View</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv4.webp" width="251" height="432" alt="image uv 5" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora ejecutamos el grafo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({opening_brace}</span><span class="s2">"foo"</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]{closing_brace})</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>---Node 1---
      ---Node 2_1---
      ---Node 3---
      ---Node 2_2---
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[3]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>{opening_brace}'foo': [1, 2, 3, 3, 4]{closing_brace}</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Lo que ha pasado es que primero se ha ejecutado el nodo 1, a continuaci贸n el nodo 2_1, despu茅s, en paralelo, los nodos 2_2 y 3, y por 煤ltimo el nodo <code>END</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como hemos definido <code>foo</code> como una lista de enteros, y est谩 tipada, si inicializamos el estado con <code>None</code> obtenemos un error</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">try</span><span class="p">:</span>',
          '    <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"foo"</span> <span class="p">:</span> <span class="kc">None</span><span class="p">})</span>',
          '<span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"TypeError occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'TypeError occurred: can only concatenate list (not "NoneType") to list',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver c贸mo arreglarlo con <code>reducers personalizados</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Reducers-personalizados">Reducers personalizados<a class="anchor-link" href="#Reducers-personalizados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A veces no podemos usar un <code>Reducer</code> predefinido y tenemos que crear el nuestro</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      
      <span class="k">def</span> <span class="nf">reducer_function</span><span class="p">(</span><span class="n">current_list</span><span class="p">,</span> <span class="n">new_item</span><span class="p">:</span> <span class="nb">list</span> <span class="o">|</span> <span class="kc">None</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">current_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
              <span class="n">current_list</span> <span class="o">=</span> <span class="p">[]</span>
      
          <span class="k">if</span> <span class="n">new_item</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
              <span class="k">return</span> <span class="n">current_list</span> <span class="o">+</span> <span class="n">new_item</span>
          <span class="k">return</span> <span class="n">current_list</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">foo</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">reducer_function</span><span class="p">]</span>
      
      <span class="k">def</span> <span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 1---"</span><span class="p">)</span>
          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
              <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s1">'foo'</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]{closing_brace}</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_2</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 2---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_3</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 3---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"foo"</span><span class="p">:</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="s1">'foo'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]{closing_brace}</span>
      
      <span class="c1"># Build graph</span>
      <span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_2"</span><span class="p">,</span> <span class="n">node_2</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_3"</span><span class="p">,</span> <span class="n">node_3</span><span class="p">)</span>
      
      <span class="c1"># Logic</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"node_1"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="s2">"node_2"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="s2">"node_3"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_2"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_3"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Add</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># View</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv5.webp" width="233" height="333" alt="image uv 6" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora iniciamos el grafo con un valor <code>None</code> ya no nos da un error</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">try</span><span class="p">:</span>',
          '    <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"foo"</span> <span class="p">:</span> <span class="kc">None</span><span class="p">})</span>',
          '<span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"TypeError occurred: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '---Node 1---',
          '---Node 2---',
          '---Node 3---',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="M%C3%BAltiples-estados">M煤ltiples estados<a class="anchor-link" href="#M%C3%BAltiples-estados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Estados-privados">Estados privados<a class="anchor-link" href="#Estados-privados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Supongamos que queremos ocultar variables del estado, por la raz贸n que sea, porque algunas variables solo aportan ruido o porque queremos mantener alguna variable privada.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si queremos tener un estado privado, simplemente lo creamos.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      
      <span class="k">class</span> <span class="nc">OverallState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">public_var</span><span class="p">:</span> <span class="nb">int</span>
      
      <span class="k">class</span> <span class="nc">PrivateState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">private_var</span><span class="p">:</span> <span class="nb">int</span>
      
      <span class="k">def</span> <span class="nf">node_1</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PrivateState</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 1---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"private_var"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">'public_var'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">node_2</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">PrivateState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OverallState</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"---Node 2---"</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"public_var"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s1">'private_var'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">{closing_brace}</span>
      
      <span class="c1"># Build graph</span>
      <span class="n">builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">OverallState</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="n">node_1</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"node_2"</span><span class="p">,</span> <span class="n">node_2</span><span class="p">)</span>
      
      <span class="c1"># Logic</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"node_1"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_1"</span><span class="p">,</span> <span class="s2">"node_2"</span><span class="p">)</span>
      <span class="n">builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"node_2"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Add</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># View</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv6.webp" width="106" height="333" alt="image uv 7" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos hemos creado el estado privado <code>PrivateState</code> y el estado p煤blico <code>OverallState</code>. Cada uno con una variable privada y una p煤blica. Primero se ejecuta el nodo 1, que modifica la variable privada y la devuelve. Luego se ejecuta el nodo 2, que modifica la variable p煤blica y la devuelve. Vamos a ejecutar el grafo para ver qu茅 pasa</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({opening_brace}</span><span class="s2">"public_var"</span> <span class="p">:</span> <span class="mi">1</span><span class="p">{closing_brace})</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>---Node 1---
      ---Node 2---
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[2]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>{opening_brace}'public_var': 3{closing_brace}</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos al ejecutar el grafo, pasamos la variable p煤blica <code>public_var</code> y obtenemos a la salida otra variable p煤blica <code>public_var</code> con el valor modificado, pero nunca se ha accedido a la variable privada <code>private_var</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Estados-de-entrada-y-salida">Estados de entrada y salida<a class="anchor-link" href="#Estados-de-entrada-y-salida"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos definir las variables de entrada y salida del grafo. Aunque internamente el estado puede tener m谩s variables, definimos qu茅 variables son de entrada al grafo y qu茅 variables son de salida.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      
      <span class="k">class</span> <span class="nc">InputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">question</span><span class="p">:</span> <span class="nb">str</span>
      
      <span class="k">class</span> <span class="nc">OutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
      
      <span class="k">class</span> <span class="nc">OverallState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">question</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">notes</span><span class="p">:</span> <span class="nb">str</span>
      
      <span class="k">def</span> <span class="nf">thinking_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">InputState</span><span class="p">):</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"answer"</span><span class="p">:</span> <span class="s2">"bye"</span><span class="p">,</span> <span class="s2">"notes"</span><span class="p">:</span> <span class="s2">"... his is name is Lance"</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">answer_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OutputState</span><span class="p">:</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"answer"</span><span class="p">:</span> <span class="s2">"bye Lance"</span><span class="p">{closing_brace}</span>
      
      <span class="n">graph</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">OverallState</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">InputState</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">OutputState</span><span class="p">)</span>
      
      <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"answer_node"</span><span class="p">,</span> <span class="n">answer_node</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"thinking_node"</span><span class="p">,</span> <span class="n">thinking_node</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"thinking_node"</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"thinking_node"</span><span class="p">,</span> <span class="s2">"answer_node"</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"answer_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># View</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv7.webp" width="146" height="333" alt="image uv 8" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este caso el estado tiene 3 variables, <code>question</code>, <code>answer</code> y <code>notes</code>. Sin embargo definimos como entrada al grafo <code>question</code> y como salida del grafo <code>answer</code>. Por lo tanto, el estado interno puede tener m谩s variables, pero no se tienen en cuenta a la hora de invocar el grafo. Vamos a ejecutar el grafo para ver qu茅 pasa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"question"</span><span class="p">:</span><span class="s2">"hi"</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'answer\': \'bye Lance\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, le hemos metido <code>question</code> al grafo y hemos obtenido <code>answer</code> a la salida.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Manejo-del-contexto">Manejo del contexto<a class="anchor-link" href="#Manejo-del-contexto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a volver a ver el c贸digo del chatbot b谩sico</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Define the chatbot function</span>
      <span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
      
      <span class="c1"># Connect nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv8.webp" width="143" height="234" alt="image uv 9" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a crear un contexto que le pasaremos al modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>',
          '',
          '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"So you said you were researching ocean mammals?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">)]</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Yes, I know about whales. But what others should I learn about?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '',
          '<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'So you said you were researching ocean mammals?',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Yes, I know about whales. But what others should I learn about?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si se lo pasamos al grafo, obtendremos la salida</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">\'messages\'</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>',
          '<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="s1">\'messages\'</span><span class="p">]:</span>',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'So you said you were researching ocean mammals?',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Yes, I know about whales. But what others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'That\'s a great topic! Besides whales, there are several other fascinating ocean mammals you might want to learn about. Here are a few:',
          '1. **Dolphins**: Highly intelligent and social, dolphins are found in all oceans of the world. They are known for their playful behavior and communication skills.',
          '2. **Porpoises**: Similar to dolphins but generally smaller and stouter, porpoises are less social and more elusive. They are found in coastal waters around the world.',
          '3. **Seals and Sea Lions**: These are semi-aquatic mammals that can be found in both Arctic and Antarctic regions, as well as in more temperate waters. They are known for their sleek bodies and flippers, and they differ in their ability to walk on land (sea lions can "walk" on their flippers, while seals can only wriggle or slide).',
          '4. **Walruses**: Known for their large tusks and whiskers, walruses are found in the Arctic. They are well-adapted to cold waters and have a thick layer of blubber to keep them warm.',
          '5. **Manatees and Dugongs**: These gentle, herbivorous mammals are often called "sea cows." They live in shallow, coastal areas and are found in tropical and subtropical regions. Manatees are found in the Americas, while dugongs are found in the Indo-Pacific region.',
          '6. **Otters**: While not fully aquatic, sea otters spend most of their lives in the water and are excellent swimmers. They are known for their dense fur, which keeps them warm in cold waters.',
          '7. **Polar Bears**: Although primarily considered land animals, polar bears are excellent swimmers and spend a significant amount of time in the water, especially when hunting for seals.',
          'Each of these mammals has unique adaptations and behaviors that make them incredibly interesting to study. If you have any specific questions or topics you\'d like to explore further, feel free to ask!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos ahora en la salida tenemos un mensaje m谩s.</p>
      <p>Si esto sigue creciendo llegar谩 un momento en el que tendremos un contexto muy largo, por lo que supondr谩 un mayor gasto de tokens, que puede acarrear un mayor gasto econ贸mico, y conlleva tambi茅n mayor latencia.</p>
      <p>Adem谩s con contextos muy largos los LLMs empiezan a rendir peor.</p>
      <p>En los 煤ltimos modelos, a d铆a de la escritura de este post, por encima de 8k tokens de contexto, empieza a decaer el rendimiento del LLM</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As铆 que vamos a ver varias maneras de gestionar esto</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Modificar-el-contexto-con-funciones-de-tipo-Reducer">Modificar el contexto con funciones de tipo <code>Reducer</code><a class="anchor-link" href="#Modificar-el-contexto-con-funciones-de-tipo-Reducer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos visto que con las funciones de tipo <code>Reducer</code> podemos modificar los mensajes del estado</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">RemoveMessage</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">filter_messages</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># Delete all but the 2 most recent messages</span>
          <span class="n">delete_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">RemoveMessage</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][:</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">delete_messages</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>    
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"filter_messages_node"</span><span class="p">,</span> <span class="n">filter_messages</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"filter_messages_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"filter_messages_node"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv9.webp" width="203" height="333" alt="image uv 10" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos en el grafo, primero filtramos los mensajes y luego pasamos el resultado al modelo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a crear un contexto que le pasaremos al modelo, pero ahora con m谩s mensajes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>',
          '',
          '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"So you said you were researching ocean mammals?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">)]</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Yes, I know about whales. But what others should I learn about?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"I know about sharks too"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"What others should I learn about?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"I know about dolphins too"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Tell me more about dolphins"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '',
          '<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'So you said you were researching ocean mammals?',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Yes, I know about whales. But what others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about sharks too',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'What others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about dolphins too',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Tell me more about dolphins',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si se lo pasamos al grafo, obtendremos la salida</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">\'messages\'</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>',
          '<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="s1">\'messages\'</span><span class="p">]:</span>',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about dolphins too',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Tell me more about dolphins',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Dolphins are highly intelligent marine mammals that are part of the family Delphinidae, which includes about 40 species. They are found in oceans worldwide, from tropical to temperate regions, and are known for their agility and playful behavior. Here are some interesting facts about dolphins:',
          '1. **Social Behavior**: Dolphins are highly social animals and often live in groups called pods, which can range from a few individuals to several hundred. Social interactions are complex and include cooperative behaviors, such as hunting and defending against predators.',
          '2. **Communication**: Dolphins communicate using a variety of sounds, including clicks, whistles, and body language. These sounds can be used for navigation (echolocation), communication, and social bonding. Each dolphin has a unique signature whistle that helps identify it to others in the pod.',
          '3. **Intelligence**: Dolphins are considered one of the most intelligent animals on Earth. They have large brains and display behaviors such as problem-solving, mimicry, and even the use of tools. Some studies suggest that dolphins can recognize themselves in mirrors, indicating a level of self-awareness.',
          '4. **Diet**: Dolphins are carnivores and primarily feed on fish and squid. They use echolocation to locate and catch their prey. Some species, like the bottlenose dolphin, have been observed using teamwork to herd fish into tight groups, making them easier to catch.',
          '5. **Reproduction**: Dolphins typically give birth to a single calf after a gestation period of about 10 to 12 months. Calves are born tail-first and are immediately helped to the surface for their first breath by their mother or another dolphin. Calves nurse for up to two years and remain dependent on their mothers for a significant period.',
          '6. **Conservation**: Many dolphin species are threatened by human activities such as pollution, overfishing, and habitat destruction. Some species, like the Indo-Pacific humpback dolphin and the Amazon river dolphin, are endangered. Conservation efforts are crucial to protect these animals and their habitats.',
          '7. **Human Interaction**: Dolphins have a long history of interaction with humans, often appearing in mythology and literature. In some cultures, they are considered sacred or bring good luck. Today, dolphins are popular in marine parks and are often the focus of eco-tourism activities, such as dolphin-watching tours.',
          'Dolphins continue to fascinate scientists and the general public alike, with ongoing research into their behavior, communication, and social structures providing new insights into these remarkable creatures.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver, la funci贸n de filtrado ha eliminado todos los mensajes menos los dos 煤ltimos y esos dos mensajes se han pasado como contexto al LLM.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Recortar-mensajes">Recortar mensajes<a class="anchor-link" href="#Recortar-mensajes"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Otra soluci贸n es recortar cada mensaje de la lista de mensajes que tengan muchos tokens, se establece un l铆mite de tokens y se elimina el mensaje que supera ese l铆mite.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">trim_messages</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">trim_messages_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># Trim the messages based on the specified parameters</span>
          <span class="n">trimmed_messages</span> <span class="o">=</span> <span class="n">trim_messages</span><span class="p">(</span>
              <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">],</span>
              <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>       <span class="c1"># Maximum tokens allowed in the trimmed list</span>
              <span class="n">strategy</span><span class="o">=</span><span class="s2">"last"</span><span class="p">,</span>     <span class="c1"># Keep the latest messages</span>
              <span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>   <span class="c1"># Use the LLM's tokenizer to count tokens</span>
              <span class="n">allow_partial</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Allow cutting messages mid-way if needed</span>
          <span class="p">)</span>
      
          <span class="c1"># Print the trimmed messages to see the effect of trim_messages</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"--- trimmed messages (input to LLM) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">trimmed_messages</span><span class="p">:</span>
              <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Invoke the LLM with the trimmed messages</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">trimmed_messages</span><span class="p">)</span>
      
          <span class="c1"># Return the LLM's response in the correct state format</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"trim_messages_node"</span><span class="p">,</span> <span class="n">trim_messages_node</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"trim_messages_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"trim_messages_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv10.webp" width="199" height="234" alt="image uv 11" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos en el grafo, primero filtramos los mensajes y luego pasamos el resultado al modelo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a crear un contexto que le pasaremos al modelo, pero ahora con m谩s mensajes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>',
          '',
          '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"So you said you were researching ocean mammals?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">)]</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Yes, I know about whales. But what others should I learn about?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"""I know about sharks too. They are very dangerous, but they are also very beautiful.</span>',
          '<span class="s2">Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.</span>',
          '<span class="s2">They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.</span>',
          '<span class="s2">Often they live in groups of 20 or more, but they are not very common.</span>',
          '<span class="s2">They should eat a lot of food. Normally they eat a lot of fish.</span>',
          '<span class="s2">The white shark is the largest of the sharks and is the most dangerous.</span>',
          '<span class="s2">The great white shark is the most famous of the sharks and is the most dangerous.</span>',
          '<span class="s2">The tiger shark is the most aggressive of the sharks and is the most dangerous.</span>',
          '<span class="s2">The hammerhead shark is the most beautiful of the sharks and is the most dangerous.</span>',
          '<span class="s2">The mako shark is the fastest of the sharks and is the most dangerous.</span>',
          '<span class="s2">The bull shark is the most common of the sharks and is the most dangerous.</span>',
          '<span class="s2">"""</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"What others should I learn about?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"I know about dolphins too"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Tell me more about dolphins"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '',
          '<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'So you said you were researching ocean mammals?',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Yes, I know about whales. But what others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about sharks too. They are very dangerous, but they are also very beautiful.',
          'Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.',
          'They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.',
          'Often they live in groups of 20 or more, but they are not very common.',
          'They should eat a lot of food. Normally they eat a lot of fish.',
          'The white shark is the largest of the sharks and is the most dangerous.',
          'The great white shark is the most famous of the sharks and is the most dangerous.',
          'The tiger shark is the most aggressive of the sharks and is the most dangerous.',
          'The hammerhead shark is the most beautiful of the sharks and is the most dangerous.',
          'The mako shark is the fastest of the sharks and is the most dangerous.',
          'The bull shark is the most common of the sharks and is the most dangerous.',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'What others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about dolphins too',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Tell me more about dolphins',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si se lo pasamos al grafo obtendremos la salida</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">\'messages\'</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '--- trimmed messages (input to LLM) ---',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'The tiger shark is the most aggressive of the sharks and is the most dangerous.',
          'The hammerhead shark is the most beautiful of the sharks and is the most dangerous.',
          'The mako shark is the fastest of the sharks and is the most dangerous.',
          'The bull shark is the most common of the sharks and is the most dangerous.',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'What others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about dolphins too',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Tell me more about dolphins',
          '------------------------------------------------',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver se ha recortado el contexto que se le pasa al LLM, el mensaje que era muy largo y ten铆a muchos tokens se ha recortado. Vamos a ver la salida del LLM</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="s1">\'messages\'</span><span class="p">]:</span>',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'So you said you were researching ocean mammals?',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Yes, I know about whales. But what others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about sharks too. They are very dangerous, but they are also very beautiful.',
          'Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.',
          'They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.',
          'Often they live in groups of 20 or more, but they are not very common.',
          'They should eat a lot of food. Normally they eat a lot of fish.',
          'The white shark is the largest of the sharks and is the most dangerous.',
          'The great white shark is the most famous of the sharks and is the most dangerous.',
          'The tiger shark is the most aggressive of the sharks and is the most dangerous.',
          'The hammerhead shark is the most beautiful of the sharks and is the most dangerous.',
          'The mako shark is the fastest of the sharks and is the most dangerous.',
          'The bull shark is the most common of the sharks and is the most dangerous.',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'What others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about dolphins too',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Tell me more about dolphins',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Certainly! Dolphins are intelligent marine mammals that are part of the family Delphinidae, which includes nearly 40 species. Here are some interesting facts about dolphins:',
          '1. **Intelligence**: Dolphins are known for their high intelligence and have large brains relative to their body size. They exhibit behaviors that suggest social complexity, self-awareness, and problem-solving skills. For example, they can recognize themselves in mirrors, a trait shared by only a few other species.',
          '2. **Communication**: Dolphins communicate using a variety of clicks, whistles, and body language. Each dolphin has a unique "signature whistle" that helps identify it to others, similar to a human name. They use echolocation to navigate and locate prey by emitting clicks and interpreting the echoes that bounce back.',
          '3. **Social Structure**: Dolphins are highly social animals and often live in groups called pods. These pods can vary in size from a few individuals to several hundred. Within these groups, dolphins form complex social relationships and often cooperate to hunt and protect each other from predators.',
          '4. **Habitat**: Dolphins are found in all the world\'s oceans and in some rivers. Different species have adapted to various environments, from tropical waters to the cooler regions of the open sea. Some species, like the Amazon river dolphin (also known as the boto), live in freshwater rivers.',
          '5. **Diet**: Dolphins are carnivores and primarily eat fish, squid, and crustaceans. Their diet can vary depending on the species and their habitat. Some species, like the killer whale (which is actually a large dolphin), can even hunt larger marine mammals.',
          '6. **Reproduction**: Dolphins have a long gestation period, typically around 10 to 12 months. Calves are born tail-first and are nursed by their mothers for up to two years. Dolphins often form strong bonds with their offspring and other members of their pod.',
          '7. **Conservation**: Many species of dolphins face threats such as pollution, overfishing, and entanglement in fishing nets. Conservation efforts are ongoing to protect these animals and their habitats. Organizations like the International Union for Conservation of Nature (IUCN) and the World Wildlife Fund (WWF) work to raise awareness and implement conservation measures.',
          '8. **Cultural Significance**: Dolphins have been a source of fascination and inspiration for humans for centuries. They appear in myths, legends, and art across many cultures and are often seen as symbols of intelligence, playfulness, and freedom.',
          'Dolphins are truly remarkable creatures with a lot to teach us about social behavior, communication, and the complexities of marine ecosystems. If you have any specific questions or want to know more about a particular species, feel free to ask!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Con un contexto recortado, el LLM sigue contestando</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Modificaci%C3%B3n-del-contexto-y-recorte-de-mensajes">Modificaci贸n del contexto y recorte de mensajes<a class="anchor-link" href="#Modificaci%C3%B3n-del-contexto-y-recorte-de-mensajes"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a juntar las dos t茅cnicas anteriores, modificaremos el contexto y recortaremos los mensajes.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">RemoveMessage</span><span class="p">,</span> <span class="n">trim_messages</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">filter_messages</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># Delete all but the 2 most recent messages</span>
          <span class="n">delete_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">RemoveMessage</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][:</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">delete_messages</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">trim_messages_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># print the messages</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"--- messages (input to trim_messages) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span>
              <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Trim the messages based on the specified parameters</span>
          <span class="n">trimmed_messages</span> <span class="o">=</span> <span class="n">trim_messages</span><span class="p">(</span>
              <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">],</span>
              <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>       <span class="c1"># Maximum tokens allowed in the trimmed list</span>
              <span class="n">strategy</span><span class="o">=</span><span class="s2">"last"</span><span class="p">,</span>     <span class="c1"># Keep the latest messages</span>
              <span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>   <span class="c1"># Use the LLM's tokenizer to count tokens</span>
              <span class="n">allow_partial</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Allow cutting messages mid-way if needed</span>
          <span class="p">)</span>
      
          <span class="c1"># Print the trimmed messages to see the effect of trim_messages</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"--- trimmed messages (input to LLM) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">trimmed_messages</span><span class="p">:</span>
              <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Invoke the LLM with the trimmed messages</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">trimmed_messages</span><span class="p">)</span>
      
          <span class="c1"># Return the LLM's response in the correct state format</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>    
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"filter_messages_node"</span><span class="p">,</span> <span class="n">filter_messages</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"trim_messages_node"</span><span class="p">,</span> <span class="n">trim_messages_node</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"filter_messages_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"filter_messages_node"</span><span class="p">,</span> <span class="s2">"trim_messages_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"trim_messages_node"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv11.webp" width="203" height="432" alt="image uv 12" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora filtramos qued谩ndonos con los dos 煤ltimos mensajes, luego trrimamos el contexto para que no se gasten muchos tokens y finalmente pasamos el resultado al modelo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un contexto para pas谩rselo al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>',
          '',
          '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"So you said you were researching ocean mammals?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">)]</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Yes, I know about whales. But what others should I learn about?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"I know about dolphins too"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"What others should I learn about?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">AIMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"""I know about sharks too. They are very dangerous, but they are also very beautiful.</span>',
          '<span class="s2">Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.</span>',
          '<span class="s2">They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.</span>',
          '<span class="s2">Often they live in groups of 20 or more, but they are not very common.</span>',
          '<span class="s2">They should eat a lot of food. Normally they eat a lot of fish.</span>',
          '<span class="s2">The white shark is the largest of the sharks and is the most dangerous.</span>',
          '<span class="s2">The great white shark is the most famous of the sharks and is the most dangerous.</span>',
          '<span class="s2">The tiger shark is the most aggressive of the sharks and is the most dangerous.</span>',
          '<span class="s2">The hammerhead shark is the most beautiful of the sharks and is the most dangerous.</span>',
          '<span class="s2">The mako shark is the fastest of the sharks and is the most dangerous.</span>',
          '<span class="s2">The bull shark is the most common of the sharks and is the most dangerous.</span>',
          '<span class="s2">"""</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Bot"</span><span class="p">))</span>',
          '<span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">HumanMessage</span><span class="p">(</span><span class="sa">f</span><span class="s2">"What others should I learn about?"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"Lance"</span><span class="p">))</span>',
          '',
          '<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'So you said you were researching ocean mammals?',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'Yes, I know about whales. But what others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about dolphins too',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'What others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about sharks too. They are very dangerous, but they are also very beautiful.',
          'Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.',
          'They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.',
          'Often they live in groups of 20 or more, but they are not very common.',
          'They should eat a lot of food. Normally they eat a lot of fish.',
          'The white shark is the largest of the sharks and is the most dangerous.',
          'The great white shark is the most famous of the sharks and is the most dangerous.',
          'The tiger shark is the most aggressive of the sharks and is the most dangerous.',
          'The hammerhead shark is the most beautiful of the sharks and is the most dangerous.',
          'The mako shark is the fastest of the sharks and is the most dangerous.',
          'The bull shark is the most common of the sharks and is the most dangerous.',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'What others should I learn about?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se lo pasamos al grafo y obtenemos la salida</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">\'messages\'</span><span class="p">:</span> <span class="n">messages</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '--- messages (input to trim_messages) ---',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about sharks too. They are very dangerous, but they are also very beautiful.',
          'Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.',
          'They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.',
          'Often they live in groups of 20 or more, but they are not very common.',
          'They should eat a lot of food. Normally they eat a lot of fish.',
          'The white shark is the largest of the sharks and is the most dangerous.',
          'The great white shark is the most famous of the sharks and is the most dangerous.',
          'The tiger shark is the most aggressive of the sharks and is the most dangerous.',
          'The hammerhead shark is the most beautiful of the sharks and is the most dangerous.',
          'The mako shark is the fastest of the sharks and is the most dangerous.',
          'The bull shark is the most common of the sharks and is the most dangerous.',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'What others should I learn about?',
          '------------------------------------------------',
          '--- trimmed messages (input to LLM) ---',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'What others should I learn about?',
          '------------------------------------------------',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se ve, solo nos hemos quedado con el 煤ltimo mensaje, ha sido porque la funci贸n de filtrado ha devuelto los dos 煤ltimos mensajes, pero la funci贸n de trimado ha eliminado el pen煤ltimo mensaje por tener m谩s de 100 tokens.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver qu茅 tenemos a la salida del modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="s1">\'messages\'</span><span class="p">]:</span>',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Name: Bot',
          'I know about sharks too. They are very dangerous, but they are also very beautiful.',
          'Sometimes have been seen in the wild, but they are not very common. In the wild, they are very dangerous, but they are also very beautiful.',
          'They live in the sea and in the ocean. They can travel long distances and can be found in many parts of the world.',
          'Often they live in groups of 20 or more, but they are not very common.',
          'They should eat a lot of food. Normally they eat a lot of fish.',
          'The white shark is the largest of the sharks and is the most dangerous.',
          'The great white shark is the most famous of the sharks and is the most dangerous.',
          'The tiger shark is the most aggressive of the sharks and is the most dangerous.',
          'The hammerhead shark is the most beautiful of the sharks and is the most dangerous.',
          'The mako shark is the fastest of the sharks and is the most dangerous.',
          'The bull shark is the most common of the sharks and is the most dangerous.',
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Name: Lance',
          'What others should I learn about?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Certainly! To provide a more tailored response, it would be helpful to know what areas or topics you\'re interested in. However, heres a general list of areas that are often considered valuable for personal and professional development:',
          '1. **Technology &amp; Digital Skills**: ',
          '   - Programming languages (Python, JavaScript, etc.)',
          '   - Web development (HTML, CSS, React, etc.)',
          '   - Data analysis and visualization (SQL, Tableau, Power BI)',
          '   - Machine learning and AI',
          '   - Cloud computing (AWS, Azure, Google Cloud)',
          '2. **Business &amp; Entrepreneurship**:',
          '   - Marketing (digital marketing, SEO, content marketing)',
          '   - Project management',
          '   - Financial literacy',
          '   - Leadership and management',
          '   -Startup and venture capital',
          '3. **Science &amp; Engineering**:',
          '   - Biology and genetics',
          '   - Physics and materials science',
          '   - Environmental science and sustainability',
          '   - Robotics and automation',
          '   - Aerospace engineering',
          '4. **Health &amp; Wellness**:',
          '   - Nutrition and dietetics',
          '   - Mental health and psychology',
          '   - Exercise science',
          '   - Yoga and mindfulness',
          '   - Traditional and alternative medicine',
          '5. **Arts &amp; Humanities**:',
          '   - Creative writing and storytelling',
          '   - Music and sound production',
          '   - Visual arts and design (graphic design, photography)',
          '   - Philosophy and ethics',
          '   - History and cultural studies',
          '6. **Communication &amp; Languages**:',
          '   - Public speaking and presentation skills',
          '   - Conflict resolution and negotiation',
          '   - Learning a new language (Spanish, Mandarin, French, etc.)',
          '   - Writing and editing',
          '7. **Personal Development**:',
          '   - Time management and productivity',
          '   - Mindfulness and stress management',
          '   - Goal setting and motivation',
          '   - Personal finance and budgeting',
          '   - Critical thinking and problem solving',
          '8. **Social &amp; Environmental Impact**:',
          '   - Social entrepreneurship',
          '   - Community organizing and activism',
          '   - Sustainable living practices',
          '   - Climate change and environmental policy',
          'If you have a specific area of interest or a particular goal in mind, feel free to share, and I can provide more detailed recommendations!',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hemos filtrado tanto el estado que el LLM no tiene contexto suficiente, m谩s adelante veremos una manera de solucionarlo a帽adiendo al estado un resumen de la conversaci贸n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Modos-de-streaming">Modos de streaming<a class="anchor-link" href="#Modos-de-streaming"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Streaming-s%C3%ADncrono">Streaming s铆ncrono<a class="anchor-link" href="#Streaming-s%C3%ADncrono"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este caso vamos a recibir el resultado del LLM completo de una vez haya terminado de generar el texto.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para explicar los modos de streaming s铆ncrono, primero vamos a crear un grafo b谩sico.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># Return the LLM's response in the correct state format</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv12.webp" width="143" height="234" alt="image uv 13" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora tenemos dos maneras de obtener el resultado del LLM, una es mediante el modo <code>updates</code> y la otra mediante el modo <code>values</code>.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="updates vs values" src="https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png" width="643" height="188"/></p>
      <p>Mientras que <code>updates</code> nos da cada nuevo resultado, <code>values</code> nos da todo el historial de resultados.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h6 id="Updates">Updates<a class="anchor-link" href="#Updates"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h6>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"hi! I\'m M谩ximo"</span><span class="p">)]},</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"updates"</span><span class="p">):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s1">\'chatbot_node\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Hello M谩ximo! It\'s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h6 id="Values">Values<a class="anchor-link" href="#Values"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h6>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"hi! I\'m M谩ximo"</span><span class="p">)]},</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'hi! I\'m M谩ximo',
          'Hello M谩ximo! It\'s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Streaming-as%C3%ADncrono">Streaming as铆ncrono<a class="anchor-link" href="#Streaming-as%C3%ADncrono"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vamos a recibir el resultado del LLM token a token. Para ello tenemos que a帽adir <code>streaming=True</code> cuando creamos el LLM de HuggingFace y tenemos que cambiar la funci贸n del nodo del chatbot para que sea as铆ncrona.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
          <span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">async</span> <span class="k">def</span> <span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="k">async</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">llm</span><span class="o">.</span><span class="n">astream_log</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]):</span>
              <span class="k">yield</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">token</span><span class="p">]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv13.webp" width="143" height="234" alt="image uv 14" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver, la funci贸n se ha creado as铆ncrona y se ha convertido en un generador ya que el <code>yield</code> devuelve un valor y pausa la ejecuci贸n de la funci贸n hasta que se llame de nuevo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ejecutar el grafo de forma as铆ncrona y vemos los tipos de eventos que se generan.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="k">try</span><span class="p">:</span>
          <span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">astream_events</span><span class="p">({opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"hi! I'm M谩ximo"</span><span class="p">)]{closing_brace},</span> <span class="n">version</span><span class="o">=</span><span class="s2">"v2"</span><span class="p">):</span>
              <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"event: </span><span class="si">{opening_brace}</span><span class="n">event</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error: </span><span class="si">{opening_brace}</span><span class="n">e</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>event: {opening_brace}'event': 'on_chain_start', 'data': {opening_brace}'input': {opening_brace}'messages': &lbrack;HumanMessage(content="hi! I'm M谩ximo", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace})]{closing_brace}{closing_brace}, 'name': 'LangGraph', 'tags': [], 'run_id': 'c9c40a00-157a-4229-a0d1-fda00e7bfd34', 'metadata': {opening_brace}{closing_brace}, 'parent_ids': []{closing_brace}
      event: {opening_brace}'event': 'on_chain_start', 'data': {opening_brace}'input': {opening_brace}'input': &lbrack;HumanMessage(content="hi! I'm M谩ximo", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='6469501c-07b0-42e4-a3e6-f133ace1860c')]{closing_brace}{closing_brace}, 'name': 'chatbot_node', 'tags': ['graph:step:1'], 'run_id': '638828c0-4add-4141-b6b6-484446100237', 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34']{closing_brace}
      event: {opening_brace}'event': 'on_chain_start', 'data': {opening_brace}{closing_brace}, 'name': 'chatbot_node', 'tags': ['seq:step:1'], 'run_id': '15247b1a-1cd6-4863-9402-66499f921244', 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_start', 'data': {opening_brace}'input': {opening_brace}'input': [&lbrack;HumanMessage(content="hi! I'm M谩ximo", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='6469501c-07b0-42e4-a3e6-f133ace1860c')]]{closing_brace}{closing_brace}, 'name': 'ChatHuggingFace', 'tags': [], 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chain_stream', 'run_id': '15247b1a-1cd6-4863-9402-66499f921244', 'name': 'chatbot_node', 'tags': ['seq:step:1'], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd'{closing_brace}, 'data': {opening_brace}'chunk': {opening_brace}'input': [RunLogPatch({closing_brace}'op': 'replace',
        'path': '',
        'value': {opening_brace}'final_output': None,
                  'id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3',
                  'logs': {opening_brace}{closing_brace},
                  'name': 'ChatHuggingFace',
                  'streamed_output': [],
                  'type': 'llm'{closing_brace}{closing_brace})]{closing_brace}{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content='Hello', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' M谩', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content='ximo', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content='!', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' It', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content="'s", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' nice', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' to', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' meet', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' you', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content='.', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' How', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' can', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' I', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' assist', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' you', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' today', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content='?', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' Feel', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' free', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' to', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' ask', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' me', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' any', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' questions', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/Users/macm1/miniforge3/envs/langgraph/lib/python3.13/site-packages/huggingface_hub/inference/_generated/_async_client.py:2308: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' or', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' let', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' me', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' know', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' if', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' you', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' need', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' help', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' with', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' anything', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content=' specific', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content='.', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_stream', 'data': {opening_brace}'chunk': AIMessageChunk(content='&lt;|im_end|&gt;', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}){closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chat_model_end', 'data': {closing_brace}'output': AIMessage(content="Hello M谩ximo! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&lt;|im_end|&gt;", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0'), 'input': {opening_brace}'input': [&lbrack;HumanMessage(content="hi! I'm M谩ximo", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='6469501c-07b0-42e4-a3e6-f133ace1860c')]]{closing_brace}{closing_brace}, 'run_id': '74dfdbb9-4c2d-4a08-ad7d-795b5953cae3', 'name': 'ChatHuggingFace', 'tags': [], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'ls_provider': 'huggingface', 'ls_model_type': 'chat'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237', '15247b1a-1cd6-4863-9402-66499f921244']{closing_brace}
      event: {opening_brace}'event': 'on_chain_stream', 'run_id': '15247b1a-1cd6-4863-9402-66499f921244', 'name': 'chatbot_node', 'tags': ['seq:step:1'], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd'{closing_brace}, 'data': {opening_brace}'chunk': {opening_brace}'input': [RunLogPatch({closing_brace}'op': 'add',
        'path': '/streamed_output/-',
        'value': AIMessage(content="Hello M谩ximo! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&lt;|im_end|&gt;", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0'){closing_brace},
       {closing_brace}'op': 'replace',
        'path': '/final_output',
        'value': AIMessage(content="Hello M谩ximo! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&lt;|im_end|&gt;", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0')})]{closing_brace}{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237']{closing_brace}
      event: {opening_brace}'event': 'on_chain_end', 'data': {closing_brace}'output': {opening_brace}'input': [RunLogPatch({closing_brace}'op': 'add',
        'path': '/streamed_output/-',
        'value': AIMessage(content="Hello M谩ximo! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&lt;|im_end|&gt;", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0'){closing_brace},
       {closing_brace}'op': 'replace',
        'path': '/final_output',
        'value': AIMessage(content="Hello M谩ximo! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&lt;|im_end|&gt;", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0')})]}, 'input': {opening_brace}'input': &lbrack;HumanMessage(content="hi! I'm M谩ximo", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='6469501c-07b0-42e4-a3e6-f133ace1860c')]{closing_brace}{closing_brace}, 'run_id': '15247b1a-1cd6-4863-9402-66499f921244', 'name': 'chatbot_node', 'tags': ['seq:step:1'], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd', 'checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34', '638828c0-4add-4141-b6b6-484446100237']{closing_brace}
      event: {opening_brace}'event': 'on_chain_stream', 'run_id': '638828c0-4add-4141-b6b6-484446100237', 'name': 'chatbot_node', 'tags': ['graph:step:1'], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd'{closing_brace}, 'data': {opening_brace}'chunk': {opening_brace}'input': [RunLogPatch({closing_brace}'op': 'add',
        'path': '/streamed_output/-',
        'value': AIMessage(content="Hello M谩ximo! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&lt;|im_end|&gt;", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0'){closing_brace},
       {closing_brace}'op': 'replace',
        'path': '/final_output',
        'value': AIMessage(content="Hello M谩ximo! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&lt;|im_end|&gt;", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0')})]{closing_brace}{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34']{closing_brace}
      event: {opening_brace}'event': 'on_chain_end', 'data': {closing_brace}'output': {opening_brace}'input': [RunLogPatch({closing_brace}'op': 'add',
        'path': '/streamed_output/-',
        'value': AIMessage(content="Hello M谩ximo! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&lt;|im_end|&gt;", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0'){closing_brace},
       {closing_brace}'op': 'replace',
        'path': '/final_output',
        'value': AIMessage(content="Hello M谩ximo! It's nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.&lt;|im_end|&gt;", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='run-74dfdbb9-4c2d-4a08-ad7d-795b5953cae3-0')})]}, 'input': {opening_brace}'input': &lbrack;HumanMessage(content="hi! I'm M谩ximo", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id='6469501c-07b0-42e4-a3e6-f133ace1860c')]{closing_brace}{closing_brace}, 'run_id': '638828c0-4add-4141-b6b6-484446100237', 'name': 'chatbot_node', 'tags': ['graph:step:1'], 'metadata': {opening_brace}'langgraph_step': 1, 'langgraph_node': 'chatbot_node', 'langgraph_triggers': ('branch:to:chatbot_node',), 'langgraph_path': ('__pregel_pull', 'chatbot_node'), 'langgraph_checkpoint_ns': 'chatbot_node:b7599990-0c1a-4133-fb2c-f32105784fbd'{closing_brace}, 'parent_ids': ['c9c40a00-157a-4229-a0d1-fda00e7bfd34']{closing_brace}
      Error: Unsupported message type: &lt;class 'langchain_core.tracers.log_stream.RunLogPatch'&gt;
      For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE 
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver los tokens llegan con el evento <code>on_chat_model_stream</code>, as铆 que vamos a capturarlo e imprimirlo.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="k">try</span><span class="p">:</span>
          <span class="k">async</span> <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">astream_events</span><span class="p">({opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"hi! I'm M谩ximo"</span><span class="p">)]{closing_brace},</span> <span class="n">version</span><span class="o">=</span><span class="s2">"v2"</span><span class="p">):</span>
              <span class="k">if</span> <span class="n">event</span><span class="p">[</span><span class="s2">"event"</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"on_chat_model_stream"</span><span class="p">:</span>
                  <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s2">"data"</span><span class="p">][</span><span class="s2">"chunk"</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">" | "</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="k">pass</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/Users/macm1/miniforge3/envs/langgraph/lib/python3.13/site-packages/huggingface_hub/inference/_generated/_async_client.py:2308: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Hello |  M谩 | ximo | ! |  It | 's |  nice |  to |  meet |  you | . |  How |  can |  I |  assist |  you |  today | ? |  Feel |  free |  to |  ask |  me |  any |  questions |  or |  let |  me |  know |  if |  you |  need |  help |  with |  anything |  specific | . | &lt;|im_end|&gt; | </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Sub-grafos">Sub grafos<a class="anchor-link" href="#Sub-grafos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 27" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes hemos visto c贸mo bifurcar un grafo de manera que se ejecuten nodos en paralelo, pero supongamos el caso de que ahora lo que queremos es que lo que se ejecute en paralelo sean subgrafos. As铆 que vamos a ver c贸mo hacerlo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver c贸mo hacer un grafo de gesti贸n de logs que va a tener un subgrafo de resumen de logs y otro subgrafo de an谩lisis de errores en los logs.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="grafo con subgrafos" src="https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb1abf89f2d847ee6f1ff_sub-graph1.png" width="1509" height="720"/></p>
      <p>As铆 que lo que vamos a hacer es primero definir cada uno de los subgrafos por separado y luego a帽adirlos al grafo principal.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Subgrafo-de-an%C3%A1lisis-de-errores-en-logs">Subgrafo de an谩lisis de errores en logs<a class="anchor-link" href="#Subgrafo-de-an%C3%A1lisis-de-errores-en-logs"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 28" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importamos las librer铆as necesarias</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>',
      '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span>',
      '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
      '<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Annotated</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Creamos una clase con la estructura de los logs</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># The structure of the logs</span>',
      '<span class="k">class</span> <span class="nc">Log</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="nb">id</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">question</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">docs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span>',
      '    <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">grade</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>',
      '    <span class="n">grader</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>',
      '    <span class="n">feedback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <p>Creamos ahora dos clases, una con la estructura de los errores de los logs y otra con el an谩lisis que reportar谩 a la salida</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Failure Analysis Sub-graph</span>',
      '<span class="k">class</span> <span class="nc">FailureAnalysisState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>',
      '    <span class="n">failures</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>',
      '    <span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>',
      ' ',
      '<span class="k">class</span> <span class="nc">FailureAnalysisOutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <p>Creamos ahora las funciones de los nodos, una obtendr谩 los fallos en los logs, para ello buscar谩 los logs que tengan alg煤n valor en el campo <code>grade</code>. Otra generar谩 un resumen de los fallos. Adem谩s vamos a poner <code>print</code>s para poder ver qu茅 est谩 pasando internamente.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">get_failures</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>',
      '<span class="w">    </span><span class="sd">""" Get logs that contain a failure """</span>',
      '    <span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"cleaned_logs"</span><span class="p">]</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug get_failures: cleaned_logs: </span><span class="si">{</span><span class="n">cleaned_logs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      ' ',
      '    <span class="n">failures</span> <span class="o">=</span> <span class="p">[</span><span class="n">log</span> <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">cleaned_logs</span> <span class="k">if</span> <span class="s2">"grade"</span> <span class="ow">in</span> <span class="n">log</span><span class="p">]</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug get_failures: failures: </span><span class="si">{</span><span class="n">failures</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      ' ',
      '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"failures"</span><span class="p">:</span> <span class="n">failures</span><span class="p">{closing_brace}</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">generate_summary</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>',
      '<span class="w">    </span><span class="sd">""" Generate summary of failures """</span>',
      '    <span class="n">failures</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"failures"</span><span class="p">]</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug generate_summary: failures: </span><span class="si">{</span><span class="n">failures</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      ' ',
      '    <span class="n">fa_summary</span> <span class="o">=</span> <span class="s2">"Poor quality retrieval of documentation."</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug generate_summary: fa_summary: </span><span class="si">{</span><span class="n">fa_summary</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      ' ',
      '    <span class="n">processed_logs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"failure-analysis-on-log-</span><span class="si">{</span><span class="n">failure</span><span class="p">[</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">failure</span> <span class="ow">in</span> <span class="n">failures</span><span class="p">]</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug generate_summary: processed_logs: </span><span class="si">{</span><span class="n">processed_logs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '    ',
      '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"fa_summary"</span><span class="p">:</span> <span class="n">fa_summary</span><span class="p">,</span> <span class="s2">"processed_logs"</span><span class="p">:</span> <span class="n">processed_logs</span><span class="p">{closing_brace}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>



























      
      <section class="section-block-markdown-cell">
      <p>Por 煤ltimo, creamos el grafo, a帽adimos los nodos y los <code>edge</code>s y lo compilamos</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">fa_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">FailureAnalysisState</span><span class="p">,</span><span class="n">output</span><span class="o">=</span><span class="n">FailureAnalysisOutputState</span><span class="p">)</span>
      
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"get_failures"</span><span class="p">,</span> <span class="n">get_failures</span><span class="p">)</span>
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"generate_summary"</span><span class="p">,</span> <span class="n">generate_summary</span><span class="p">)</span>
      
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"get_failures"</span><span class="p">)</span>
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"get_failures"</span><span class="p">,</span> <span class="s2">"generate_summary"</span><span class="p">)</span>
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"generate_summary"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="n">graph</span> <span class="o">=</span> <span class="n">fa_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv14.webp" width="184" height="333" alt="image uv 15" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a crear un log de prueba</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">failure_log</span> <span class="o">=</span> <span class="p">{opening_brace}</span>',
      '    <span class="s2">"id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">,</span> ',
      '    <span class="s2">"question"</span><span class="p">:</span> <span class="s2">"What is the meaning of life?"</span><span class="p">,</span> ',
      '    <span class="s2">"docs"</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> ',
      '    <span class="s2">"answer"</span><span class="p">:</span> <span class="s2">"42"</span><span class="p">,</span> ',
      '    <span class="s2">"grade"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> ',
      '    <span class="s2">"grader"</span><span class="p">:</span> <span class="s2">"AI"</span><span class="p">,</span> ',
      '    <span class="s2">"feedback"</span><span class="p">:</span> <span class="s2">"Good job!"</span>',
      '<span class="p">{closing_brace}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <p>Ejecutamos el grafo con el log de prueba. Como la funci贸n <code>get_failures</code> coge la key <code>cleaned_logs</code> del estado, tenemos que pasarle el log al grafo en esa misma key.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({opening_brace}</span><span class="s2">"cleaned_logs"</span><span class="p">:</span> <span class="p">[</span><span class="n">failure_log</span><span class="p">]{closing_brace})</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>	 debug get_failures: cleaned_logs: [{opening_brace}'id': '1', 'question': 'What is the meaning of life?', 'docs': None, 'answer': '42', 'grade': 1, 'grader': 'AI', 'feedback': 'Good job!'{closing_brace}&rbrack;
      	 debug get_failures: failures: [{opening_brace}'id': '1', 'question': 'What is the meaning of life?', 'docs': None, 'answer': '42', 'grade': 1, 'grader': 'AI', 'feedback': 'Good job!'{closing_brace}&rbrack;
      	 debug generate_summary: failures: [{opening_brace}'id': '1', 'question': 'What is the meaning of life?', 'docs': None, 'answer': '42', 'grade': 1, 'grader': 'AI', 'feedback': 'Good job!'{closing_brace}&rbrack;
      	 debug generate_summary: fa_summary: Poor quality retrieval of documentation.
      	 debug generate_summary: processed_logs: ['failure-analysis-on-log-1']
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[16]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>{opening_brace}'fa_summary': 'Poor quality retrieval of documentation.',
       'processed_logs': ['failure-analysis-on-log-1']{closing_brace}</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se puede ver que ha encontrado el log de prueba, ya que tiene un valor de <code>1</code> en el campo <code>grade</code> y luego ha generado un resumen de los fallos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a definir todo el subgrafo junto otra vez para que se vea m谩s claro y adem谩s para quitar los <code>print</code>s que hemos puesto para debug.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      
      <span class="kn">from</span> <span class="nn">operator</span> <span class="kn">import</span> <span class="n">add</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Annotated</span>
      
      <span class="c1"># The structure of the logs</span>
      <span class="k">class</span> <span class="nc">Log</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="nb">id</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">question</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">docs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span>
          <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">grade</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
          <span class="n">grader</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
          <span class="n">feedback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
      
      <span class="c1"># Failure clases</span>
      <span class="k">class</span> <span class="nc">FailureAnalysisState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
          <span class="n">failures</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
          <span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
      
      <span class="k">class</span> <span class="nc">FailureAnalysisOutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
      
      <span class="c1"># Functions</span>
      <span class="k">def</span> <span class="nf">get_failures</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
      <span class="w">    </span><span class="sd">""" Get logs that contain a failure """</span>
          <span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"cleaned_logs"</span><span class="p">]</span>
          <span class="n">failures</span> <span class="o">=</span> <span class="p">[</span><span class="n">log</span> <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">cleaned_logs</span> <span class="k">if</span> <span class="s2">"grade"</span> <span class="ow">in</span> <span class="n">log</span><span class="p">]</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"failures"</span><span class="p">:</span> <span class="n">failures</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">generate_summary</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
      <span class="w">    </span><span class="sd">""" Generate summary of failures """</span>
          <span class="n">failures</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"failures"</span><span class="p">]</span>
          <span class="n">fa_summary</span> <span class="o">=</span> <span class="s2">"Poor quality retrieval of documentation."</span>
          <span class="n">processed_logs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"failure-analysis-on-log-</span><span class="si">{opening_brace}</span><span class="n">failure</span><span class="p">[</span><span class="s1">'id'</span><span class="p">]</span><span class="si">{closing_brace}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">failure</span> <span class="ow">in</span> <span class="n">failures</span><span class="p">]</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"fa_summary"</span><span class="p">:</span> <span class="n">fa_summary</span><span class="p">,</span> <span class="s2">"processed_logs"</span><span class="p">:</span> <span class="n">processed_logs</span><span class="p">{closing_brace}</span>
      
      <span class="c1"># Build the graph</span>
      <span class="n">fa_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">FailureAnalysisState</span><span class="p">,</span><span class="n">output</span><span class="o">=</span><span class="n">FailureAnalysisOutputState</span><span class="p">)</span>
      
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"get_failures"</span><span class="p">,</span> <span class="n">get_failures</span><span class="p">)</span>
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"generate_summary"</span><span class="p">,</span> <span class="n">generate_summary</span><span class="p">)</span>
      
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"get_failures"</span><span class="p">)</span>
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"get_failures"</span><span class="p">,</span> <span class="s2">"generate_summary"</span><span class="p">)</span>
      <span class="n">fa_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"generate_summary"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="n">graph</span> <span class="o">=</span> <span class="n">fa_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv15.webp" width="184" height="333" alt="image uv 16" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora lo volvemos a ejecutar obtenemos el mismo resultado, pero sin los <code>print</code>s.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"cleaned_logs"</span><span class="p">:</span> <span class="p">[</span><span class="n">failure_log</span><span class="p">]})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'fa_summary\': \'Poor quality retrieval of documentation.\',',
          ' \'processed_logs\': [\'failure-analysis-on-log-1\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Subgrafo-de-resumen-de-logs">Subgrafo de resumen de logs<a class="anchor-link" href="#Subgrafo-de-resumen-de-logs"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 29" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos el subgrafo de resumen de logs. En este caso no hace falta volver a crear la clase con la estructura de los logs, por lo que creamos las clases con la estructura para los res煤menes de los logs y con la estructura de la salida.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Summarization subgraph</span>',
      '<span class="k">class</span> <span class="nc">QuestionSummarizationState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>',
      '    <span class="n">qs_summary</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">report</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>',
      ' ',
      '<span class="k">class</span> <span class="nc">QuestionSummarizationOutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">report</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <p>Ahora definimos las funciones de los nodos, una generar谩 el resumen de los logs y otra "enviar谩 el resumen a Slack".</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_summary</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>',
      '    <span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"cleaned_logs"</span><span class="p">]</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug generate_summary: cleaned_logs: </span><span class="si">{</span><span class="n">cleaned_logs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      ' ',
      '    <span class="n">summary</span> <span class="o">=</span> <span class="s2">"Questions focused on ..."</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug generate_summary: summary: </span><span class="si">{</span><span class="n">summary</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      ' ',
      '    <span class="n">processed_logs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"summary-on-log-</span><span class="si">{</span><span class="n">log</span><span class="p">[</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">cleaned_logs</span><span class="p">]</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug generate_summary: processed_logs: </span><span class="si">{</span><span class="n">processed_logs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      ' ',
      '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"qs_summary"</span><span class="p">:</span> <span class="n">summary</span><span class="p">,</span> <span class="s2">"processed_logs"</span><span class="p">:</span> <span class="n">processed_logs</span><span class="p">{closing_brace}</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">send_to_slack</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>',
      '    <span class="n">qs_summary</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"qs_summary"</span><span class="p">]</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug send_to_slack: qs_summary: </span><span class="si">{</span><span class="n">qs_summary</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      ' ',
      '    <span class="n">report</span> <span class="o">=</span> <span class="s2">"foo bar baz"</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2"> debug send_to_slack: report: </span><span class="si">{</span><span class="n">report</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      ' ',
      '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"report"</span><span class="p">:</span> <span class="n">report</span><span class="p">{closing_brace}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>

























      
      <section class="section-block-markdown-cell">
      <p>Por 煤ltimo, creamos el grafo, a帽adimos los nodos y los <code>edge</code>s y lo compilamos.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="c1"># Build the graph</span>
      <span class="n">qs_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">QuestionSummarizationState</span><span class="p">,</span><span class="n">output</span><span class="o">=</span><span class="n">QuestionSummarizationOutputState</span><span class="p">)</span>
      
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"generate_summary"</span><span class="p">,</span> <span class="n">generate_summary</span><span class="p">)</span>
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"send_to_slack"</span><span class="p">,</span> <span class="n">send_to_slack</span><span class="p">)</span>
      
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"generate_summary"</span><span class="p">)</span>
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"generate_summary"</span><span class="p">,</span> <span class="s2">"send_to_slack"</span><span class="p">)</span>
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"send_to_slack"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="n">graph</span> <span class="o">=</span> <span class="n">qs_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv16.webp" width="184" height="333" alt="image uv 17" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a probar con el log que creamos antes.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({opening_brace}</span><span class="s2">"cleaned_logs"</span><span class="p">:</span> <span class="p">[</span><span class="n">failure_log</span><span class="p">]{closing_brace})</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>	 debug generate_summary: cleaned_logs: [{opening_brace}'id': '1', 'question': 'What is the meaning of life?', 'docs': None, 'answer': '42', 'grade': 1, 'grader': 'AI', 'feedback': 'Good job!'{closing_brace}&rbrack;
      	 debug generate_summary: summary: Questions focused on ...
      	 debug generate_summary: processed_logs: ['summary-on-log-1']
      	 debug send_to_slack: qs_summary: Questions focused on ...
      	 debug send_to_slack: report: foo bar baz
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[25]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>{opening_brace}'report': 'foo bar baz', 'processed_logs': ['summary-on-log-1']{closing_brace}</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a escribir el subgrafo, todo junto para ver con mayor claridad y sin los <code>print</code>s.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="c1"># Summarization clases</span>
      <span class="k">class</span> <span class="nc">QuestionSummarizationState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
          <span class="n">qs_summary</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">report</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
      
      <span class="k">class</span> <span class="nc">QuestionSummarizationOutputState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">report</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">processed_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
      
      <span class="c1"># Functions</span>
      <span class="k">def</span> <span class="nf">generate_summary</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"cleaned_logs"</span><span class="p">]</span>
          <span class="n">summary</span> <span class="o">=</span> <span class="s2">"Questions focused on ..."</span>
          <span class="n">processed_logs</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"summary-on-log-</span><span class="si">{opening_brace}</span><span class="n">log</span><span class="p">[</span><span class="s1">'id'</span><span class="p">]</span><span class="si">{closing_brace}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">cleaned_logs</span><span class="p">]</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"qs_summary"</span><span class="p">:</span> <span class="n">summary</span><span class="p">,</span> <span class="s2">"processed_logs"</span><span class="p">:</span> <span class="n">processed_logs</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">send_to_slack</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="n">qs_summary</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"qs_summary"</span><span class="p">]</span>
          <span class="n">report</span> <span class="o">=</span> <span class="s2">"foo bar baz"</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"report"</span><span class="p">:</span> <span class="n">report</span><span class="p">{closing_brace}</span>
      
      <span class="c1"># Build the graph</span>
      <span class="n">qs_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">QuestionSummarizationState</span><span class="p">,</span><span class="n">output</span><span class="o">=</span><span class="n">QuestionSummarizationOutputState</span><span class="p">)</span>
      
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"generate_summary"</span><span class="p">,</span> <span class="n">generate_summary</span><span class="p">)</span>
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"send_to_slack"</span><span class="p">,</span> <span class="n">send_to_slack</span><span class="p">)</span>
      
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"generate_summary"</span><span class="p">)</span>
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"generate_summary"</span><span class="p">,</span> <span class="s2">"send_to_slack"</span><span class="p">)</span>
      <span class="n">qs_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"send_to_slack"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="n">graph</span> <span class="o">=</span> <span class="n">qs_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv17.webp" width="184" height="333" alt="image uv 18" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ejecutar el grafo con el log de prueba.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"cleaned_logs"</span><span class="p">:</span> <span class="p">[</span><span class="n">failure_log</span><span class="p">]})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'report\': \'foo bar baz\', \'processed_logs\': [\'summary-on-log-1\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Grafo-principal">Grafo principal<a class="anchor-link" href="#Grafo-principal"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos los dos subgrafos, podemos crear el grafo principal que los usar谩. Para ello creamos la clase <code>EntryGraphState</code> que tendr谩 el estado de los dos subgrafos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Entry Graph</span>',
      '<span class="k">class</span> <span class="nc">EntryGraphState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">raw_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>',
      '    <span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>',
      '    <span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span> <span class="c1"># This will only be generated in the FA sub-graph</span>',
      '    <span class="n">report</span><span class="p">:</span> <span class="nb">str</span> <span class="c1"># This will only be generated in the QS sub-graph</span>',
      '    <span class="n">processed_logs</span><span class="p">:</span>  <span class="n">Annotated</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">add</span><span class="p">]</span> <span class="c1"># This will be generated in BOTH sub-graphs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>












      
      <section class="section-block-markdown-cell">
      <p>Creamos una funci贸n de limpieza de logs, que ser谩 un nodo que se ejecutar谩 antes de los dos subgrafos y que les aportar谩 los logs limpios y a trav茅s de la key <code>cleaned_logs</code>, que es la que los dos subgrafos toman del estado.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">clean_logs</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>',
      '    <span class="c1"># Get logs</span>',
      '    <span class="n">raw_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"raw_logs"</span><span class="p">]</span>',
      '    <span class="c1"># Data cleaning raw_logs -&gt; docs </span>',
      '    <span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">raw_logs</span>',
      '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"cleaned_logs"</span><span class="p">:</span> <span class="n">cleaned_logs</span><span class="p">{closing_brace}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos el grafo principal</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Build the graph</span>',
      '<span class="n">entry_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">EntryGraphState</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>A帽adimos los nodos. Para a帽adir un subgrafo como nodo, lo que hacemos es a帽adir su compilaci贸n</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Add nodes</span>',
          '<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"clean_logs"</span><span class="p">,</span> <span class="n">clean_logs</span><span class="p">)</span>',
          '<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"question_summarization"</span><span class="p">,</span> <span class="n">qs_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">())</span>',
          '<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"failure_analysis"</span><span class="p">,</span> <span class="n">fa_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">())</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x107985ef0&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A partir de aqu铆 ya es como si siempre, a帽adimos los <code>edge</code>s y lo compilamos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Add edges</span>',
          '<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"clean_logs"</span><span class="p">)</span>',
          '<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"clean_logs"</span><span class="p">,</span> <span class="s2">"failure_analysis"</span><span class="p">)</span>',
          '<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"clean_logs"</span><span class="p">,</span> <span class="s2">"question_summarization"</span><span class="p">)</span>',
          '<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"failure_analysis"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>',
          '<span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"question_summarization"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>',
          '',
          '<span class="c1"># Compile the graph</span>',
          '<span class="n">graph</span> <span class="o">=</span> <span class="n">entry_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.',
          'Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.',
          'Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.',
          'Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.',
          'Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Por 煤ltimo, mostramos el grafo. A帽adimos <code>xray=1</code> para que se vea el estado interno del grafo.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="c1"># Setting xray to 1 will show the internal structure of the nested graph</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">(</span><span class="n">xray</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv18.webp" width="512" height="482" alt="image uv 19" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si no hubi茅semos a帽adido <code>xray=1</code>, el grafo se ver铆a as铆</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv19.webp" width="410" height="333" alt="image uv 20" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos dos logs de prueba, en uno habr谩 un error (un valor en <code>grade</code>) y en el otro no.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Dummy logs</span>',
      '<span class="n">question_answer</span> <span class="o">=</span> <span class="n">Log</span><span class="p">(</span>',
      '    <span class="nb">id</span><span class="o">=</span><span class="s2">"1"</span><span class="p">,</span>',
      '    <span class="n">question</span><span class="o">=</span><span class="s2">"How can I import ChatOllama?"</span><span class="p">,</span>',
      '    <span class="n">answer</span><span class="o">=</span><span class="s2">"To import ChatOllama, use: \'from langchain_community.chat_models import ChatOllama.\'"</span><span class="p">,</span>',
      '<span class="p">)</span>',
      ' ',
      '<span class="n">question_answer_feedback</span> <span class="o">=</span> <span class="n">Log</span><span class="p">(</span>',
      '    <span class="nb">id</span><span class="o">=</span><span class="s2">"2"</span><span class="p">,</span>',
      '    <span class="n">question</span><span class="o">=</span><span class="s2">"How can I use Chroma vector store?"</span><span class="p">,</span>',
      '    <span class="n">answer</span><span class="o">=</span><span class="s2">"To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain)."</span><span class="p">,</span>',
      '    <span class="n">grade</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>',
      '    <span class="n">grader</span><span class="o">=</span><span class="s2">"Document Relevance Recall"</span><span class="p">,</span>',
      '    <span class="n">feedback</span><span class="o">=</span><span class="s2">"The retrieved documents discuss vector stores in general, but not Chroma specifically"</span><span class="p">,</span>',
      '<span class="p">)</span>',
      ' ',
      '<span class="n">raw_logs</span> <span class="o">=</span> <span class="p">[</span><span class="n">question_answer</span><span class="p">,</span><span class="n">question_answer_feedback</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






















      
      <section class="section-block-markdown-cell">
      <p>Se los pasamos al grafo principal</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"raw_logs"</span><span class="p">:</span> <span class="n">raw_logs</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'raw_logs\': &lbrack;{opening_brace}\'id\': \'1\',',
          '   \'question\': \'How can I import ChatOllama?\',',
          '   \'answer\': "To import ChatOllama, use: \'from langchain_community.chat_models import ChatOllama.\'"{closing_brace},',
          '  {opening_brace}\'id\': \'2\',',
          '   \'question\': \'How can I use Chroma vector store?\',',
          '   \'answer\': \'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\',',
          '   \'grade\': 0,',
          '   \'grader\': \'Document Relevance Recall\',',
          '   \'feedback\': \'The retrieved documents discuss vector stores in general, but not Chroma specifically\'{closing_brace}&rbrack;,',
          ' \'cleaned_logs\': &lbrack;{opening_brace}\'id\': \'1\',',
          '   \'question\': \'How can I import ChatOllama?\',',
          '   \'answer\': "To import ChatOllama, use: \'from langchain_community.chat_models import ChatOllama.\'"{closing_brace},',
          '  {opening_brace}\'id\': \'2\',',
          '   \'question\': \'How can I use Chroma vector store?\',',
          '   \'answer\': \'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\',',
          '   \'grade\': 0,',
          '   \'grader\': \'Document Relevance Recall\',',
          '   \'feedback\': \'The retrieved documents discuss vector stores in general, but not Chroma specifically\'{closing_brace}&rbrack;,',
          ' \'fa_summary\': \'Poor quality retrieval of documentation.\',',
          ' \'report\': \'foo bar baz\',',
          ' \'processed_logs\': [\'failure-analysis-on-log-2\',',
          '  \'summary-on-log-1\',',
          '  \'summary-on-log-2\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Al igual que antes, escribimos todo el grafo para verlo con mayor claridad</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="c1"># Entry Graph</span>
      <span class="k">class</span> <span class="nc">EntryGraphState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">raw_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
          <span class="n">cleaned_logs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Log</span><span class="p">]</span>
          <span class="n">fa_summary</span><span class="p">:</span> <span class="nb">str</span> <span class="c1"># This will only be generated in the FA sub-graph</span>
          <span class="n">report</span><span class="p">:</span> <span class="nb">str</span> <span class="c1"># This will only be generated in the QS sub-graph</span>
          <span class="n">processed_logs</span><span class="p">:</span>  <span class="n">Annotated</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">add</span><span class="p">]</span> <span class="c1"># This will be generated in BOTH sub-graphs</span>
      
      <span class="c1"># Functions</span>
      <span class="k">def</span> <span class="nf">clean_logs</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
          <span class="c1"># Get logs</span>
          <span class="n">raw_logs</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"raw_logs"</span><span class="p">]</span>
          <span class="c1"># Data cleaning raw_logs -&gt; docs </span>
          <span class="n">cleaned_logs</span> <span class="o">=</span> <span class="n">raw_logs</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"cleaned_logs"</span><span class="p">:</span> <span class="n">cleaned_logs</span><span class="p">{closing_brace}</span>
      
      <span class="c1"># Build the graph</span>
      <span class="n">entry_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">EntryGraphState</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"clean_logs"</span><span class="p">,</span> <span class="n">clean_logs</span><span class="p">)</span>
      <span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"question_summarization"</span><span class="p">,</span> <span class="n">qs_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">())</span>
      <span class="n">entry_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"failure_analysis"</span><span class="p">,</span> <span class="n">fa_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">())</span>
      
      <span class="c1"># Add edges</span>
      <span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"clean_logs"</span><span class="p">)</span>
      <span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"clean_logs"</span><span class="p">,</span> <span class="s2">"failure_analysis"</span><span class="p">)</span>
      <span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"clean_logs"</span><span class="p">,</span> <span class="s2">"question_summarization"</span><span class="p">)</span>
      <span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"failure_analysis"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      <span class="n">entry_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"question_summarization"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">entry_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># Setting xray to 1 will show the internal structure of the nested graph</span>
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">(</span><span class="n">xray</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv20.webp" width="512" height="482" alt="image uv 21" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Le pasamos los logs de prueba al grafo principal</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"raw_logs"</span><span class="p">:</span> <span class="n">raw_logs</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'raw_logs\': &lbrack;{opening_brace}\'id\': \'1\',',
          '   \'question\': \'How can I import ChatOllama?\',',
          '   \'answer\': "To import ChatOllama, use: \'from langchain_community.chat_models import ChatOllama.\'"{closing_brace},',
          '  {opening_brace}\'id\': \'2\',',
          '   \'question\': \'How can I use Chroma vector store?\',',
          '   \'answer\': \'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\',',
          '   \'grade\': 0,',
          '   \'grader\': \'Document Relevance Recall\',',
          '   \'feedback\': \'The retrieved documents discuss vector stores in general, but not Chroma specifically\'{closing_brace}&rbrack;,',
          ' \'cleaned_logs\': &lbrack;{opening_brace}\'id\': \'1\',',
          '   \'question\': \'How can I import ChatOllama?\',',
          '   \'answer\': "To import ChatOllama, use: \'from langchain_community.chat_models import ChatOllama.\'"{closing_brace},',
          '  {opening_brace}\'id\': \'2\',',
          '   \'question\': \'How can I use Chroma vector store?\',',
          '   \'answer\': \'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\',',
          '   \'grade\': 0,',
          '   \'grader\': \'Document Relevance Recall\',',
          '   \'feedback\': \'The retrieved documents discuss vector stores in general, but not Chroma specifically\'{closing_brace}&rbrack;,',
          ' \'fa_summary\': \'Poor quality retrieval of documentation.\',',
          ' \'report\': \'foo bar baz\',',
          ' \'processed_logs\': [\'failure-analysis-on-log-2\',',
          '  \'summary-on-log-1\',',
          '  \'summary-on-log-2\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Ramas-din%C3%A1micas">Ramas din谩micas<a class="anchor-link" href="#Ramas-din%C3%A1micas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hasta ahora hemos creado nodos y <code>edge</code>s est谩ticos, pero hay veces en las que no sabemos si vamos a necesitar una rama hasta que se ejecute el grafo. Para ello, podemos usar el m茅todo <code>SEND</code> de langgraph, que permite crear ramas din谩micamente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para verlo, vamos a crear un grafo que genere chistes sobre unos temas, pero como no sabemos de antemano sobre cu谩ntos temas queremos generar chistes, mediante el m茅todo <code>SEND</code> vamos a crear ramas din谩micamente, de manera que si quedan temas por generar, se crear谩 una nueva rama.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Nota: Este apartado lo vamos a hacer usando Sonnet 3.7, ya que la integraci贸n de HuggingFace no tiene la funcionalidad de <code>with_structured_output</code> que proporciona una salida extructurada con un estructura definida.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero importamos las librer铆as necesarias.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">operator</span>',
      '<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
      '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>',
      ' ',
      '<span class="kn">import</span> <span class="nn">os</span>',
      '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
      ' ',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      '<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>





















      
      <section class="section-block-markdown-cell">
      <p>Creamos las clases con la estructura del estado.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">class</span> <span class="nc">OverallState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">topic</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">subjects</span><span class="p">:</span> <span class="nb">list</span>',
      '    <span class="n">jokes</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">]</span>',
      '    <span class="n">best_selected_joke</span><span class="p">:</span> <span class="nb">str</span>',
      ' ',
      '<span class="k">class</span> <span class="nc">JokeState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">subject</span><span class="p">:</span> <span class="nb">str</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>Creamos el LLM</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Create the LLM model</span>',
      '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Creamos la funci贸n que generar谩 los temas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a usar <code>with_structured_output</code> para que el LLM genere una salida con una estructura definida por nosotros, esa estructura la vamos a definir con la clase <code>Subjects</code> que es una clase de tipo <code>BaseModel</code> de <code>Pydantic</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>',
      ' ',
      '<span class="k">class</span> <span class="nc">Subjects</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>',
      '    <span class="n">subjects</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>',
      ' ',
      '<span class="n">subjects_prompt</span> <span class="o">=</span> <span class="s2">"""Generate a list of 3 sub-topics that are all related to this overall topic: </span><span class="si">{topic}</span><span class="s2">."""</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">generate_topics</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>',
      '    <span class="n">prompt</span> <span class="o">=</span> <span class="n">subjects_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">"topic"</span><span class="p">])</span>',
      '    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Subjects</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>',
      '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"subjects"</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">subjects</span><span class="p">{closing_brace}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
















      
      <section class="section-block-markdown-cell">
      <p>Ahora definimos la funci贸n que generar谩 los chistes.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">class</span> <span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>',
      '    <span class="n">joke</span><span class="p">:</span> <span class="nb">str</span>',
      '    ',
      '<span class="n">joke_prompt</span> <span class="o">=</span> <span class="s2">"""Generate a joke about </span><span class="si">{subject}</span><span class="s2">"""</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">generate_joke</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">JokeState</span><span class="p">):</span>',
      '    <span class="n">prompt</span> <span class="o">=</span> <span class="n">joke_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">subject</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">"subject"</span><span class="p">])</span>',
      '    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Joke</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>',
      '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"jokes"</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="o">.</span><span class="n">joke</span><span class="p">]}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <p>Y por 煤ltimo la funci贸n que seleccionar谩 el mejor chiste.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">class</span> <span class="nc">BestJoke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>',
      '    <span class="nb">id</span><span class="p">:</span> <span class="nb">int</span>',
      ' ',
      '<span class="n">best_joke_prompt</span> <span class="o">=</span> <span class="s2">"""Below are a bunch of jokes about </span><span class="si">{topic}</span><span class="s2">. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: </span><span class="se">\\n\\n</span><span class="s2">  </span><span class="si">{jokes}</span><span class="s2">"""</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">best_joke</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>',
      '    <span class="n">jokes</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\n\\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"jokes"</span><span class="p">])</span>',
      '    <span class="n">prompt</span> <span class="o">=</span> <span class="n">best_joke_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">"topic"</span><span class="p">],</span> <span class="n">jokes</span><span class="o">=</span><span class="n">jokes</span><span class="p">)</span>',
      '    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">BestJoke</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>',
      '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"best_selected_joke"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">"jokes"</span><span class="p">][</span><span class="n">response</span><span class="o">.</span><span class="n">id</span><span class="p">]}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <p>Ahora vamos a crear una funci贸n que decida si crear una nueva rama con <code>SEND</code> o no, y para decidirlo comprobar谩 si quedan temas por generar.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langgraph.constants</span> <span class="kn">import</span> <span class="n">Send</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">continue_to_jokes</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>',
      '    <span class="k">return</span> <span class="p">[</span><span class="n">Send</span><span class="p">(</span><span class="s2">"generate_joke"</span><span class="p">,</span> <span class="p">{opening_brace}</span><span class="s2">"subject"</span><span class="p">:</span> <span class="n">s</span><span class="p">})</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"subjects"</span><span class="p">]]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Construimos el grafo, a帽adimos los nodos y los <code>edge</code>s.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="c1"># Build the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">OverallState</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"generate_topics"</span><span class="p">,</span> <span class="n">generate_topics</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"generate_joke"</span><span class="p">,</span> <span class="n">generate_joke</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"best_joke"</span><span class="p">,</span> <span class="n">best_joke</span><span class="p">)</span>
      
      <span class="c1"># Add edges</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"generate_topics"</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">"generate_topics"</span><span class="p">,</span> <span class="n">continue_to_jokes</span><span class="p">,</span> <span class="p">[</span><span class="s2">"generate_joke"</span><span class="p">])</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"generate_joke"</span><span class="p">,</span> <span class="s2">"best_joke"</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"best_joke"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">app</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># Display the graph</span>
      <span class="n">Image</span><span class="p">(</span><span class="n">app</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">())</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt-output-prompt">Out[8]:</div>
      <div class="output-png-output-subareaoutput_execute_result">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv21.webp" width="160" height="432" alt="image uv 22" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver el <code>edge</code> entre <code>generate_topics</code> y <code>generate_joke</code> se representa con una l铆nea discontinua, lo que indica que es una rama din谩mica.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos ahora un diccionario con la key <code>topic</code> que es la que necesita el nodo <code>generate_topics</code> para generar los temas y se lo pasamos al grafo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Call the graph: here we call it to generate a list of jokes</span>',
          '<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">app</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"topic"</span><span class="p">:</span> <span class="s2">"animals"</span><span class="p">}):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'generate_topics\': {\'subjects\': [\'Marine Animals\', \'Endangered Species\', \'Animal Behavior\']{closing_brace}{closing_brace}',
          '{opening_brace}\'generate_joke\': {\'jokes\': ["Why don\'t cats play poker in the wild? Too many cheetahs!"]{closing_brace}{closing_brace}',
          '{opening_brace}\'generate_joke\': {\'jokes\': ["Why don\'t sharks eat clownfish? Because they taste funny!"]{closing_brace}{closing_brace}',
          '{opening_brace}\'generate_joke\': {\'jokes\': ["Why don\'t endangered species tell jokes? Because they\'re afraid of dying out from laughter!"]{closing_brace}{closing_brace}',
          '{opening_brace}\'best_joke\': {\'best_selected_joke\': "Why don\'t cats play poker in the wild? Too many cheetahs!"{closing_brace}{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a crear el grafo con todo el c贸digo junto para mayor claridad.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">operator</span>
      <span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
      
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">END</span><span class="p">,</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span>
      <span class="kn">from</span> <span class="nn">langgraph.constants</span> <span class="kn">import</span> <span class="n">Send</span>
      
      <span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>
      
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="kn">import</span> <span class="nn">dotenv</span>
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>
      
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
      
      <span class="c1"># Prompts we will use</span>
      <span class="n">subjects_prompt</span> <span class="o">=</span> <span class="s2">"""Generate a list of 3 sub-topics that are all related to this overall topic: </span><span class="si">{opening_brace}topic{closing_brace}</span><span class="s2">."""</span>
      <span class="n">joke_prompt</span> <span class="o">=</span> <span class="s2">"""Generate a joke about </span><span class="si">{opening_brace}subject{closing_brace}</span><span class="s2">"""</span>
      <span class="n">best_joke_prompt</span> <span class="o">=</span> <span class="s2">"""Below are a bunch of jokes about </span><span class="si">{opening_brace}topic{closing_brace}</span><span class="s2">. Select the best one! Return the ID of the best one, starting 0 as the ID for the first joke. Jokes: </span><span class="se">\n\n</span><span class="s2">  </span><span class="si">{opening_brace}jokes{closing_brace}</span><span class="s2">"""</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">Subjects</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
          <span class="n">subjects</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
      
      <span class="k">class</span> <span class="nc">BestJoke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
          <span class="nb">id</span><span class="p">:</span> <span class="nb">int</span>
          
      <span class="k">class</span> <span class="nc">OverallState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">topic</span><span class="p">:</span> <span class="nb">str</span>
          <span class="n">subjects</span><span class="p">:</span> <span class="nb">list</span>
          <span class="n">jokes</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">operator</span><span class="o">.</span><span class="n">add</span><span class="p">]</span>
          <span class="n">best_selected_joke</span><span class="p">:</span> <span class="nb">str</span>
      
      <span class="k">class</span> <span class="nc">JokeState</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">subject</span><span class="p">:</span> <span class="nb">str</span>
      
      <span class="k">class</span> <span class="nc">Joke</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
          <span class="n">joke</span><span class="p">:</span> <span class="nb">str</span>
      
      <span class="k">def</span> <span class="nf">generate_topics</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>
          <span class="n">prompt</span> <span class="o">=</span> <span class="n">subjects_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">"topic"</span><span class="p">])</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Subjects</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"subjects"</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">subjects</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">continue_to_jokes</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>
          <span class="k">return</span> <span class="p">[</span><span class="n">Send</span><span class="p">(</span><span class="s2">"generate_joke"</span><span class="p">,</span> <span class="p">{opening_brace}</span><span class="s2">"subject"</span><span class="p">:</span> <span class="n">s</span><span class="p">{closing_brace})</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"subjects"</span><span class="p">]]</span>
      
      <span class="k">def</span> <span class="nf">generate_joke</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">JokeState</span><span class="p">):</span>
          <span class="n">prompt</span> <span class="o">=</span> <span class="n">joke_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">subject</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">"subject"</span><span class="p">])</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">Joke</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"jokes"</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="o">.</span><span class="n">joke</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">best_joke</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">OverallState</span><span class="p">):</span>
          <span class="n">jokes</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"jokes"</span><span class="p">])</span>
          <span class="n">prompt</span> <span class="o">=</span> <span class="n">best_joke_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">topic</span><span class="o">=</span><span class="n">state</span><span class="p">[</span><span class="s2">"topic"</span><span class="p">],</span> <span class="n">jokes</span><span class="o">=</span><span class="n">jokes</span><span class="p">)</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">BestJoke</span><span class="p">)</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"best_selected_joke"</span><span class="p">:</span> <span class="n">state</span><span class="p">[</span><span class="s2">"jokes"</span><span class="p">][</span><span class="n">response</span><span class="o">.</span><span class="n">id</span><span class="p">]{closing_brace}</span>
      
      <span class="c1"># Build the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">OverallState</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"generate_topics"</span><span class="p">,</span> <span class="n">generate_topics</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"generate_joke"</span><span class="p">,</span> <span class="n">generate_joke</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"best_joke"</span><span class="p">,</span> <span class="n">best_joke</span><span class="p">)</span>
      
      <span class="c1"># Add edges</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"generate_topics"</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">"generate_topics"</span><span class="p">,</span> <span class="n">continue_to_jokes</span><span class="p">,</span> <span class="p">[</span><span class="s2">"generate_joke"</span><span class="p">])</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"generate_joke"</span><span class="p">,</span> <span class="s2">"best_joke"</span><span class="p">)</span>
      <span class="n">graph</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"best_joke"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">app</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="c1"># Display the graph</span>
      <span class="n">Image</span><span class="p">(</span><span class="n">app</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">())</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt-output-prompt">Out[1]:</div>
      <div class="output-png-output-subareaoutput_execute_result">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv22.webp" width="160" height="432" alt="image uv 23" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ejecutarlo, pero ahora, en vez de con <code>animales</code>, lo vamos a hacer con <code>coches</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">app</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"topic"</span><span class="p">:</span> <span class="s2">"cars"</span><span class="p">}):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'generate_topics\': {\'subjects\': [\'Car Maintenance and Repair\', \'Electric and Hybrid Vehicles\', \'Automotive Design and Engineering\']{closing_brace}{closing_brace}',
          '{opening_brace}\'generate_joke\': {\'jokes\': ["Why don\'t electric cars tell jokes? They\'re afraid of running out of charge before they get to the punchline!"]{closing_brace}{closing_brace}',
          '{opening_brace}\'generate_joke\': {\'jokes\': ["Why don\'t automotive engineers play hide and seek? Because good luck hiding when you\'re always making a big noise about torque!"]{closing_brace}{closing_brace}',
          '{opening_brace}\'generate_joke\': {\'jokes\': ["Why don\'t cars ever tell their own jokes? Because they always exhaust themselves during the delivery! Plus, their timing belts are always a little off."]{closing_brace}{closing_brace}',
          '{opening_brace}\'best_joke\': {\'best_selected_joke\': "Why don\'t electric cars tell jokes? They\'re afraid of running out of charge before they get to the punchline!"{closing_brace}{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Mejorar-el-chatbot-con-herramientas">Mejorar el chatbot con herramientas<a class="anchor-link" href="#Mejorar-el-chatbot-con-herramientas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 32" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para manejar algunas consultas, nuestro chatbot no puede responder <code>desde su conocimiento</code>, as铆 que vamos a integrar una herramienta de b煤squeda web. Nuestro bot puede utilizar esta herramienta para encontrar informaci贸n relevante y proporcionar mejores respuestas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Requisitos">Requisitos<a class="anchor-link" href="#Requisitos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de comenzar, tenemos que instalar el buscador <a href="https://python.langchain.com/docs/integrations/tools/tavily_search/" target="_blank" rel="nofollow noreferrer">Tavily</a> que es un buscador web que nos permite buscar informaci贸n en la web.</p>
      <div class="highlight">
        <pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>tavily-python<span class="w"> </span>langchain_community</pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Despu茅s, tenemos que crear una <a href="https://app.tavily.com/home" target="_blank" rel="nofollow noreferrer">API KEY</a>, la escribimos en nuestro archivo <code>.env</code> y la cargamos en una variable.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">dotenv</span>',
      '<span class="kn">import</span> <span class="nn">os</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      ' ',
      '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
      '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>












      
      <section class="section-block-markdown-cell">
      <h3 id="Chatbot-con-tools">Chatbot con tools<a class="anchor-link" href="#Chatbot-con-tools"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero creamos el estado y el LLM</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
      '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
      '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
      '<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>',
      ' ',
      '<span class="kn">import</span> <span class="nn">json</span>',
      '<span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>',
      ' ',
      '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
      ' ',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
      ' ',
      '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
      ' ',
      '<span class="c1"># Create the LLM</span>',
      '<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>',
      '<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
      '    <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>',
      '    <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>',
      '    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
      '    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="c1"># Create the chat model</span>',
      '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







































      
      <section class="section-block-markdown-cell">
      <p>Ahora, definimos la herramienta de b煤squeda web mediante <a href="https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html" target="_blank" rel="nofollow noreferrer">TavilySearchResults</a></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
      '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
      ' ',
      '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
      ' ',
      '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
      '<span class="n">tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>












      
      <section class="section-block-markdown-cell">
      <p>Probamos la herramienta, vamos a hacer una b煤squeda en Internet</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">tool</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">"What was the result of Real Madrid's at last match in the Champions League?"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[3]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>[{opening_brace}'title': 'HIGHLIGHTS | Real Madrid 3-2 Legan茅s | LaLiga 2024/25 - YouTube',
        'url': 'https://www.youtube.com/watch?v=Np-Kwz4RDpY',
        'content': "20:14 路 Go to channel 路 RONALDO'S LAST MATCH WITH REAL MADRID: THE MOST THRILLING FINAL EVER! ... Champions League 1/4 Final | PES. Football",
        'score': 0.65835214{closing_brace},
       {opening_brace}'title': 'Real Madrid | History | UEFA Champions League',
        'url': 'https://www.uefa.com/uefachampionsleague/history/clubs/50051--real-madrid/',
        'content': '1955/56 P W D L Final 7 5 0 2\nUEFA Champions League [...] 2010/11 P W D L Semi-finals 12 8 3 1\n2009/10 P W D L Round of 16 8 4 2 2\n2000s\n2008/09 P W D L Round of 16 8 4 0 4\n2007/08 P W D L Round of 16 8 3 2 3\n2006/07 P W D L Round of 16 8 4 2 2\n2005/06 P W D L Round of 16 8 3 2 3\n2004/05 P W D L Round of 16 10 6 2 2\n2003/04 P W D L Quarter-finals 10 6 3 1\n2002/03 P W D L Semi-finals 16 7 5 4\n2001/02 P W D L Final 17 12 3 2\n2000/01 P W D L Semi-finals 16 9 2 5\n1990s\n1999/00 P W D L Final 17 10 3 4\n1998/99 P W D L Quarter-finals 8 4 1 3 [...] 1969/70 P W D L Second round 4 2 0 2\n1968/69 P W D L Second round 4 3 0 1\n1967/68 P W D L Semi-finals 8 2 4 2\n1966/67 P W D L Quarter-finals 4 1 0 3\n1965/66 P W D L Final 9 5 2 2\n1964/65 P W D L Quarter-finals 6 4 1 1\n1963/64 P W D L Final 9 7 0 2\n1962/63 P W D L Preliminary round 2 0 1 1\n1961/62 P W D L Final 10 8 0 2\n1960/61 P W D L First round 2 0 1 1\n1950s\n1959/60 P W D L Final 7 6 0 1\n1958/59 P W D L Final 8 5 2 1\n1957/58 P W D L Final 7 5 1 1\n1956/57 P W D L Final 8 6 1 1',
        'score': 0.6030211{closing_brace}]</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los resultados son res煤menes de p谩ginas que nuestro chatbot puede usar para responder preguntas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una lista de herramientas, porque nuestro grafo necesita definir las herramientas mediante una lista.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos la lista de <code>tool</code>s creamos un <code>llm_with_tools</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Modification: tell the LLM which tools it can call</span>',
      '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Definimos la funci贸n que ir谩 en el nodo chat bot</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Define the chatbot function</span>',
      '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
      '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Necesitamos crear una funci贸n para ejecutar las <code>tools_list</code> si se llaman. Agregamos las <code>tools_list</code> a un nuevo nodo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>M谩s tarde haremos esto con el m茅todo <a href="https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode" target="_blank" rel="nofollow noreferrer">ToolNode</a> de <code>LangGraph</code>, pero primero lo construiremos nosotros mismos para entender c贸mo funciona.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a implementar la clase <code>BasicToolNode</code>, que comprueba el mensaje m谩s reciente en el estado y llama a las <code>tools_list</code> si el mensaje contiene <code>tool_calls</code>.</p>
      <p>Se basa en el soporte de <code>tool_calling</code> de los <code>LLM</code>s , que est谩 disponible en <code>Anthropic</code>, <code>HuggingFace</code>, <code>Google Gemini</code>, <code>OpenAI</code> y varios otros proveedores de <code>LLM</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">ToolMessage</span>',
      ' ',
      '<span class="k">class</span> <span class="nc">BasicToolNode</span><span class="p">:</span>',
      '<span class="w">    </span><span class="sd">"""A node that runs the tools requested in the last AIMessage."""</span>',
      ' ',
      '    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tools</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>',
      '<span class="w">        </span><span class="sd">"""</span>',
      '<span class="sd">        Initialize the tools</span>',
      ' ',
      '<span class="sd">        Args:</span>',
      '<span class="sd">            tools (list): The tools to use</span>',
      ' ',
      '<span class="sd">        Returns:</span>',
      '<span class="sd">            None</span>',
      '<span class="sd">        """</span>',
      '        <span class="c1"># Initialize the tools</span>',
      '        <span class="bp">self</span><span class="o">.</span><span class="n">tools_by_name</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="n">tool</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">tool</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span><span class="p">{closing_brace}</span>',
      ' ',
      '    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>',
      '<span class="w">        </span><span class="sd">"""</span>',
      '<span class="sd">        Call the node</span>',
      ' ',
      '<span class="sd">        Args:</span>',
      '<span class="sd">            inputs (dict): The inputs to the node</span>',
      ' ',
      '<span class="sd">        Returns:</span>',
      '<span class="sd">            dict: The outputs of the node</span>',
      '<span class="sd">        """</span>',
      '        <span class="c1"># Get the last message</span>',
      '        <span class="k">if</span> <span class="n">messages</span> <span class="o">:=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"messages"</span><span class="p">,</span> <span class="p">[]):</span>',
      '            <span class="n">message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '        <span class="k">else</span><span class="p">:</span>',
      '            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"No message found in input"</span><span class="p">)</span>',
      '        ',
      '        <span class="c1"># Execute the tools</span>',
      '        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>',
      '        <span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">:</span>',
      '            <span class="n">tool_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tools_by_name</span><span class="p">[</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]]</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>',
      '                <span class="n">tool_call</span><span class="p">[</span><span class="s2">"args"</span><span class="p">]</span>',
      '            <span class="p">)</span>',
      '            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>',
      '                <span class="n">ToolMessage</span><span class="p">(</span>',
      '                    <span class="n">content</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tool_result</span><span class="p">),</span>',
      '                    <span class="n">name</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span>',
      '                    <span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"id"</span><span class="p">],</span>',
      '                <span class="p">)</span>',
      '            <span class="p">)</span>',
      '        <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">outputs</span><span class="p">{closing_brace}</span>',
      ' ',
      '<span class="n">basic_tool_node</span> <span class="o">=</span> <span class="n">BasicToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>























































      
      <section class="section-block-markdown-cell">
      <p>Hemos usado <a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html" target="_blank" rel="nofollow noreferrer">ToolMessage</a> que pasa el resultado de ejecutar una <code>tool</code> de nuevo al <code>LLM</code>.</p>
      <p><code>ToolMessage</code> contiene el resultado de una invocaci贸n de una <code>tool</code>.</p>
      <p>Es decir, en cuanto tenemos el resultado de usar una <code>Tool</code> se lo pasamos al LLM para que lo procese</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Con el objeto de <code>basic_tool_node</code> (que es un objeto de la clase <code>BasicToolNode</code> que hemos creado) ya podemos hacer que el LLM ejecute <code>tool</code>s</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora, igual que hicimos cuando construimos un chatbot b谩sico, vamos a crear el grafo y a帽adirle nodos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Create graph</span>',
          '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add the chatbot node</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools_node"</span><span class="p">,</span> <span class="n">basic_tool_node</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x14996cd70&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Cuando el LLM reciba un mensaje, como conoce las <code>tool</code>s que tiene a disposici贸n, decidir谩 si contestar o usar una <code>tool</code>. As铆 que vamos a crear una funci贸n de rutado, que ejecutar谩 una <code>tool</code> si el LLM decide usarla, o si no terminar谩 la ejecuci贸n del grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">route_tools_function</span><span class="p">(</span>',
      '    <span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">,</span>',
      '<span class="p">):</span>',
      '<span class="w">    </span><span class="sd">"""</span>',
      '<span class="sd">    Use in the conditional_edge to route to the ToolNode if the last message</span>',
      '<span class="sd">    has tool calls. Otherwise, route to the end.</span>',
      '<span class="sd">    """</span>',
      '    <span class="c1"># Get last message</span>',
      '    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>',
      '        <span class="n">ai_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '    <span class="k">elif</span> <span class="n">messages</span> <span class="o">:=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"messages"</span><span class="p">,</span> <span class="p">[]):</span>',
      '        <span class="n">ai_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '    <span class="k">else</span><span class="p">:</span>',
      '        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"No messages found in input state to tool_edge: </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '    ',
      '    <span class="c1"># Router in function of last message</span>',
      '    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ai_message</span><span class="p">,</span> <span class="s2">"tool_calls"</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ai_message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>',
      '        <span class="k">return</span> <span class="s2">"tools_node"</span>',
      '    <span class="k">return</span> <span class="n">END</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
























      
      <section class="section-block-markdown-cell">
      <p>A帽adimos los <code>edge</code>s.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tenemos que a帽adir un <code>edge</code> especial mediante <code>add_conditional_edges</code>, que crear谩 un nodo condicional. Une el nodo <code>chatbot_node</code> con la funci贸n de rutado que hemos creado antes <code>route_tools_function</code>. Con este nodo, si obtenemos a la salida de <code>route_tools_function</code> el string <code>tools_node</code> rutar谩 el grafo al nodo <code>tools_node</code>, pero si recibimos <code>END</code> rutar谩 el grafo al nodo <code>END</code> y terminar谩 la ejecuci贸n del grafo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>M谩s tarde, reemplazaremos esto con el m茅todo preconstruido <code>tools_condition</code>, pero ahora lo implementamos nosotros mismos para ver c贸mo funciona.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por 煤ltimo, se a帽ade otro <code>edge</code> que une <code>tools_node</code> con <code>chatbot_node</code>, para que cuando termine de ejecutarse una <code>tool</code> el grafo vuelva al nodo del <code>LLM</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Add edges</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>',
          '    <span class="s2">"chatbot_node"</span><span class="p">,</span>',
          '    <span class="n">route_tools_function</span><span class="p">,</span>',
          '    <span class="c1"># The following dictionary lets you tell the graph to interpret the condition\'s outputs as a specific node</span>',
          '    <span class="c1"># It defaults to the identity function, but if you</span>',
          '    <span class="c1"># want to use a node named something else apart from "tools",</span>',
          '    <span class="c1"># You can update the value of the dictionary to something else</span>',
          '    <span class="c1"># e.g., "tools": "my_tools"</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"tools_node"</span><span class="p">:</span> <span class="s2">"tools_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">:</span> <span class="n">END</span><span class="p">},</span>',
          '<span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools_node"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x14996cd70&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Compilamos el nodo y lo representamos</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
      
      <span class="k">try</span><span class="p">:</span>
          <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{opening_brace}</span><span class="n">e</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv23.webp" width="260" height="249" alt="image uv 24" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora podemos hacer preguntas al bot fuera de sus datos de entrenamiento</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Colors for the terminal</span>',
          '<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;32m"</span>',
          '<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;33m"</span>',
          '<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;0m"</span>',
          '',
          '',
          '<span class="k">def</span> <span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>',
          '    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace}):</span>',
          '        <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">user_input</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">value</span><span class="p">[</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
          '',
          '<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>',
          '    <span class="k">try</span><span class="p">:</span>',
          '        <span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">"User: "</span><span class="p">)</span>',
          '        <span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"quit"</span><span class="p">,</span> <span class="s2">"exit"</span><span class="p">,</span> <span class="s2">"q"</span><span class="p">]:</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">user_input</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">Goodbye!"</span><span class="p">)</span>',
          '            <span class="k">break</span>',
          '',
          '        <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>',
          '    <span class="k">except</span><span class="p">:</span>',
          '        <span class="c1"># fallback if input() is not available</span>',
          '        <span class="n">user_input</span> <span class="o">=</span> <span class="s2">"What do you know about LangGraph?"</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="s2">"User: "</span> <span class="o">+</span> <span class="n">user_input</span><span class="p">)</span>',
          '        <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>',
          '        <span class="k">break</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '<span class="ansi-green-fg">User: </span>How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span>How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid 3-2 Leganes: Goals and highlights - LaLiga 24/25 | Marca", "url": "https://www.marca.com/en/soccer/laliga/r-madrid-leganes/2025/03/29/01_0101_20250329_186_957-live.html", "content": "While their form has varied throughout the campaign there is no denying Real Madrid are a force at home in LaLiga this season, as they head into Saturday\'s match having picked up 34 points from 13 matches.\n\nAs for Leganes they currently sit 18th in the table, though they are level with Alaves for 17th as both teams look to stay in the top flight. [...] The two teams have already played twice this season, with Real Madrid securing a 3-0 win in the reverse league fixture. They also met in the quarter-finals of the Copa del Rey, a game Real won 3-2.\n\nReal Madrid vs Leganes LIVE - Latest Updates\n\nMatch ends, Real Madrid 3, Leganes 2.\n\nSecond Half ends, Real Madrid 3, Leganes 2.\n\nFoul by Vin\u00edcius J\u00fanior (Real Madrid).\n\nSeydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half. [...] Goal! Real Madrid 1, Leganes 1. Diego Garc\u00eda (Leganes) left footed shot from very close range.\n\nAttempt missed. \u00d3scar Rodr\u00edguez (Leganes) left footed shot from the centre of the box.\n\nGoal! Real Madrid 1, Leganes 0. Kylian Mbapp\u00e9 (Real Madrid) converts the penalty with a right footed shot.\n\nPenalty Real Madrid. Arda G\u00fcler draws a foul in the penalty area.\n\nPenalty conceded by \u00d3scar Rodr\u00edguez (Leganes) after a foul in the penalty area.\n\nDelay over. They are ready to continue.", "score": 0.8548001}, {opening_brace}"title": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Game Analysis - ESPN", "url": "https://www.espn.com/soccer/report/_/gameId/704946", "content": "Real Madrid\n\nLegan\u00e9s\n\nMbapp\u00e9 nets twice to keep Real Madrid\'s title hopes alive\n\nReal Madrid vs. Legan\u00e9s - Game Highlights\n\nWatch the Game Highlights from Real Madrid vs. Legan\u00e9s, 03/30/2025\n\nReal Madrid\'s Kylian Mbapp\u00e9 struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n\"We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\" Leganes striker Garcia said.\n\n\"Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\"\n\nGame Information", "score": 0.82220376}]',
          '<span class="ansi-green-fg">User: </span>How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span>How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid vs Leganes 3-2 | Highlights &amp; All Goals - YouTube", "url": "https://www.youtube.com/watch?v=ngBWsjmeHEk", "content": "Real Madrid secured a dramatic 3-2 victory over Leganes in an intense La Liga showdown on 29 March 2025! \u26bd Watch all the goals and", "score": 0.5157425}, {opening_brace}"title": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Game Analysis - ESPN", "url": "https://www.espn.com/soccer/report/_/gameId/704946", "content": "\"We know what we always have to do: win. We started well, in the opposition half, and we scored a goal. Then we didn\'t play well for 20 minutes and conceded two goals,\" said Mbapp\u00e9.\n\n\"But we know that if we play well we\'ll score and in the second half we scored two goals. We won the game and we\'re very happy.\n\n\"We worked on [the set piece] a few weeks ago with the staff. I knew I could shoot this way, I saw the space. I asked the others to let me shoot and it worked out well.\" [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n\"We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\" Leganes striker Garcia said.\n\n\"Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\"\n\nGame Information [...] However, Leganes responded almost immediately as Diego Garcia tapped in a loose ball at the far post to equalise in the following minute before Rodriguez set up Dani Raba to slot past goalkeeper Andriy Lunin in the 41st.\n\nReal midfielder Jude Bellingham brought the scores level two minutes after the break, sliding the ball into the net after a rebound off the crossbar. Mbapp\u00e9 then bagged the winner with a brilliant curled free kick in the 76th minute for his second.", "score": 0.50944775}]',
          '<span class="ansi-green-fg">User: </span>How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span>How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Game Analysis - ESPN", "url": "https://www.espn.com/soccer/report/_/gameId/704946", "content": "Real Madrid\n\nLegan\u00e9s\n\nMbapp\u00e9 nets twice to keep Real Madrid\'s title hopes alive\n\nReal Madrid vs. Legan\u00e9s - Game Highlights\n\nWatch the Game Highlights from Real Madrid vs. Legan\u00e9s, 03/30/2025\n\nReal Madrid\'s Kylian Mbapp\u00e9 struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n\"We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\" Leganes striker Garcia said.\n\n\"Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\"\n\nGame Information [...] However, Leganes responded almost immediately as Diego Garcia tapped in a loose ball at the far post to equalise in the following minute before Rodriguez set up Dani Raba to slot past goalkeeper Andriy Lunin in the 41st.\n\nReal midfielder Jude Bellingham brought the scores level two minutes after the break, sliding the ball into the net after a rebound off the crossbar. Mbapp\u00e9 then bagged the winner with a brilliant curled free kick in the 76th minute for his second.", "score": 0.93666285}, {opening_brace}"title": "MBAPPE BRACE Leganes vs. Real Madrid - ESPN FC - YouTube", "url": "https://www.youtube.com/watch?v=0xwUhzx19_4", "content": "MBAPPE BRACE \ud83d\udd25 Leganes vs. Real Madrid | LALIGA Highlights | ESPN FC \n ESPN FC \n 6836 likes \n 550646 views \n 29 Mar 2025 \n Watch these highlights as Kylian Mbappe scores 2 goals to give Real Madrid the 3-2 victory over Leganes in their LALIGA matchup.\n\n\u2714 Subscribe to ESPN+: http://espnplus.com/soccer/youtube\n\u2714 Subscribe to ESPN FC on YouTube: http://bit.ly/SUBSCRIBEtoESPNFC \n 790 comments", "score": 0.92857105}]',
          '<span class="ansi-green-fg">User: </span>How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span>How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "(VIDEO) All Goals from Real Madrid vs Leganes in La Liga", "url": "https://www.beinsports.com/en-us/soccer/la-liga/articles-video/-video-all-goals-from-real-madrid-vs-leganes-in-la-liga-2025-03-29?ess=", "content": "Real Madrid will host CD Leganes this Saturday, March 29, 2025, at the Santiago Bernab\u00e9u in a Matchday 29 clash of LaLiga EA Sports.", "score": 0.95628047}, {opening_brace}"title": "Real Madrid v Leganes | March 29, 2025 | Goal.com US", "url": "https://www.goal.com/en-us/match/real-madrid-vs-leganes/sZTw_SnjyKCcntxKHHQI7", "content": "Latest news, stats and live commentary for the LaLiga\'s meeting between Real Madrid v Leganes on the March 29, 2025.", "score": 0.9522955}]',
          '<span class="ansi-green-fg">User: </span>How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>Real Madrid faced Leganes in La Liga this weekend and came away with a 3-2 victory at the Santiago Bernab茅u. The match was intense, with Kylian Mbapp茅 scoring twice for Real Madrid, including a curled free kick in the 76th minute that proved to be the winner. Leganes managed to take the lead briefly with goals from Diego Garc铆a and Dani Raba, but Real Madrid leveled through Jude Bellingham before Mbapp茅\'s second goal secured the win. This result keeps Real Madrid\'s title hopes alive, moving them level on points with leaders Barcelona.',
          '<span class="ansi-green-fg">User: </span>Which players played the match?',
          '<span class="ansi-yellow-fg">Assistant: </span>The question is too vague and doesn\'t provide context such as the sport, league, or specific match in question. Could you please provide more details?',
          '<span class="ansi-green-fg">User: </span>q',
          '<span class="ansi-yellow-fg">Assistant: </span>Goodbye!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como ves, primero le he preguntado c贸mo qued贸 el Real Madrid en su 煤ltimo partido en la Liga contra el Legan茅s
      , como es algo de actualidad, ha decidido usar la herramienta de b煤squeda, con lo que ha obtenido el resultado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Sin embargo, a continuaci贸n le he preguntado qu茅 jugadores jugaron y no sab铆a de qu茅 le hablaba, eso es porque no se mantiene el contexto de la conversaci贸n. As铆 que lo siguiente que vamos a hacer es agregar una memoria al agente para que pueda mantener el contexto de la conversaci贸n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a escribir todo junto para que sea m谩s legible</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
          '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
          '<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
          '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">ToolMessage</span>',
          '',
          '<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>',
          '<span class="kn">import</span> <span class="nn">json</span>',
          '',
          '<span class="kn">import</span> <span class="nn">os</span>',
          '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
          '',
          '<span class="kn">import</span> <span class="nn">dotenv</span>',
          '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
          '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
          '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
          '',
          '<span class="c1"># State</span>',
          '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
          '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
          '',
          '<span class="c1"># Tools</span>',
          '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
          '<span class="n">tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>',
          '<span class="c1"># Create the LLM model</span>',
          '<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>',
          '<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
          '    <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>',
          '    <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>',
          '    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
          '    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="c1"># Create the chat model</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>',
          '<span class="c1"># Create the LLM with tools</span>',
          '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># BasicToolNode class</span>',
          '<span class="k">class</span> <span class="nc">BasicToolNode</span><span class="p">:</span>',
          '<span class="w">    </span><span class="sd">"""A node that runs the tools requested in the last AIMessage."""</span>',
          '',
          '    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tools</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>',
          '<span class="w">        </span><span class="sd">"""</span>',
          '<span class="sd">        Initialize the tools</span>',
          '',
          '<span class="sd">        Args:</span>',
          '<span class="sd">            tools (list): The tools to use</span>',
          '',
          '<span class="sd">        Returns:</span>',
          '<span class="sd">            None</span>',
          '<span class="sd">        """</span>',
          '        <span class="c1"># Initialize the tools</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">tools_by_name</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="n">tool</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">tool</span> <span class="k">for</span> <span class="n">tool</span> <span class="ow">in</span> <span class="n">tools</span><span class="p">{closing_brace}</span>',
          '',
          '    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>',
          '<span class="w">        </span><span class="sd">"""</span>',
          '<span class="sd">        Call the node</span>',
          '',
          '<span class="sd">        Args:</span>',
          '<span class="sd">            inputs (dict): The inputs to the node</span>',
          '',
          '<span class="sd">        Returns:</span>',
          '<span class="sd">            dict: The outputs of the node</span>',
          '<span class="sd">        """</span>',
          '        <span class="c1"># Get the last message</span>',
          '        <span class="k">if</span> <span class="n">messages</span> <span class="o">:=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"messages"</span><span class="p">,</span> <span class="p">[]):</span>',
          '            <span class="n">message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
          '        <span class="k">else</span><span class="p">:</span>',
          '            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"No message found in input"</span><span class="p">)</span>',
          '        ',
          '        <span class="c1"># Execute the tools</span>',
          '        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>',
          '        <span class="k">for</span> <span class="n">tool_call</span> <span class="ow">in</span> <span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">:</span>',
          '            <span class="n">tool_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tools_by_name</span><span class="p">[</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">]]</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>',
          '                <span class="n">tool_call</span><span class="p">[</span><span class="s2">"args"</span><span class="p">]</span>',
          '            <span class="p">)</span>',
          '            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>',
          '                <span class="n">ToolMessage</span><span class="p">(</span>',
          '                    <span class="n">content</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tool_result</span><span class="p">),</span>',
          '                    <span class="n">name</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"name"</span><span class="p">],</span>',
          '                    <span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call</span><span class="p">[</span><span class="s2">"id"</span><span class="p">],</span>',
          '                <span class="p">)</span>',
          '            <span class="p">)</span>',
          '        <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">outputs</span><span class="p">{closing_brace}</span>',
          '',
          '<span class="n">basic_tool_node</span> <span class="o">=</span> <span class="n">BasicToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Functions</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]}</span>',
          '',
          '<span class="c1"># Route function</span>',
          '<span class="k">def</span> <span class="nf">route_tools_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '<span class="w">    </span><span class="sd">"""</span>',
          '<span class="sd">    Use in the conditional_edge to route to the ToolNode if the last message</span>',
          '<span class="sd">    has tool calls. Otherwise, route to the end.</span>',
          '<span class="sd">    """</span>',
          '    <span class="c1"># Get last message</span>',
          '    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>',
          '        <span class="n">ai_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
          '    <span class="k">elif</span> <span class="n">messages</span> <span class="o">:=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"messages"</span><span class="p">,</span> <span class="p">[]):</span>',
          '        <span class="n">ai_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"No messages found in input state to tool_edge: </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '    ',
          '    <span class="c1"># Router in function of last message</span>',
          '    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ai_message</span><span class="p">,</span> <span class="s2">"tool_calls"</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ai_message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>',
          '        <span class="k">return</span> <span class="s2">"tools_node"</span>',
          '    <span class="k">return</span> <span class="n">END</span>',
          '',
          '<span class="c1"># Start to build the graph</span>',
          '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add nodes to the graph</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools_node"</span><span class="p">,</span> <span class="n">basic_tool_node</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add edges</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>',
          '    <span class="s2">"chatbot_node"</span><span class="p">,</span>',
          '    <span class="n">route_tools_function</span><span class="p">,</span>',
          '    <span class="p">{opening_brace}</span>',
          '        <span class="s2">"tools_node"</span><span class="p">:</span> <span class="s2">"tools_node"</span><span class="p">,</span>',
          '        <span class="n">END</span><span class="p">:</span> <span class="n">END</span>',
          '    <span class="p">},</span>',
          '<span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools_node"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '',
          '<span class="c1"># Compile the graph</span>',
          '<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>',
          '',
          '<span class="c1"># Display the graph</span>',
          '<span class="k">try</span><span class="p">:</span>',
          '    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>',
          '<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:',
          '1. Check your internet connection and try again',
          '2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`',
          '3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ejecutamos el grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Colors for the terminal</span>',
          '<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;32m"</span>',
          '<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;33m"</span>',
          '<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;0m"</span>',
          '',
          '',
          '<span class="k">def</span> <span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>',
          '    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace}):</span>',
          '        <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">user_input</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">value</span><span class="p">[</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
          '',
          '<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>',
          '    <span class="k">try</span><span class="p">:</span>',
          '        <span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">"User: "</span><span class="p">)</span>',
          '        <span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"quit"</span><span class="p">,</span> <span class="s2">"exit"</span><span class="p">,</span> <span class="s2">"q"</span><span class="p">]:</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">user_input</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">Goodbye!"</span><span class="p">)</span>',
          '            <span class="k">break</span>',
          '',
          '        <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>',
          '    <span class="k">except</span><span class="p">:</span>',
          '        <span class="c1"># fallback if input() is not available</span>',
          '        <span class="n">user_input</span> <span class="o">=</span> <span class="s2">"What do you know about LangGraph?"</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="s2">"User: "</span> <span class="o">+</span> <span class="n">user_input</span><span class="p">)</span>',
          '        <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>',
          '        <span class="k">break</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid 3-2 Leganes: Mbappe, Bellingham inspire comeback to ...", "url": "https://www.nbcsports.com/soccer/news/how-to-watch-real-madrid-vs-leganes-live-stream-link-tv-team-news-prediction", "content": "Real Madrid fought back to beat struggling Leganes 3-2 at the Santiago Bernabeu on Saturday as Kylian Mbappe scored twice and Jude", "score": 0.78749067}, {opening_brace}"title": "Real Madrid vs Leganes 3-2: LaLiga \u2013 as it happened - Al Jazeera", "url": "https://www.aljazeera.com/sports/liveblog/2025/3/29/live-real-madrid-vs-leganes-laliga", "content": "Defending champions Real Madrid beat 3-2 Leganes in Spain\'s LaLiga. The match at Santiago Bernabeu in Madrid, Spain saw Real trail 2-1 at half-", "score": 0.7485182}]',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid vs Legan\u00e9s: Spanish La Liga stats &amp; head-to-head - BBC", "url": "https://www.bbc.com/sport/football/live/cm2ndndvdgmt", "content": "Mbappe scores winner as Real Madrid survive Leganes scare\nMatch Summary\nSat 29 Mar 2025\n\u2027\nSpanish La Liga\nReal Madrid 3 , Legan\u00e9s 2 at Full time\nReal MadridReal MadridReal Madrid\n\n3\n2\n\nLegan\u00e9sLegan\u00e9sLegan\u00e9s\nFull time\nFT\nHalf Time Real Madrid 1 , Legan\u00e9s 2\nHT 1-2\nKey Events\nReal Madrid\n\nK. Mbapp\u00e9 (32\' pen, 76\')Penalty 32 minutes, Goal 76 minutes\nJ. Bellingham (47\')Goal 47 minutes\n\nLegan\u00e9s\n\nDiego Garc\u00eda (34\')Goal 34 minutes\nDani Raba (41\')Goal 41 minutes [...] Good nightpublished at 22:14 Greenwich Mean Time 29 March\n22:14 GMT 29 March\nThanks for joining us, that was a great game.\nSee you again soon for more La Liga action.\n13\n2\nShare\nclose panel\nShare page\nCopy link\nAbout sharing\n\n\nPostpublished at 22:10 Greenwich Mean Time 29 March\n22:10 GMT 29 March\nFT: Real Madrid 3-2 Leganes [...] Postpublished at 22:02 Greenwich Mean Time 29 March\n22:02 GMT 29 March\nFT: Real Madrid 3-2 Leganes\nOver to you, Barcelona.\nHansi Flick\'s side face Girona tomorrow (15:15 BST) and have the chance to regain their three point lead if they are victorious.\n18\n6\nShare\nclose panel\nShare page\nCopy link\nAbout sharing", "score": 0.86413884}, {opening_brace}"title": "Real Madrid 3 - 2 CD Legan\u00e9s (03/29) - Game Report - 365Scores", "url": "https://www.365scores.com/en-us/football/match/laliga-11/cd-leganes-real-madrid-131-9242-11", "content": "The game between Real Madrid and CD Legan\u00e9s ended with a score of Real Madrid 3 - 2 CD Legan\u00e9s. On 365Scores, you can check all the head-to-head results between", "score": 0.8524574}]',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN", "url": "https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid", "content": "Game Information\nSantiago Bernab\u00e9u\n8:00 PM, March 29, 2025Coverage: ESPN Deportes/ESPN+\nMadrid, Spain\nAttendance: 73,641 [...] Match Commentary\n-Match ends, Real Madrid 3, Leganes 2.90\'+9\'Second Half ends, Real Madrid 3, Leganes 2.90\'+7\'Seydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half.\nFull Commentary\nMatch Stats\nRMALEG\nPossession\n70.7%\n29.3%\n\nShots on Goal\n10\n4\nShot Attempts\n24\n10\nYellow Cards\n1\n4\nCorner Kicks\n8\n3\nSaves\n2\n6\n4-2-3-1\n\n\n13\nLunin\n*   20\nGarc\u00eda\n*   22\nR\u00fcdiger\n*   35\nAsencio\n*   17\nV\u00e1zquez\n\n\n6\nCamavinga\n*   10\nModric\n\n\n21\nD\u00edaz\n\n\n5\nBellingham\n*   15\nG\u00fcler\n\n\n9\nMbapp\u00e9 [...] | Rayo Vallecano | 35 | 12 | 11 | 12 | -5 | 47 |\n| Mallorca | 35 | 13 | 8 | 14 | -7 | 47 |\n| Valencia | 35 | 11 | 12 | 12 | -8 | 45 |\n| Osasuna | 35 | 10 | 15 | 10 | -8 | 45 |\n| Real Sociedad | 35 | 12 | 7 | 16 | -9 | 43 |\n| Getafe | 35 | 10 | 9 | 16 | -3 | 39 |\n| Espanyol | 35 | 10 | 9 | 16 | -9 | 39 |\n| Girona | 35 | 10 | 8 | 17 | -12 | 38 |\n| Sevilla | 35 | 9 | 11 | 15 | -10 | 38 |\n| Alav\u00e9s | 35 | 8 | 11 | 16 | -12 | 35 |\n| Legan\u00e9s | 35 | 7 | 13 | 15 | -18 | 34 |", "score": 0.93497354}, {opening_brace}"title": "Real Madrid v Leganes | March 29, 2025 | Goal.com US", "url": "https://www.goal.com/en-us/match/real-madrid-vs-leganes/sZTw_SnjyKCcntxKHHQI7", "content": "Latest news, stats and live commentary for the LaLiga\'s meeting between Real Madrid v Leganes on the March 29, 2025.", "score": 0.921929}]',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN", "url": "https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid", "content": "Game Information\nSantiago Bernab\u00e9u\n8:00 PM, March 29, 2025Coverage: ESPN Deportes/ESPN+\nMadrid, Spain\nAttendance: 73,641 [...] Match Commentary\n-Match ends, Real Madrid 3, Leganes 2.90\'+9\'Second Half ends, Real Madrid 3, Leganes 2.90\'+7\'Seydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half.\nFull Commentary\nMatch Stats\nRMALEG\nPossession\n70.7%\n29.3%\n\nShots on Goal\n10\n4\nShot Attempts\n24\n10\nYellow Cards\n1\n4\nCorner Kicks\n8\n3\nSaves\n2\n6\n4-2-3-1\n\n\n13\nLunin\n*   20\nGarc\u00eda\n*   22\nR\u00fcdiger\n*   35\nAsencio\n*   17\nV\u00e1zquez\n\n\n6\nCamavinga\n*   10\nModric\n\n\n21\nD\u00edaz\n\n\n5\nBellingham\n*   15\nG\u00fcler\n\n\n9\nMbapp\u00e9 [...] Mbapp\u00e9 nets twice to maintain Madrid title hopes ------------------------------------------------ Kylian Mbapp\u00e9 struck twice to guide Real Madrid to a 3-2 home win over relegation-threatened Leganes on Saturday. Mar 29, 2025, 10:53 pm - Reuters\nMatch Timeline\nReal Madrid\nLegan\u00e9s\n\nKO\n\n32\n\n\n34\n\n\n41\n\n\nHT\n\n\n47\n\n\n62\n\n\n62\n\n\n62\n\n\n65\n\n\n66\n\n\n72\n\n\n74\n\n\n76\n\n\n81\n\n\n83\n\n\n86\n\n\n89\n\n\nFT", "score": 0.96213967}]',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN", "url": "https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid", "content": "Game Information\nSantiago Bernab\u00e9u\n8:00 PM, March 29, 2025Coverage: ESPN Deportes/ESPN+\nMadrid, Spain\nAttendance: 73,641 [...] Match Commentary\n-Match ends, Real Madrid 3, Leganes 2.90\'+9\'Second Half ends, Real Madrid 3, Leganes 2.90\'+7\'Seydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half.\nFull Commentary\nMatch Stats\nRMALEG\nPossession\n70.7%\n29.3%\n\nShots on Goal\n10\n4\nShot Attempts\n24\n10\nYellow Cards\n1\n4\nCorner Kicks\n8\n3\nSaves\n2\n6\n4-2-3-1\n\n\n13\nLunin\n*   20\nGarc\u00eda\n*   22\nR\u00fcdiger\n*   35\nAsencio\n*   17\nV\u00e1zquez\n\n\n6\nCamavinga\n*   10\nModric\n\n\n21\nD\u00edaz\n\n\n5\nBellingham\n*   15\nG\u00fcler\n\n\n9\nMbapp\u00e9 [...] -550\n\no3.5\n+105\n-1.5\n-165\nLEGLegan\u00e9sLegan\u00e9s\n(6-9-14)\n(6-9-14, 27 pts)\nu3.5\n-120\n+950\nu3.5\n-135", "score": 0.9635647}, {opening_brace}"title": "Real Madrid v Leganes | March 29, 2025 | Goal.com US", "url": "https://www.goal.com/en-us/match/real-madrid-vs-leganes/sZTw_SnjyKCcntxKHHQI7", "content": "Latest news, stats and live commentary for the LaLiga\'s meeting between Real Madrid v Leganes on the March 29, 2025.", "score": 0.95921934}]',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN", "url": "https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid", "content": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN Real Madrid -Match ends, Real Madrid 3, Leganes 2.90\'+9\'Second Half ends, Real Madrid 3, Leganes 2.90\'+7\'Seydouba Ciss\u00e9 (Leganes) wins a free kick in the defensive half. Freedom from Property StressJohn buys bay area houses | [Sponsored](https://popup.taboola.com/en/?template=colorbox&amp;utm_source=espnnetwork-espn&amp;utm_medium=referral&amp;utm_content=thumbs-feed-01-b:gamepackage-thumbnails-3x1-b%20|%20Card%201:)[Sponsored](https://popup.taboola.com/en/?template=colorbox&amp;utm_source=espnnetwork-espn&amp;utm_medium=referral&amp;utm_content=thumbs-feed-01-b:gamepackage-thumbnails-3x1-b%20|%20Card%201:) Get Offer Brand-New 2-Bedroom Senior Apartment in Mountain View: You Won\'t Believe the Price2-Bedroom Senior Apartment | [Sponsored](https://popup.taboola.com/en/?template=colorbox&amp;utm_source=espnnetwork-espn&amp;utm_medium=referral&amp;utm_content=thumbs-feed-01-b:gamepackage-thumbnails-3x1-b%20|%20Card%201:)[Sponsored](https://popup.taboola.com/en/?template=colorbox&amp;utm_source=espnnetwork-espn&amp;utm_medium=referral&amp;utm_content=thumbs-feed-01-b:gamepackage-thumbnails-3x1-b%20|%20Card%201:) Read More | Real Madrid | 35 | 23 | 6 | 6 | +35 | 75 | Real Madrid woes continue as Vin\u00edcius J\u00fanior injury confirmed ------------------------------------------------------------- Injuries to Vin\u00edcius J\u00fanior and Lucas V\u00e1zquez added to Real Madrid\'s problems on Monday. To learn more, visit \"Do Not Sell or Share My Personal Information\" and \"Targeted Advertising\" Opt-Out Rights.", "score": 0.98565}, {opening_brace}"title": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Game Analysis - ESPN", "url": "https://www.espn.com/soccer/report/_/gameId/704946", "content": "Real Madrid\'s Kylian Mbapp\u00e9 struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened", "score": 0.98277}]',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>[{opening_brace}"title": "Real Madrid 3 - 2 CD Legan\u00e9s (03/29) - Game Report - 365Scores", "url": "https://www.365scores.com/en-us/football/match/laliga-11/cd-leganes-real-madrid-131-9242-11", "content": "The game between Real Madrid and CD Legan\u00e9s in the Regular Season of LaLiga, held on Saturday, March 29, 2025 at Estadio Santiago Bernab\u00e9u, ended with a score", "score": 0.96686727}, {opening_brace}"title": "Real Madrid 3-2 Legan\u00e9s (Mar 29, 2025) Final Score - ESPN", "url": "https://www.espn.com/soccer/match/_/gameId/704946/leganes-real-madrid", "content": "Game Information\nSantiago Bernab\u00e9u\n8:00 PM, March 29, 2025Coverage: ESPN Deportes/ESPN+\nMadrid, Spain\nAttendance: 73,641 [...] -550\n\no3.5\n+105\n-1.5\n-165\nLEGLegan\u00e9sLegan\u00e9s\n(6-9-14)\n(6-9-14, 27 pts)\nu3.5\n-120\n+950\nu3.5\n-135 [...] Referees:\nPablo Gonz\u00e1lez Fuertes", "score": 0.9595845}]',
          '<span class="ansi-green-fg">User: </span> How did Real Madrid fare this weekend against Leganes in La Liga?',
          '<span class="ansi-yellow-fg">Assistant: </span>Real Madrid faced CD Legan茅s in a La Liga match on Saturday, March 29, 2025, at the Estadio Santiago Bernab茅u. The match was a thrilling encounter, with Real Madrid coming from behind to secure a 3-2 victory. ',
          'Key points from the match include:',
          '- **Scoreline**: Real Madrid 3, Legan茅s 2.',
          '- **Goals**:',
          '  - **Real Madrid**: Kylian Mbapp茅 scored twice, including a penalty, and Jude Bellingham also found the net.',
          '  - **Legan茅s**: Goals were scored by Diego Garc铆a and Dani Raba.',
          '- **Attendance**: The match was played in front of 73,641 spectators.',
          '- **Key Moments**:',
          '  - Real Madrid trailed 2-1 at half-time but mounted a comeback in the second half.',
          '  - Mbapp茅\'s penalty in the 32nd minute and his second goal in the 76th minute were crucial in turning the game around.',
          '  - Bellingham\'s goal in the 47th minute shortly after the break tied the game.',
          'This victory is significant for Real Madrid as they continue their push for the La Liga title, while Legan茅s remains in a difficult position, fighting against relegation.',
          '<span class="ansi-green-fg">User: </span>Which players played the match?',
          '<span class="ansi-yellow-fg">Assistant: </span>I\'m sorry, but I need more information to answer your question. Could you please specify which match you\'re referring to, including the sport, the teams, or any other relevant details? This will help me provide you with the correct information.',
          '<span class="ansi-green-fg">User: </span>q',
          '<span class="ansi-yellow-fg">Assistant: </span>Goodbye!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver que el problema es que no recuerda el contexto de la conversaci贸n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Agregar-memoria-al-chatbot---memoria-a-corto-plazo,-memoria-dentro-del-hilo">Agregar memoria al chatbot - memoria a corto plazo, memoria dentro del hilo<a class="anchor-link" href="#Agregar-memoria-al-chatbot---memoria-a-corto-plazo,-memoria-dentro-del-hilo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Nuestro chatbot ahora puede usar herramientas para responder preguntas de los usuarios, pero no recuerda el contexto de las interacciones anteriores. Esto limita su capacidad de tener conversaciones coherentes y de m煤ltiples turnos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>LangGraph</code> resuelve este problema a trav茅s de puntos de control persistentes o <code>checkpoints</code>. Si le proporcionamos un <code>checkpointer</code> al compilar el grafo y un <code>thread_id</code> al llamar al grafo, <code>LangGraph</code> guarda autom谩ticamente el estado despu茅s de cada iteraci贸n en la conversaci贸n.</p>
      <p>Cuando invoquemos el grafo nuevamente usando el mismo <code>thread_id</code>, el grafo cargar谩 su estado guardado, permitiendo que el chatbot contin煤e donde lo dej贸.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Veremos m谩s tarde que ese <code>checkpointing</code> es mucho m谩s potente que la simple memoria de chat: le permite guardar y reanudar estados complejos en cualquier momento para la recuperaci贸n de errores, flujos de trabajo con <code>human in the loop</code>, interacciones en el tiempo y m谩s. Pero antes de ver todo eso, vamos a agregar puntos de control para permitir conversaciones de varias iteraciones.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      ' ',
      '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
      '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>












      
      <section class="section-block-markdown-cell">
      <p>Para empezar, creamos un <code>checkpointer</code> <a href="https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver" target="_blank" rel="nofollow noreferrer">MemorySaver</a>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>',
      ' ',
      '<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p><strong>Aviso</strong></p>
      <p>Estamos usando un <code>checkpointer</code> en memoria, es decir, se guarda en la RAM y cuando se termine de ejecutar el grafo se elimina. Esto nos vale para nuestro caso, ya que es un ejemplo para aprender a usar <code>LangGraph</code>. En una aplicaci贸n de producci贸n, es probable que se necesite cambiar esto para usarlo con <code>SqliteSaver</code> o <code>PostgresSaver</code> y con茅ctarnos a nuestra propia base de datos.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A continuaci贸n, definimos el grafo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
      '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
      ' ',
      ' ',
      '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
      ' ',
      ' ',
      '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>


















      
      <section class="section-block-markdown-cell">
      <p>Definimos la <code>tool</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
      '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
      ' ',
      '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
      '<span class="n">tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
      '<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>A continuaci贸n, el <code>LLM</code> con las <code>bind_tools</code> y lo a帽adimos al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
          '<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>',
          '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
          '',
          '<span class="c1"># Create the LLM</span>',
          '<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>',
          '<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
          '    <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>',
          '    <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>',
          '    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
          '    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="c1"># Create the chat model</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>',
          '',
          '<span class="c1"># Modification: tell the LLM which tools it can call</span>',
          '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Define the chatbot function</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]}</span>',
          '',
          '<span class="c1"># Add the chatbot node</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x1173534d0&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Antes construimos nuestro propio <code>BasicToolNode</code> para aprender c贸mo funciona, ahora lo reemplazaremos con el m茅todo de LangGraph <code>ToolNode</code> y <code>tools_condition</code>, ya que estos hacen algunas cosas buenas como la ejecuci贸n paralela de API. Aparte de eso, el resto es igual que antes.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>',
          '',
          '<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">tool</span><span class="p">])</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x1173534d0&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos el nodo de <code>tools_condition</code> al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span>',
          '    <span class="s2">"chatbot_node"</span><span class="p">,</span>',
          '    <span class="n">tools_condition</span><span class="p">,</span>',
          '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x1173534d0&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos el nodo de <code>tools</code> al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x1173534d0&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos el nodo de <code>START</code> al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x1173534d0&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Compilamos el grafo a帽adiendo el <code>checkpointer</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Lo representamos gr谩ficamente</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      
      <span class="k">try</span><span class="p">:</span>
          <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{opening_brace}</span><span class="n">e</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv24.webp" width="216" height="249" alt="image uv 25" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una configuraci贸n con un <code>thread_id</code> de un usuario</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">"1"</span>',
      '<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">{closing_brace}{closing_brace}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">"Hi there! My name is Maximo."</span>',
          '',
          '<span class="c1"># The config is the **second positional argument** to stream() or invoke()!</span>',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config_USER1</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Hi there! My name is Maximo.',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query:  does not reside in any location,{closing_brace}{closing_brace},',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Determining an individual\'s tax residency status - IRS", "url": "https://www.irs.gov/individuals/international-taxpayers/determining-an-individuals-tax-residency-status", "content": "If you are not a U.S. citizen, you are considered a nonresident of the United States for U.S. tax purposes unless you meet one of two tests.", "score": 0.1508904}, {opening_brace}"title": "Fix \"Location Is Not Available\", C:\\WINDOWS\\system32 ... - YouTube", "url": "https://www.youtube.com/watch?v=QFD-Ptp0SJw", "content": "Fix Error \"Location is not available\" C:\\WINDOWS\\system32\\config\\systemprofile\\Desktop is unavailable. If the location is on this PC,", "score": 0.07777658}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Invalid Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    {"query": "Arguments["image={"}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">"Do you remember my name?"</span>',
          '',
          '<span class="c1"># The config is the **second positional argument** to stream() or invoke()!</span>',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config_USER1</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Do you remember my name?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Of course! You mentioned your name is Maximo.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver, no hemos pasado una lista con los mensajes, todo est谩 siendo gestionado por el <code>checkpointer</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora probamos con otro usuario, es decir, con otro <code>thread_id</code>, veremos que el grafo no recuerda la conversaci贸n anterior.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">USER2_THREAD_ID</span> <span class="o">=</span> <span class="s2">"2"</span>',
          '<span class="n">config_USER2</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">USER2_THREAD_ID</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="n">user_input</span> <span class="o">=</span> <span class="s2">"Do you remember my name?"</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config_USER2</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Do you remember my name?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: Do you Remember My Name',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Sam Fender - Remember My Name (Official Video) - YouTube", "url": "https://www.youtube.com/watch?v=uaQm48G6IjY", "content": "Sam Fender - Remember My Name (Official Video) \n SamFenderVEVO \n 10743 likes \n 862209 views \n 14 Feb 2025 \n Remember My Name is a love song dedicated to my late Grandparents - they were always so fiercely proud of our family so I wrote the song in honour of them, from the perspective of my Grandad who was looking after my Grandma when she was suffering from dementia. This video is a really special one for me and I want to say thank you to everyone involved in making it. I hope you like it わ [...] If I was wanting of anymore\nId be as greedy as those men on the hill\nBut I remain forlorn\nIn the memory of what once was\n\nChasing a cross in from the wing\nOur boys a whippet, hes faster than anything\nRemember the pride that we felt\nFor the two of us made him ourselves\n\nHumour me\nMake my day\nIll tell you stories\nKiss your face\nAnd Ill pray\nYoull remember\nMy name\n\nIm not sure of what awaits\nWasnt a fan of St Peter and his gates\nBut by god I pray\nThat Ill see you in some way [...] Oh 11 Walk Avenue\nSomething to behold\nTo them its a council house\nTo me its a home\nAnd a home that you made\nWhere the grandkids could play\nBut its never the same without you\n\nHumour me\nMake my day\nIll tell you stories\nIll kiss your face\nAnd Ill pray\nYoull remember\nMy name\n\nAnd Ill pray you remember my name\nAnd Ill pray you remember my name\n\n---", "score": 0.6609831}, {opening_brace}"title": "Do You Remember My Name? - Novel Updates", "url": "https://www.novelupdates.com/series/do-you-remember-my-name/", "content": "This is a Cute, Tender, and Heartwarming High School Romance. It\'s not Heavy. It\'s not so Emotional too, but it does have Emotional moments. It\'s story Full of", "score": 0.608897}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: do you remember my name',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Sam Fender - Remember My Name (Official Video) - YouTube", "url": "https://www.youtube.com/watch?v=uaQm48G6IjY", "content": "Sam Fender - Remember My Name (Official Video) \n SamFenderVEVO \n 10743 likes \n 862209 views \n 14 Feb 2025 \n Remember My Name is a love song dedicated to my late Grandparents - they were always so fiercely proud of our family so I wrote the song in honour of them, from the perspective of my Grandad who was looking after my Grandma when she was suffering from dementia. This video is a really special one for me and I want to say thank you to everyone involved in making it. I hope you like it わ [...] Oh 11 Walk Avenue\nSomething to behold\nTo them its a council house\nTo me its a home\nAnd a home that you made\nWhere the grandkids could play\nBut its never the same without you\n\nHumour me\nMake my day\nIll tell you stories\nIll kiss your face\nAnd Ill pray\nYoull remember\nMy name\n\nAnd Ill pray you remember my name\nAnd Ill pray you remember my name\n\n--- [...] If I was wanting of anymore\nId be as greedy as those men on the hill\nBut I remain forlorn\nIn the memory of what once was\n\nChasing a cross in from the wing\nOur boys a whippet, hes faster than anything\nRemember the pride that we felt\nFor the two of us made him ourselves\n\nHumour me\nMake my day\nIll tell you stories\nKiss your face\nAnd Ill pray\nYoull remember\nMy name\n\nIm not sure of what awaits\nWasnt a fan of St Peter and his gates\nBut by god I pray\nThat Ill see you in some way", "score": 0.7123327}, {opening_brace}"title": "Do you remember my name? - song and lyrics by Alea, Mama Marjas", "url": "https://open.spotify.com/track/3GVBn3rEQLxZl4zJ4dG8UJ", "content": "Listen to Do you remember my name? on Spotify. Song 路 Alea, Mama Marjas 路 2023.", "score": 0.6506676}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: do you remember my name',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Sam Fender - Remember My Name (Official Video) - YouTube", "url": "https://www.youtube.com/watch?v=uaQm48G6IjY", "content": "Sam Fender - Remember My Name (Official Video) \n SamFenderVEVO \n 10743 likes \n 862209 views \n 14 Feb 2025 \n Remember My Name is a love song dedicated to my late Grandparents - they were always so fiercely proud of our family so I wrote the song in honour of them, from the perspective of my Grandad who was looking after my Grandma when she was suffering from dementia. This video is a really special one for me and I want to say thank you to everyone involved in making it. I hope you like it わ [...] Oh 11 Walk Avenue\nSomething to behold\nTo them its a council house\nTo me its a home\nAnd a home that you made\nWhere the grandkids could play\nBut its never the same without you\n\nHumour me\nMake my day\nIll tell you stories\nIll kiss your face\nAnd Ill pray\nYoull remember\nMy name\n\nAnd Ill pray you remember my name\nAnd Ill pray you remember my name\n\n--- [...] If I was wanting of anymore\nId be as greedy as those men on the hill\nBut I remain forlorn\nIn the memory of what once was\n\nChasing a cross in from the wing\nOur boys a whippet, hes faster than anything\nRemember the pride that we felt\nFor the two of us made him ourselves\n\nHumour me\nMake my day\nIll tell you stories\nKiss your face\nAnd Ill pray\nYoull remember\nMy name\n\nIm not sure of what awaits\nWasnt a fan of St Peter and his gates\nBut by god I pray\nThat Ill see you in some way", "score": 0.7123327}, {opening_brace}"title": "Do you remember my name? - song and lyrics by Alea, Mama Marjas", "url": "https://open.spotify.com/track/3GVBn3rEQLxZl4zJ4dG8UJ", "content": "Listen to Do you remember my name? on Spotify. Song 路 Alea, Mama Marjas 路 2023.", "score": 0.6506676}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'I\'m here to assist you, but I don\'t actually have the ability to remember names or personal information from previous conversations. How can I assist you today?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que nuestro chatbot tiene herramientas de b煤squeda y memoria, vamos a repetir el ejemplo anterior, donde le pregunto por el resultado del 煤ltimo partido del Real Madrid en la Liga y luego por qu茅 jugadores jugaron.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">USER3_THREAD_ID</span> <span class="o">=</span> <span class="s2">"3"</span>',
          '<span class="n">config_USER3</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">USER3_THREAD_ID</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="n">user_input</span> <span class="o">=</span> <span class="s2">"How did Real Madrid fare this weekend against Leganes in La Liga?"</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config_USER3</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'How did Real Madrid fare this weekend against Leganes in La Liga?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: Real Madrid vs Leganes La Liga this weekend',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Real Madrid 3-2 Leganes: Goals and highlights - LaLiga 24/25 | Marca", "url": "https://www.marca.com/en/soccer/laliga/r-madrid-leganes/2025/03/29/01_0101_20250329_186_957-live.html", "content": "While their form has varied throughout the campaign there is no denying Real Madrid are a force at home in LaLiga this season, as they head into Saturday\'s match having picked up 34 points from 13 matches.\n\nAs for Leganes they currently sit 18th in the table, though they are level with Alaves for 17th as both teams look to stay in the top flight. [...] The two teams have already played twice this season, with Real Madrid securing a 3-0 win in the reverse league fixture. They also met in the quarter-finals of the Copa del Rey, a game Real won 3-2.\n\nReal Madrid vs Leganes LIVE - Latest Updates\n\nMatch ends, Real Madrid 3, Leganes 2.\n\nSecond Half ends, Real Madrid 3, Leganes 2.\n\nFoul by Vin铆cius J煤nior (Real Madrid).\n\nSeydouba Ciss茅 (Leganes) wins a free kick in the defensive half. [...] Goal! Real Madrid 1, Leganes 1. Diego Garc铆a (Leganes) left footed shot from very close range.\n\nAttempt missed. scar Rodr铆guez (Leganes) left footed shot from the centre of the box.\n\nGoal! Real Madrid 1, Leganes 0. Kylian Mbapp茅 (Real Madrid) converts the penalty with a right footed shot.\n\nPenalty Real Madrid. Arda G眉ler draws a foul in the penalty area.\n\nPenalty conceded by scar Rodr铆guez (Leganes) after a foul in the penalty area.\n\nDelay over. They are ready to continue.", "score": 0.8548001}, {opening_brace}"title": "Real Madrid 3-2 Legan茅s (Mar 29, 2025) Game Analysis - ESPN", "url": "https://www.espn.com/soccer/report/_/gameId/704946", "content": "Real Madrid\n\nLegan茅s\n\nMbapp茅 nets twice to keep Real Madrid\'s title hopes alive\n\nReal Madrid vs. Legan茅s - Game Highlights\n\nWatch the Game Highlights from Real Madrid vs. Legan茅s, 03/30/2025\n\nReal Madrid\'s Kylian Mbapp茅 struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. [...] Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n\"We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\" Leganes striker Garcia said.\n\n\"Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\"\n\nGame Information", "score": 0.82220376}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Real Madrid secured a 3-2 victory against Leganes this weekend in their La Liga match. Kylian Mbapp茅 scored twice, including a penalty, to help his team come from behind and claim the win, keeping Real Madrid\'s title hopes alive. Leganes, now sitting 18th in the table, continues to face challenges in their fight against relegation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora le preguntamos por los jugadores que jugaron en el partido.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">"Which players played the match?"</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config_USER3</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Which players played the match?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: Real Madrid vs Leganes match report players lineup',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Real Madrid vs. Leganes final score: La Liga result, updates, stats ...", "url": "https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-score-result-updates-stats-la-liga/8ecf730cfcb9b6c5f6693a0d", "content": "Real Madrid came through a topsy-turvy game with Leganes to claim a 3-2 victory and put pressure back on Barcelona in La Liga\'s title race. Kylian Mbappe scored in each half either side of a Jude Bellingham goal  his first in the league since January 3  to seal all three points for the champions after Leganes had come from behind to lead at the interval. Rodrygo won back the ball in the Leganes half and earned a free-kick on the edge of the box, and Mbappe found the bottom corner after rolling the ball short to Fran Garcia to work an angle. Leganes lead Real Madrid at the Bernabeu for the very first time! *Real Madrid starting lineup (4-3-3, right to left):*Lunin (GK)  Vazquez, Rudiger, Asencio, Garcia  Modric, Bellingham, Camavinga  B.", "score": 0.88372874}, {opening_brace}"title": "CONFIRMED lineups: Real Madrid vs Legan茅s, 2025 La Liga", "url": "https://www.managingmadrid.com/2025/3/29/24396638/real-madrid-vs-leganes-2025-la-liga-live-online-stream", "content": "Real Madrid starting XI: Lunin, Vazquez, Rudiger, Asencio, Fran Garcia, Camavinga, Guler, Modric, Bellingham, Brahim, Mbappe. Leganes starting", "score": 0.83452857}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: Real Madrid vs Leganes players 2025',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Player Ratings: Real Madrid 3-2 Leganes; 2025 La Liga", "url": "https://www.managingmadrid.com/2025/3/30/24396688/player-ratings-real-madrid-3-2-leganes-2025-la-liga", "content": "Ra煤l Asencio7: Applauded by the Bernabeu on multiple occasions with good sweeping up defensively.\n\nFran Garc铆a6: Better on the offensive end, getting into the final third and playing some dagger crosses.\n\nEduardo Camavinga6: Modric and Camavinga struggled to deal with Leganes counter attacks and Diego, playing as a #10 for Leganes, got the better of both of them. [...] Follow Managing Madrid online:\n\nSite search\n\nManaging Madrid main menu\n\nFiled under:\n\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\n\nKylian Mbappe scores a brace to help Madrid secure a nervy 3-2 victory.\n\nShare this story\n\nShare\nAll sharing options for:\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\n\nFull match player ratings below:\n\nAndriy Lunin7: Not at fault for the goals, was left with the opposition taking a shot from near the six yard box. [...] Lucas V谩zquez4: Exposed in transition and lacking the speed and athleticism to cover the gaps he leaves when venturing forward. Needs a more pessimistic attitude when the ball is on the opposite flank, occupying better spots in rest defense.\n\nAntonio Rudiger5: Several unnecessary long distance shots to hurt Madrids rhythm and reinforce Leganes game plan. Playing with too many matches in his legs and its beginning to show.", "score": 0.8832463}, {opening_brace}"title": "Real Madrid vs. Legan茅s (Mar 29, 2025) Live Score - ESPN", "url": "https://www.espn.com/soccer/match/_/gameId/704946", "content": "Match Formations 路 13. Lunin 路 20. Garc铆a 路 22. R眉diger 路 35. Asencio 路 17. V谩zquez 路 5. Bellingham 路 10. Modric 路 6. Camavinga.", "score": 0.86413884}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: Real Madrid vs Leganes starting lineup',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Starting lineups of Real Madrid and Legan茅s", "url": "https://www.realmadrid.com/en-US/news/football/first-team/latest-news/once-inicial-del-real-madrid-contra-el-leganes-29-03-2025", "content": "Starting lineups of Real Madrid and Legan漏s\n\n\n\nThe Whites芒 team is: Lunin, Lucas V., Asencio, R录diger, Fran Garc颅a, Arda G录ler, Modri, Camavinga, Bellingham, Brahim and Mbapp漏.\n\n\n\n\n\nReal Madridhave named their starting line-up for the game against Legan漏s on matchday 29 ofLaLiga, which will be played at theSantiago Bernab漏u(9 pm CET). [...] Real Madrid starting line-up:\n13. Lunin\n17. Lucas V.\n35. Asencio\n22. R录diger\n20. Fran Garc颅a\n15. Arda G录ler\n10. Modri\n6. Camavinga\n5. Bellingham\n21. Brahim\n9. Mbapp漏.\n\nSubstitutes:\n26. Fran Gonz隆lez\n34. Sergio Mestre\n4. Alaba\n7. Vini Jr.\n8. Valverde\n11. Rodrygo\n14. Tchouameni\n16. Endrick\n18. Vallejo\n43. Diego Aguado.\n\nLegan漏s starting line-up:\n13. Dmitrovic\n5. Tapia\n6. Sergio G.\n7. scar\n10. Raba\n11. Cruz\n12. V. Rosier\n17. Neyou\n19. Diego G.\n20. Javi Hern隆ndez\n22. Nastasic. [...] Suplentes:\n1. Juan Soriano\n36. Abajas\n2. A. Alti\n3. Jorge S隆enz\n8. Cisse\n9. Miguel\n14. Darko\n18. Duk\n21. R. L鲁pez\n23. Munir\n24. Chicco\n30. I. Diomande.\n\n\n\nDownload Now\n\nOfficial App Fan\n\nReal Madrid 漏 2025 All rights reserved", "score": 0.9465623}, {opening_brace}"title": "Real Madrid vs. Leganes lineups, confirmed starting 11, team news ...", "url": "https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-lineups-starting-11-team-news-injuries/aac757d10cc7b9a084995b4d", "content": "Real Madrid starting lineup (4-3-3, right to left): Lunin (GK)  Vazquez, Rudiger, Asencio, Garcia  Modric, Bellingham, Camavinga  B. Diaz,", "score": 0.9224337}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'The starting lineup for Real Madrid in their match against Legan茅s was: Lunin (GK), V谩zquez, R眉diger, Asencio, Fran Garc铆a, Modric, Bellingham, Camavinga, Brahim, Arda G眉ler, and Mbapp茅. Notable players like Vini Jr., Rodrygo, and Valverde were on the bench.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Tras mucho buscar, al final lo encuentra. Por lo que ya tenemos un chatbot con <code>tools</code> y memoria.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por ahora, hemos creado unos <code>checkpoints</code> en tres hilos diferentes. Pero, 驴qu茅 entra en cada <code>checkpoint</code>? Para inspeccionar el estado de un grafo para una configuraci贸n dada podemos usar el m茅todo <code>get_state(config)</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config_USER3</span><span class="p">)</span>',
          '<span class="n">snapshot</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'StateSnapshot(values={opening_brace}\'messages\': &lbrack;HumanMessage(content=\'How did Real Madrid fare this weekend against Leganes in La Liga?\', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id=\'a33f5825-1ae4-4717-ad17-8e306f35b027\'), AIMessage(content=\'\', additional_kwargs={opening_brace}\'tool_calls\': &lbrack;{opening_brace}\'function\': {\'arguments\': {opening_brace}\'query\': \'Real Madrid vs Leganes La Liga this weekend\'{closing_brace}, \'name\': \'tavily_search_results_json\', \'description\': None}, \'id\': \'0\', \'type\': \'function\'{closing_brace}&rbrack;}, response_metadata={opening_brace}\'token_usage\': {\'completion_tokens\': 25, \'prompt_tokens\': 296, \'total_tokens\': 321}, \'model\': \'\', \'finish_reason\': \'stop\'{closing_brace}, id=\'run-7905b5ae-5dee-4641-b012-396affde984c-0\', tool_calls=&lbrack;{opening_brace}\'name\': \'tavily_search_results_json\', \'args\': {opening_brace}\'query\': \'Real Madrid vs Leganes La Liga this weekend\'{closing_brace}, \'id\': \'0\', \'type\': \'tool_call\'{closing_brace}&rbrack;), ToolMessage(content=\'&lbrack;{opening_brace}"title": "Real Madrid 3-2 Leganes: Goals and highlights - LaLiga 24/25 | Marca", "url": "https://www.marca.com/en/soccer/laliga/r-madrid-leganes/2025/03/29/01_0101_20250329_186_957-live.html", "content": "While their form has varied throughout the campaign there is no denying Real Madrid are a force at home in LaLiga this season, as they head into Saturday\'s match having picked up 34 points from 13 matches.\\n\\nAs for Leganes they currently sit 18th in the table, though they are level with Alaves for 17th as both teams look to stay in the top flight. &lbrack;...&rbrack; The two teams have already played twice this season, with Real Madrid securing a 3-0 win in the reverse league fixture. They also met in the quarter-finals of the Copa del Rey, a game Real won 3-2.\\n\\nReal Madrid vs Leganes LIVE - Latest Updates\\n\\nMatch ends, Real Madrid 3, Leganes 2.\\n\\nSecond Half ends, Real Madrid 3, Leganes 2.\\n\\nFoul by Vin铆cius J煤nior (Real Madrid).\\n\\nSeydouba Ciss茅 (Leganes) wins a free kick in the defensive half. &lbrack;...&rbrack; Goal! Real Madrid 1, Leganes 1. Diego Garc铆a (Leganes) left footed shot from very close range.\\n\\nAttempt missed. scar Rodr铆guez (Leganes) left footed shot from the centre of the box.\\n\\nGoal! Real Madrid 1, Leganes 0. Kylian Mbapp茅 (Real Madrid) converts the penalty with a right footed shot.\\n\\nPenalty Real Madrid. Arda G眉ler draws a foul in the penalty area.\\n\\nPenalty conceded by scar Rodr铆guez (Leganes) after a foul in the penalty area.\\n\\nDelay over. They are ready to continue.", "score": 0.8548001}, {opening_brace}"title": "Real Madrid 3-2 Legan茅s (Mar 29, 2025) Game Analysis - ESPN", "url": "https://www.espn.com/soccer/report/_/gameId/704946", "content": "Real Madrid\\n\\nLegan茅s\\n\\nMbapp茅 nets twice to keep Real Madrid\'s title hopes alive\\n\\nReal Madrid vs. Legan茅s - Game Highlights\\n\\nWatch the Game Highlights from Real Madrid vs. Legan茅s, 03/30/2025\\n\\nReal Madrid\'s Kylian Mbapp茅 struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. &lbrack;...&rbrack; Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\\n\\n\\"We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here,\\" Leganes striker Garcia said.\\n\\n\\"Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week.\\"\\n\\nGame Information", "score": 0.82220376}&rbrack;\', name=\'tavily_search_results_json\', id=\'0e02fce3-a6f0-4cce-9217-04c8c3219265\', tool_call_id=\'0\', artifact={opening_brace}\'query\': \'Real Madrid vs Leganes La Liga this weekend\', \'follow_up_questions\': None, \'answer\': None, \'images\': &lbrack;&rbrack;, \'results\': &lbrack;{\'url\': \'https://www.marca.com/en/soccer/laliga/r-madrid-leganes/2025/03/29/01_0101_20250329_186_957-live.html\', \'title\': \'Real Madrid 3-2 Leganes: Goals and highlights - LaLiga 24/25 | Marca\', \'content\': "While their form has varied throughout the campaign there is no denying Real Madrid are a force at home in LaLiga this season, as they head into Saturday\'s match having picked up 34 points from 13 matches.\n\nAs for Leganes they currently sit 18th in the table, though they are level with Alaves for 17th as both teams look to stay in the top flight. &lbrack;...&rbrack; The two teams have already played twice this season, with Real Madrid securing a 3-0 win in the reverse league fixture. They also met in the quarter-finals of the Copa del Rey, a game Real won 3-2.\n\nReal Madrid vs Leganes LIVE - Latest Updates\n\nMatch ends, Real Madrid 3, Leganes 2.\n\nSecond Half ends, Real Madrid 3, Leganes 2.\n\nFoul by Vin铆cius J煤nior (Real Madrid).\n\nSeydouba Ciss茅 (Leganes) wins a free kick in the defensive half. &lbrack;...&rbrack; Goal! Real Madrid 1, Leganes 1. Diego Garc铆a (Leganes) left footed shot from very close range.\n\nAttempt missed. scar Rodr铆guez (Leganes) left footed shot from the centre of the box.\n\nGoal! Real Madrid 1, Leganes 0. Kylian Mbapp茅 (Real Madrid) converts the penalty with a right footed shot.\n\nPenalty Real Madrid. Arda G眉ler draws a foul in the penalty area.\n\nPenalty conceded by scar Rodr铆guez (Leganes) after a foul in the penalty area.\n\nDelay over. They are ready to continue.", \'score\': 0.8548001, \'raw_content\': None}, {\'url\': \'https://www.espn.com/soccer/report/_/gameId/704946\', \'title\': \'Real Madrid 3-2 Legan茅s (Mar 29, 2025) Game Analysis - ESPN\', \'content\': \'Real Madrid\n\nLegan茅s\n\nMbapp茅 nets twice to keep Real Madrid\'s title hopes alive\n\nReal Madrid vs. Legan茅s - Game Highlights\n\nWatch the Game Highlights from Real Madrid vs. Legan茅s, 03/30/2025\n\nReal Madrid\'s Kylian Mbapp茅 struck twice to help his side come from behind to claim a hard-fought 3-2 home win over relegation-threatened Leganes on Saturday to move the second-placed reigning champions level on points with leaders Barcelona. &lbrack;...&rbrack; Leganes pushed for an equaliser but fell to a third consecutive defeat to sit 18th on 27 points, level with Alaves who are one place higher in the safety zone on goal difference.\n\n"We have done a tremendous job. We leave with our heads held high because we were fighting until the end to score here," Leganes striker Garcia said.\n\n"Ultimately, it was down to the details that they took it. We played a very serious game and now we have to think about next week."\n\nGame Information\', \'score\': 0.82220376, \'raw_content\': None}&rbrack;, \'response_time\': 1.47}), AIMessage(content="Real Madrid secured a 3-2 victory against Leganes this weekend in their La Liga match. Kylian Mbapp茅 scored twice, including a penalty, to help his team come from behind and claim the win, keeping Real Madrid\'s title hopes alive. Leganes, now sitting 18th in the table, continues to face challenges in their fight against relegation.", additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}\'token_usage\': {\'completion_tokens\': 92, \'prompt_tokens\': 1086, \'total_tokens\': 1178}, \'model\': \'\', \'finish_reason\': \'stop\'{closing_brace}, id=\'run-22226dda-0475-49b7-882f-fe7bd63ef025-0\'), HumanMessage(content=\'Which players played the match?\', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id=\'3e6d9f84-06a2-4148-8f2b-d8ef42c3bea1\'), AIMessage(content=\'\', additional_kwargs={opening_brace}\'tool_calls\': &lbrack;{opening_brace}\'function\': {\'arguments\': {opening_brace}\'query\': \'Real Madrid vs Leganes match report players lineup\'{closing_brace}, \'name\': \'tavily_search_results_json\', \'description\': None}, \'id\': \'0\', \'type\': \'function\'{closing_brace}&rbrack;}, response_metadata={opening_brace}\'token_usage\': {\'completion_tokens\': 29, \'prompt_tokens\': 1178, \'total_tokens\': 1207}, \'model\': \'\', \'finish_reason\': \'stop\'{closing_brace}, id=\'run-025d3235-61b9-4add-8e1b-5b1bc795a9d3-0\', tool_calls=&lbrack;{opening_brace}\'name\': \'tavily_search_results_json\', \'args\': {opening_brace}\'query\': \'Real Madrid vs Leganes match report players lineup\'{closing_brace}, \'id\': \'0\', \'type\': \'tool_call\'{closing_brace}&rbrack;), ToolMessage(content=\'&lbrack;{opening_brace}"title": "Real Madrid vs. Leganes final score: La Liga result, updates, stats ...", "url": "https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-score-result-updates-stats-la-liga/8ecf730cfcb9b6c5f6693a0d", "content": "Real Madrid came through a topsy-turvy game with Leganes to claim a 3-2 victory and put pressure back on Barcelona in La Liga\'s title race. Kylian Mbappe scored in each half either side of a Jude Bellingham goal  his first in the league since January 3  to seal all three points for the champions after Leganes had come from behind to lead at the interval. Rodrygo won back the ball in the Leganes half and earned a free-kick on the edge of the box, and Mbappe found the bottom corner after rolling the ball short to Fran Garcia to work an angle. Leganes lead Real Madrid at the Bernabeu for the very first time! *Real Madrid starting lineup (4-3-3, right to left):*\xa0Lunin (GK)  Vazquez, Rudiger, Asencio, Garcia  Modric, Bellingham, Camavinga  B.", "score": 0.88372874}, {opening_brace}"title": "CONFIRMED lineups: Real Madrid vs Legan茅s, 2025 La Liga", "url": "https://www.managingmadrid.com/2025/3/29/24396638/real-madrid-vs-leganes-2025-la-liga-live-online-stream", "content": "Real Madrid starting XI: Lunin, Vazquez, Rudiger, Asencio, Fran Garcia, Camavinga, Guler, Modric, Bellingham, Brahim, Mbappe. Leganes starting", "score": 0.83452857}&rbrack;\', name=\'tavily_search_results_json\', id=\'2dbc1324-2c20-406a-b2d7-a3d6fc609537\', tool_call_id=\'0\', artifact={opening_brace}\'query\': \'Real Madrid vs Leganes match report players lineup\', \'follow_up_questions\': None, \'answer\': None, \'images\': &lbrack;&rbrack;, \'results\': &lbrack;{\'url\': \'https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-score-result-updates-stats-la-liga/8ecf730cfcb9b6c5f6693a0d\', \'title\': \'Real Madrid vs. Leganes final score: La Liga result, updates, stats ...\', \'content\': "Real Madrid came through a topsy-turvy game with Leganes to claim a 3-2 victory and put pressure back on Barcelona in La Liga\'s title race. Kylian Mbappe scored in each half either side of a Jude Bellingham goal  his first in the league since January 3  to seal all three points for the champions after Leganes had come from behind to lead at the interval. Rodrygo won back the ball in the Leganes half and earned a free-kick on the edge of the box, and Mbappe found the bottom corner after rolling the ball short to Fran Garcia to work an angle. Leganes lead Real Madrid at the Bernabeu for the very first time! *Real Madrid starting lineup (4-3-3, right to left):*\xa0Lunin (GK)  Vazquez, Rudiger, Asencio, Garcia  Modric, Bellingham, Camavinga  B.", \'score\': 0.88372874, \'raw_content\': None}, {\'url\': \'https://www.managingmadrid.com/2025/3/29/24396638/real-madrid-vs-leganes-2025-la-liga-live-online-stream\', \'title\': \'CONFIRMED lineups: Real Madrid vs Legan茅s, 2025 La Liga\', \'content\': \'Real Madrid starting XI: Lunin, Vazquez, Rudiger, Asencio, Fran Garcia, Camavinga, Guler, Modric, Bellingham, Brahim, Mbappe. Leganes starting\', \'score\': 0.83452857, \'raw_content\': None}&rbrack;, \'response_time\': 3.36}), AIMessage(content=\'\', additional_kwargs={opening_brace}\'tool_calls\': &lbrack;{opening_brace}\'function\': {\'arguments\': {opening_brace}\'query\': \'Real Madrid vs Leganes players 2025\'{closing_brace}, \'name\': \'tavily_search_results_json\', \'description\': None}, \'id\': \'0\', \'type\': \'function\'{closing_brace}&rbrack;}, response_metadata={opening_brace}\'token_usage\': {\'completion_tokens\': 31, \'prompt_tokens\': 1630, \'total_tokens\': 1661}, \'model\': \'\', \'finish_reason\': \'stop\'{closing_brace}, id=\'run-d6b4c4ff-0923-4082-9dea-7c51b2a4fc60-0\', tool_calls=&lbrack;{opening_brace}\'name\': \'tavily_search_results_json\', \'args\': {opening_brace}\'query\': \'Real Madrid vs Leganes players 2025\'{closing_brace}, \'id\': \'0\', \'type\': \'tool_call\'{closing_brace}&rbrack;), ToolMessage(content=\'&lbrack;{opening_brace}"title": "Player Ratings: Real Madrid 3-2 Leganes; 2025 La Liga", "url": "https://www.managingmadrid.com/2025/3/30/24396688/player-ratings-real-madrid-3-2-leganes-2025-la-liga", "content": "Ra煤l Asencio7: Applauded by the Bernabeu on multiple occasions with good sweeping up defensively.\\n\\nFran Garc铆a6: Better on the offensive end, getting into the final third and playing some dagger crosses.\\n\\nEduardo Camavinga6: Modric and Camavinga struggled to deal with Leganes counter attacks and Diego, playing as a #10 for Leganes, got the better of both of them. &lbrack;...&rbrack; Follow Managing Madrid online:\\n\\nSite search\\n\\nManaging Madrid main menu\\n\\nFiled under:\\n\\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\\n\\nKylian Mbappe scores a brace to help Madrid secure a nervy 3-2 victory.\\n\\nShare this story\\n\\nShare\\nAll sharing options for:\\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\\n\\nFull match player ratings below:\\n\\nAndriy Lunin7: Not at fault for the goals, was left with the opposition taking a shot from near the six yard box. &lbrack;...&rbrack; Lucas V谩zquez4: Exposed in transition and lacking the speed and athleticism to cover the gaps he leaves when venturing forward. Needs a more pessimistic attitude when the ball is on the opposite flank, occupying better spots in rest defense.\\n\\nAntonio Rudiger5: Several unnecessary long distance shots to hurt Madrids rhythm and reinforce Leganes game plan. Playing with too many matches in his legs and its beginning to show.", "score": 0.8832463}, {opening_brace}"title": "Real Madrid vs. Legan茅s (Mar 29, 2025) Live Score - ESPN", "url": "https://www.espn.com/soccer/match/_/gameId/704946", "content": "Match Formations 路 13. Lunin 路 20. Garc铆a 路 22. R眉diger 路 35. Asencio 路 17. V谩zquez 路 5. Bellingham 路 10. Modric 路 6. Camavinga.", "score": 0.86413884}&rbrack;\', name=\'tavily_search_results_json\', id=\'ac15dd6e-09b1-4075-834e-d869f4079285\', tool_call_id=\'0\', artifact={opening_brace}\'query\': \'Real Madrid vs Leganes players 2025\', \'follow_up_questions\': None, \'answer\': None, \'images\': &lbrack;&rbrack;, \'results\': &lbrack;{\'url\': \'https://www.managingmadrid.com/2025/3/30/24396688/player-ratings-real-madrid-3-2-leganes-2025-la-liga\', \'title\': \'Player Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\', \'content\': \'Ra煤l Asencio7: Applauded by the Bernabeu on multiple occasions with good sweeping up defensively.\n\nFran Garc铆a6: Better on the offensive end, getting into the final third and playing some dagger crosses.\n\nEduardo Camavinga6: Modric and Camavinga struggled to deal with Leganes counter attacks and Diego, playing as a #10 for Leganes, got the better of both of them. &lbrack;...&rbrack; Follow Managing Madrid online:\n\nSite search\n\nManaging Madrid main menu\n\nFiled under:\n\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\n\nKylian Mbappe scores a brace to help Madrid secure a nervy 3-2 victory.\n\nShare this story\n\nShare\nAll sharing options for:\nPlayer Ratings: Real Madrid 3-2 Leganes; 2025 La Liga\n\nFull match player ratings below:\n\nAndriy Lunin7: Not at fault for the goals, was left with the opposition taking a shot from near the six yard box. &lbrack;...&rbrack; Lucas V谩zquez4: Exposed in transition and lacking the speed and athleticism to cover the gaps he leaves when venturing forward. Needs a more pessimistic attitude when the ball is on the opposite flank, occupying better spots in rest defense.\n\nAntonio Rudiger5: Several unnecessary long distance shots to hurt Madrids rhythm and reinforce Leganes game plan. Playing with too many matches in his legs and its beginning to show.\', \'score\': 0.8832463, \'raw_content\': None}, {\'url\': \'https://www.espn.com/soccer/match/_/gameId/704946\', \'title\': \'Real Madrid vs. Legan茅s (Mar 29, 2025) Live Score - ESPN\', \'content\': \'Match Formations 路 13. Lunin 路 20. Garc铆a 路 22. R眉diger 路 35. Asencio 路 17. V谩zquez 路 5. Bellingham 路 10. Modric 路 6. Camavinga.\', \'score\': 0.86413884, \'raw_content\': None}&rbrack;, \'response_time\': 0.89}), AIMessage(content=\'\', additional_kwargs={opening_brace}\'tool_calls\': &lbrack;{opening_brace}\'function\': {\'arguments\': {opening_brace}\'query\': \'Real Madrid vs Leganes starting lineup\'{closing_brace}, \'name\': \'tavily_search_results_json\', \'description\': None}, \'id\': \'0\', \'type\': \'function\'{closing_brace}&rbrack;}, response_metadata={opening_brace}\'token_usage\': {\'completion_tokens\': 27, \'prompt_tokens\': 2212, \'total_tokens\': 2239}, \'model\': \'\', \'finish_reason\': \'stop\'{closing_brace}, id=\'run-68867df1-2012-47ac-9f01-42b071ef3a1f-0\', tool_calls=&lbrack;{opening_brace}\'name\': \'tavily_search_results_json\', \'args\': {opening_brace}\'query\': \'Real Madrid vs Leganes starting lineup\'{closing_brace}, \'id\': \'0\', \'type\': \'tool_call\'{closing_brace}&rbrack;), ToolMessage(content=\'&lbrack;{opening_brace}"title": "Starting lineups of Real Madrid and Legan茅s", "url": "https://www.realmadrid.com/en-US/news/football/first-team/latest-news/once-inicial-del-real-madrid-contra-el-leganes-29-03-2025", "content": "Starting lineups of Real Madrid and Legan漏s\\n\\n\\n\\nThe Whites芒\x80\x99 team is: Lunin, Lucas V., Asencio, R录diger, Fran Garc\xada, Arda G录ler, Modri\x87, Camavinga, Bellingham, Brahim and Mbapp漏.\\n\\n\\n\\n\\n\\nReal Madrid\xa0have named their starting line-up for the game against Legan漏s on matchday 29 of\xa0LaLiga, which will be played at the\xa0Santiago Bernab漏u\xa0(9 pm CET). &lbrack;...&rbrack; Real Madrid starting line-up:\\n13. Lunin\\n17. Lucas V.\\n35. Asencio\\n22. R录diger\\n20. Fran Garc\xada\\n15. Arda G录ler\\n10. Modri\x87\\n6. Camavinga\\n5. Bellingham\\n21. Brahim\\n9. Mbapp漏.\\n\\nSubstitutes:\\n26. Fran Gonz隆lez\\n34. Sergio Mestre\\n4. Alaba\\n7. Vini Jr.\\n8. Valverde\\n11. Rodrygo\\n14. Tchouameni\\n16. Endrick\\n18. Vallejo\\n43. Diego Aguado.\\n\\nLegan漏s starting line-up:\\n13. Dmitrovic\\n5. Tapia\\n6. Sergio G.\\n7. \x93scar\\n10. Raba\\n11. Cruz\\n12. V. Rosier\\n17. Neyou\\n19. Diego G.\\n20. Javi Hern隆ndez\\n22. Nastasic. &lbrack;...&rbrack; Suplentes:\\n1. Juan Soriano\\n36. Abajas\\n2. A. Alti\\n3. Jorge S隆enz\\n8. Cisse\\n9. Miguel\\n14. Darko\\n18. Duk\\n21. R. L鲁pez\\n23. Munir\\n24. Chicco\\n30. I. Diomande.\\n\\n\\n\\nDownload Now\\n\\nOfficial App Fan\\n\\nReal Madrid 漏 2025 All rights reserved", "score": 0.9465623}, {opening_brace}"title": "Real Madrid vs. Leganes lineups, confirmed starting 11, team news ...", "url": "https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-lineups-starting-11-team-news-injuries/aac757d10cc7b9a084995b4d", "content": "Real Madrid starting lineup (4-3-3, right to left): Lunin (GK)  Vazquez, Rudiger, Asencio, Garcia  Modric, Bellingham, Camavinga  B. Diaz,", "score": 0.9224337}&rbrack;\', name=\'tavily_search_results_json\', id=\'46721f2b-2df2-4da2-831a-ce94f6b4ff8f\', tool_call_id=\'0\', artifact={opening_brace}\'query\': \'Real Madrid vs Leganes starting lineup\', \'follow_up_questions\': None, \'answer\': None, \'images\': &lbrack;&rbrack;, \'results\': &lbrack;{\'url\': \'https://www.realmadrid.com/en-US/news/football/first-team/latest-news/once-inicial-del-real-madrid-contra-el-leganes-29-03-2025\', \'title\': \'Starting lineups of Real Madrid and Legan茅s\', \'content\': \'Starting lineups of Real Madrid and Legan漏s\n\n\n\nThe Whites芒\x80\x99 team is: Lunin, Lucas V., Asencio, R录diger, Fran Garc\xada, Arda G录ler, Modri\x87, Camavinga, Bellingham, Brahim and Mbapp漏.\n\n\n\n\n\nReal Madrid\xa0have named their starting line-up for the game against Legan漏s on matchday 29 of\xa0LaLiga, which will be played at the\xa0Santiago Bernab漏u\xa0(9 pm CET). &lbrack;...&rbrack; Real Madrid starting line-up:\n13. Lunin\n17. Lucas V.\n35. Asencio\n22. R录diger\n20. Fran Garc\xada\n15. Arda G录ler\n10. Modri\x87\n6. Camavinga\n5. Bellingham\n21. Brahim\n9. Mbapp漏.\n\nSubstitutes:\n26. Fran Gonz隆lez\n34. Sergio Mestre\n4. Alaba\n7. Vini Jr.\n8. Valverde\n11. Rodrygo\n14. Tchouameni\n16. Endrick\n18. Vallejo\n43. Diego Aguado.\n\nLegan漏s starting line-up:\n13. Dmitrovic\n5. Tapia\n6. Sergio G.\n7. \x93scar\n10. Raba\n11. Cruz\n12. V. Rosier\n17. Neyou\n19. Diego G.\n20. Javi Hern隆ndez\n22. Nastasic. &lbrack;...&rbrack; Suplentes:\n1. Juan Soriano\n36. Abajas\n2. A. Alti\n3. Jorge S隆enz\n8. Cisse\n9. Miguel\n14. Darko\n18. Duk\n21. R. L鲁pez\n23. Munir\n24. Chicco\n30. I. Diomande.\n\n\n\nDownload Now\n\nOfficial App Fan\n\nReal Madrid 漏 2025 All rights reserved\', \'score\': 0.9465623, \'raw_content\': None}, {\'url\': \'https://www.sportingnews.com/us/soccer/news/real-madrid-leganes-lineups-starting-11-team-news-injuries/aac757d10cc7b9a084995b4d\', \'title\': \'Real Madrid vs. Leganes lineups, confirmed starting 11, team news ...\', \'content\': \'Real Madrid starting lineup (4-3-3, right to left): Lunin (GK)  Vazquez, Rudiger, Asencio, Garcia  Modric, Bellingham, Camavinga  B. Diaz,\', \'score\': 0.9224337, \'raw_content\': None}&rbrack;, \'response_time\': 2.3}), AIMessage(content=\'The starting lineup for Real Madrid in their match against Legan茅s was: Lunin (GK), V谩zquez, R眉diger, Asencio, Fran Garc铆a, Modric, Bellingham, Camavinga, Brahim, Arda G眉ler, and Mbapp茅. Notable players like Vini Jr., Rodrygo, and Valverde were on the bench.\', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}\'token_usage\': {\'completion_tokens\': 98, \'prompt_tokens\': 2954, \'total_tokens\': 3052}, \'model\': \'\', \'finish_reason\': \'stop\'{closing_brace}, id=\'run-0bd921c6-1d94-4a4c-9d9c-d255d301e2d5-0\')&rbrack;}, next=(), config={\'configurable\': {\'thread_id\': \'3\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1f010a50-49f2-6904-800c-ec8d67fe5b92\'{closing_brace}{closing_brace}, metadata={\'source\': \'loop\', \'writes\': {opening_brace}\'chatbot_node\': {opening_brace}\'messages\': &lbrack;AIMessage(content=\'The starting lineup for Real Madrid in their match against Legan茅s was: Lunin (GK), V谩zquez, R眉diger, Asencio, Fran Garc铆a, Modric, Bellingham, Camavinga, Brahim, Arda G眉ler, and Mbapp茅. Notable players like Vini Jr., Rodrygo, and Valverde were on the bench.\', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}\'token_usage\': {\'completion_tokens\': 98, \'prompt_tokens\': 2954, \'total_tokens\': 3052}, \'model\': \'\', \'finish_reason\': \'stop\'{closing_brace}, id=\'run-0bd921c6-1d94-4a4c-9d9c-d255d301e2d5-0\')&rbrack;{closing_brace}{closing_brace}, \'thread_id\': \'3\', \'step\': 12, \'parents\': {opening_brace}{closing_brace{closing_brace}{closing_brace}, created_at=\'2025-04-03T16:02:18.167222+00:00\', parent_config={\'configurable\': {\'thread_id\': \'3\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1f010a50-1feb-6534-800b-079c102aaa71\'{closing_brace}{closing_brace}, tasks=())',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si queremos ver el siguiente nodo a procesar, podemos usar el atributo <code>next</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">snapshot</span><span class="o">.</span><span class="n">next</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '()',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Dado que el grafo ha finalizado, <code>next</code> est谩 vac铆o. Si obtienes un estado desde dentro de una invocaci贸n del grafo, <code>next</code> indica qu茅 nodo se ejecutar谩 a continuaci贸n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La instant谩nea anterior (<code>snapshot</code>) contiene los valores de estado actuales, la configuraci贸n correspondiente y el siguiente nodo (<code>next</code>) a procesar. En nuestro caso, el gr谩fico ha alcanzado el estado <code>END</code>, por eso <code>next</code> est谩 vac铆o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a volver a escribir todo el c贸digo para que sea m谩s legible.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
          '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
          '<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
          '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">ToolMessage</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>',
          '',
          '<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>',
          '<span class="kn">import</span> <span class="nn">json</span>',
          '',
          '<span class="kn">import</span> <span class="nn">os</span>',
          '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
          '',
          '<span class="kn">import</span> <span class="nn">dotenv</span>',
          '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
          '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
          '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
          '',
          '<span class="c1"># State</span>',
          '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
          '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
          '',
          '<span class="c1"># Tools</span>',
          '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
          '<span class="n">tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool</span><span class="p">]</span>',
          '',
          '<span class="c1"># Create the LLM model</span>',
          '<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>',
          '<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
          '    <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>',
          '    <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>',
          '    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
          '    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="c1"># Create the chat model</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>',
          '<span class="c1"># Create the LLM with tools</span>',
          '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Tool node</span>',
          '<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Functions</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]}</span>',
          '',
          '<span class="c1"># Start to build the graph</span>',
          '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add nodes to the graph</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add edges</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span> <span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '',
          '<span class="c1"># Compile the graph</span>',
          '<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>',
          '<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>',
          '',
          '<span class="c1"># Display the graph</span>',
          '<span class="k">try</span><span class="p">:</span>',
          '    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>',
          '<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:',
          '1. Check your internet connection and try again',
          '2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`',
          '3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">"1"</span>',
          '<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="n">user_input</span> <span class="o">=</span> <span class="s2">"Hi there! My name is Maximo."</span>',
          '',
          '<span class="c1"># The config is the **second positional argument** to stream() or invoke()!</span>',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config_USER1</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Hi there! My name is Maximo.',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Hello Maximo! It\'s nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">"Do you remember my name?"</span>',
          '',
          '<span class="c1"># The config is the **second positional argument** to stream() or invoke()!</span>',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config_USER1</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Do you remember my name?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Yes, I remember your name! You mentioned it\'s Maximo. It\'s nice to chat with you, Maximo. How can I assist you today?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p><strong>隆Felicidades!</strong> Nuestro chatbot ahora puede mantener el estado de conversaci贸n en todas las sesiones gracias al sistema de puntos de control (<code>checkpoints</code>) de <code>LangGraph</code>. Esto abre posibilidades para interacciones m谩s naturales y contextuales. El control de <code>LangGraph</code> incluso maneja estados de grafos complejos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="M%C3%A1s">M谩s<a class="anchor-link" href="#M%C3%A1s"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Chatbot-con-mensaje-de-resumen">Chatbot con mensaje de resumen<a class="anchor-link" href="#Chatbot-con-mensaje-de-resumen"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 37" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si vamos a manejar el contexto de la conversaci贸n para no gastar muchos tokens, una cosa que podemos hacer para mejorar la conversaci贸n es a帽adir un mensaje con el resumen de la conversaci贸n. Esto puede ser 煤til para el ejemplo anterior, en el que hemos filtrado tanto el estado que el LLM no tiene contexto suficiente.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">RemoveMessage</span><span class="p">,</span> <span class="n">trim_messages</span><span class="p">,</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">RemoveMessage</span>
      <span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="n">memory_saver</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
          <span class="n">summary</span><span class="p">:</span> <span class="nb">str</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Print functions</span>
      <span class="k">def</span> <span class="nf">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
          <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">):</span>
              <span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
              <span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
                  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="s2">&lbrack;HumanMessage]: </span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">SystemMessage</span><span class="p">):</span>
              <span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
              <span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
                  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="s2">[SystemMessage]: </span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>
              <span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
              <span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
                  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="s2">[AIMessage]: </span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">RemoveMessage</span><span class="p">):</span>
              <span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
              <span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
                  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="s2">[RemoveMessage]: </span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">message_content</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">content</span>
              <span class="n">message_lines</span> <span class="o">=</span> <span class="n">message_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">message_lines</span><span class="p">):</span>
                  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="s2">[</span><span class="si">{opening_brace}</span><span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span><span class="si">{closing_brace}</span><span class="s2">]: </span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
      <span class="k">def</span> <span class="nf">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"summary"</span><span class="p">):</span>
              <span class="n">summary_lines</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"summary"</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">summary_lines</span><span class="p">):</span>
                  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="s2">Summary of the conversation: </span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t\t</span><span class="s2">No summary of the conversation"</span><span class="p">)</span>
      
      <span class="k">def</span> <span class="nf">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">summary</span><span class="p">:</span>
              <span class="n">summary_lines</span> <span class="o">=</span> <span class="n">summary</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">summary_lines</span><span class="p">):</span>
                  <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="s2">Summary of the conversation: </span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t\t</span><span class="s2">No summary of the conversation"</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">filter_messages</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">--- 1 messages (input to filter_messages) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span>
              <span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
          <span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Delete all but the 2 most recent messages if there are more than 2</span>
          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
              <span class="n">delete_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">RemoveMessage</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">id</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][:</span><span class="o">-</span><span class="mi">2</span><span class="p">]]</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">delete_messages</span> <span class="o">=</span> <span class="p">[]</span>
      
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">--- 1 messages (output of filter_messages) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">delete_messages</span><span class="p">:</span>
              <span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
          <span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">delete_messages</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">trim_messages_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># print the messages received from filter_messages_node</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n\t</span><span class="s2">--- 2 messages (input to trim_messages) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span>
              <span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
          <span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Trim the messages based on the specified parameters</span>
          <span class="n">trimmed_messages</span> <span class="o">=</span> <span class="n">trim_messages</span><span class="p">(</span>
              <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">],</span>
              <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>       <span class="c1"># Maximum tokens allowed in the trimmed list</span>
              <span class="n">strategy</span><span class="o">=</span><span class="s2">"last"</span><span class="p">,</span>     <span class="c1"># Keep the latest messages</span>
              <span class="n">token_counter</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>   <span class="c1"># Use the LLM's tokenizer to count tokens</span>
              <span class="n">allow_partial</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Allow cutting messages mid-way if needed</span>
          <span class="p">)</span>
      
          <span class="c1"># Identify the messages that must be removed</span>
          <span class="c1"># This is crucial: determine which messages are in 'state["messages"]' but not in 'trimmed_messages'</span>
          <span class="n">original_ids</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="n">m</span><span class="o">.</span><span class="n">id</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]{closing_brace}</span>
          <span class="n">trimmed_ids</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="n">m</span><span class="o">.</span><span class="n">id</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">trimmed_messages</span><span class="p">{closing_brace}</span>
          <span class="n">ids_to_remove</span> <span class="o">=</span> <span class="n">original_ids</span> <span class="o">-</span> <span class="n">trimmed_ids</span>
          
          <span class="c1"># Create a RemoveMessage for each message that must be removed</span>
          <span class="n">messages_to_remove</span> <span class="o">=</span> <span class="p">[</span><span class="n">RemoveMessage</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="n">msg_id</span><span class="p">)</span> <span class="k">for</span> <span class="n">msg_id</span> <span class="ow">in</span> <span class="n">ids_to_remove</span><span class="p">]</span>
      
          <span class="c1"># Print the result of the trimming</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">--- 2 messages (output of trim_messages - after trimming) ---"</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">trimmed_messages</span><span class="p">:</span>
              <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">trimmed_messages</span><span class="p">:</span>
                  <span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="s2">"[Empty list - No messages after trimming]"</span><span class="p">)</span>
          <span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">messages_to_remove</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># Get summary of the conversation if it exists</span>
          <span class="n">summary</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"summary"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
      
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n\t</span><span class="s2">--- 3 messages (input to chat_model_node) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span>
              <span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
          <span class="n">print_state_summary</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># If there is a summary, add it to the system message</span>
          <span class="k">if</span> <span class="n">summary</span><span class="p">:</span>
              <span class="c1"># Add the summary to the system message</span>
              <span class="n">system_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Summary of the conversation earlier: </span><span class="si">{opening_brace}</span><span class="n">summary</span><span class="si">{closing_brace}</span><span class="s2">"</span>
      
              <span class="c1"># Add the system message to the messages at the beginning</span>
              <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_message</span><span class="p">)]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]</span>
          
          <span class="c1"># If there is no summary, just return the messages</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">--- 3 messages (input to chat_model_node) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
              <span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
          <span class="n">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Invoke the LLM with the messages</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
      
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">--- 3 messages (output of chat_model_node) ---"</span><span class="p">)</span>
          <span class="n">print_message</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
          <span class="n">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Return the LLM's response in the correct state format</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">summarize_conversation</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># Get summary of the conversation if it exists</span>
          <span class="n">summary</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"summary"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
      
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n\t</span><span class="s2">--- 4 messages (input to summarize_conversation) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span>
              <span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
          <span class="n">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># If there is a summary, add it to the system message</span>
          <span class="k">if</span> <span class="n">summary</span><span class="p">:</span>
              <span class="n">summary_message</span> <span class="o">=</span> <span class="p">(</span>
                  <span class="sa">f</span><span class="s2">"This is a summary of the conversation to date: </span><span class="si">{opening_brace}</span><span class="n">summary</span><span class="si">{closing_brace}</span><span class="se">\n\n</span><span class="s2">"</span>
                  <span class="s2">"Extend the summary by taking into account the new messages above."</span>
              <span class="p">)</span>
          
          <span class="c1"># If there is no summary, create a new one</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">summary_message</span> <span class="o">=</span> <span class="s2">"Create a summary of the conversation above."</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">--- 4 summary message ---"</span><span class="p">)</span>
          <span class="n">summary_lines</span> <span class="o">=</span> <span class="n">summary_message</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">summary_lines</span><span class="p">):</span>
              <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
              <span class="k">else</span><span class="p">:</span>
                  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t\t</span><span class="si">{opening_brace}</span><span class="n">line</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="n">print_summary</span><span class="p">(</span><span class="n">summary</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Add prompt to the messages</span>
          <span class="n">messages</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">summary_message</span><span class="p">)]</span>
      
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">--- 4 messages (input to summarize_conversation with summary) ---"</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">:</span>
              <span class="n">print_message</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Invoke the LLM with the messages</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
      
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">--- 4 messages (output of summarize_conversation) ---"</span><span class="p">)</span>
          <span class="n">print_message</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\t</span><span class="s2">------------------------------------------------"</span><span class="p">)</span>
      
          <span class="c1"># Return the summary message in the correct state format</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"summary"</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"filter_messages_node"</span><span class="p">,</span> <span class="n">filter_messages</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"trim_messages_node"</span><span class="p">,</span> <span class="n">trim_messages_node</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"summarize_conversation_node"</span><span class="p">,</span> <span class="n">summarize_conversation</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"filter_messages_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"filter_messages_node"</span><span class="p">,</span> <span class="s2">"trim_messages_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"trim_messages_node"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="s2">"summarize_conversation_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"summarize_conversation_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory_saver</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv25.webp" width="268" height="531" alt="image uv 26" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, tenemos:</p>
      <ul>
      <li>Funci贸n de filtrado de mensajes: Si en el estado hay m谩s de 2 mensajes, se eliminan todos los mensajes excepto los 2 煤ltimos.</li>
      <li>Funci贸n de trimado de mensajes: Se eliminan los mensajes que superan los 100 tokens.</li>
      <li>Funci贸n de chatbot: Se ejecuta el modelo con los mensajes filtrados y trimados. Adem谩s, si existe un resumen, se a帽ade al mensaje de sistema.</li>
      <li>Funci贸n de resumen: Se crea un resumen de la conversaci贸n.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una funci贸n para imprimir los mensajes del grafo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Colors for the terminal</span>',
      '<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;32m"</span>',
      '<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;33m"</span>',
      '<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;0m"</span>',
      ' ',
      ' ',
      '<span class="k">def</span> <span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>',
      '    <span class="c1"># Initialize a flag to track if an assistant response has been printed</span>',
      '    <span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">False</span>',
      ' ',
      '    <span class="c1"># Print the user\'s input immediately</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n\\n</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">user_input</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
      '    ',
      '    <span class="c1"># Create the user\'s message with the HumanMessage class</span>',
      '    <span class="n">user_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">user_input</span><span class="p">)</span>',
      '    ',
      '    <span class="c1"># Stream events from the graph execution</span>',
      '    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">user_message</span><span class="p">]},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
      '        <span class="c1"># event is a dictionary mapping node names to their output</span>',
      '        <span class="c1"># Example: {opening_brace}\'chatbot_node\': {opening_brace}\'messages\': [...]{closing_brace}{closing_brace} or {opening_brace}\'summarize_conversation_node\': {opening_brace}\'summary\': \'...\'{closing_brace}{closing_brace}</span>',
      ' ',
      '        <span class="c1"># Iterate through node name and its output</span>',
      '        <span class="k">for</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>',
      '            <span class="c1"># Check if this event is from the chatbot node which should contain the assistant\'s reply</span>',
      '            <span class="k">if</span> <span class="n">node_name</span> <span class="o">==</span> <span class="s1">\'messages\'</span><span class="p">:</span>',
      '                <span class="c1"># Ensure the output format is as expected (list of messages)</span>',
      '                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>',
      '                    <span class="c1"># Get the messages from the event</span>',
      '                    <span class="n">messages</span> <span class="o">=</span> <span class="n">value</span>',
      '                    <span class="c1"># Ensure \'messages\' is a non-empty list</span>',
      '                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="n">messages</span><span class="p">:</span>',
      '                        <span class="c1"># Get the last message (presumably the assistant\'s reply)</span>',
      '                        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '                        <span class="c1"># Ensure the message is an instance of AIMessage</span>',
      '                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>',
      '                            <span class="c1"># Ensure the message has content to display</span>',
      '                            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="s1">\'content\'</span><span class="p">):</span>',
      '                                <span class="c1"># Print the assistant\'s message content</span>',
      '                                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{</span><span class="n">last_message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '                                <span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># Mark that we\'ve printed the response</span>',
      '    ',
      '    <span class="c1"># Fallback if no assistant response was printed (e.g., graph error before chatbot_node)</span>',
      '    <span class="k">if</span> <span class="ow">not</span> <span class="n">assistant_response_printed</span><span class="p">:</span>',
      '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">[No response generated or error occurred]"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>

















































      
      <section class="section-block-markdown-cell">
      <p>Ahora ejecutamos el grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">"1"</span>',
          '<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">{closing_brace}{closing_brace}</span>',
          '<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>',
          '    <span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n\\n</span><span class="s2">User: "</span><span class="p">)</span>',
          '    <span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"quit"</span><span class="p">,</span> <span class="s2">"exit"</span><span class="p">,</span> <span class="s2">"q"</span><span class="p">]:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">Exiting..."</span><span class="p">)</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">Goodbye!"</span><span class="p">)</span>',
          '        <span class="k">break</span>',
          '    ',
          '    <span class="n">events</span> <span class="o">=</span> <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">,</span> <span class="n">config_USER1</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '<span class="ansi-green-fg">User: </span>Hello',
          '	--- 1 messages (input to filter_messages) ---',
          '		&lbrack;HumanMessage]: Hello',
          '		No summary of the conversation',
          '	------------------------------------------------',
          '	--- 1 messages (output of filter_messages) ---',
          '		No summary of the conversation',
          '	------------------------------------------------',
          '	--- 2 messages (input to trim_messages) ---',
          '		&lbrack;HumanMessage]: Hello',
          '		No summary of the conversation',
          '	------------------------------------------------',
          '	--- 2 messages (output of trim_messages - after trimming) ---',
          '		&lbrack;HumanMessage]: Hello',
          '		No summary of the conversation',
          '	------------------------------------------------',
          '	--- 3 messages (input to chat_model_node) ---',
          '		&lbrack;HumanMessage]: Hello',
          '		No summary of the conversation',
          '	------------------------------------------------',
          '	--- 3 messages (input to chat_model_node) ---',
          '		&lbrack;HumanMessage]: Hello',
          '		No summary of the conversation',
          '	------------------------------------------------',
          '	--- 3 messages (output of chat_model_node) ---',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		No summary of the conversation',
          '	------------------------------------------------',
          '<span class="ansi-yellow-fg">Assistant: </span>Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '	--- 4 messages (input to summarize_conversation) ---',
          '		&lbrack;HumanMessage]: Hello',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		No summary of the conversation',
          '	------------------------------------------------',
          '	--- 4 summary message ---',
          '		Create a summary of the conversation above.',
          '		No summary of the conversation',
          '	------------------------------------------------',
          '	--- 4 messages (input to summarize_conversation with summary) ---',
          '		&lbrack;HumanMessage]: Hello',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		&lbrack;HumanMessage]: Create a summary of the conversation above.',
          '	------------------------------------------------',
          '	--- 4 messages (output of summarize_conversation) ---',
          '		[AIMessage]: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '<span class="ansi-yellow-fg">Assistant: </span>Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '<span class="ansi-green-fg">User: </span>I am studying about langgraph, do you know it?',
          '	--- 1 messages (input to filter_messages) ---',
          '		&lbrack;HumanMessage]: Hello',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		&lbrack;HumanMessage]: I am studying about langgraph, do you know it?',
          '		Summary of the conversation: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 1 messages (output of filter_messages) ---',
          '		[RemoveMessage]: ',
          '		Summary of the conversation: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 2 messages (input to trim_messages) ---',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		&lbrack;HumanMessage]: I am studying about langgraph, do you know it?',
          '		Summary of the conversation: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 2 messages (output of trim_messages - after trimming) ---',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		&lbrack;HumanMessage]: I am studying about langgraph, do you know it?',
          '		Summary of the conversation: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 3 messages (input to chat_model_node) ---',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		&lbrack;HumanMessage]: I am studying about langgraph, do you know it?',
          '		Summary of the conversation: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 3 messages (input to chat_model_node) ---',
          '		[SystemMessage]: Summary of the conversation earlier: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		&lbrack;HumanMessage]: I am studying about langgraph, do you know it?',
          '		Summary of the conversation: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 3 messages (output of chat_model_node) ---',
          '		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.',
          '		',
          '		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		',
          '		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.',
          '		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.',
          '		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.',
          '		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		Summary of the conversation: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '<span class="ansi-yellow-fg">Assistant: </span>Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.',
          'LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.',
          '2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.',
          '3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.',
          '4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.',
          'If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '	--- 4 messages (input to summarize_conversation) ---',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		&lbrack;HumanMessage]: I am studying about langgraph, do you know it?',
          '		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.',
          '		',
          '		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		',
          '		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.',
          '		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.',
          '		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.',
          '		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		Summary of the conversation: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 4 summary message ---',
          '		This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		Extend the summary by taking into account the new messages above.',
          '		Summary of the conversation: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 4 messages (input to summarize_conversation with summary) ---',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		&lbrack;HumanMessage]: I am studying about langgraph, do you know it?',
          '		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.',
          '		',
          '		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		',
          '		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.',
          '		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.',
          '		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.',
          '		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		&lbrack;HumanMessage]: This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		Extend the summary by taking into account the new messages above.',
          '	------------------------------------------------',
          '	--- 4 messages (output of summarize_conversation) ---',
          '		[AIMessage]: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '<span class="ansi-yellow-fg">Assistant: </span>Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.',
          'LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.',
          '2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.',
          '3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.',
          '4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.',
          'If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '<span class="ansi-green-fg">User: </span>I would like to know about using langsmith with huggingface llms, the integration of huggingface',
          '	--- 1 messages (input to filter_messages) ---',
          '		[AIMessage]: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		&lbrack;HumanMessage]: I am studying about langgraph, do you know it?',
          '		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.',
          '		',
          '		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		',
          '		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.',
          '		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.',
          '		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.',
          '		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		&lbrack;HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface',
          '		Summary of the conversation: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 1 messages (output of filter_messages) ---',
          '		[RemoveMessage]: ',
          '		[RemoveMessage]: ',
          '		Summary of the conversation: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 2 messages (input to trim_messages) ---',
          '		[AIMessage]: Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.',
          '		',
          '		LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		',
          '		1. **Visualizing Model Architecture**: Provides a clear and detailed view of how different components of a language model are connected.',
          '		2. **Comparing Models**: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.',
          '		3. **Understanding Training Processes**: Helps in understanding the training dynamics and the flow of data through the model.',
          '		4. **Identifying Bottlenecks**: Can help in identifying potential bottlenecks or areas for improvement in the model.',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		&lbrack;HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface',
          '		Summary of the conversation: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 2 messages (output of trim_messages - after trimming) ---',
          '		&lbrack;HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface',
          '		Summary of the conversation: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 3 messages (input to chat_model_node) ---',
          '		&lbrack;HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface',
          '		Summary of the conversation: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 3 messages (input to chat_model_node) ---',
          '		[SystemMessage]: Summary of the conversation earlier: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '		&lbrack;HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface',
          '		Summary of the conversation: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 3 messages (output of chat_model_node) ---',
          '		[AIMessage]: Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Heres a detailed look at how you can use LangSmith with Hugging Face models:',
          '		',
          '		### What is LangSmith?',
          '		LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:',
          '		- **Model Management**: Manage and version control your language models.',
          '		- **Data Labeling**: Annotate and label data for training and evaluation.',
          '		- **Model Evaluation**: Evaluate and compare different models and versions.',
          '		- **API Integration**: Integrate with various NLP tools and platforms.',
          '		',
          '		### What is Hugging Face?',
          '		Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:',
          '		- **Pre-trained Models**: Access to a vast library of pre-trained models.',
          '		- **Transformers Library**: A powerful library for working with transformer models.',
          '		- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.',
          '		',
          '		### Integrating LangSmith with Hugging Face Models',
          '		',
          '		#### Step-by-Step Guide',
          '		',
          '		1. **Install Required Libraries**',
          '		   Ensure you have the necessary libraries installed:',
          '		   ```bash',
          '		   pip install transformers datasets langsmith',
          '		   ```',
          '		',
          '		2. **Load a Hugging Face Model**',
          '		   Use the `transformers` library to load a pre-trained model:',
          '		   ```python',
          '		   from transformers import AutoModelForSequenceClassification, AutoTokenizer',
          '		',
          '		   model_name = "distilbert-base-uncased"',
          '		   tokenizer = AutoTokenizer.from_pretrained(model_name)',
          '		   model = AutoModelForSequenceClassification.from_pretrained(model_name)',
          '		   ```',
          '		',
          '		3. **Integrate with LangSmith**',
          '		   - **Initialize LangSmith Client**:',
          '		     ```python',
          '		     from langsmith import Client',
          '		',
          '		     client = Client()',
          '		     ```',
          '		',
          '		   - **Create or Load a Dataset**:',
          '		     ```python',
          '		     from datasets import Dataset',
          '		',
          '		     # Example dataset',
          '		     data = {',
          '		         "text": ["This is a positive review.", "This is a negative review."],',
          '		         "label": [1, 0]',
          '		     }',
          '		     dataset = Dataset.from_dict(data)',
          '		',
          '		     # Save dataset to LangSmith',
          '		     dataset_id = client.create_dataset(name="my_dataset", data=dataset)',
          '		     ```',
          '		',
          '		   - **Evaluate the Model**:',
          '		     ```python',
          '		     from langsmith import EvaluationResult',
          '		',
          '		     def evaluate_model(dataset, tokenizer, model):',
          '		         results = []',
          '		         for example in dataset:',
          '		             inputs = tokenizer(example["text"], return_tensors="pt")',
          '		             outputs = model(**inputs)',
          '		             predicted_label = outputs.logits.argmax().item()',
          '		             result = EvaluationResult(',
          '		                 example_id=example["id"],',
          '		                 predicted_label=predicted_label,',
          '		                 ground_truth_label=example["label"]',
          '		             )',
          '		             results.append(result)',
          '		         return results',
          '		',
          '		     evaluation_results = evaluate_model(dataset, tokenizer, model)',
          '		     ```',
          '		',
          '		   - **Upload Evaluation Results to LangSmith**:',
          '		     ```python',
          '		     client.log_results(dataset_id, evaluation_results)',
          '		     ```',
          '		',
          '		4. **Visualize and Analyze Results**',
          '		   - Use LangSmiths web interface to visualize the evaluation results.',
          '		   - Compare different models and versions to identify the best performing model.',
          '		',
          '		### Additional Tips',
          '		- **Model Tuning**: Use Hugging Faces `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.',
          '		- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.',
          '		- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.',
          '		',
          '		By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.',
          '		',
          '		If you have any specific questions or need further assistance, feel free to ask!',
          '		Summary of the conversation: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '<span class="ansi-yellow-fg">Assistant: </span>Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Heres a detailed look at how you can use LangSmith with Hugging Face models:',
          '### What is LangSmith?',
          'LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:',
          '- **Model Management**: Manage and version control your language models.',
          '- **Data Labeling**: Annotate and label data for training and evaluation.',
          '- **Model Evaluation**: Evaluate and compare different models and versions.',
          '- **API Integration**: Integrate with various NLP tools and platforms.',
          '### What is Hugging Face?',
          'Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:',
          '- **Pre-trained Models**: Access to a vast library of pre-trained models.',
          '- **Transformers Library**: A powerful library for working with transformer models.',
          '- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.',
          '### Integrating LangSmith with Hugging Face Models',
          '#### Step-by-Step Guide',
          '1. **Install Required Libraries**',
          '   Ensure you have the necessary libraries installed:',
          '   ```bash',
          '   pip install transformers datasets langsmith',
          '   ```',
          '2. **Load a Hugging Face Model**',
          '   Use the `transformers` library to load a pre-trained model:',
          '   ```python',
          '   from transformers import AutoModelForSequenceClassification, AutoTokenizer',
          '   model_name = "distilbert-base-uncased"',
          '   tokenizer = AutoTokenizer.from_pretrained(model_name)',
          '   model = AutoModelForSequenceClassification.from_pretrained(model_name)',
          '   ```',
          '3. **Integrate with LangSmith**',
          '   - **Initialize LangSmith Client**:',
          '     ```python',
          '     from langsmith import Client',
          '     client = Client()',
          '     ```',
          '   - **Create or Load a Dataset**:',
          '     ```python',
          '     from datasets import Dataset',
          '     # Example dataset',
          '     data = {',
          '         "text": ["This is a positive review.", "This is a negative review."],',
          '         "label": [1, 0]',
          '     }',
          '     dataset = Dataset.from_dict(data)',
          '     # Save dataset to LangSmith',
          '     dataset_id = client.create_dataset(name="my_dataset", data=dataset)',
          '     ```',
          '   - **Evaluate the Model**:',
          '     ```python',
          '     from langsmith import EvaluationResult',
          '     def evaluate_model(dataset, tokenizer, model):',
          '         results = []',
          '         for example in dataset:',
          '             inputs = tokenizer(example["text"], return_tensors="pt")',
          '             outputs = model(**inputs)',
          '             predicted_label = outputs.logits.argmax().item()',
          '             result = EvaluationResult(',
          '                 example_id=example["id"],',
          '                 predicted_label=predicted_label,',
          '                 ground_truth_label=example["label"]',
          '             )',
          '             results.append(result)',
          '         return results',
          '     evaluation_results = evaluate_model(dataset, tokenizer, model)',
          '     ```',
          '   - **Upload Evaluation Results to LangSmith**:',
          '     ```python',
          '     client.log_results(dataset_id, evaluation_results)',
          '     ```',
          '4. **Visualize and Analyze Results**',
          '   - Use LangSmiths web interface to visualize the evaluation results.',
          '   - Compare different models and versions to identify the best performing model.',
          '### Additional Tips',
          '- **Model Tuning**: Use Hugging Faces `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.',
          '- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.',
          '- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.',
          'By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.',
          'If you have any specific questions or need further assistance, feel free to ask!',
          '	--- 4 messages (input to summarize_conversation) ---',
          '		&lbrack;HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface',
          '		[AIMessage]: Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Heres a detailed look at how you can use LangSmith with Hugging Face models:',
          '		',
          '		### What is LangSmith?',
          '		LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:',
          '		- **Model Management**: Manage and version control your language models.',
          '		- **Data Labeling**: Annotate and label data for training and evaluation.',
          '		- **Model Evaluation**: Evaluate and compare different models and versions.',
          '		- **API Integration**: Integrate with various NLP tools and platforms.',
          '		',
          '		### What is Hugging Face?',
          '		Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:',
          '		- **Pre-trained Models**: Access to a vast library of pre-trained models.',
          '		- **Transformers Library**: A powerful library for working with transformer models.',
          '		- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.',
          '		',
          '		### Integrating LangSmith with Hugging Face Models',
          '		',
          '		#### Step-by-Step Guide',
          '		',
          '		1. **Install Required Libraries**',
          '		   Ensure you have the necessary libraries installed:',
          '		   ```bash',
          '		   pip install transformers datasets langsmith',
          '		   ```',
          '		',
          '		2. **Load a Hugging Face Model**',
          '		   Use the `transformers` library to load a pre-trained model:',
          '		   ```python',
          '		   from transformers import AutoModelForSequenceClassification, AutoTokenizer',
          '		',
          '		   model_name = "distilbert-base-uncased"',
          '		   tokenizer = AutoTokenizer.from_pretrained(model_name)',
          '		   model = AutoModelForSequenceClassification.from_pretrained(model_name)',
          '		   ```',
          '		',
          '		3. **Integrate with LangSmith**',
          '		   - **Initialize LangSmith Client**:',
          '		     ```python',
          '		     from langsmith import Client',
          '		',
          '		     client = Client()',
          '		     ```',
          '		',
          '		   - **Create or Load a Dataset**:',
          '		     ```python',
          '		     from datasets import Dataset',
          '		',
          '		     # Example dataset',
          '		     data = {',
          '		         "text": ["This is a positive review.", "This is a negative review."],',
          '		         "label": [1, 0]',
          '		     }',
          '		     dataset = Dataset.from_dict(data)',
          '		',
          '		     # Save dataset to LangSmith',
          '		     dataset_id = client.create_dataset(name="my_dataset", data=dataset)',
          '		     ```',
          '		',
          '		   - **Evaluate the Model**:',
          '		     ```python',
          '		     from langsmith import EvaluationResult',
          '		',
          '		     def evaluate_model(dataset, tokenizer, model):',
          '		         results = []',
          '		         for example in dataset:',
          '		             inputs = tokenizer(example["text"], return_tensors="pt")',
          '		             outputs = model(**inputs)',
          '		             predicted_label = outputs.logits.argmax().item()',
          '		             result = EvaluationResult(',
          '		                 example_id=example["id"],',
          '		                 predicted_label=predicted_label,',
          '		                 ground_truth_label=example["label"]',
          '		             )',
          '		             results.append(result)',
          '		         return results',
          '		',
          '		     evaluation_results = evaluate_model(dataset, tokenizer, model)',
          '		     ```',
          '		',
          '		   - **Upload Evaluation Results to LangSmith**:',
          '		     ```python',
          '		     client.log_results(dataset_id, evaluation_results)',
          '		     ```',
          '		',
          '		4. **Visualize and Analyze Results**',
          '		   - Use LangSmiths web interface to visualize the evaluation results.',
          '		   - Compare different models and versions to identify the best performing model.',
          '		',
          '		### Additional Tips',
          '		- **Model Tuning**: Use Hugging Faces `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.',
          '		- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.',
          '		- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.',
          '		',
          '		By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.',
          '		',
          '		If you have any specific questions or need further assistance, feel free to ask!',
          '		Summary of the conversation: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 4 summary message ---',
          '		This is a summary of the conversation to date: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		Extend the summary by taking into account the new messages above.',
          '		Summary of the conversation: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '	--- 4 messages (input to summarize_conversation with summary) ---',
          '		&lbrack;HumanMessage]: I would like to know about using langsmith with huggingface llms, the integration of huggingface',
          '		[AIMessage]: Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Heres a detailed look at how you can use LangSmith with Hugging Face models:',
          '		',
          '		### What is LangSmith?',
          '		LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:',
          '		- **Model Management**: Manage and version control your language models.',
          '		- **Data Labeling**: Annotate and label data for training and evaluation.',
          '		- **Model Evaluation**: Evaluate and compare different models and versions.',
          '		- **API Integration**: Integrate with various NLP tools and platforms.',
          '		',
          '		### What is Hugging Face?',
          '		Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:',
          '		- **Pre-trained Models**: Access to a vast library of pre-trained models.',
          '		- **Transformers Library**: A powerful library for working with transformer models.',
          '		- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.',
          '		',
          '		### Integrating LangSmith with Hugging Face Models',
          '		',
          '		#### Step-by-Step Guide',
          '		',
          '		1. **Install Required Libraries**',
          '		   Ensure you have the necessary libraries installed:',
          '		   ```bash',
          '		   pip install transformers datasets langsmith',
          '		   ```',
          '		',
          '		2. **Load a Hugging Face Model**',
          '		   Use the `transformers` library to load a pre-trained model:',
          '		   ```python',
          '		   from transformers import AutoModelForSequenceClassification, AutoTokenizer',
          '		',
          '		   model_name = "distilbert-base-uncased"',
          '		   tokenizer = AutoTokenizer.from_pretrained(model_name)',
          '		   model = AutoModelForSequenceClassification.from_pretrained(model_name)',
          '		   ```',
          '		',
          '		3. **Integrate with LangSmith**',
          '		   - **Initialize LangSmith Client**:',
          '		     ```python',
          '		     from langsmith import Client',
          '		',
          '		     client = Client()',
          '		     ```',
          '		',
          '		   - **Create or Load a Dataset**:',
          '		     ```python',
          '		     from datasets import Dataset',
          '		',
          '		     # Example dataset',
          '		     data = {',
          '		         "text": ["This is a positive review.", "This is a negative review."],',
          '		         "label": [1, 0]',
          '		     }',
          '		     dataset = Dataset.from_dict(data)',
          '		',
          '		     # Save dataset to LangSmith',
          '		     dataset_id = client.create_dataset(name="my_dataset", data=dataset)',
          '		     ```',
          '		',
          '		   - **Evaluate the Model**:',
          '		     ```python',
          '		     from langsmith import EvaluationResult',
          '		',
          '		     def evaluate_model(dataset, tokenizer, model):',
          '		         results = []',
          '		         for example in dataset:',
          '		             inputs = tokenizer(example["text"], return_tensors="pt")',
          '		             outputs = model(**inputs)',
          '		             predicted_label = outputs.logits.argmax().item()',
          '		             result = EvaluationResult(',
          '		                 example_id=example["id"],',
          '		                 predicted_label=predicted_label,',
          '		                 ground_truth_label=example["label"]',
          '		             )',
          '		             results.append(result)',
          '		         return results',
          '		',
          '		     evaluation_results = evaluate_model(dataset, tokenizer, model)',
          '		     ```',
          '		',
          '		   - **Upload Evaluation Results to LangSmith**:',
          '		     ```python',
          '		     client.log_results(dataset_id, evaluation_results)',
          '		     ```',
          '		',
          '		4. **Visualize and Analyze Results**',
          '		   - Use LangSmiths web interface to visualize the evaluation results.',
          '		   - Compare different models and versions to identify the best performing model.',
          '		',
          '		### Additional Tips',
          '		- **Model Tuning**: Use Hugging Faces `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.',
          '		- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.',
          '		- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.',
          '		',
          '		By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.',
          '		',
          '		If you have any specific questions or need further assistance, feel free to ask!',
          '		&lbrack;HumanMessage]: This is a summary of the conversation to date: Sure! Here\'s an extended summary of the conversation:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		Extend the summary by taking into account the new messages above.',
          '	------------------------------------------------',
          '	--- 4 messages (output of summarize_conversation) ---',
          '		[AIMessage]: Sure! Here\'s the extended summary of the conversation to date:',
          '		',
          '		---',
          '		',
          '		**User:** Hello',
          '		',
          '		**Qwen:** Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		',
          '		**User:** I am studying about langgraph, do you know it?',
          '		',
          '		**Qwen:** Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:',
          '		1. Visualizing Model Architecture',
          '		2. Comparing Models',
          '		3. Understanding Training Processes',
          '		4. Identifying Bottlenecks',
          '		',
          '		If you have specific questions or aspects of LangGraph you\'re interested in, feel free to let me know!',
          '		',
          '		**User:** This is a summary of the conversation to date: Sure! Here\'s a summary of the conversation above:',
          '		User: Hello',
          '		Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I\'m here to help.',
          '		User: Create a summary of the conversation above.',
          '		Qwen: [Provided the summary you are now reading.]',
          '		',
          '		Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** [Extended the summary you are now reading.]',
          '		',
          '		**User:** Is there anything else you need assistance with?',
          '		',
          '		**Qwen:** If you have any more questions about LangGraph or any other topic, feel free to ask! I\'m here to help with any information or assistance you need.',
          '		',
          '		---',
          '		',
          '		Is there anything else you need assistance with?',
          '	------------------------------------------------',
          '<span class="ansi-yellow-fg">Assistant: </span>Certainly! LangSmith and Hugging Face are both powerful tools in the domain of natural language processing (NLP), and integrating them can significantly enhance your workflow. Heres a detailed look at how you can use LangSmith with Hugging Face models:',
          '### What is LangSmith?',
          'LangSmith is a platform designed to help developers and researchers build, test, and deploy natural language applications. It offers features such as:',
          '- **Model Management**: Manage and version control your language models.',
          '- **Data Labeling**: Annotate and label data for training and evaluation.',
          '- **Model Evaluation**: Evaluate and compare different models and versions.',
          '- **API Integration**: Integrate with various NLP tools and platforms.',
          '### What is Hugging Face?',
          'Hugging Face is a leading company in the NLP domain, known for its transformers library. Hugging Face provides a wide array of pre-trained models and tools for NLP tasks, including:',
          '- **Pre-trained Models**: Access to a vast library of pre-trained models.',
          '- **Transformers Library**: A powerful library for working with transformer models.',
          '- **Hugging Face Hub**: A platform for sharing and accessing models, datasets, and metrics.',
          '### Integrating LangSmith with Hugging Face Models',
          '#### Step-by-Step Guide',
          '1. **Install Required Libraries**',
          '   Ensure you have the necessary libraries installed:',
          '   ```bash',
          '   pip install transformers datasets langsmith',
          '   ```',
          '2. **Load a Hugging Face Model**',
          '   Use the `transformers` library to load a pre-trained model:',
          '   ```python',
          '   from transformers import AutoModelForSequenceClassification, AutoTokenizer',
          '   model_name = "distilbert-base-uncased"',
          '   tokenizer = AutoTokenizer.from_pretrained(model_name)',
          '   model = AutoModelForSequenceClassification.from_pretrained(model_name)',
          '   ```',
          '3. **Integrate with LangSmith**',
          '   - **Initialize LangSmith Client**:',
          '     ```python',
          '     from langsmith import Client',
          '     client = Client()',
          '     ```',
          '   - **Create or Load a Dataset**:',
          '     ```python',
          '     from datasets import Dataset',
          '     # Example dataset',
          '     data = {',
          '         "text": ["This is a positive review.", "This is a negative review."],',
          '         "label": [1, 0]',
          '     }',
          '     dataset = Dataset.from_dict(data)',
          '     # Save dataset to LangSmith',
          '     dataset_id = client.create_dataset(name="my_dataset", data=dataset)',
          '     ```',
          '   - **Evaluate the Model**:',
          '     ```python',
          '     from langsmith import EvaluationResult',
          '     def evaluate_model(dataset, tokenizer, model):',
          '         results = []',
          '         for example in dataset:',
          '             inputs = tokenizer(example["text"], return_tensors="pt")',
          '             outputs = model(**inputs)',
          '             predicted_label = outputs.logits.argmax().item()',
          '             result = EvaluationResult(',
          '                 example_id=example["id"],',
          '                 predicted_label=predicted_label,',
          '                 ground_truth_label=example["label"]',
          '             )',
          '             results.append(result)',
          '         return results',
          '     evaluation_results = evaluate_model(dataset, tokenizer, model)',
          '     ```',
          '   - **Upload Evaluation Results to LangSmith**:',
          '     ```python',
          '     client.log_results(dataset_id, evaluation_results)',
          '     ```',
          '4. **Visualize and Analyze Results**',
          '   - Use LangSmiths web interface to visualize the evaluation results.',
          '   - Compare different models and versions to identify the best performing model.',
          '### Additional Tips',
          '- **Model Tuning**: Use Hugging Faces `Trainer` class to fine-tune models on your datasets and then evaluate them using LangSmith.',
          '- **Custom Metrics**: Define custom evaluation metrics and use them to assess model performance.',
          '- **Collaboration**: Share datasets and models with team members using the Hugging Face Hub and LangSmith.',
          'By following these steps, you can effectively integrate Hugging Face models with LangSmith, leveraging the strengths of both platforms to build and evaluate robust NLP applications.',
          'If you have any specific questions or need further assistance, feel free to ask!',
          '<span class="ansi-green-fg">User: </span>Exiting...',
          '<span class="ansi-yellow-fg">Assistant: </span>Goodbye!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si nos vamos al final de la conversaci贸n, podemos ver</p>
      <div class="highlight">
        <pre><span></span>--- 2 messages (input to trim_messages) ---</pre>
        <pre>    	[<span class="nl">AIMessage</span>]: <span class="na">Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.</span></pre>
        <pre>    	</pre>
        <pre>    	LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:</pre>
        <pre><span class="w">    	</span></pre>
        <pre><span class="w">    	</span><span class="k">1.</span> <span class="gs">**Visualizing Model Architecture**</span>: Provides a clear and detailed view of how different components of a language model are connected.</pre>
        <pre><span class="w">    	</span><span class="k">2.</span> <span class="gs">**Comparing Models**</span>: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.</pre>
        <pre><span class="w">    	</span><span class="k">3.</span> <span class="gs">**Understanding Training Processes**</span>: Helps in understanding the training dynamics and the flow of data through the model.</pre>
        <pre><span class="w">    	</span><span class="k">4.</span> <span class="gs">**Identifying Bottlenecks**</span>: Can help in identifying potential bottlenecks or areas for improvement in the model.</pre>
        <pre>    	</pre>
        <pre>    	If you have specific questions or aspects of LangGraph you're interested in, feel free to let me know!</pre>
        <pre>    	[<span class="nl">HumanMessage</span>]: <span class="na">I would like to know about using langsmith with huggingface llms, the integration of huggingface</span></pre>
        <pre>    	Summary of the conversation: Sure! Here's an extended summary of the conversation:</pre>
        <pre>    	</pre>
        <pre>    	---</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**User:**</span> Hello</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**Qwen:**</span> Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I'm here to help.</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**User:**</span> I am studying about langgraph, do you know it?</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**Qwen:**</span> Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:</pre>
        <pre><span class="w">    	</span><span class="k">1.</span> Visualizing Model Architecture</pre>
        <pre><span class="w">    	</span><span class="k">2.</span> Comparing Models</pre>
        <pre><span class="w">    	</span><span class="k">3.</span> Understanding Training Processes</pre>
        <pre><span class="w">    	</span><span class="k">4.</span> Identifying Bottlenecks</pre>
        <pre>    	</pre>
        <pre>    	If you have specific questions or aspects of LangGraph you're interested in, feel free to let me know!</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**User:**</span> This is a summary of the conversation to date: Sure! Here's a summary of the conversation above:</pre>
        <pre>    	User: Hello</pre>
        <pre>    	Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I'm here to help.</pre>
        <pre>    	User: Create a summary of the conversation above.</pre>
        <pre>    	Qwen: [Provided the summary you are now reading.]</pre>
        <pre>    	</pre>
        <pre>    	Is there anything else you need assistance with?</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**Qwen:**</span> [Extended the summary you are now reading.]</pre>
        <pre>    	</pre>
        <pre>    	---</pre>
        <pre>    	</pre>
        <pre>    	Is there anything else you need assistance with?</pre>
        <pre>    ------------------------------------------------</pre>
      </div>
      <p>Vemos que en los mensajes del estado solo se conservan</p>
      <div class="highlight">
        <pre><span></span>[<span class="nl">AIMessage</span>]: <span class="na">Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models.</span></pre>
        <pre>    	</pre>
        <pre>    	LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:</pre>
        <pre><span class="w">    	</span></pre>
        <pre><span class="w">    	</span><span class="k">1.</span> <span class="gs">**Visualizing Model Architecture**</span>: Provides a clear and detailed view of how different components of a language model are connected.</pre>
        <pre><span class="w">    	</span><span class="k">2.</span> <span class="gs">**Comparing Models**</span>: Allows for easy comparison of different language models in terms of their structure, training data, and performance metrics.</pre>
        <pre><span class="w">    	</span><span class="k">3.</span> <span class="gs">**Understanding Training Processes**</span>: Helps in understanding the training dynamics and the flow of data through the model.</pre>
        <pre><span class="w">    	</span><span class="k">4.</span> <span class="gs">**Identifying Bottlenecks**</span>: Can help in identifying potential bottlenecks or areas for improvement in the model.</pre>
        <pre>    	</pre>
        <pre>    	If you have specific questions or aspects of LangGraph you're interested in, feel free to let me know!</pre>
        <pre>    	[<span class="nl">HumanMessage</span>]: <span class="na">I would like to know about using langsmith with huggingface llms, the integration of huggingface</span></pre>
      </div>
      <p>Es decir, la funci贸n de filtrado solo mantiene los 2 煤ltimos mensajes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero despu茅s podemos ver</p>
      <div class="highlight">
        <pre><span></span>--- 2 messages (output of trim_messages - after trimming) ---</pre>
        <pre>    	[<span class="nl">HumanMessage</span>]: <span class="na">I would like to know about using langsmith with huggingface llms, the integration of huggingface</span></pre>
        <pre>    	Summary of the conversation: Sure! Here's an extended summary of the conversation:</pre>
        <pre>    	</pre>
        <pre>    	---</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**User:**</span> Hello</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**Qwen:**</span> Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I'm here to help.</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**User:**</span> I am studying about langgraph, do you know it?</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**Qwen:**</span> Yes, I can help with information about LangGraph! LangGraph is a language model graph that represents the relationships and connections between different language models and their components. It can be used to visualize and understand the architecture, training processes, and performance characteristics of various language models. LangGraph can be particularly useful for researchers and developers who are working on natural language processing (NLP) tasks. It helps in:</pre>
        <pre><span class="w">    	</span><span class="k">1.</span> Visualizing Model Architecture</pre>
        <pre><span class="w">    	</span><span class="k">2.</span> Comparing Models</pre>
        <pre><span class="w">    	</span><span class="k">3.</span> Understanding Training Processes</pre>
        <pre><span class="w">    	</span><span class="k">4.</span> Identifying Bottlenecks</pre>
        <pre>    	</pre>
        <pre>    	If you have specific questions or aspects of LangGraph you're interested in, feel free to let me know!</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**User:**</span> This is a summary of the conversation to date: Sure! Here's a summary of the conversation above:</pre>
        <pre>    	User: Hello</pre>
        <pre>    	Qwen: Hello! How can I assist you today? Whether you need help with information, a specific task, or just want to chat, I'm here to help.</pre>
        <pre>    	User: Create a summary of the conversation above.</pre>
        <pre>    	Qwen: [Provided the summary you are now reading.]</pre>
        <pre>    	</pre>
        <pre>    	Is there anything else you need assistance with?</pre>
        <pre>    	</pre>
        <pre>    	<span class="gs">**Qwen:**</span> [Extended the summary you are now reading.]</pre>
        <pre>    	</pre>
        <pre>    	---</pre>
        <pre>    	</pre>
        <pre>    	Is there anything else you need assistance with?</pre>
        <pre>    ------------------------------------------------</pre>
      </div>
      <p>Es decir, la funci贸n de trimado elimina el mensaje del asistente porque supera los 100 tokens.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A煤n eliminando mensajes, por lo que no los tiene como contexto el LLM, podemos tener una conversaci贸n gracias al resumen de la conversaci贸n que vamos generando.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Guardar-estado-en-SQlite">Guardar estado en SQlite<a class="anchor-link" href="#Guardar-estado-en-SQlite"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 38" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos visto c贸mo guardar el estado del grafo en memoria, pero en cuanto terminamos el proceso, esa memoria se pierde, por lo que vamos a ver c贸mo guardarla en SQLite</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero necesitamos instalar el paquete de <code>sqlite</code> para LangGraph.</p>
      <div class="highlight">
        <pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>langgraph-checkpoint-sqlite</pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importamos las librer铆as de <code>sqlite</code> y <code>langgraph-checkpoint-sqlite</code>. Antes, cuando guard谩bamos el estado en memoria us谩bamos <code>memory_saver</code>, ahora usaremos <code>SqliteSaver</code> para guardar el estado en una base de datos SQLite.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">sqlite3</span>',
      '<span class="kn">from</span> <span class="nn">langgraph.checkpoint.sqlite</span> <span class="kn">import</span> <span class="n">SqliteSaver</span>',
      '<span class="kn">import</span> <span class="nn">os</span>',
      ' ',
      '<span class="c1"># Create the directory if it doesn\'t exist</span>',
      '<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">"state_db"</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      ' ',
      '<span class="n">db_path</span> <span class="o">=</span> <span class="s2">"state_db/langgraph_sqlite.db"</span>',
      '<span class="n">conn</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">db_path</span><span class="p">,</span> <span class="n">check_same_thread</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>',
      '<span class="n">memory</span> <span class="o">=</span> <span class="n">SqliteSaver</span><span class="p">(</span><span class="n">conn</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <p>Vamos a crear un chatbot b谩sico para no a帽adir complejidad aparte de la funcionalidad que queremos probar.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># Return the LLM's response in the correct state format</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv26.webp" width="143" height="234" alt="image uv 27" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Definimos la funci贸n para imprimir los mensajes del grafo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Colors for the terminal</span>',
      '<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;32m"</span>',
      '<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;33m"</span>',
      '<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;0m"</span>',
      ' ',
      ' ',
      '<span class="k">def</span> <span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>',
      '    <span class="c1"># Initialize a flag to track if an assistant response has been printed</span>',
      '    <span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">False</span>',
      ' ',
      '    <span class="c1"># Print the user\'s input immediately</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n\\n</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">user_input</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
      '    ',
      '    <span class="c1"># Create the user\'s message with the HumanMessage class</span>',
      '    <span class="n">user_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">user_input</span><span class="p">)</span>',
      '    ',
      '    <span class="c1"># Stream events from the graph execution</span>',
      '    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">user_message</span><span class="p">]},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
      '        <span class="c1"># event is a dictionary mapping node names to their output</span>',
      '        <span class="c1"># Example: {opening_brace}\'chatbot_node\': {opening_brace}\'messages\': [...]{closing_brace}{closing_brace} or {opening_brace}\'summarize_conversation_node\': {opening_brace}\'summary\': \'...\'{closing_brace}{closing_brace}</span>',
      ' ',
      '        <span class="c1"># Iterate through node name and its output</span>',
      '        <span class="k">for</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>',
      '            <span class="c1"># Check if this event is from the chatbot node which should contain the assistant\'s reply</span>',
      '            <span class="k">if</span> <span class="n">node_name</span> <span class="o">==</span> <span class="s1">\'messages\'</span><span class="p">:</span>',
      '                <span class="c1"># Ensure the output format is as expected (list of messages)</span>',
      '                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>',
      '                    <span class="c1"># Get the messages from the event</span>',
      '                    <span class="n">messages</span> <span class="o">=</span> <span class="n">value</span>',
      '                    <span class="c1"># Ensure \'messages\' is a non-empty list</span>',
      '                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="n">messages</span><span class="p">:</span>',
      '                        <span class="c1"># Get the last message (presumably the assistant\'s reply)</span>',
      '                        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '                        <span class="c1"># Ensure the message is an instance of AIMessage</span>',
      '                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>',
      '                            <span class="c1"># Ensure the message has content to display</span>',
      '                            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="s1">\'content\'</span><span class="p">):</span>',
      '                                <span class="c1"># Print the assistant\'s message content</span>',
      '                                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{</span><span class="n">last_message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '                                <span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># Mark that we\'ve printed the response</span>',
      '    ',
      '    <span class="c1"># Fallback if no assistant response was printed (e.g., graph error before chatbot_node)</span>',
      '    <span class="k">if</span> <span class="ow">not</span> <span class="n">assistant_response_printed</span><span class="p">:</span>',
      '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">[No response generated or error occurred]"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>

















































      
      <section class="section-block-markdown-cell">
      <p>Ejecutamos el grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">"USER1"</span>',
          '<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">{closing_brace}{closing_brace}</span>',
          '<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>',
          '    <span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n\\n</span><span class="s2">User: "</span><span class="p">)</span>',
          '    <span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"quit"</span><span class="p">,</span> <span class="s2">"exit"</span><span class="p">,</span> <span class="s2">"q"</span><span class="p">]:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">Exiting..."</span><span class="p">)</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">Goodbye!"</span><span class="p">)</span>',
          '        <span class="k">break</span>',
          '    ',
          '    <span class="n">events</span> <span class="o">=</span> <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">,</span> <span class="n">config_USER1</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '<span class="ansi-green-fg">User: </span>Hello, my name is M谩ximo',
          '<span class="ansi-yellow-fg">Assistant: </span>Hello M谩ximo! It\'s a pleasure to meet you. How can I assist you today?',
          '<span class="ansi-green-fg">User: </span>Exiting...',
          '<span class="ansi-yellow-fg">Assistant: </span>Goodbye!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver, solo le he dicho c贸mo me llamo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora reiniciamos el notebook para que se eliminen todos los datos guardados en RAM del notebook y volvemos a ejecutar el c贸digo anterior.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a crear la memoria de <code>sqlite</code> con <code>SqliteSaver</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">sqlite3</span>',
      '<span class="kn">from</span> <span class="nn">langgraph.checkpoint.sqlite</span> <span class="kn">import</span> <span class="n">SqliteSaver</span>',
      '<span class="kn">import</span> <span class="nn">os</span>',
      ' ',
      '<span class="c1"># Create the directory if it doesn\'t exist</span>',
      '<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s2">"state_db"</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      ' ',
      '<span class="n">db_path</span> <span class="o">=</span> <span class="s2">"state_db/langgraph_sqlite.db"</span>',
      '<span class="n">conn</span> <span class="o">=</span> <span class="n">sqlite3</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">db_path</span><span class="p">,</span> <span class="n">check_same_thread</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>',
      '<span class="n">memory</span> <span class="o">=</span> <span class="n">SqliteSaver</span><span class="p">(</span><span class="n">conn</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <p>Volvemos a crear el grafo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="c1"># Return the LLM's response in the correct state format</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv27.webp" width="143" height="234" alt="image uv 28" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a definir la funci贸n para imprimir los mensajes del grafo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Colors for the terminal</span>',
      '<span class="n">COLOR_GREEN</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;32m"</span>',
      '<span class="n">COLOR_YELLOW</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;33m"</span>',
      '<span class="n">COLOR_RESET</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\033</span><span class="s2">&lbrack;0m"</span>',
      ' ',
      ' ',
      '<span class="k">def</span> <span class="nf">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>',
      '    <span class="c1"># Initialize a flag to track if an assistant response has been printed</span>',
      '    <span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">False</span>',
      ' ',
      '    <span class="c1"># Print the user\'s input immediately</span>',
      '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n\\n</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{opening_brace}</span><span class="n">user_input</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>',
      '    ',
      '    <span class="c1"># Create the user\'s message with the HumanMessage class</span>',
      '    <span class="n">user_message</span> <span class="o">=</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">user_input</span><span class="p">)</span>',
      '    ',
      '    <span class="c1"># Stream events from the graph execution</span>',
      '    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">user_message</span><span class="p">]},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
      '        <span class="c1"># event is a dictionary mapping node names to their output</span>',
      '        <span class="c1"># Example: {opening_brace}\'chatbot_node\': {opening_brace}\'messages\': [...]{closing_brace}{closing_brace} or {opening_brace}\'summarize_conversation_node\': {opening_brace}\'summary\': \'...\'{closing_brace}{closing_brace}</span>',
      ' ',
      '        <span class="c1"># Iterate through node name and its output</span>',
      '        <span class="k">for</span> <span class="n">node_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">event</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>',
      '            <span class="c1"># Check if this event is from the chatbot node which should contain the assistant\'s reply</span>',
      '            <span class="k">if</span> <span class="n">node_name</span> <span class="o">==</span> <span class="s1">\'messages\'</span><span class="p">:</span>',
      '                <span class="c1"># Ensure the output format is as expected (list of messages)</span>',
      '                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>',
      '                    <span class="c1"># Get the messages from the event</span>',
      '                    <span class="n">messages</span> <span class="o">=</span> <span class="n">value</span>',
      '                    <span class="c1"># Ensure \'messages\' is a non-empty list</span>',
      '                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="n">messages</span><span class="p">:</span>',
      '                        <span class="c1"># Get the last message (presumably the assistant\'s reply)</span>',
      '                        <span class="n">last_message</span> <span class="o">=</span> <span class="n">messages</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '                        <span class="c1"># Ensure the message is an instance of AIMessage</span>',
      '                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">):</span>',
      '                            <span class="c1"># Ensure the message has content to display</span>',
      '                            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">last_message</span><span class="p">,</span> <span class="s1">\'content\'</span><span class="p">):</span>',
      '                                <span class="c1"># Print the assistant\'s message content</span>',
      '                                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}{</span><span class="n">last_message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '                                <span class="n">assistant_response_printed</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># Mark that we\'ve printed the response</span>',
      '    ',
      '    <span class="c1"># Fallback if no assistant response was printed (e.g., graph error before chatbot_node)</span>',
      '    <span class="k">if</span> <span class="ow">not</span> <span class="n">assistant_response_printed</span><span class="p">:</span>',
      '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">[No response generated or error occurred]"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>

















































      
      <section class="section-block-markdown-cell">
      <p>Y lo volvemos a ejecutar</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">USER1_THREAD_ID</span> <span class="o">=</span> <span class="s2">"USER1"</span>',
          '<span class="n">config_USER1</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="n">USER1_THREAD_ID</span><span class="p">{closing_brace}{closing_brace}</span>',
          '<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>',
          '    <span class="n">user_input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\n\\n</span><span class="s2">User: "</span><span class="p">)</span>',
          '    <span class="k">if</span> <span class="n">user_input</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"quit"</span><span class="p">,</span> <span class="s2">"exit"</span><span class="p">,</span> <span class="s2">"q"</span><span class="p">]:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_GREEN</span><span class="si">{closing_brace}</span><span class="s2">User: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">Exiting..."</span><span class="p">)</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{opening_brace}</span><span class="n">COLOR_YELLOW</span><span class="si">{closing_brace}</span><span class="s2">Assistant: </span><span class="si">{opening_brace}</span><span class="n">COLOR_RESET</span><span class="si">{closing_brace}</span><span class="s2">Goodbye!"</span><span class="p">)</span>',
          '        <span class="k">break</span>',
          '    ',
          '    <span class="n">events</span> <span class="o">=</span> <span class="n">stream_graph_updates</span><span class="p">(</span><span class="n">user_input</span><span class="p">,</span> <span class="n">config_USER1</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '<span class="ansi-green-fg">User: </span>What\'s my name?',
          '<span class="ansi-yellow-fg">Assistant: </span>Your name is M谩ximo. It\'s nice to know and use your name as we chat. How can I assist you today, M谩ximo?',
          '<span class="ansi-green-fg">User: </span>Exiting...',
          '<span class="ansi-yellow-fg">Assistant: </span>Goodbye!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver, hemos podido recuperar el estado del grafo de la base de datos SQLite.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Memoria-a-largo-plazo,-memoria-entre-hilos">Memoria a largo plazo, memoria entre hilos<a class="anchor-link" href="#Memoria-a-largo-plazo,-memoria-entre-hilos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 39" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La memoria es una funci贸n cognitiva que permite a las personas almacenar, recuperar y utilizar informaci贸n para comprender, a partir de su pasado, su presente y su futuro.</p>
      <p>Existen varios tipos de <a href="https://langchain-ai.github.io/langgraph/concepts/memory" target="_blank" rel="nofollow noreferrer">memoria</a> a largo plazo que pueden utilizarse en aplicaciones de IA.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Introducci%C3%B3n-a-LangGraph-Memory-Store">Introducci贸n a LangGraph Memory Store<a class="anchor-link" href="#Introducci%C3%B3n-a-LangGraph-Memory-Store"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 40" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>LangGraph proporciona el <a href="https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore" target="_blank" rel="nofollow noreferrer">LangGraph Memory Store</a>, que es una forma de guardar y recuperar memoria a largo plazo entre diferentes hilos. De esta manera, en una conversaci贸n, un usuario puede indicar que le gusta algo, y en otra conversaci贸n, el chatbot puede recuperar esa informaci贸n para generar una respuesta m谩s personalizada.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se trata de una clase para almacenes persistentes de clave-valor (<code>key</code>-<code>value</code>).</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cuando se almacenan objetos en la memoria se necesitan tres cosas:</p>
      <ul>
      <li>Un <code>namespace</code> para el objeto, se hace mediante una <code>tupla</code></li>
      <li>Una <code>key</code> 煤nica</li>
      <li>El <code>valor</code> del objeto</li>
      </ul>
      <p>Vamos a ver un ejemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">uuid</span>',
      '<span class="kn">from</span> <span class="nn">langgraph.store.memory</span> <span class="kn">import</span> <span class="n">InMemoryStore</span>',
      ' ',
      '<span class="n">in_memory_store</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>',
      ' ',
      '<span class="c1"># Namespace for the memory to save</span>',
      '<span class="n">user_id</span> <span class="o">=</span> <span class="s2">"1"</span>',
      '<span class="n">namespace_for_memory</span> <span class="o">=</span> <span class="p">(</span><span class="n">user_id</span><span class="p">,</span> <span class="s2">"memories"</span><span class="p">)</span>',
      ' ',
      '<span class="c1"># Save a memory to namespace as key and value</span>',
      '<span class="n">key</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())</span>',
      ' ',
      '<span class="c1"># The value needs to be a dictionary  </span>',
      '<span class="n">value</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"food_preference"</span> <span class="p">:</span> <span class="s2">"I like pizza"</span><span class="p">{closing_brace}</span>',
      ' ',
      '<span class="c1"># Save the memory</span>',
      '<span class="n">in_memory_store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace_for_memory</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






















      
      <section class="section-block-markdown-cell">
      <p>El objeto <code>in_memory_store</code> que hemos creado tiene varios <a href="https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore" target="_blank" rel="nofollow noreferrer">m茅todos</a> y uno de ellos es <code>search</code>, que nos permite buscar por <code>namespace</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Search </span>',
          '<span class="n">memories</span> <span class="o">=</span> <span class="n">in_memory_store</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace_for_memory</span><span class="p">)</span>',
          '<span class="nb">type</span><span class="p">(</span><span class="n">memories</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">memories</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(list, 1)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Es una lista de un 煤nico valor, lo cual es l贸gico, porque solo hemos almacenado un valor, as铆 que vamos a verlo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">value</span> <span class="o">=</span> <span class="n">memories</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>',
          '<span class="n">value</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'namespace\': [\'1\', \'memories\'],',
          ' \'key\': \'70006131-948a-4d7a-bdce-78351c44fc4d\',',
          ' \'value\': {\'food_preference\': \'I like pizza\'{closing_brace},',
          ' \'created_at\': \'2025-05-11T07:24:31.462465+00:00\',',
          ' \'updated_at\': \'2025-05-11T07:24:31.462468+00:00\',',
          ' \'score\': None}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver su <code>key</code> y su <code>value</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># The key, value</span>',
          '<span class="n">memories</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">memories</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(\'70006131-948a-4d7a-bdce-78351c44fc4d\', {\'food_preference\': \'I like pizza\'})',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Tambi茅n podemos usar el m茅todo <code>get</code> para obtener un objeto de la memoria a partir de su <code>namespace</code> y su <code>key</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Get the memory by namespace and key</span>',
          '<span class="n">memory</span> <span class="o">=</span> <span class="n">in_memory_store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace_for_memory</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>',
          '<span class="n">memory</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'namespace\': [\'1\', \'memories\'],',
          ' \'key\': \'70006131-948a-4d7a-bdce-78351c44fc4d\',',
          ' \'value\': {\'food_preference\': \'I like pizza\'{closing_brace},',
          ' \'created_at\': \'2025-05-11T07:24:31.462465+00:00\',',
          ' \'updated_at\': \'2025-05-11T07:24:31.462468+00:00\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Al igual que para la memoria a corto plazo hemos usado los checkpoints, para la memoria a largo plazo vamos a usar <code>LangGraph Store</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Chatbot-con-memoria-a-largo-plazo">Chatbot con memoria a largo plazo<a class="anchor-link" href="#Chatbot-con-memoria-a-largo-plazo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 41" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un chatbot b谩sico, con memoria a largo plazo y memoria a corto plazo.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
      <span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span> <span class="c1"># Short-term memory</span>
      <span class="kn">from</span> <span class="nn">langgraph.store.base</span> <span class="kn">import</span> <span class="n">BaseStore</span>          <span class="c1"># Long-term memory</span>
      <span class="kn">from</span> <span class="nn">langchain_core.runnables.config</span> <span class="kn">import</span> <span class="n">RunnableConfig</span>
      <span class="kn">from</span> <span class="nn">langgraph.store.memory</span> <span class="kn">import</span> <span class="n">InMemoryStore</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Chatbot instruction</span>
      <span class="n">MODEL_SYSTEM_MESSAGE</span> <span class="o">=</span> <span class="s2">"""You are a helpful assistant that can answer questions and help with tasks.</span>
      <span class="s2">You have access to a long-term memory that you can use to answer questions and help with tasks.</span>
      <span class="s2">Here is the memory (it may be empty): </span><span class="si">{opening_brace}memory{closing_brace}</span><span class="s2">"""</span>
      
      <span class="c1"># Create new memory from the chat history and any existing memory</span>
      <span class="n">CREATE_MEMORY_INSTRUCTION</span> <span class="o">=</span> <span class="s2">"""You are a helpful assistant that gets information from the user to personalize your responses.</span>
      
      <span class="s2"># INFORMATION FROM THE USER:</span>
      <span class="si">{opening_brace}memory{closing_brace}</span>
      
      <span class="s2"># INSTRUCTIONS:</span>
      <span class="s2">1. Carefully review the chat history</span>
      <span class="s2">2. Identify new information from the user, such as:</span>
      <span class="s2">   - Personal details (name, location)</span>
      <span class="s2">   - Preferences (likes, dislikes)</span>
      <span class="s2">   - Interests and hobbies</span>
      <span class="s2">   - Past experiences</span>
      <span class="s2">   - Goals or future plans</span>
      <span class="s2">3. Combine any new information with the existing memory</span>
      <span class="s2">4. Format the memory as a clear, bulleted list</span>
      <span class="s2">5. If new information conflicts with existing memory, keep the most recent version</span>
      
      <span class="s2">Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.</span>
      
      <span class="s2">Based on the chat history below, please update the user information:"""</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
      
      <span class="w">    </span><span class="sd">"""Load memory from the store and use it to personalize the chatbot's response."""</span>
          
          <span class="c1"># Get the user ID from the config</span>
          <span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s2">"user_id"</span><span class="p">]</span>
      
          <span class="c1"># Retrieve memory from the store</span>
          <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
          <span class="n">key</span> <span class="o">=</span> <span class="s2">"user_memory"</span>
          <span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
      
          <span class="c1"># Extract the actual memory content if it exists and add a prefix</span>
          <span class="k">if</span> <span class="n">existing_memory</span><span class="p">:</span>
              <span class="c1"># Value is a dictionary with a memory key</span>
              <span class="n">existing_memory_content</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'memory'</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">existing_memory_content</span> <span class="o">=</span> <span class="s2">"No existing memory found."</span>
          <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">existing_memory_content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
              <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Call model debug] Existing memory: </span><span class="si">{opening_brace}</span><span class="n">existing_memory_content</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Call model debug] Existing memory: </span><span class="si">{opening_brace}</span><span class="n">existing_memory_content</span><span class="o">.</span><span class="n">content</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
          <span class="c1"># Format the memory in the system prompt</span>
          <span class="n">system_msg</span> <span class="o">=</span> <span class="n">MODEL_SYSTEM_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">existing_memory_content</span><span class="p">)</span>
          
          <span class="c1"># Respond using memory as well as the chat history</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>
      
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">response</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">write_memory</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
      
      <span class="w">    </span><span class="sd">"""Reflect on the chat history and save a memory to the store."""</span>
          
          <span class="c1"># Get the user ID from the config</span>
          <span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s2">"user_id"</span><span class="p">]</span>
      
          <span class="c1"># Retrieve existing memory from the store</span>
          <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
          <span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">"user_memory"</span><span class="p">)</span>
              
          <span class="c1"># Extract the memory</span>
          <span class="k">if</span> <span class="n">existing_memory</span><span class="p">:</span>
              <span class="n">existing_memory_content</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'memory'</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">existing_memory_content</span> <span class="o">=</span> <span class="s2">"No existing memory found."</span>
          <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">existing_memory_content</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
              <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing memory: </span><span class="si">{opening_brace}</span><span class="n">existing_memory_content</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing memory: </span><span class="si">{opening_brace}</span><span class="n">existing_memory_content</span><span class="o">.</span><span class="n">content</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
          <span class="c1"># Format the memory in the system prompt</span>
          <span class="n">system_msg</span> <span class="o">=</span> <span class="n">CREATE_MEMORY_INSTRUCTION</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">existing_memory_content</span><span class="p">)</span>
          <span class="n">new_memory</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s1">'messages'</span><span class="p">])</span>
          <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_memory</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
              <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n\t</span><span class="s2">[Write memory debug] New memory: </span><span class="si">{opening_brace}</span><span class="n">new_memory</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n\t</span><span class="s2">[Write memory debug] New memory: </span><span class="si">{opening_brace}</span><span class="n">new_memory</span><span class="o">.</span><span class="n">content</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
          <span class="c1"># Overwrite the existing memory in the store </span>
          <span class="n">key</span> <span class="o">=</span> <span class="s2">"user_memory"</span>
      
          <span class="c1"># Write value as a dictionary with a memory key</span>
          <span class="n">store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="p">{opening_brace}</span><span class="s2">"memory"</span><span class="p">:</span> <span class="n">new_memory</span><span class="o">.</span><span class="n">content</span><span class="p">{closing_brace})</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"call_model"</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"write_memory"</span><span class="p">,</span> <span class="n">write_memory</span><span class="p">)</span>
      
      <span class="c1"># Connect nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"call_model"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"call_model"</span><span class="p">,</span> <span class="s2">"write_memory"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"write_memory"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Store for long-term (across-thread) memory</span>
      <span class="n">long_term_memory</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
      
      <span class="c1"># Checkpointer for short-term (within-thread) memory</span>
      <span class="n">short_term_memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">short_term_memory</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">long_term_memory</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv28.webp" width="146" height="333" alt="image uv 29" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a probarlo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>',
          '<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">,</span> <span class="s2">"user_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Hi, my name is Maximo"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Hi, my name is Maximo',
          '	[Call model debug] Existing memory: No existing memory found.',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Hello Maximo! It\'s nice to meet you. How can I assist you today?',
          '	[Write memory debug] Existing memory: No existing memory found.',
          '	[Write memory debug] New memory:  ',
          'Here\'s the updated information I have about you:',
          '- Name: Maximo',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"I like to bike around San Francisco"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'I like to bike around San Francisco',
          '	[Call model debug] Existing memory:  ',
          'Here\'s the updated information I have about you:',
          '- Name: Maximo',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'That sounds like a great way to explore the city! San Francisco has some fantastic biking routes. Are there any specific areas or routes you enjoy biking the most, or are you looking for some new recommendations?',
          '	[Write memory debug] Existing memory:  ',
          'Here\'s the updated information I have about you:',
          '- Name: Maximo',
          '	[Write memory debug] New memory:  ',
          'Here\'s the updated information about you:',
          '- Name: Maximo',
          '- Location: San Francisco',
          '- Interest: Biking around San Francisco',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si recuperamos la memoria a largo plazo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Namespace for the memory to save</span>',
          '<span class="n">user_id</span> <span class="o">=</span> <span class="s2">"1"</span>',
          '<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>',
          '<span class="n">existing_memory</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">"user_memory"</span><span class="p">)</span>',
          '<span class="n">existing_memory</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'namespace\': [\'memory\', \'1\'],',
          ' \'key\': \'user_memory\',',
          ' \'value\': {\'memory\': " \n\nHere\'s the updated information about you:\n- Name: Maximo\n- Location: San Francisco\n- Interest: Biking around San Francisco"},',
          ' \'created_at\': \'2025-05-11T09:41:26.739207+00:00\',',
          ' \'updated_at\': \'2025-05-11T09:41:26.739211+00:00\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos su valor</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">\'memory\'</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          ' ',
          'Here\'s the updated information about you:',
          '- Name: Maximo',
          '- Location: San Francisco',
          '- Interest: Biking around San Francisco',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora podemos empezar un nuevo hilo de conversaci贸n, pero con la misma memoria a largo plazo. Veremos que el chatbot recuerda la informaci贸n del usuario.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># We supply a user ID for across-thread memory as well as a new thread ID</span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"2"</span><span class="p">,</span> <span class="s2">"user_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Hi! Where would you recommend that I go biking?"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Hi! Where would you recommend that I go biking?',
          '	[Call model debug] Existing memory:  ',
          'Here\'s the updated information about you:',
          '- Name: Maximo',
          '- Location: San Francisco',
          '- Interest: Biking around San Francisco',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Hi there! Given my interest in biking around San Francisco, I\'d recommend a few great routes:',
          '1. **Golden Gate Park**: This is a fantastic place to bike, with wide paths that are separated from vehicle traffic. You can start at the eastern end near Stow Lake and bike all the way to the western end at Ocean Beach. There are plenty of scenic spots to stop and enjoy along the way.',
          '2. **The Embarcadero**: This route follows the waterfront from Fishermans Wharf to the Bay Bridge. Its relatively flat and offers beautiful views of the San Francisco Bay and the city skyline. You can also stop by the Ferry Building for some delicious food and drinks.',
          '3. **Presidio**: The Presidio is a large park with numerous trails that offer diverse landscapes, from forests to coastal bluffs. The Crissy Field area is especially popular for its views of the Golden Gate Bridge.',
          '4. **Golden Gate Bridge**: Riding across the Golden Gate Bridge is a must-do experience. You can start from the San Francisco side, bike across the bridge, and then continue into Marin County for a longer ride with stunning views.',
          '5. **Lombard Street**: While not a long ride, biking down the famous crooked section of Lombard Street can be a fun and memorable experience. Just be prepared for the steep hill on the way back up!',
          'Each of these routes offers a unique experience, so you can choose based on your interests and the type of scenery you enjoy. Happy biking!',
          '	[Write memory debug] Existing memory:  ',
          'Here\'s the updated information about you:',
          '- Name: Maximo',
          '- Location: San Francisco',
          '- Interest: Biking around San Francisco',
          '	[Write memory debug] New memory:  ',
          'Let me know if you have any other questions or if you need more recommendations!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>He abierto un nuevo hilo de conversaci贸n, le he preguntado d贸nde podr铆a ir a montar en bici, ha recordado que le hab铆a dicho que me gusta ir a montar en bici por San Francisco y me ha respondido con lugares de San Francisco a los que podr铆a ir</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Chatbot-con-perfil-de-usuario">Chatbot con perfil de usuario<a class="anchor-link" href="#Chatbot-con-perfil-de-usuario"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 42" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Nota: Este apartado lo vamos a hacer usando Sonnet 3.7, ya que la integraci贸n de HuggingFace no tiene la funcionalidad de <code>with_structured_output</code> que proporciona una salida extructurada con un estructura definida.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos crear tipados para que el LLM genere una salida con una estructura definida por nosotros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a crear un tipado para el perfil de usuario.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">TypedDict</span><span class="p">,</span> <span class="n">List</span>',
      ' ',
      '<span class="k">class</span> <span class="nc">UserProfile</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '<span class="w">    </span><span class="sd">"""User profile schema with typed fields"""</span>',
      '    <span class="n">user_name</span><span class="p">:</span> <span class="nb">str</span>  <span class="c1"># The user\'s preferred name</span>',
      '    <span class="n">interests</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>  <span class="c1"># A list of the user\'s interests</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Ahora volvemos a crear el grafo, pero ahora con el tipado <code>UserProfile</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a usar <code>with_structured_output</code> para que el LLM genere una salida con una estructura definida por nosotros, esa estructura la vamos a definir con la clase <code>Subjects</code> que es una clase de tipo <code>BaseModel</code> de <code>Pydantic</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
      <span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span> <span class="c1"># Short-term memory</span>
      <span class="kn">from</span> <span class="nn">langgraph.store.base</span> <span class="kn">import</span> <span class="n">BaseStore</span>          <span class="c1"># Long-term memory</span>
      <span class="kn">from</span> <span class="nn">langchain_core.runnables.config</span> <span class="kn">import</span> <span class="n">RunnableConfig</span>
      <span class="kn">from</span> <span class="nn">langgraph.store.memory</span> <span class="kn">import</span> <span class="n">InMemoryStore</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
      <span class="n">llm_with_structured_output</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">UserProfile</span><span class="p">)</span>
      
      <span class="c1"># Chatbot instruction</span>
      <span class="n">MODEL_SYSTEM_MESSAGE</span> <span class="o">=</span> <span class="s2">"""You are a helpful assistant with memory that provides information about the user. </span>
      <span class="s2">If you have memory for this user, use it to personalize your responses.</span>
      <span class="s2">Here is the memory (it may be empty): </span><span class="si">{opening_brace}memory{closing_brace}</span><span class="s2">"""</span>
      
      <span class="c1"># Create new memory from the chat history and any existing memory</span>
      <span class="n">CREATE_MEMORY_INSTRUCTION</span> <span class="o">=</span> <span class="s2">"""Create or update a user profile memory based on the user's chat history. </span>
      <span class="s2">This will be saved for long-term memory. If there is an existing memory, simply update it. </span>
      <span class="s2">Here is the existing memory (it may be empty): </span><span class="si">{opening_brace}memory{closing_brace}</span><span class="s2">"""</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
      
      <span class="w">    </span><span class="sd">"""Load memory from the store and use it to personalize the chatbot's response."""</span>
          
          <span class="c1"># Get the user ID from the config</span>
          <span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s2">"user_id"</span><span class="p">]</span>
      
          <span class="c1"># Retrieve memory from the store</span>
          <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
          <span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">"user_memory"</span><span class="p">)</span>
      
          <span class="c1"># Format the memories for the system prompt</span>
          <span class="k">if</span> <span class="n">existing_memory</span> <span class="ow">and</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
              <span class="n">memory_dict</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>
              <span class="n">formatted_memory</span> <span class="o">=</span> <span class="p">(</span>
                  <span class="sa">f</span><span class="s2">"Name: </span><span class="si">{opening_brace}</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'user_name'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Unknown'</span><span class="p">)</span><span class="si">{closing_brace}</span><span class="se">\n</span><span class="s2">"</span>
                  <span class="sa">f</span><span class="s2">"Interests: </span><span class="si">{opening_brace}</span><span class="s1">', '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'interests'</span><span class="p">,</span><span class="w"> </span><span class="p">[]))</span><span class="si">{closing_brace}</span><span class="s2">"</span>
              <span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">formatted_memory</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="c1"># if isinstance(existing_memory_content, str):</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Call model debug] Existing memory: </span><span class="si">{opening_brace}</span><span class="n">formatted_memory</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          <span class="c1"># else:</span>
          <span class="c1">#     print(f"\t[Call model debug] Existing memory: {opening_brace}existing_memory_content.content{closing_brace}")</span>
      
          <span class="c1"># Format the memory in the system prompt</span>
          <span class="n">system_msg</span> <span class="o">=</span> <span class="n">MODEL_SYSTEM_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">formatted_memory</span><span class="p">)</span>
      
          <span class="c1"># Respond using memory as well as the chat history</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>
      
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">response</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">write_memory</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
      
      <span class="w">    </span><span class="sd">"""Reflect on the chat history and save a memory to the store."""</span>
          
          <span class="c1"># Get the user ID from the config</span>
          <span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s2">"user_id"</span><span class="p">]</span>
      
          <span class="c1"># Retrieve existing memory from the store</span>
          <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
          <span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">"user_memory"</span><span class="p">)</span>
      
          <span class="c1"># Format the memories for the system prompt</span>
          <span class="k">if</span> <span class="n">existing_memory</span> <span class="ow">and</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
              <span class="n">memory_dict</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>
              <span class="n">formatted_memory</span> <span class="o">=</span> <span class="p">(</span>
                  <span class="sa">f</span><span class="s2">"Name: </span><span class="si">{opening_brace}</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'user_name'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Unknown'</span><span class="p">)</span><span class="si">{closing_brace}</span><span class="se">\n</span><span class="s2">"</span>
                  <span class="sa">f</span><span class="s2">"Interests: </span><span class="si">{opening_brace}</span><span class="s1">', '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'interests'</span><span class="p">,</span><span class="w"> </span><span class="p">[]))</span><span class="si">{closing_brace}</span><span class="s2">"</span>
              <span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">formatted_memory</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing memory: </span><span class="si">{opening_brace}</span><span class="n">formatted_memory</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
              
          <span class="c1"># Format the existing memory in the instruction</span>
          <span class="n">system_msg</span> <span class="o">=</span> <span class="n">CREATE_MEMORY_INSTRUCTION</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">formatted_memory</span><span class="p">)</span>
      
          <span class="c1"># Invoke the model to produce structured output that matches the schema</span>
          <span class="n">new_memory</span> <span class="o">=</span> <span class="n">llm_with_structured_output</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s1">'messages'</span><span class="p">])</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Write memory debug] New memory: </span><span class="si">{opening_brace}</span><span class="n">new_memory</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
          <span class="c1"># Overwrite the existing use profile memory</span>
          <span class="n">key</span> <span class="o">=</span> <span class="s2">"user_memory"</span>
          <span class="n">store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">new_memory</span><span class="p">)</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">MessagesState</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"call_model"</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"write_memory"</span><span class="p">,</span> <span class="n">write_memory</span><span class="p">)</span>
      
      <span class="c1"># Connect nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"call_model"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"call_model"</span><span class="p">,</span> <span class="s2">"write_memory"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"write_memory"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Store for long-term (across-thread) memory</span>
      <span class="n">long_term_memory</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
      
      <span class="c1"># Checkpointer for short-term (within-thread) memory</span>
      <span class="n">short_term_memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">short_term_memory</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">long_term_memory</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv29.webp" width="146" height="333" alt="image uv 30" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ejecutamos el grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>',
          '<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">,</span> <span class="s2">"user_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Hi, my name is Maximo and I like to bike around Madrid and eat salads."</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Hi, my name is Maximo and I like to bike around Madrid and eat salads.',
          '	[Call model debug] Existing memory: None',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Hello Maximo! It\'s nice to meet you. I see you enjoy biking around Madrid and eating salads - those are great healthy habits! Madrid has some beautiful areas to explore by bike, and the city has been improving its cycling infrastructure in recent years. ',
          'Is there anything specific about Madrid\'s cycling routes or perhaps some good places to find delicious salads in the city that you\'d like to know more about? I\'d be happy to help with any questions you might have.',
          '	[Write memory debug] Existing memory: None',
          '	[Write memory debug] New memory: {\'user_name\': \'Maximo\', \'interests\': [\'biking\', \'Madrid\', \'salads\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, el LLM ha generado una salida con la estructura definida por nosotros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver c贸mo se ha guardado la memoria a largo plazo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Namespace for the memory to save</span>',
          '<span class="n">user_id</span> <span class="o">=</span> <span class="s2">"1"</span>',
          '<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>',
          '<span class="n">existing_memory</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">"user_memory"</span><span class="p">)</span>',
          '<span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'user_name\': \'Maximo\', \'interests\': [\'biking\', \'Madrid\', \'salads\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="M%C3%A1s">M谩s<a class="anchor-link" href="#M%C3%A1s"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 43" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Actualizar-esquemas-estructurados-con-Trustcall">Actualizar esquemas estructurados con Trustcall<a class="anchor-link" href="#Actualizar-esquemas-estructurados-con-Trustcall"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 44" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el ejemplo anterior, hemos creado perfiles de usuario con datos estructurados</p>
      <p>Enn realidad, lo que se hace por debajo, es regenerar el perfil de usuario en cada interacci贸n. Lo cual genera un gasto innecesario de tokens y puede hacer que se pierda informaci贸n importante del perfil del usuario.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As铆 que para solucionarlo vamos a usar la librer铆a <a href="https://github.com/hinthornw/trustcall" target="_blank" rel="nofollow noreferrer">TrustCall</a>, que es una librer铆a open source para actualizar esquemas JSON. Cuando tiene que actualizar un esquema JSON, lo hace de manera incremental, es decir, no borra el esquema anterior, sino que va a帽adiendo los nuevos campos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a crear un ejemplo de conversaci贸n para ver c贸mo funciona.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>',
      ' ',
      '<span class="c1"># Conversation</span>',
      '<span class="n">conversation</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Hi, I\'m Maximo."</span><span class="p">),</span> ',
      '                <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Nice to meet you, Maximo."</span><span class="p">),</span> ',
      '                <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"I really like playing soccer."</span><span class="p">)]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Creamos un esquema estructurado y un modelo de LLM</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>',
      '<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>',
      ' ',
      '<span class="c1"># Schema</span>',
      '<span class="k">class</span> <span class="nc">UserProfile</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>',
      '<span class="w">    </span><span class="sd">"""User profile schema with typed fields"""</span>',
      '    <span class="n">user_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"The user\'s preferred name"</span><span class="p">)</span>',
      '    <span class="n">interests</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"A list of the user\'s interests"</span><span class="p">)</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>',
      '<span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      '<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
      '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
      ' ',
      '<span class="c1"># Create the LLM model</span>',
      '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
























      
      <section class="section-block-markdown-cell">
      <p>Utilizamos la funci贸n <code>create_extractor</code> de <code>trustcall</code> para crear un extractor de datos estructurados</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">trustcall</span> <span class="kn">import</span> <span class="n">create_extractor</span>',
      ' ',
      '<span class="c1"># Create the extractor</span>',
      '<span class="n">trustcall_extractor</span> <span class="o">=</span> <span class="n">create_extractor</span><span class="p">(</span>',
      '    <span class="n">llm</span><span class="p">,</span>',
      '    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">UserProfile</span><span class="p">],</span>',
      '    <span class="n">tool_choice</span><span class="o">=</span><span class="s2">"UserProfile"</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver, al m茅todo <code>trustcall_extractor</code> se le da un llm, que va a usarse como motor de b煤squeda</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Extrajimos los datos estructurados</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">SystemMessage</span>',
          '',
          '<span class="c1"># Instruction</span>',
          '<span class="n">system_msg</span> <span class="o">=</span> <span class="s2">"Extract the user profile from the following conversation"</span>',
          '',
          '<span class="c1"># Invoke the extractor</span>',
          '<span class="n">result</span> <span class="o">=</span> <span class="n">trustcall_extractor</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">conversation</span><span class="p">})</span>',
          '<span class="n">result</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'messages\': [AIMessage(content=&lbrack;{opening_brace}\'id\': \'toolu_01WfgbD1fG3rJYAXGrjqjfVY\', \'input\': {\'user_name\': \'Maximo\', \'interests\': [\'soccer\']{closing_brace}, \'name\': \'UserProfile\', \'type\': \'tool_use\'{closing_brace}&rbrack;, additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}\'id\': \'msg_01TEB3FeDKLAeHJtbKo5noyW\', \'model\': \'claude-3-7-sonnet-20250219\', \'stop_reason\': \'tool_use\', \'stop_sequence\': None, \'usage\': {\'cache_creation_input_tokens\': 0, \'cache_read_input_tokens\': 0, \'input_tokens\': 497, \'output_tokens\': 56}, \'model_name\': \'claude-3-7-sonnet-20250219\'{closing_brace}, id=\'run-8a15289b-fd39-4a2d-878a-fa6feaa805c5-0\', tool_calls=[{opening_brace}\'name\': \'UserProfile\', \'args\': {\'user_name\': \'Maximo\', \'interests\': [\'soccer\']{closing_brace}, \'id\': \'toolu_01WfgbD1fG3rJYAXGrjqjfVY\', \'type\': \'tool_call\'{closing_brace}&rbrack;, usage_metadata={\'input_tokens\': 497, \'output_tokens\': 56, \'total_tokens\': 553, \'input_token_details\': {\'cache_read\': 0, \'cache_creation\': 0{closing_brace}{closing_brace})],',
          ' \'responses\': [UserProfile(user_name=\'Maximo\', interests=[\'soccer\'])],',
          ' \'response_metadata\': &lbrack;{opening_brace}\'id\': \'toolu_01WfgbD1fG3rJYAXGrjqjfVY\'{closing_brace}&rbrack;,',
          ' \'attempts\': 1}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver los mensajes que se han generado para extraer los datos estructurados</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span> ',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          '&lbrack;{opening_brace}\'id\': \'toolu_01WfgbD1fG3rJYAXGrjqjfVY\', \'input\': {\'user_name\': \'Maximo\', \'interests\': [\'soccer\']{closing_brace}, \'name\': \'UserProfile\', \'type\': \'tool_use\'{closing_brace}&rbrack;',
          'Tool Calls:',
          '  UserProfile (toolu_01WfgbD1fG3rJYAXGrjqjfVY)',
          ' Call ID: toolu_01WfgbD1fG3rJYAXGrjqjfVY',
          '  Args:',
          '    user_name: Maximo',
          '    interests: [\'soccer\']',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>El esquema de <code>UserProfile</code> se ha actualizado con el nuevo dato.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">schema</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">"responses"</span><span class="p">]</span>',
          '<span class="n">schema</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[UserProfile(user_name=\'Maximo\', interests=[\'soccer\'])]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, el esquema es una lista, vamos a ver el tipo de dato de su 煤nico elemento</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">type</span><span class="p">(</span><span class="n">schema</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '__main__.UserProfile',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos convertirlo a un diccionario con <code>model_dump</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">schema</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'user_name\': \'Maximo\', \'interests\': [\'soccer\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Gracias a haberle dado un LLM a <code>trustcall_extractor</code>, podemos pedirle qu茅 queremos que extraiga</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a simular que continua la conversaci贸n para ver c贸mo se actualiza el esquema</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="c1"># Update the conversation</span>',
      '<span class="n">updated_conversation</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Hi, I\'m Maximo."</span><span class="p">),</span> ',
      '                <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Nice to meet you, Maximo."</span><span class="p">),</span> ',
      '                <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"I really like playing soccer."</span><span class="p">),</span>',
      '                <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"It is great to play soccer! Where do you go after playing soccer?"</span><span class="p">),</span>',
      '                <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"I really like to go to a bakery after playing soccer."</span><span class="p">),]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Le pedimos al modelo que actualice el esquema (un <code>JSON</code>) mediante la librer铆a <code>trustcall</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Update the instruction</span>',
          '<span class="n">system_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"""Update the memory (JSON doc) to incorporate new information from the following conversation"""</span>',
          '',
          '<span class="c1"># Invoke the extractor with the updated instruction and existing profile with the corresponding tool name (UserProfile)</span>',
          '<span class="n">result</span> <span class="o">=</span> <span class="n">trustcall_extractor</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">updated_conversation</span><span class="p">},</span> ',
          '                                    <span class="p">{opening_brace}</span><span class="s2">"existing"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"UserProfile"</span><span class="p">:</span> <span class="n">schema</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(){closing_brace}{closing_brace})</span>',
          '<span class="n">result</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'messages\': [AIMessage(content=&lbrack;{opening_brace}\'id\': \'toolu_01K1zTh33kXDAw1h18Yh2HBb\', \'input\': {\'user_name\': \'Maximo\', \'interests\': [\'soccer\', \'bakeries\']{closing_brace}, \'name\': \'UserProfile\', \'type\': \'tool_use\'{closing_brace}&rbrack;, additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}\'id\': \'msg_01RYUJvCdzL4b8kBYKo4BtQf\', \'model\': \'claude-3-7-sonnet-20250219\', \'stop_reason\': \'tool_use\', \'stop_sequence\': None, \'usage\': {\'cache_creation_input_tokens\': 0, \'cache_read_input_tokens\': 0, \'input_tokens\': 538, \'output_tokens\': 60}, \'model_name\': \'claude-3-7-sonnet-20250219\'{closing_brace}, id=\'run-06994472-5ba0-46cc-a512-5fcacce283fc-0\', tool_calls=[{opening_brace}\'name\': \'UserProfile\', \'args\': {\'user_name\': \'Maximo\', \'interests\': [\'soccer\', \'bakeries\']{closing_brace}, \'id\': \'toolu_01K1zTh33kXDAw1h18Yh2HBb\', \'type\': \'tool_call\'{closing_brace}&rbrack;, usage_metadata={\'input_tokens\': 538, \'output_tokens\': 60, \'total_tokens\': 598, \'input_token_details\': {\'cache_read\': 0, \'cache_creation\': 0{closing_brace}{closing_brace})],',
          ' \'responses\': [UserProfile(user_name=\'Maximo\', interests=[\'soccer\', \'bakeries\'])],',
          ' \'response_metadata\': &lbrack;{opening_brace}\'id\': \'toolu_01K1zTh33kXDAw1h18Yh2HBb\'{closing_brace}&rbrack;,',
          ' \'attempts\': 1}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver los mensajes que se han generado para actualizar el esquema</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span> ',
          '    <span class="n">m</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          '&lbrack;{opening_brace}\'id\': \'toolu_01K1zTh33kXDAw1h18Yh2HBb\', \'input\': {\'user_name\': \'Maximo\', \'interests\': [\'soccer\', \'bakeries\']{closing_brace}, \'name\': \'UserProfile\', \'type\': \'tool_use\'{closing_brace}&rbrack;',
          'Tool Calls:',
          '  UserProfile (toolu_01K1zTh33kXDAw1h18Yh2HBb)',
          ' Call ID: toolu_01K1zTh33kXDAw1h18Yh2HBb',
          '  Args:',
          '    user_name: Maximo',
          '    interests: [\'soccer\', \'bakeries\']',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos el esquema actualizado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">updated_schema</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">"responses"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>',
          '<span class="n">updated_schema</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'user_name\': \'Maximo\', \'interests\': [\'soccer\', \'bakeries\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Chatbot-con-perfil-de-usuario-actualizado-con-Trustcall">Chatbot con perfil de usuario actualizado con Trustcall<a class="anchor-link" href="#Chatbot-con-perfil-de-usuario-actualizado-con-Trustcall"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 45" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a crear el grafo que actualiza el perfil de usuario, pero ahora con la librer铆a <code>trustcall</code></p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
      <span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span> <span class="c1"># Short-term memory</span>
      <span class="kn">from</span> <span class="nn">langgraph.store.base</span> <span class="kn">import</span> <span class="n">BaseStore</span>          <span class="c1"># Long-term memory</span>
      <span class="kn">from</span> <span class="nn">langchain_core.runnables.config</span> <span class="kn">import</span> <span class="n">RunnableConfig</span>
      <span class="kn">from</span> <span class="nn">langgraph.store.memory</span> <span class="kn">import</span> <span class="n">InMemoryStore</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      <span class="kn">from</span> <span class="nn">trustcall</span> <span class="kn">import</span> <span class="n">create_extractor</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Schema </span>
      <span class="k">class</span> <span class="nc">UserProfile</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
      <span class="w">    </span><span class="sd">""" Profile of a user """</span>
          <span class="n">user_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"The user's preferred name"</span><span class="p">)</span>
          <span class="n">user_location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"The user's location"</span><span class="p">)</span>
          <span class="n">interests</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"A list of the user's interests"</span><span class="p">)</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
      
      <span class="c1"># Create the extractor</span>
      <span class="n">trustcall_extractor</span> <span class="o">=</span> <span class="n">create_extractor</span><span class="p">(</span>
          <span class="n">llm</span><span class="p">,</span>
          <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">UserProfile</span><span class="p">],</span>
          <span class="n">tool_choice</span><span class="o">=</span><span class="s2">"UserProfile"</span><span class="p">,</span> <span class="c1"># Enforces use of the UserProfile tool</span>
      <span class="p">)</span>
      
      <span class="c1"># Chatbot instruction</span>
      <span class="n">MODEL_SYSTEM_MESSAGE</span> <span class="o">=</span> <span class="s2">"""You are a helpful assistant with memory that provides information about the user. </span>
      <span class="s2">If you have memory for this user, use it to personalize your responses.</span>
      <span class="s2">Here is the memory (it may be empty): </span><span class="si">{opening_brace}memory{closing_brace}</span><span class="s2">"""</span>
      
      <span class="c1"># Create new memory from the chat history and any existing memory</span>
      <span class="n">TRUSTCALL_INSTRUCTION</span> <span class="o">=</span> <span class="s2">"""Create or update the memory (JSON doc) to incorporate information from the following conversation:"""</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
      
      <span class="w">    </span><span class="sd">"""Load memory from the store and use it to personalize the chatbot's response."""</span>
      <span class="w">    </span>
      <span class="w">    </span><span class="sd">"""Load memory from the store and use it to personalize the chatbot's response."""</span>
          
          <span class="c1"># Get the user ID from the config</span>
          <span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s2">"user_id"</span><span class="p">]</span>
      
          <span class="c1"># Retrieve memory from the store</span>
          <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
          <span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">"user_memory"</span><span class="p">)</span>
      
          <span class="c1"># Format the memories for the system prompt</span>
          <span class="k">if</span> <span class="n">existing_memory</span> <span class="ow">and</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
              <span class="n">memory_dict</span> <span class="o">=</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>
              <span class="n">formatted_memory</span> <span class="o">=</span> <span class="p">(</span>
                  <span class="sa">f</span><span class="s2">"Name: </span><span class="si">{opening_brace}</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'user_name'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Unknown'</span><span class="p">)</span><span class="si">{closing_brace}</span><span class="se">\n</span><span class="s2">"</span>
                  <span class="sa">f</span><span class="s2">"Location: </span><span class="si">{opening_brace}</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'user_location'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Unknown'</span><span class="p">)</span><span class="si">{closing_brace}</span><span class="se">\n</span><span class="s2">"</span>
                  <span class="sa">f</span><span class="s2">"Interests: </span><span class="si">{opening_brace}</span><span class="s1">', '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">memory_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">'interests'</span><span class="p">,</span><span class="w"> </span><span class="p">[]))</span><span class="si">{closing_brace}</span><span class="s2">"</span>      
              <span class="p">)</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">formatted_memory</span> <span class="o">=</span> <span class="kc">None</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Call model debug] Existing memory: </span><span class="si">{opening_brace}</span><span class="n">formatted_memory</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
          <span class="c1"># Format the memory in the system prompt</span>
          <span class="n">system_msg</span> <span class="o">=</span> <span class="n">MODEL_SYSTEM_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">formatted_memory</span><span class="p">)</span>
      
          <span class="c1"># Respond using memory as well as the chat history</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>
      
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">response</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">write_memory</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
      
      <span class="w">    </span><span class="sd">"""Reflect on the chat history and save a memory to the store."""</span>
          
          <span class="c1"># Get the user ID from the config</span>
          <span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s2">"user_id"</span><span class="p">]</span>
      
          <span class="c1"># Retrieve existing memory from the store</span>
          <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
          <span class="n">existing_memory</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">"user_memory"</span><span class="p">)</span>
              
          <span class="c1"># Get the profile as the value from the list, and convert it to a JSON doc</span>
          <span class="n">existing_profile</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"UserProfile"</span><span class="p">:</span> <span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span><span class="p">{closing_brace}</span> <span class="k">if</span> <span class="n">existing_memory</span> <span class="k">else</span> <span class="kc">None</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing profile: </span><span class="si">{opening_brace}</span><span class="n">existing_profile</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
          
          <span class="c1"># Invoke the extractor</span>
          <span class="n">result</span> <span class="o">=</span> <span class="n">trustcall_extractor</span><span class="o">.</span><span class="n">invoke</span><span class="p">({opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">TRUSTCALL_INSTRUCTION</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">],</span> <span class="s2">"existing"</span><span class="p">:</span> <span class="n">existing_profile</span><span class="p">{closing_brace})</span>
          
          <span class="c1"># Get the updated profile as a JSON object</span>
          <span class="n">updated_profile</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">"responses"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">model_dump</span><span class="p">()</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Write memory debug] Updated profile: </span><span class="si">{opening_brace}</span><span class="n">updated_profile</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
          <span class="c1"># Save the updated profile</span>
          <span class="n">key</span> <span class="o">=</span> <span class="s2">"user_memory"</span>
          <span class="n">store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">updated_profile</span><span class="p">)</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">MessagesState</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"call_model"</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"write_memory"</span><span class="p">,</span> <span class="n">write_memory</span><span class="p">)</span>
      
      <span class="c1"># Connect nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"call_model"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"call_model"</span><span class="p">,</span> <span class="s2">"write_memory"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"write_memory"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Store for long-term (across-thread) memory</span>
      <span class="n">long_term_memory</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
      
      <span class="c1"># Checkpointer for short-term (within-thread) memory</span>
      <span class="n">short_term_memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">short_term_memory</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">long_term_memory</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv30.webp" width="146" height="333" alt="image uv 31" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Iniciamos la conversaci贸n</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>',
          '<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">,</span> <span class="s2">"user_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Hi, my name is Maximo"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Hi, my name is Maximo',
          '	[Call model debug] Existing memory: None',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Hello Maximo! It\'s nice to meet you. How can I help you today? Whether you have questions, need information, or just want to chat, I\'m here to assist you. Is there something specific you\'d like to talk about?',
          '	[Write memory debug] Existing profile: None',
          '	[Write memory debug] Updated profile: {\'user_name\': \'Maximo\', \'user_location\': \'&lt;UNKNOWN&gt;\', \'interests\': []}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, no sabe ni la localizaci贸n ni los intereses del usuario. Vamos a actualizar el perfil del usuario.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"I like to play soccer and I live in Madrid"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'I like to play soccer and I live in Madrid',
          '	[Call model debug] Existing memory: Name: Maximo',
          'Location: &lt;UNKNOWN&gt;',
          'Interests: ',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Hello Maximo! It\'s great to learn that you live in Madrid and enjoy playing soccer. Madrid is a fantastic city with a rich soccer culture, being home to world-famous clubs like Real Madrid and Atl茅tico Madrid.',
          'Soccer is truly a way of life in Spain, so you\'re in a perfect location for your interest. Do you support any particular team in Madrid? Or perhaps you enjoy playing soccer recreationally in the city\'s parks and facilities?',
          'Is there anything specific about Madrid or soccer you\'d like to discuss further?',
          '	[Write memory debug] Existing profile: {\'UserProfile\': {\'user_name\': \'Maximo\', \'user_location\': \'&lt;UNKNOWN&gt;\', \'interests\': []{closing_brace}{closing_brace}',
          '	[Write memory debug] Updated profile: {\'user_name\': \'Maximo\', \'user_location\': \'Madrid\', \'interests\': [\'soccer\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ha actualizado el perfil con la localizaci贸n y los intereses del usuario</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver la memoria actualizada</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Namespace for the memory to save</span>',
          '<span class="n">user_id</span> <span class="o">=</span> <span class="s2">"1"</span>',
          '<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>',
          '<span class="n">existing_memory</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">"user_memory"</span><span class="p">)</span>',
          '<span class="n">existing_memory</span><span class="o">.</span><span class="n">dict</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'namespace\': [\'memory\', \'1\'],',
          ' \'key\': \'user_memory\',',
          ' \'value\': {\'user_name\': \'Maximo\',',
          '  \'user_location\': \'Madrid\',',
          '  \'interests\': [\'soccer\']{closing_brace},',
          ' \'created_at\': \'2025-05-12T17:35:03.583258+00:00\',',
          ' \'updated_at\': \'2025-05-12T17:35:03.583259+00:00\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos el esquema con el perfil del usuario actualizado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># The user profile saved as a JSON object</span>',
          '<span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'user_name\': \'Maximo\', \'user_location\': \'Madrid\', \'interests\': [\'soccer\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a a帽adir un nuevo inter茅s del usuario</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"I also like to play basketball"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'I also like to play basketball',
          '	[Call model debug] Existing memory: Name: Maximo',
          'Location: Madrid',
          'Interests: soccer',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'That\'s great to know, Maximo! It\'s nice that you enjoy both soccer and basketball. Basketball is also quite popular in Spain, with Liga ACB being one of the strongest basketball leagues in Europe. ',
          'In Madrid, you have the opportunity to follow Real Madrid\'s basketball section, which is one of the most successful basketball teams in Europe. The city offers plenty of courts and facilities where you can play basketball too.',
          'Do you play basketball casually with friends, or are you part of any local leagues in Madrid? And how do you balance your time between soccer and basketball?',
          '	[Write memory debug] Existing profile: {\'UserProfile\': {\'user_name\': \'Maximo\', \'user_location\': \'Madrid\', \'interests\': [\'soccer\']{closing_brace}{closing_brace}',
          '	[Write memory debug] Updated profile: {\'user_name\': \'Maximo\', \'user_location\': \'Madrid\', \'interests\': [\'soccer\', \'basketball\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver la memoria actualizada</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Namespace for the memory to save</span>',
          '<span class="n">user_id</span> <span class="o">=</span> <span class="s2">"1"</span>',
          '<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memory"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>',
          '<span class="n">existing_memory</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span> <span class="s2">"user_memory"</span><span class="p">)</span>',
          '<span class="n">existing_memory</span><span class="o">.</span><span class="n">value</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'user_name\': \'Maximo\',',
          ' \'user_location\': \'Madrid\',',
          ' \'interests\': [\'soccer\', \'basketball\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ha a帽adido bien el nuevo inter茅s del usuario.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Con esta memoria a largo plazo guardada, podemos iniciar un nuevo hilo y el chatbot tendr谩 acceso a nuestro perfil actualizado.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>',
          '<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"2"</span><span class="p">,</span> <span class="s2">"user_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"What soccer players do you recommend for me?"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'What soccer players do you recommend for me?',
          '	[Call model debug] Existing memory: Name: Maximo',
          'Location: Madrid',
          'Interests: soccer, basketball',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Based on your interest in soccer, I can recommend some players who might appeal to you. Since you\'re from Madrid, you might already follow Real Madrid or Atl茅tico Madrid players, but here are some recommendations:',
          'From La Liga:',
          '- Vin铆cius J煤nior and Jude Bellingham (Real Madrid)',
          '- Antoine Griezmann (Atl茅tico Madrid)',
          '- Robert Lewandowski (Barcelona)',
          '- Lamine Yamal (Barcelona\'s young talent)',
          'International stars:',
          '- Kylian Mbapp茅',
          '- Erling Haaland',
          '- Mohamed Salah',
          '- Kevin De Bruyne',
          'You might also enjoy watching players with creative playing styles since you\'re interested in basketball as well, which is a sport that values creativity and flair - players like Rodrigo De Paul or Jo茫o F茅lix.',
          'Is there a particular league or playing style you prefer in soccer?',
          '	[Write memory debug] Existing profile: {\'UserProfile\': {\'user_name\': \'Maximo\', \'user_location\': \'Madrid\', \'interests\': [\'soccer\', \'basketball\']{closing_brace}{closing_brace}',
          '	[Write memory debug] Updated profile: {\'user_name\': \'Maximo\', \'user_location\': \'Madrid\', \'interests\': [\'soccer\', \'basketball\']{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como sabe que vivo en Madrid, primero me ha sugerido jugadores de f煤tbol de la Liga espa帽ola. Y luego me ha sugerido jugadores de otras ligas</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Chatbot-con-colecciones-de-documentos-de-usuario-actualizadas-con-Trustcall">Chatbot con colecciones de documentos de usuario actualizadas con Trustcall<a class="anchor-link" href="#Chatbot-con-colecciones-de-documentos-de-usuario-actualizadas-con-Trustcall"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 46" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Otro enfoque es que en vez de guardar en un solo documento el perfil del usuario, guardar una colecci贸n de documentos, de esta manera no estamos atados a un solo esquema cerrado</p>
      <p>Vamos a ver c贸mo hacerlo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span><span class="p">,</span> <span class="n">SystemMessage</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">merge_message_runs</span>
      <span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span> <span class="c1"># Short-term memory</span>
      <span class="kn">from</span> <span class="nn">langgraph.store.base</span> <span class="kn">import</span> <span class="n">BaseStore</span>          <span class="c1"># Long-term memory</span>
      <span class="kn">from</span> <span class="nn">langchain_core.runnables.config</span> <span class="kn">import</span> <span class="n">RunnableConfig</span>
      <span class="kn">from</span> <span class="nn">langgraph.store.memory</span> <span class="kn">import</span> <span class="n">InMemoryStore</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">from</span> <span class="nn">trustcall</span> <span class="kn">import</span> <span class="n">create_extractor</span>
      <span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
      <span class="kn">import</span> <span class="nn">uuid</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Memory schema</span>
      <span class="k">class</span> <span class="nc">Memory</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
      <span class="w">    </span><span class="sd">"""A memory item representing a piece of information learned about the user."""</span>
          <span class="n">content</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">"The main content of the memory. For example: User expressed interest in learning about French."</span><span class="p">)</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
      
      <span class="c1"># Create the extractor</span>
      <span class="n">trustcall_extractor</span> <span class="o">=</span> <span class="n">create_extractor</span><span class="p">(</span>
          <span class="n">llm</span><span class="p">,</span>
          <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">Memory</span><span class="p">],</span>
          <span class="n">tool_choice</span><span class="o">=</span><span class="s2">"Memory"</span><span class="p">,</span>
          <span class="c1"># This allows the extractor to insert new memories</span>
          <span class="n">enable_inserts</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
      <span class="p">)</span>
      
      <span class="c1"># Chatbot instruction</span>
      <span class="n">MODEL_SYSTEM_MESSAGE</span> <span class="o">=</span> <span class="s2">"""You are a helpful chatbot. You are designed to be a companion to a user. </span>
      <span class="s2">You have a long term memory which keeps track of information you learn about the user over time.</span>
      <span class="s2">Current Memory (may include updated memories from this conversation): </span>
      <span class="si">{opening_brace}memory{closing_brace}</span><span class="s2">"""</span>
      
      <span class="c1"># Create new memory from the chat history and any existing memory</span>
      <span class="n">TRUSTCALL_INSTRUCTION</span> <span class="o">=</span> <span class="s2">"""Reflect on following interaction. </span>
      <span class="s2">Use the provided tools to retain any necessary memories about the user. </span>
      <span class="s2">Use parallel tool calling to handle updates and insertions simultaneously:"""</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">call_model</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
      
      <span class="w">    </span><span class="sd">"""Load memory from the store and use it to personalize the chatbot's response."""</span>
          
          <span class="c1"># Get the user ID from the config</span>
          <span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s2">"user_id"</span><span class="p">]</span>
      
          <span class="c1"># Retrieve memory from the store</span>
          <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memories"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
          <span class="n">memories</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace</span><span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Call model debug] Memories: </span><span class="si">{opening_brace}</span><span class="n">memories</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
          <span class="c1"># Format the memories for the system prompt</span>
          <span class="n">info</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s2">"- </span><span class="si">{opening_brace}</span><span class="n">mem</span><span class="o">.</span><span class="n">value</span><span class="p">[</span><span class="s1">'content'</span><span class="p">]</span><span class="si">{closing_brace}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">mem</span> <span class="ow">in</span> <span class="n">memories</span><span class="p">)</span>
          <span class="n">system_msg</span> <span class="o">=</span> <span class="n">MODEL_SYSTEM_MESSAGE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">memory</span><span class="o">=</span><span class="n">info</span><span class="p">)</span>
      
          <span class="c1"># Respond using memory as well as the chat history</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_msg</span><span class="p">)]</span><span class="o">+</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>
      
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">response</span><span class="p">{closing_brace}</span>
      
      <span class="k">def</span> <span class="nf">write_memory</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">MessagesState</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">RunnableConfig</span><span class="p">,</span> <span class="n">store</span><span class="p">:</span> <span class="n">BaseStore</span><span class="p">):</span>
      
      <span class="w">    </span><span class="sd">"""Reflect on the chat history and save a memory to the store."""</span>
          
          <span class="c1"># Get the user ID from the config</span>
          <span class="n">user_id</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s2">"user_id"</span><span class="p">]</span>
      
          <span class="c1"># Define the namespace for the memories</span>
          <span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memories"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>
      
          <span class="c1"># Retrieve the most recent memories for context</span>
          <span class="n">existing_items</span> <span class="o">=</span> <span class="n">store</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace</span><span class="p">)</span>
      
          <span class="c1"># Format the existing memories for the Trustcall extractor</span>
          <span class="n">tool_name</span> <span class="o">=</span> <span class="s2">"Memory"</span>
          <span class="n">existing_memories</span> <span class="o">=</span> <span class="p">([(</span><span class="n">existing_item</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">tool_name</span><span class="p">,</span> <span class="n">existing_item</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
                                <span class="k">for</span> <span class="n">existing_item</span> <span class="ow">in</span> <span class="n">existing_items</span><span class="p">]</span>
                                <span class="k">if</span> <span class="n">existing_items</span>
                                <span class="k">else</span> <span class="kc">None</span>
                              <span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Write memory debug] Existing memories: </span><span class="si">{opening_brace}</span><span class="n">existing_memories</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
          <span class="c1"># Merge the chat history and the instruction</span>
          <span class="n">updated_messages</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">merge_message_runs</span><span class="p">(</span><span class="n">messages</span><span class="o">=</span><span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">TRUSTCALL_INSTRUCTION</span><span class="p">)]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]))</span>
      
          <span class="c1"># Invoke the extractor</span>
          <span class="n">result</span> <span class="o">=</span> <span class="n">trustcall_extractor</span><span class="o">.</span><span class="n">invoke</span><span class="p">({opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">updated_messages</span><span class="p">,</span> 
                                              <span class="s2">"existing"</span><span class="p">:</span> <span class="n">existing_memories</span><span class="p">{closing_brace})</span>
      
          <span class="c1"># Save the memories from Trustcall to the store</span>
          <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">rmeta</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s2">"responses"</span><span class="p">],</span> <span class="n">result</span><span class="p">[</span><span class="s2">"response_metadata"</span><span class="p">]):</span>
              <span class="n">store</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">namespace</span><span class="p">,</span>
                        <span class="n">rmeta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"json_doc_id"</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())),</span>
                        <span class="n">r</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">"json"</span><span class="p">),</span>
                  <span class="p">)</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">[Write memory debug] Saved memories: </span><span class="si">{opening_brace}</span><span class="n">result</span><span class="p">[</span><span class="s1">'responses'</span><span class="p">]</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">MessagesState</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"call_model"</span><span class="p">,</span> <span class="n">call_model</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"write_memory"</span><span class="p">,</span> <span class="n">write_memory</span><span class="p">)</span>
      
      <span class="c1"># Connect nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"call_model"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"call_model"</span><span class="p">,</span> <span class="s2">"write_memory"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"write_memory"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Store for long-term (across-thread) memory</span>
      <span class="n">long_term_memory</span> <span class="o">=</span> <span class="n">InMemoryStore</span><span class="p">()</span>
      
      <span class="c1"># Checkpointer for short-term (within-thread) memory</span>
      <span class="n">short_term_memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">short_term_memory</span><span class="p">,</span> <span class="n">store</span><span class="o">=</span><span class="n">long_term_memory</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv31.webp" width="146" height="333" alt="image uv 32" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Empezamos una nueva conversaci贸n</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>',
          '<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">,</span> <span class="s2">"user_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Hi, my name is Maximo"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Hi, my name is Maximo',
          '	[Call model debug] Memories: []',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Hello Maximo! It\'s nice to meet you. I\'m your companion chatbot, here to chat, help answer questions, or just be someone to talk to. ',
          'I\'ll remember your name is Maximo for our future conversations. What would you like to talk about today? How are you doing?',
          '	[Write memory debug] Existing memories: None',
          '	[Write memory debug] Saved memories: [Memory(content="User\'s name is Maximo.")]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos un nuevo inter茅s del usuario</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"I like to play soccer"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'I like to play soccer',
          '	[Call model debug] Memories: [Item(namespace=[\'memories\', \'1\'], key=\'6d06c4f5-3a74-46b2-92b4-1e29ba128c90\', value={\'content\': "User\'s name is Maximo."}, created_at=\'2025-05-12T18:32:38.070902+00:00\', updated_at=\'2025-05-12T18:32:38.070903+00:00\', score=None)]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'That\'s great to know, Maximo! Soccer is such a wonderful sport. Do you play on a team, or more casually with friends? I\'d also be curious to know what position you typically play, or if you have a favorite professional team you follow. I\'ll remember that you enjoy soccer for our future conversations.',
          '	[Write memory debug] Existing memories: [(\'6d06c4f5-3a74-46b2-92b4-1e29ba128c90\', \'Memory\', {\'content\': "User\'s name is Maximo."})]',
          '	[Write memory debug] Saved memories: [Memory(content=\'User enjoys playing soccer.\')]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, se ha a帽adido el nuevo inter茅s del usuario a la memoria.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver la memoria actualizada</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Namespace for the memory to save</span>',
          '<span class="n">user_id</span> <span class="o">=</span> <span class="s2">"1"</span>',
          '<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memories"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>',
          '<span class="n">memories</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">memories</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">dict</span><span class="p">())</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'namespace\': [\'memories\', \'1\'], \'key\': \'6d06c4f5-3a74-46b2-92b4-1e29ba128c90\', \'value\': {\'content\': "User\'s name is Maximo."}, \'created_at\': \'2025-05-12T18:32:38.070902+00:00\', \'updated_at\': \'2025-05-12T18:32:38.070903+00:00\', \'score\': None}',
          '{opening_brace}\'namespace\': [\'memories\', \'1\'], \'key\': \'25d2ee8c-5890-415b-85e0-d9fb0ea4cd43\', \'value\': {\'content\': \'User enjoys playing soccer.\'{closing_brace}, \'created_at\': \'2025-05-12T18:32:42.558787+00:00\', \'updated_at\': \'2025-05-12T18:32:42.558789+00:00\', \'score\': None}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">memories</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'content\': "User\'s name is Maximo."}',
          '{opening_brace}\'content\': \'User enjoys playing soccer.\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que se guardan documentos de memoria, no un perfil del usuario.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a a帽adir un nuevo inter茅s del usuario</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"I also like to play basketball"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'I also like to play basketball',
          '	[Call model debug] Memories: [Item(namespace=[\'memories\', \'1\'], key=\'6d06c4f5-3a74-46b2-92b4-1e29ba128c90\', value={\'content\': "User\'s name is Maximo."}, created_at=\'2025-05-12T18:32:38.070902+00:00\', updated_at=\'2025-05-12T18:32:38.070903+00:00\', score=None), Item(namespace=[\'memories\', \'1\'], key=\'25d2ee8c-5890-415b-85e0-d9fb0ea4cd43\', value={\'content\': \'User enjoys playing soccer.\'{closing_brace}, created_at=\'2025-05-12T18:32:42.558787+00:00\', updated_at=\'2025-05-12T18:32:42.558789+00:00\', score=None)]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'That\'s awesome, Maximo! Both soccer and basketball are fantastic sports. I\'ll remember that you enjoy basketball as well. Do you find yourself playing one more than the other? And similar to soccer, do you play basketball with a team or more casually? Many people enjoy the different skills and dynamics each sport offers - soccer with its continuous flow and footwork, and basketball with its fast pace and shooting precision. Any favorite basketball teams you follow?',
          '	[Write memory debug] Existing memories: [(\'6d06c4f5-3a74-46b2-92b4-1e29ba128c90\', \'Memory\', {\'content\': "User\'s name is Maximo."}), (\'25d2ee8c-5890-415b-85e0-d9fb0ea4cd43\', \'Memory\', {\'content\': \'User enjoys playing soccer.\'})]',
          '	[Write memory debug] Saved memories: [Memory(content=\'User enjoys playing basketball.\')]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver la memoria actualizada</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Namespace for the memory to save</span>',
          '<span class="n">user_id</span> <span class="o">=</span> <span class="s2">"1"</span>',
          '<span class="n">namespace</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"memories"</span><span class="p">,</span> <span class="n">user_id</span><span class="p">)</span>',
          '<span class="n">memories</span> <span class="o">=</span> <span class="n">long_term_memory</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">namespace</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">memories</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'content\': "User\'s name is Maximo."}',
          '{opening_brace}\'content\': \'User enjoys playing soccer.\'}',
          '{opening_brace}\'content\': \'User enjoys playing basketball.\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Iniciamos una nueva conversaci贸n con un nuevo hilo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># We supply a thread ID for short-term (within-thread) memory</span>',
          '<span class="c1"># We supply a user ID for long-term (across-thread) memory </span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"2"</span><span class="p">,</span> <span class="s2">"user_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># User input </span>',
          '<span class="n">input_messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"What soccer players do you recommend for me?"</span><span class="p">)]</span>',
          '',
          '<span class="c1"># Run the graph</span>',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">input_messages</span><span class="p">},</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">chunk</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'What soccer players do you recommend for me?',
          '	[Call model debug] Memories: [Item(namespace=[\'memories\', \'1\'], key=\'6d06c4f5-3a74-46b2-92b4-1e29ba128c90\', value={\'content\': "User\'s name is Maximo."}, created_at=\'2025-05-12T18:32:38.070902+00:00\', updated_at=\'2025-05-12T18:32:38.070903+00:00\', score=None), Item(namespace=[\'memories\', \'1\'], key=\'25d2ee8c-5890-415b-85e0-d9fb0ea4cd43\', value={\'content\': \'User enjoys playing soccer.\'{closing_brace}, created_at=\'2025-05-12T18:32:42.558787+00:00\', updated_at=\'2025-05-12T18:32:42.558789+00:00\', score=None), Item(namespace=[\'memories\', \'1\'], key=\'965f2e52-bea0-44d4-8534-4fce2bbc1c4b\', value={\'content\': \'User enjoys playing basketball.\'{closing_brace}, created_at=\'2025-05-12T18:33:38.613626+00:00\', updated_at=\'2025-05-12T18:33:38.613629+00:00\', score=None)]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Hi Maximo! Since you enjoy soccer, I\'d be happy to recommend some players you might find interesting to follow or learn from.',
          'Based on your interests in both soccer and basketball, I might suggest players who are known for their athleticism and skill:',
          '1. Lionel Messi - Widely considered one of the greatest players of all time',
          '2. Cristiano Ronaldo - Known for incredible athleticism and dedication',
          '3. Kylian Mbapp茅 - Young talent with amazing speed and technical ability',
          '4. Kevin De Bruyne - Master of passing and vision',
          '5. Erling Haaland - Goal-scoring phenomenon',
          'Is there a particular position or playing style you\'re most interested in? That would help me refine my recommendations further. I could also suggest players from specific leagues or teams if you have preferences!',
          '	[Write memory debug] Existing memories: [(\'6d06c4f5-3a74-46b2-92b4-1e29ba128c90\', \'Memory\', {\'content\': "User\'s name is Maximo."}), (\'25d2ee8c-5890-415b-85e0-d9fb0ea4cd43\', \'Memory\', {\'content\': \'User enjoys playing soccer.\'}), (\'965f2e52-bea0-44d4-8534-4fce2bbc1c4b\', \'Memory\', {\'content\': \'User enjoys playing basketball.\'})]',
          '	[Write memory debug] Saved memories: [Memory(content=\'User asked for soccer player recommendations, suggesting an active interest in following professional soccer beyond just playing it.\')]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que se acordaba que nos gustaba el f煤tbol y el baloncesto.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Human-in-the-loop">Human in the loop<a class="anchor-link" href="#Human-in-the-loop"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 47" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aunque un agente puede realizar tareas, para seg煤n qu茅 tareas, es necesario que haya una supervisi贸n humana. A esto se le llama <code>human in the loop</code>. As铆 que vamos a ver c贸mo se puede hacer esto con <code>LangGraph</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La capa de <a href="https://langchain-ai.github.io/langgraph/concepts/persistence/" target="_blank" rel="nofollow noreferrer">persistencia</a> de <code>LangGraph</code> admite flujos de trabajo con humanos en el bucle, lo que permite que la ejecuci贸n se detenga y reanude en funci贸n de los comentarios de los usuarios. La interfaz principal de esta funcionalidad es la funci贸n <a href="https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#interrupt">interrupt</a>. Llamando a <code>interrupt</code> dentro de un nodo se detendr谩 la ejecuci贸n. La ejecuci贸n se puede reanudar, junto con la nueva aportaci贸n del humano, pasada en una primitiva <a href="https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#the-command-primitive">Command</a>. <code>interrupt</code> es similar al comando de Python <code>input()</code>, pero con algunas consideraciones extra.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a a帽adir al chatbot que tiene memoria a corto plazo y acceso a tools, pero haremos un cambio, que es agregar una simple herramienta <code>human_assistance</code>. Esta herramienta utiliza <code>interrupt</code> para recibir informaci贸n de un humano.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero cargamos los valores de las API KEYs</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      ' ',
      '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
      '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>












      
      <section class="section-block-markdown-cell">
      <p>Creamos el grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
      '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
      ' ',
      ' ',
      '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
      ' ',
      '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>

















      
      <section class="section-block-markdown-cell">
      <p>Definimos la <code>tool</code> de b煤squeda</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
      '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
      ' ',
      '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
      '<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos la <code>tool</code> de ayuda humana</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langgraph.types</span> <span class="kn">import</span> <span class="n">Command</span><span class="p">,</span> <span class="n">interrupt</span>',
      '<span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">tool</span>',
      ' ',
      '<span class="nd">@tool</span>',
      '<span class="k">def</span> <span class="nf">human_assistance</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>',
      '<span class="w">    </span><span class="sd">"""</span>',
      '<span class="sd">    Request assistance from a human expert. Use this tool ONLY ONCE per conversation.</span>',
      '<span class="sd">    After receiving the expert\'s response, you should provide an elaborated response to the user based on the information received</span>',
      '<span class="sd">    based on the information received, without calling this tool again.</span>',
      ' ',
      '<span class="sd">    Args:</span>',
      '<span class="sd">        query: The query to ask the human expert.</span>',
      ' ',
      '<span class="sd">    Returns:</span>',
      '<span class="sd">        The response from the human expert.</span>',
      '<span class="sd">    """</span>',
      '    <span class="n">human_response</span> <span class="o">=</span> <span class="n">interrupt</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>',
      '    <span class="k">return</span> <span class="n">human_response</span><span class="p">[</span><span class="s2">"data"</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>























      
      <section class="section-block-markdown-cell">
      <p><code>LangGraph</code> obtiene informaci贸n de las herramientas mediante la documentaci贸n de la herramienta, es decir, el <code>docstring</code> de la funci贸n. Por lo que es muy importante generar un buen <code>docstring</code> para la herramienta.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una lista de <code>tools</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">,</span> <span class="n">human_assistance</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>A continuaci贸n, el <code>LLM</code> con las <code>bind_tools</code> y lo a帽adimos al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
          '<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>',
          '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
          '',
          '<span class="c1"># Create the LLM</span>',
          '<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>',
          '<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
          '    <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>',
          '    <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>',
          '    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
          '    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="c1"># Create the chat model</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>',
          '',
          '<span class="c1"># Modification: tell the LLM which tools it can call</span>',
          '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Define the chatbot function</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>',
          '    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>',
          '',
          '<span class="c1"># Add the chatbot node</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x10764b380&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si te fijas, hemos cambiado la forma de definir la funci贸n <code>chatbot_function</code>, ya que ahora tiene que manejar la interrupci贸n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos la <code>tool_node</code> al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>',
          '',
          '<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x10764b380&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos el nodo de <code>START</code> al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x10764b380&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un <code>checkpointer</code> <a href="https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver" target="_blank" rel="nofollow noreferrer">MemorySaver</a>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>',
      ' ',
      '<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Compilamos el grafo con el <code>checkpointer</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Lo representamos gr谩ficamente</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      
      <span class="k">try</span><span class="p">:</span>
          <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{opening_brace}</span><span class="n">e</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv32.webp" width="216" height="249" alt="image uv 33" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora solicitemos al chatbot con una pregunta que involucrar谩 a la nueva herramienta <code>human_assistance</code>:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">"I need some expert guidance for building an AI agent. Could you request assistance for me?"</span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'I need some expert guidance for building an AI agent. Could you request assistance for me?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  human_assistance (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: I need some expert guidance for building an AI agent. Could you provide me with some advice?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver, el chatbot gener贸 una llamada a la herramienta de asistencia humana.</p>
      <div class="highlight">
        <pre><span></span>Tool Calls:</pre>
        <pre>  human_assistance (0)</pre>
        <pre> Call ID: 0</pre>
        <pre>  Args:</pre>
        <pre>    query: I need some expert guidance for building an AI agent. Could you provide advice on key considerations, best practices, and potential pitfalls to avoid?</pre>
      </div>
      <p>Pero luego la ejecuci贸n se ha interrumpido. Vamos a ver el estado del grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '<span class="n">snapshot</span><span class="o">.</span><span class="n">next</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(\'tools\',)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que se detuvo en el nodo de <code>tools</code>. Analizamos c贸mo se ha definido la herramienta <code>human_assistance</code>.</p>
      <div class="highlight">
        <pre><span></span><span class="kn">from</span> <span class="nn">langgraph.types</span> <span class="kn">import</span> <span class="n">Command</span><span class="p">,</span> <span class="n">interrupt</span></pre>
        <pre><span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">tool</span></pre>
        <pre><span class="nd">@tool</span></pre>
        <pre><span class="k">def</span> <span class="nf">human_assistance</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span></pre>
        <pre><span class="w">    </span><span class="sd">"""</span></pre>
        <pre><span class="sd">    Request assistance from a human expert. Use this tool ONLY ONCE per conversation.</span></pre>
        <pre><span class="sd">    After receiving the expert's response, you should provide an elaborated response to the user based on the information received</span></pre>
        <pre><span class="sd">    based on the information received, without calling this tool again.</span></pre>
        <pre><span class="sd">    Args:</span></pre>
        <pre><span class="sd">        query: The query to ask the human expert.</span></pre>
        <pre><span class="sd">    Returns:</span></pre>
        <pre><span class="sd">        The response from the human expert.</span></pre>
        <pre><span class="sd">    """</span></pre>
        <pre>    <span class="n">human_response</span> <span class="o">=</span> <span class="n">interrupt</span><span class="p">({opening_brace}</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">query</span><span class="p">{closing_brace})</span></pre>
        <pre>    <span class="k">return</span> <span class="n">human_response</span><span class="p">[</span><span class="s2">"data"</span><span class="p">]</span></pre>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Llamando a la herramienta <code>interrupt</code> se detendr谩 la ejecuci贸n, similar a la funci贸n de Python <code>input()</code>.</p>
      <p>El progreso se mantiene en funci贸n de nuestra elecci贸n de <a href="https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpointer-libraries" target="_blank" rel="nofollow noreferrer">checkpointer</a>. Es decir, la elecci贸n de d贸nde se guarda el estado del grafo. As铆 que si estamos persistiendo (guardando el estado del grafo) con una base de datos como <code>SQLite</code>, <code>Postgres</code>, etc, podemos reanudar la ejecuci贸n en cualquier momento siempre y cuando la base de datos est茅 viva.</p>
      <p>Aqu铆 estamos persistiendo (guardando el estado del grafo) con el puntero de verificaci贸n en memoria RAM, por lo que podemos reanudar en cualquier momento mientras nuestro kernel de Python se est茅 ejecutando. En mi caso, mientras no resetee el kernel de mi Jupyter Notebook.</p>
      <p>Para reanudar la ejecuci贸n, pasamos un objeto <a href="https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#the-command-primitive" target="_blank" rel="nofollow noreferrer">Command</a> que contiene los datos esperados por la herramienta. El formato de estos datos se puede personalizar en funci贸n de nuestras necesidades. Aqu铆, solo necesitamos un diccionario con una key <code>data</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">human_response</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"We, the experts are here to help! We\'d recommend you check out LangGraph to build your agent."</span>',
          '    <span class="s2">"It\'s much more reliable and extensible than simple autonomous agents."</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">human_command</span> <span class="o">=</span> <span class="n">Command</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="p">{opening_brace}</span><span class="s2">"data"</span><span class="p">:</span> <span class="n">human_response</span><span class="p">})</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">human_command</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  human_assistance (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: I need some expert guidance for building an AI agent. Could you provide me with some advice?',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: human_assistance',
          'We, the experts are here to help! We\'d recommend you check out LangGraph to build your agent.It\'s much more reliable and extensible than simple autonomous agents.',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'The experts recommend checking out LangGraph for building your AI agent. It\'s known for being more reliable and extensible compared to simple autonomous agents.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, el chatbot ha esperado a que un humano le proporcione la respuesta y luego ha generado una respuesta basada en la informaci贸n recibida. Le hemos pedido por la ayuda sobre un experto sobre c贸mo crear agentes, el humano le ha dicho que lo mejor es usar LangGraph, y el chatbot ha generado una respuesta basada en esa informaci贸n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero sigue teniendo la posibilidad de realizar b煤squedas en la web. As铆 que ahora le vamos a pedir las 煤ltimas noticias sobre LangGraph.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">"What\'s the latest news about LangGraph?"</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'What\'s the latest news about LangGraph?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain - Changelog", "url": "https://changelog.langchain.com/", "content": "LangGraph `interrupt`: Simplifying human-in-the-loop agents --------------------------------------------------- Our latest feature in LangGraph, interrupt , makes building human-in-the-loop workflows easier. Agents arent perfect, so keeping humans in the loop... December 16, 2024 [...] LangGraph  Modify graph state from tools in LangGraph --------------------------------------------- LangGraph\'s latest update gives you greater control over your agents by enabling tools to directly update the graph state. This is a game-changer for use... December 18, 2024 [...] LangGraph Platform Custom authentication &amp; access control for LangGraph Platform ------------------------------------------------------------- Today, we\'re thrilled to announce Custom Authentication and Resource-Level Access Control for Python deployments in LangGraph Cloud and self-hosted... December 20, 2024", "score": 0.78650844}, {opening_brace}"title": "LangGraph 0.3 Release: Prebuilt Agents - LangChain Blog", "url": "https://blog.langchain.dev/langgraph-0-3-release-prebuilt-agents/", "content": "LangGraph 0.3 Release: Prebuilt Agents\n2 min read Feb 27, 2025\nBy Nuno Campos and Vadym Barda\nOver the past year, weve invested heavily in making LangGraph the go-to framework for building AI agents. With companies like Replit, Klarna, LinkedIn and Uber choosing to build on top of LangGraph, we have more conviction than ever that we are on the right path. [...] Up to this point, weve had one higher level abstraction and its lived in the main langgraph package. It was create_react_agent, a wrapper for creating a simple tool calling agent. Today, we are splitting that out of langgraph as part of a 0.3 release, and moving it into langgraph-prebuilt.\nWe are also introducing a new set of prebuilt agents built on top of LangGraph, in both Python and JavaScript.\nOver the past three weeks, weve already released a few of these: [...] Published Time: 2025-02-27T15:09:15.000Z\nLangGraph 0.3 Release: Prebuilt Agents\nSkip to content\n\n\nCase Studies\nIn the Loop\nLangChain\nDocs\nChangelog\n\nSign in Subscribe", "score": 0.72348577}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain - Changelog", "url": "https://changelog.langchain.com/", "content": "LangGraph  Modify graph state from tools in LangGraph --------------------------------------------- LangGraph\'s latest update gives you greater control over your agents by enabling tools to directly update the graph state. This is a game-changer for use... December 18, 2024 [...] LangGraph `interrupt`: Simplifying human-in-the-loop agents --------------------------------------------------- Our latest feature in LangGraph, interrupt , makes building human-in-the-loop workflows easier. Agents arent perfect, so keeping humans in the loop... December 16, 2024 [...] LangGraph Platform Custom authentication &amp; access control for LangGraph Platform ------------------------------------------------------------- Today, we\'re thrilled to announce Custom Authentication and Resource-Level Access Control for Python deployments in LangGraph Cloud and self-hosted... December 20, 2024", "score": 0.79732054}, {opening_brace}"title": "LangGraph 0.3 Release: Prebuilt Agents - LangChain Blog", "url": "https://blog.langchain.dev/langgraph-0-3-release-prebuilt-agents/", "content": "LangGraph 0.3 Release: Prebuilt Agents\n2 min read Feb 27, 2025\nBy Nuno Campos and Vadym Barda\nOver the past year, weve invested heavily in making LangGraph the go-to framework for building AI agents. With companies like Replit, Klarna, LinkedIn and Uber choosing to build on top of LangGraph, we have more conviction than ever that we are on the right path. [...] Up to this point, weve had one higher level abstraction and its lived in the main langgraph package. It was create_react_agent, a wrapper for creating a simple tool calling agent. Today, we are splitting that out of langgraph as part of a 0.3 release, and moving it into langgraph-prebuilt.\nWe are also introducing a new set of prebuilt agents built on top of LangGraph, in both Python and JavaScript.\nOver the past three weeks, weve already released a few of these: [...] Published Time: 2025-02-27T15:09:15.000Z\nLangGraph 0.3 Release: Prebuilt Agents\nSkip to content\n\n\nCase Studies\nIn the Loop\nLangChain\nDocs\nChangelog\n\nSign in Subscribe", "score": 0.7552947}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'The latest news about LangGraph includes several updates and releases. Firstly, the \'interrupt\' feature has been added, which simplifies creating human-in-the-loop workflows, essential for maintaining oversight of AI agents. Secondly, an update allows tools to modify the graph state directly, providing more control over the agents. Lastly, custom authentication and resource-level access control have been implemented for Python deployments in LangGraph Cloud and self-hosted environments. In addition, LangGraph released version 0.3, which introduces prebuilt agents in both Python and JavaScript, aimed at making it even easier to develop AI agents.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ha buscado las 煤ltimas noticias sobre LangGraph y ha generado una respuesta basada en la informaci贸n recibida.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a escribir todo junto para que sea m谩s comprensible</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
          '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
          '<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
          '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">ToolMessage</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langgraph.types</span> <span class="kn">import</span> <span class="n">Command</span><span class="p">,</span> <span class="n">interrupt</span>',
          '<span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">tool</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>',
          '',
          '<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>',
          '<span class="kn">import</span> <span class="nn">json</span>',
          '',
          '<span class="kn">import</span> <span class="nn">os</span>',
          '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
          '',
          '<span class="kn">import</span> <span class="nn">dotenv</span>',
          '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
          '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
          '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
          '',
          '<span class="c1"># State</span>',
          '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
          '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
          '',
          '<span class="c1"># Tools</span>',
          '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
          '<span class="n">tool_search</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '',
          '<span class="nd">@tool</span>',
          '<span class="k">def</span> <span class="nf">human_assistance</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>',
          '<span class="w">    </span><span class="sd">"""</span>',
          '<span class="sd">    Request assistance from a human expert. Use this tool ONLY ONCE per conversation.</span>',
          '<span class="sd">    After receiving the expert\'s response, you should provide an elaborated response to the user based on the information received</span>',
          '<span class="sd">    based on the information received, without calling this tool again.</span>',
          '',
          '<span class="sd">    Args:</span>',
          '<span class="sd">        query: The query to ask the human expert.</span>',
          '',
          '<span class="sd">    Returns:</span>',
          '<span class="sd">        The response from the human expert.</span>',
          '<span class="sd">    """</span>',
          '    <span class="n">human_response</span> <span class="o">=</span> <span class="n">interrupt</span><span class="p">({</span><span class="s2">"query"</span><span class="p">:</span> <span class="n">query</span><span class="p">})</span>',
          '    <span class="k">return</span> <span class="n">human_response</span><span class="p">[</span><span class="s2">"data"</span><span class="p">]</span>',
          '',
          '<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tool_search</span><span class="p">,</span> <span class="n">human_assistance</span><span class="p">]</span>',
          '',
          '<span class="c1"># Create the LLM model</span>',
          '<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>',
          '<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
          '    <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>',
          '    <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>',
          '    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
          '    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="c1"># Create the chat model</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>',
          '<span class="c1"># Create the LLM with tools</span>',
          '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Tool node</span>',
          '<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Functions</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>',
          '    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>',
          '',
          '<span class="c1"># Start to build the graph</span>',
          '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add nodes to the graph</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add edges</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span> <span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '',
          '<span class="c1"># Compile the graph</span>',
          '<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>',
          '<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>',
          '',
          '<span class="c1"># Display the graph</span>',
          '<span class="k">try</span><span class="p">:</span>',
          '    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>',
          '<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:',
          '1. Check your internet connection and try again',
          '2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`',
          '3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a pedirle ayuda al chatbot para crear agentes. Le pedimos que busque ayuda</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">"I need some expert guidance for building an AI agent. Could you request assistance for me?"</span>',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'I need some expert guidance for building an AI agent. Could you request assistance for me?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  human_assistance (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: I need expert guidance for building an AI agent.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos en qu茅 estado se ha quedado el grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '<span class="n">snapshot</span><span class="o">.</span><span class="n">next</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(\'tools\',)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Le damos la asistencia que est谩 pidiendo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">human_response</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"We, the experts are here to help! We\'d recommend you check out LangGraph to build your agent."</span>',
          '    <span class="s2">"It\'s much more reliable and extensible than simple autonomous agents."</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">human_command</span> <span class="o">=</span> <span class="n">Command</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="p">{opening_brace}</span><span class="s2">"data"</span><span class="p">:</span> <span class="n">human_response</span><span class="p">})</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">human_command</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  human_assistance (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: I need expert guidance for building an AI agent.',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: human_assistance',
          'We, the experts are here to help! We\'d recommend you check out LangGraph to build your agent.It\'s much more reliable and extensible than simple autonomous agents.',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  human_assistance (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: I need some expert guidance for building an AI agent. Could you recommend a platform and any tips for getting started?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Y por 煤ltimo le pedimos que busque en internet las 煤ltimas noticias de LangGraph</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="s2">"What\'s the latest news about LangGraph?"</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'What\'s the latest news about LangGraph?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain Blog", "url": "https://blog.langchain.dev/", "content": "LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna\'s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.", "score": 0.6811549}, {opening_brace}"title": "LangGraph + UiPath: advancing agentic automation together", "url": "https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together", "content": "Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath\'s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, were excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChains cloud or self-hosted on-premises. This feature is currently in private preview.", "score": 0.6557114}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain Blog", "url": "https://blog.langchain.dev/", "content": "LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna\'s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.", "score": 0.6811549}, {opening_brace}"title": "LangGraph + UiPath: advancing agentic automation together", "url": "https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together", "content": "Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath\'s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, were excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChains cloud or self-hosted on-premises. This feature is currently in private preview.", "score": 0.6557114}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain Blog", "url": "https://blog.langchain.dev/", "content": "LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna\'s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.", "score": 0.6811549}, {opening_brace}"title": "LangGraph + UiPath: advancing agentic automation together", "url": "https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together", "content": "Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath\'s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, were excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChains cloud or self-hosted on-premises. This feature is currently in private preview.", "score": 0.6557114}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain Blog", "url": "https://blog.langchain.dev/", "content": "LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna\'s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.", "score": 0.6811549}, {opening_brace}"title": "LangGraph + UiPath: advancing agentic automation together", "url": "https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together", "content": "Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath\'s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, were excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChains cloud or self-hosted on-premises. This feature is currently in private preview.", "score": 0.6557114}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain Blog", "url": "https://blog.langchain.dev/", "content": "LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna\'s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.", "score": 0.6811549}, {opening_brace}"title": "LangGraph + UiPath: advancing agentic automation together", "url": "https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together", "content": "Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath\'s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, were excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChains cloud or self-hosted on-premises. This feature is currently in private preview.", "score": 0.6557114}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain Blog", "url": "https://blog.langchain.dev/", "content": "LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna\'s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.", "score": 0.6811549}, {opening_brace}"title": "LangGraph + UiPath: advancing agentic automation together", "url": "https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together", "content": "Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath\'s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, were excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChains cloud or self-hosted on-premises. This feature is currently in private preview.", "score": 0.6557114}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain Blog", "url": "https://blog.langchain.dev/", "content": "LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna\'s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.", "score": 0.6811549}, {opening_brace}"title": "LangGraph + UiPath: advancing agentic automation together", "url": "https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together", "content": "Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath\'s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, were excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChains cloud or self-hosted on-premises. This feature is currently in private preview.", "score": 0.6557114}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain Blog", "url": "https://blog.langchain.dev/", "content": "LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna\'s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.", "score": 0.6811549}, {opening_brace}"title": "LangGraph + UiPath: advancing agentic automation together", "url": "https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together", "content": "Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath\'s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, were excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChains cloud or self-hosted on-premises. This feature is currently in private preview.", "score": 0.6557114}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: Latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangChain Blog", "url": "https://blog.langchain.dev/", "content": "LangSmith Incident on May 1, 2025\n\nRequests to the US LangSmith API from both the web application and SDKs experienced an elevated error rate for 28 minutes on May 1, 2025\n\nFeatured\n\nHow Klarna\'s AI assistant redefined customer support at scale for 85 million active users\n\nIs LangGraph Used In Production?\n\nIntroducing Interrupt: The AI Agent Conference by LangChain\n\nTop 5 LangGraph Agents in Production 2024 [...] See how Harmonic uses LangSmith and LangGraph products to streamline venture investing workflows.\n\nWhy Definely chose LangGraph for building their multi-agent AI system\n\nSee how Definely used LangGraph to design a multi-agent system to help lawyers speed up their workflows.\n\nIntroducing End-to-End OpenTelemetry Support in LangSmith\n\nLangSmith now provides end-to-end OpenTelemetry (OTel) support for applications built on LangChain and/or LangGraph.", "score": 0.67758125}, {opening_brace}"title": "LangGraph + UiPath: advancing agentic automation together", "url": "https://www.uipath.com/blog/product-and-updates/langgraph-uipath-advancing-agentic-automation-together", "content": "Raghu Malpani, Chief Technology Officer at UiPath, emphasizes the significance of these announcements for the UiPath developer community:\n\nOur collaboration with LangChain on LangSmith and Agent Protocol advances interoperability across agent frameworks. Further, by enabling the deployment of LangGraph agents into UiPath\'s enterprise-grade infrastructure, we are expanding the capabilities of our platform and opening up more possibilities for our developer community. [...] Today, were excited to announce:\n\nNative support for LangSmith observability in the UiPath LLM Gateway via OpenTelemetry (OTLP), enabling developers to monitor, debug, and evaluate LLM-powered features in UiPath using LangSmith either in LangChains cloud or self-hosted on-premises. This feature is currently in private preview.", "score": 0.6522641}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: latest news about LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangGraph - LangChain", "url": "https://www.langchain.com/langgraph", "content": "As Ally advances its exploration of Generative AI, our tech labs is excited by LangGraph, the new library from LangChain, which is central to our experiments", "score": 0.98559}, {opening_brace}"title": "Evaluating LangGraph Framework : Series 1 | by Jalaj Agrawal", "url": "https://medium.com/@jalajagr/evaluating-langgraph-as-a-multiagent-framework-a-10-dimensional-framework-series-1-c7203b7f4659", "content": ": LangGraph excels with its intuitive graph-based abstraction that allows new developers to build working multi-agent systems within hours.", "score": 0.98196}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'It looks like LangGraph has been generating some significant buzz in the AI community, especially for its capabilities in building multi-agent systems. Here are a few highlights from the latest news:',
          '1. **LangGraph in Production**: Companies like Klarna and Definely are already using LangGraph to build and optimize their AI systems. Klarna has leveraged LangGraph to enhance their customer support, and Definely has used it to design a multi-agent system to speed up legal workflows.',
          '2. **Integration with UiPath**: LangChain and UiPath have collaborated to advance agentic automation. This partnership includes native support for LangSmith observability in UiPaths LLM Gateway via OpenTelemetry, which will allow developers to monitor, debug, and evaluate LLM-powered features more effectively.',
          '3. **Intuitive Design**: LangGraph is praised for its intuitive graph-based abstraction, which enables developers to build working multi-agent systems quickly, even if they are new to the field.',
          '4. **Community and Conferences**: LangChain is also hosting an AI Agent Conference called "Interrupt," which could be a great opportunity to learn more about the latest developments and best practices in building AI agents.',
          'If you\'re considering using LangGraph for your project, these resources and updates might provide valuable insights and support. Would you like more detailed information on any specific aspect of LangGraph?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="M%C3%A1s">M谩s<a class="anchor-link" href="#M%C3%A1s"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 48" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Aprobaci%C3%B3n-del-uso-de-herramientas">Aprobaci贸n del uso de herramientas<a class="anchor-link" href="#Aprobaci%C3%B3n-del-uso-de-herramientas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 49" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Nota: Este apartado lo vamos a hacer usando Sonnet 3.7, ya que a d铆a de la escritura del post, es el mejor modelo para uso con agentes, y es el 煤nico que entiende cu谩ndo tiene que llamar a las tools y cu谩ndo no para este ejemplo</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos meter un <code>human in the loop</code> para aprobar el uso de herramientas. Vamos a crear un chatbot con varias herramientas para hacer operaciones matem谩ticas, para ello a la hora de construir el grafo indicamos d贸nde queremos meter el <code>breakpoint</code> (<code>graph_builder.compile(interrupt_before=["tools"], checkpointer=memory)</code>)</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
      <span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
      <span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>
      <span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">tool</span>
      <span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>
      
      <span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Tools</span>
      <span class="nd">@tool</span>
      <span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
      <span class="w">    </span><span class="sd">"""Multiply a and b.</span>
      
      <span class="sd">    Args:</span>
      <span class="sd">        a: first int</span>
      <span class="sd">        b: second int</span>
      
      <span class="sd">    Returns:</span>
      <span class="sd">        The product of a and b.</span>
      <span class="sd">    """</span>
          <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
      
      <span class="nd">@tool</span>
      <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
      <span class="w">    </span><span class="sd">"""Adds a and b.</span>
      
      <span class="sd">    Args:</span>
      <span class="sd">        a: first int</span>
      <span class="sd">        b: second int</span>
      
      <span class="sd">    Returns:</span>
      <span class="sd">        The sum of a and b.</span>
      <span class="sd">    """</span>
          <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
      
      <span class="nd">@tool</span>
      <span class="k">def</span> <span class="nf">subtract</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
      <span class="w">    </span><span class="sd">"""Subtract b from a.</span>
      
      <span class="sd">    Args:</span>
      <span class="sd">        a: first int</span>
      <span class="sd">        b: second int</span>
      
      <span class="sd">    Returns:</span>
      <span class="sd">        The difference between a and b.</span>
      <span class="sd">    """</span>
          <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
      
      <span class="nd">@tool</span>
      <span class="k">def</span> <span class="nf">divide</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
      <span class="w">    </span><span class="sd">"""Divide a by b.</span>
      
      <span class="sd">    Args:</span>
      <span class="sd">        a: first int</span>
      <span class="sd">        b: second int</span>
      
      <span class="sd">    Returns:</span>
      <span class="sd">        The quotient of a and b.</span>
      <span class="sd">    """</span>
          <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
      
      <span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">multiply</span><span class="p">,</span> <span class="n">add</span><span class="p">,</span> <span class="n">subtract</span><span class="p">,</span> <span class="n">divide</span><span class="p">]</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
      <span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="n">system_message</span> <span class="o">=</span> <span class="s2">"You are a helpful assistant that can use tools to answer questions. Once you have the result of a tool, provide a final answer without calling more tools."</span>
          <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_message</span><span class="p">)]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
      <span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">interrupt_before</span><span class="o">=</span><span class="p">[</span><span class="s2">"tools"</span><span class="p">],</span> <span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv33.webp" width="299" height="291" alt="image uv 34" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos en el grafo, hay un <code>interrupt</code> antes de usar las <code>tool</code>s. Eso significa que se va a parar antes de usarlas para pedirnos permiso</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Input</span>',
          '<span class="n">initial_input</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Multiply 2 and 3"</span><span class="p">)}</span>',
          '',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># Run the graph until the first interruption</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">initial_input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"updates"</span><span class="p">):</span>',
          '    <span class="k">if</span> <span class="s1">\'chatbot_node\'</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">\'chatbot_node\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          '[{\'text\': "I\'ll multiply 2 and 3 for you.", \'type\': \'text\'{closing_brace}, {opening_brace}\'id\': \'toolu_01QDuind1VBHWtvifELN9SPf\', \'input\': {\'a\': 2, \'b\': 3}, \'name\': \'multiply\', \'type\': \'tool_use\'{closing_brace}&rbrack;',
          'Tool Calls:',
          '  multiply (toolu_01QDuind1VBHWtvifELN9SPf)',
          ' Call ID: toolu_01QDuind1VBHWtvifELN9SPf',
          '  Args:',
          '    a: 2',
          '    b: 3',
          'None',
          '{opening_brace}\'__interrupt__\': ()}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, el LLM sabe que tiene que usar la herramienta <code>multiply</code>, pero se interrumpe la ejecuci贸n, porque tiene que esperar a que un humano le autorice el uso de la herramienta.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver el estado en el que se ha quedado el grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '<span class="n">state</span><span class="o">.</span><span class="n">next</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(\'tools\',)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, se ha quedado en el nodo de <code>tools</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos crear una funci贸n (no en el grafo, sino fuera del grafo, para mejorar la experiencia de usuario y que entienda por qu茅 se para la ejecuci贸n) que le pida al usuario que apruebe el uso de la herramienta.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un nuevo <code>thread_id</code> para que se cree un nuevo estado.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Input</span>',
          '<span class="n">initial_input</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Multiply 2 and 3"</span><span class="p">)}</span>',
          '',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"2"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># Run the graph until the first interruption</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">initial_input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"updates"</span><span class="p">):</span>',
          '    <span class="n">function_name</span> <span class="o">=</span> <span class="kc">None</span>',
          '    <span class="n">function_args</span> <span class="o">=</span> <span class="kc">None</span>',
          '',
          '    <span class="k">if</span> <span class="s1">\'chatbot_node\'</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">event</span><span class="p">[</span><span class="s1">\'chatbot_node\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">:</span>',
          '            <span class="k">if</span> <span class="n">element</span><span class="p">[</span><span class="s1">\'type\'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">\'text\'</span><span class="p">:</span>',
          '                <span class="nb">print</span><span class="p">(</span><span class="n">element</span><span class="p">[</span><span class="s1">\'text\'</span><span class="p">])</span>',
          '            <span class="k">elif</span> <span class="n">element</span><span class="p">[</span><span class="s1">\'type\'</span><span class="p">]</span> <span class="o">==</span> <span class="s1">\'tool_use\'</span><span class="p">:</span>',
          '                <span class="n">function_name</span> <span class="o">=</span> <span class="n">element</span><span class="p">[</span><span class="s1">\'name\'</span><span class="p">]</span>',
          '                <span class="n">function_args</span> <span class="o">=</span> <span class="n">element</span><span class="p">[</span><span class="s1">\'input\'</span><span class="p">]</span>',
          '                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The LLM wants to use the tool </span><span class="si">{</span><span class="n">function_name</span><span class="si">}</span><span class="s2"> with the arguments </span><span class="si">{</span><span class="n">function_args</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '    ',
          '    <span class="k">elif</span> <span class="s1">\'__interrupt__\'</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="k">pass</span>',
          '    ',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>',
          '',
          '<span class="n">question</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Do you approve the use of the tool </span><span class="si">{</span><span class="n">function_name</span><span class="si">}</span><span class="s2"> with the arguments </span><span class="si">{</span><span class="n">function_args</span><span class="si">}</span><span class="s2">? (y/n)"</span>',
          '<span class="n">user_approval</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">user_approval</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '',
          '<span class="k">if</span> <span class="n">user_approval</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s1">\'y\'</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="s2">"User approved the use of the tool"</span><span class="p">)</span>',
          '    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"updates"</span><span class="p">):</span>',
          '        <span class="k">if</span> <span class="s1">\'chatbot_node\'</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '            <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">event</span><span class="p">[</span><span class="s1">\'chatbot_node\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span><span class="p">:</span>',
          '                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>',
          '                    <span class="nb">print</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">""</span><span class="p">)</span>',
          '        ',
          '        <span class="k">elif</span> <span class="s1">\'tools\'</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '            <span class="n">result</span> <span class="o">=</span> <span class="n">event</span><span class="p">[</span><span class="s1">\'tools\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">content</span>',
          '            <span class="n">tool_used</span> <span class="o">=</span> <span class="n">event</span><span class="p">[</span><span class="s1">\'tools\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The result of the tool </span><span class="si">{</span><span class="n">tool_used</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '        ',
          '        <span class="k">else</span><span class="p">:</span>',
          '            <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'I\'ll multiply 2 and 3 for you.',
          'The LLM wants to use the tool multiply with the arguments {\'a\': 2, \'b\': 3}',
          'Do you approve the use of the tool None with the arguments None? (y/n): y',
          'User approved the use of the tool',
          'The result of the tool multiply is 6',
          'The result of multiplying 2 and 3 is 6.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que nos ha preguntado si aprobamos el uso de la <code>tool</code> de multiplicaci贸n, la hemos aprobado y el grafo ha terminado la ejecuci贸n. Viendo el estado del grafo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '<span class="n">state</span><span class="o">.</span><span class="n">next</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '()',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que el siguiente estado del grafo est谩 vac铆o, eso indica que ha terminado la ejecuci贸n del grafo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Modificaci%C3%B3n-del-estado">Modificaci贸n del estado<a class="anchor-link" href="#Modificaci%C3%B3n-del-estado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 50" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Nota: Este apartado lo vamos a hacer usando Sonnet 3.7, ya que a d铆a de la escritura del post, es el mejor modelo para uso con agentes, y es el 煤nico que entiende cu谩ndo tiene que llamar a las tools y cu谩ndo no para este ejemplo</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a repetir el ejemplo de antes, pero en vez de interrumpir el grafo antes del uso de una <code>tool</code>, lo vamos a interrumpir en el LLM, para ello, a la hora de construir el grafo indicamos que queremos pararlo en el agente (<code>graph_builder.compile(interrupt_before=["chatbot_node"], checkpointer=memory)</code>)</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
      <span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>
      <span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>
      <span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">tool</span>
      <span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>
      
      <span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Tools</span>
      <span class="nd">@tool</span>
      <span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
      <span class="w">    </span><span class="sd">"""Multiply a and b.</span>
      
      <span class="sd">    Args:</span>
      <span class="sd">        a: first int</span>
      <span class="sd">        b: second int</span>
      
      <span class="sd">    Returns:</span>
      <span class="sd">        The product of a and b.</span>
      <span class="sd">    """</span>
          <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
      
      <span class="nd">@tool</span>
      <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
      <span class="w">    </span><span class="sd">"""Adds a and b.</span>
      
      <span class="sd">    Args:</span>
      <span class="sd">        a: first int</span>
      <span class="sd">        b: second int</span>
      
      <span class="sd">    Returns:</span>
      <span class="sd">        The sum of a and b.</span>
      <span class="sd">    """</span>
          <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
      
      <span class="nd">@tool</span>
      <span class="k">def</span> <span class="nf">subtract</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
      <span class="w">    </span><span class="sd">"""Subtract b from a.</span>
      
      <span class="sd">    Args:</span>
      <span class="sd">        a: first int</span>
      <span class="sd">        b: second int</span>
      
      <span class="sd">    Returns:</span>
      <span class="sd">        The difference between a and b.</span>
      <span class="sd">    """</span>
          <span class="k">return</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span>
      
      <span class="nd">@tool</span>
      <span class="k">def</span> <span class="nf">divide</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
      <span class="w">    </span><span class="sd">"""Divide a by b.</span>
      
      <span class="sd">    Args:</span>
      <span class="sd">        a: first int</span>
      <span class="sd">        b: second int</span>
      
      <span class="sd">    Returns:</span>
      <span class="sd">        The quotient of a and b.</span>
      <span class="sd">    """</span>
          <span class="k">return</span> <span class="n">a</span> <span class="o">/</span> <span class="n">b</span>
      
      <span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">multiply</span><span class="p">,</span> <span class="n">add</span><span class="p">,</span> <span class="n">subtract</span><span class="p">,</span> <span class="n">divide</span><span class="p">]</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>
      <span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">chat_model_node</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="n">system_message</span> <span class="o">=</span> <span class="s2">"You are a helpful assistant that can use tools to answer questions. Once you have the result of a tool, provide a final answer without calling more tools."</span>
          <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">system_message</span><span class="p">)]</span> <span class="o">+</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chat_model_node</span><span class="p">)</span>
      <span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">interrupt_before</span><span class="o">=</span><span class="p">[</span><span class="s2">"chatbot_node"</span><span class="p">],</span> <span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv34.webp" width="216" height="291" alt="image uv 35" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos en la representaci贸n del grafo que hay un <code>interrupt</code> antes de la ejecuci贸n de <code>chatbot_node</code>, as铆 que antes de que se ejecute el chatbot se interrumpir谩 la ejecuci贸n y tendremos que hacer nosotros que contin煤e</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora le volvemos a pedir una multiplicaci贸n</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="c1"># Input</span>',
          '<span class="n">initial_input</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Multiply 2 and 3"</span><span class="p">)}</span>',
          '',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># Run the graph until the first interruption</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">initial_input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"updates"</span><span class="p">):</span>',
          '    <span class="k">if</span> <span class="s1">\'chatbot_node\'</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">\'chatbot_node\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'__interrupt__\': ()}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que no ha hecho nada. Si vemos el estado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '<span class="n">state</span><span class="o">.</span><span class="n">next</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(\'chatbot_node\',)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que el siguiente nodo es el de chatbot. Adem谩s, si vemos sus valores, vemos el mensaje que le hemos mandado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">state</span><span class="o">.</span><span class="n">values</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'messages\': &lbrack;HumanMessage(content=\'Multiply 2 and 3\', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id=\'08fd6084-ecd2-4156-ab24-00d2d5c26f00\')]}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora procedemos a modificar el estado, a帽adiendo un nuevo mensaje</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"No, actually multiply 3 and 3!"</span><span class="p">)]}</span>',
          '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'configurable\': {\'thread_id\': \'1\',',
          '  \'checkpoint_ns\': \'\',',
          '  \'checkpoint_id\': \'1f027eb6-6c8b-6b6a-8001-bc0f8942566c\'{closing_brace}{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos el nuevo estado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">new_state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '<span class="n">new_state</span><span class="o">.</span><span class="n">next</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(\'chatbot_node\',)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>El siguiente nodo sigue siendo el del chatbot, pero si ahora vemos los mensajes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">new_state</span><span class="o">.</span><span class="n">values</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'messages\': &lbrack;HumanMessage(content=\'Multiply 2 and 3\', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id=\'08fd6084-ecd2-4156-ab24-00d2d5c26f00\'),',
          '  HumanMessage(content=\'No, actually multiply 3 and 3!\', additional_kwargs={opening_brace}{closing_brace}, response_metadata={opening_brace}{closing_brace}, id=\'e95394c2-e62e-47d2-b9b2-51eba40f3e22\')]}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que se ha a帽adido el nuevo. As铆 que hacemos que contin煤e la ejecuci贸n</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">):</span>',
          '    <span class="n">event</span><span class="p">[</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'No, actually multiply 3 and 3!',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          '[{\'text\': "I\'ll multiply 3 and 3 for you.", \'type\': \'text\'{closing_brace}, {opening_brace}\'id\': \'toolu_01UABhLnEdg5ZqxVQTE5pGUx\', \'input\': {\'a\': 3, \'b\': 3}, \'name\': \'multiply\', \'type\': \'tool_use\'{closing_brace}&rbrack;',
          'Tool Calls:',
          '  multiply (toolu_01UABhLnEdg5ZqxVQTE5pGUx)',
          ' Call ID: toolu_01UABhLnEdg5ZqxVQTE5pGUx',
          '  Args:',
          '    a: 3',
          '    b: 3',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: multiply',
          '9',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se ha hecho la multiplicaci贸n de 3 por 3, que es la modificaci贸n del estado que hemos hecho, y no 2 por 3, que es lo que le pedimos la primera vez</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Esto puede ser 煤til cuando tenemos un agente y queremos revisar que lo que hace est茅 bien, por lo que podemos entrar en la ejecuci贸n y modificar el estado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Breakpoints-din%C3%A1micos">Breakpoints din谩micos<a class="anchor-link" href="#Breakpoints-din%C3%A1micos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 51" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hasta ahora hemos creado breakpoints est谩ticos mediante la compilaci贸n del grafo, pero podemos crear breakpoints din谩micos mediante <code>NodeInterrupt</code>. Esto es 煤til porque se puede interrumpir la ejecuci贸n por reglas l贸gicas introducidas por programaci贸n</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Estos <code>NodeInterrupt</code> permiten personalizar c贸mo se va a notificar al usuario de la interrupci贸n</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>
      <span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>
      <span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>
      <span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>
      <span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
      <span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>
      <span class="kn">from</span> <span class="nn">langgraph.errors</span> <span class="kn">import</span> <span class="n">NodeInterrupt</span>
      <span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
      <span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      <span class="kn">import</span> <span class="nn">os</span>
      <span class="kn">import</span> <span class="nn">dotenv</span>
      
      <span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>
      <span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>
      
      <span class="n">memory_saver</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>
      
      <span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>
          <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>
      
      <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>
      
      <span class="c1"># Create the LLM model</span>
      <span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>  <span class="c1"># Login to HuggingFace to use the model</span>
      <span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>
          <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>
          <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>
          <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
          <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># Create the chat model</span>
      <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
      
      <span class="c1"># Nodes</span>
      <span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>
          <span class="n">max_len</span> <span class="o">=</span> <span class="mi">15</span>
          <span class="n">input_message</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
      
          <span class="c1"># Check len message</span>
          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_len</span><span class="p">:</span>
              <span class="k">raise</span> <span class="n">NodeInterrupt</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Received input is longer than </span><span class="si">{opening_brace}</span><span class="n">max_len</span><span class="si">{closing_brace}</span><span class="s2"> characters --&gt; </span><span class="si">{opening_brace}</span><span class="n">input_message</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      
          <span class="c1"># Invoke the LLM with the messages</span>
          <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>
      
          <span class="c1"># Return the LLM's response in the correct state format</span>
          <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">response</span><span class="p">]{closing_brace}</span>
      
      <span class="c1"># Create graph builder</span>
      <span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>
      
      <span class="c1"># Add nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>
      
      <span class="c1"># Connecto nodes</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>
      <span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">END</span><span class="p">)</span>
      
      <span class="c1"># Compile the graph</span>
      <span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory_saver</span><span class="p">)</span>
      
      <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv35.webp" width="143" height="234" alt="image uv 36" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como se puede ver hemos creado una interrupci贸n en caso de que el mensaje sea largo. Vamos a probarlo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">initial_input</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"Hello, how are you? My name is M谩ximo"</span><span class="p">)}</span>',
          '',
          '<span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="c1"># Run the graph until the first interruption</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">initial_input</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"updates"</span><span class="p">):</span>',
          '    <span class="k">if</span> <span class="s1">\'chatbot_node\'</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">\'chatbot_node\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'__interrupt__\': (Interrupt(value="Received input is longer than 15 characters --&gt; content=\'Hello, how are you? My name is M谩ximo\' additional_kwargs={opening_brace}{closing_brace} response_metadata={opening_brace}{closing_brace} id=\'2bdc6d41-0cfe-4d3c-8748-ca7d46fd5a60\'", resumable=False, ns=None),)}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Efectivamente se ha parado la interrupci贸n y nos ha dado el mensaje de error que hemos creado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si vemos el nodo en el que se ha parado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '<span class="n">state</span><span class="o">.</span><span class="n">next</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(\'chatbot_node\',)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que est谩 parado en el nodo del chatbot. Podemos volver a hacer que continue con la ejecuci贸n, pero nos va a dar el mismo error</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"updates"</span><span class="p">):</span>',
          '    <span class="k">if</span> <span class="s1">\'chatbot_node\'</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">\'chatbot_node\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'__interrupt__\': (Interrupt(value="Received input is longer than 15 characters --&gt; content=\'Hello, how are you? My name is M谩ximo\' additional_kwargs={opening_brace}{closing_brace} response_metadata={opening_brace}{closing_brace} id=\'2bdc6d41-0cfe-4d3c-8748-ca7d46fd5a60\'", resumable=False, ns=None),)}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As铆 que tenemos que modificar el estado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">"How are you?"</span><span class="p">)]}</span>',
          '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'configurable\': {\'thread_id\': \'1\',',
          '  \'checkpoint_ns\': \'\',',
          '  \'checkpoint_id\': \'1f027f13-5827-6a18-8001-4209d5a866f0\'{closing_brace}{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver el estado y sus valores</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">new_state</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Siguiente nodo: </span><span class="si">{</span><span class="n">new_state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="s2">"Valores:"</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">new_state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\\t</span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Siguiente nodo: (\'chatbot_node\',)',
          'Valores:',
          '	Hello, how are you? My name is M谩ximo',
          '	How are you?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>El 煤ltimo mensaje es m谩s corto, por lo que intentamos reanudar la ejecuci贸n del grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"updates"</span><span class="p">):</span>',
          '    <span class="k">if</span> <span class="s1">\'chatbot_node\'</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s1">\'chatbot_node\'</span><span class="p">][</span><span class="s1">\'messages\'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">())</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Hello M谩ximo! I\'m doing well, thank you for asking. How about you? How can I assist you today?',
          'None',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Personalizaci%C3%B3n-del-estado">Personalizaci贸n del estado<a class="anchor-link" href="#Personalizaci%C3%B3n-del-estado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 52" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Nota: Este apartado lo vamos a hacer usando Sonnet 3.7, ya que a d铆a de la escritura del post, es el mejor modelo para uso con agentes, y es el 煤nico que entiende cuando tiene que llamar a las tools y cuando no</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hasta ahora, hemos confiado en un estado simple con una entrada, una lista de mensajes. Se puede llegar lejos con este estado simple, pero si se desea definir un comportamiento complejo sin depender de la lista de mensajes, se pueden agregar campos adicionales al estado.</p>
      <p>Aqu铆 vamos a ver un nuevo escenario, en el que el chatbot est谩 utilizando la herramienta de b煤squeda para encontrar informaci贸n espec铆fica, y reenvi谩ndola a un ser humano para su revisi贸n. Vamos a hacer que el chatbot investigue el cumplea帽os de una entidad. Agregaremos <code>name</code> y <code>birthday</code> como claves del estado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero cargamos los valores de las API KEYs</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      ' ',
      '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
      '<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>












      
      <section class="section-block-markdown-cell">
      <p>Creamos el nuevo estado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
      ' ',
      '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
      '    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>',
      '    <span class="n">birthday</span><span class="p">:</span> <span class="nb">str</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <p>Agregar esta informaci贸n al estado hace que sea f谩cilmente accesible por otros nodos del grafo (por ejemplo, un nodo que almacena o procesa la informaci贸n), as铆 como la capa de persistencia del grafo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos el grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
      ' ',
      '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Definimos la <code>tool</code> de b煤squeda</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
      '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
      ' ',
      '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
      '<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos la tool de asistencia humana. En esta tool rellenaremos las claves de estado dentro de nuestra herramienta <code>human_assistance</code>. Esto permite a un ser humano revisar la informaci贸n antes de que se almacene en el estado. Volveremos a usar <code>Command</code>, esta vez para emitir una actualizaci贸n de estado desde el interior de nuestra herramienta.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">ToolMessage</span>',
      '<span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">InjectedToolCallId</span><span class="p">,</span> <span class="n">tool</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langgraph.types</span> <span class="kn">import</span> <span class="n">Command</span><span class="p">,</span> <span class="n">interrupt</span>',
      ' ',
      ' ',
      '<span class="nd">@tool</span>',
      '<span class="c1"># Note that because we are generating a ToolMessage for a state update, we</span>',
      '<span class="c1"># generally require the ID of the corresponding tool call. We can use</span>',
      '<span class="c1"># LangChain\'s InjectedToolCallId to signal that this argument should not</span>',
      '<span class="c1"># be revealed to the model in the tool\'s schema.</span>',
      '<span class="k">def</span> <span class="nf">human_assistance</span><span class="p">(</span>',
      '    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">birthday</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">InjectedToolCallId</span><span class="p">]</span>',
      '<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>',
      '<span class="w">    </span><span class="sd">"""</span>',
      '<span class="sd">    Request assistance from a human expert. Use this tool ONLY ONCE per conversation.</span>',
      '<span class="sd">    After receiving the expert\'s response, you should provide an elaborated response to the user based on the information received</span>',
      '<span class="sd">    based on the information received, without calling this tool again.</span>',
      ' ',
      '<span class="sd">    Args:</span>',
      '<span class="sd">        query: The query to ask the human expert.</span>',
      ' ',
      '<span class="sd">    Returns:</span>',
      '<span class="sd">        The response from the human expert.</span>',
      '<span class="sd">    """</span>',
      '    <span class="n">human_response</span> <span class="o">=</span> <span class="n">interrupt</span><span class="p">(</span>',
      '        <span class="p">{opening_brace}</span>',
      '            <span class="s2">"question"</span><span class="p">:</span> <span class="s2">"Is this correct?"</span><span class="p">,</span>',
      '            <span class="s2">"name"</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>',
      '            <span class="s2">"birthday"</span><span class="p">:</span> <span class="n">birthday</span><span class="p">,</span>',
      '        <span class="p">},</span>',
      '    <span class="p">)</span>',
      ' ',
      '    <span class="c1"># If the information is correct, update the state as-is.</span>',
      '    <span class="k">if</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"correct"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"y"</span><span class="p">):</span>',
      '        <span class="n">verified_name</span> <span class="o">=</span> <span class="n">name</span>',
      '        <span class="n">verified_birthday</span> <span class="o">=</span> <span class="n">birthday</span>',
      '        <span class="n">response</span> <span class="o">=</span> <span class="s2">"Correct"</span>',
      ' ',
      '    <span class="c1"># Otherwise, receive information from the human reviewer.</span>',
      '    <span class="k">else</span><span class="p">:</span>',
      '        <span class="n">verified_name</span> <span class="o">=</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>',
      '        <span class="n">verified_birthday</span> <span class="o">=</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"birthday"</span><span class="p">,</span> <span class="n">birthday</span><span class="p">)</span>',
      '        <span class="n">response</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Made a correction: </span><span class="si">{</span><span class="n">human_response</span><span class="si">}</span><span class="s2">"</span>',
      ' ',
      '    <span class="c1"># This time we explicitly update the state with a ToolMessage inside</span>',
      '    <span class="c1"># the tool.</span>',
      '    <span class="n">state_update</span> <span class="o">=</span> <span class="p">{opening_brace}</span>',
      '        <span class="s2">"name"</span><span class="p">:</span> <span class="n">verified_name</span><span class="p">,</span>',
      '        <span class="s2">"birthday"</span><span class="p">:</span> <span class="n">verified_birthday</span><span class="p">,</span>',
      '        <span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">ToolMessage</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call_id</span><span class="p">)],</span>',
      '    <span class="p">{closing_brace}</span>',
      '    ',
      '    <span class="c1"># We return a Command object in the tool to update our state.</span>',
      '    <span class="k">return</span> <span class="n">Command</span><span class="p">(</span><span class="n">update</span><span class="o">=</span><span class="n">state_update</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>




























































      
      <section class="section-block-markdown-cell">
      <p>Hemos usado <a href="https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html" target="_blank" rel="nofollow noreferrer">ToolMessage</a> que se usa para pasar el resultado de ejecutar una <code>tool</code> de nuevo a un modelo y <a href="https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.InjectedToolCallId.html">InjectedToolCallId</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una lista de <code>tools</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">,</span> <span class="n">human_assistance</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>A continuaci贸n, el <code>LLM</code> con las <code>bind_tools</code> y lo a帽adimos al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
          '<span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>',
          '',
          '',
          '<span class="c1"># Create the LLM</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>',
          '',
          '<span class="c1"># Modification: tell the LLM which tools it can call</span>',
          '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Define the chatbot function</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>',
          '    <span class="c1"># Because we will be interrupting during tool execution,</span>',
          '    <span class="c1"># we disable parallel tool calling to avoid repeating any</span>',
          '    <span class="c1"># tool invocations when we resume.</span>',
          '    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>',
          '',
          '<span class="c1"># Add the chatbot node</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x120b4f380&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos la <code>tool</code> al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>',
          '',
          '<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x120b4f380&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos el nodo de <code>START</code> al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x120b4f380&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un <code>checkpointer</code> <a href="https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver" target="_blank" rel="nofollow noreferrer">MemorySaver</a>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>',
      ' ',
      '<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Compilamos el grafo con el <code>checkpointer</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Lo representamos gr谩ficamente</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      
      <span class="k">try</span><span class="p">:</span>
          <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{opening_brace}</span><span class="n">e</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv36.webp" width="216" height="249" alt="image uv 37" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a pedir a nuestro chatbot que busque el "cumplea帽os" de la biblioteca de <code>LangGraph</code>.</p>
      <p>Dirigiremos al chatbot hasta la herramienta <code>human_assistance</code> una vez que tenga la informaci贸n requerida. Los argumentos <code>name</code> y <code>birthday</code> son obligatorios para la herramienta <code>human_assistance</code>, as铆 que obligan al chatbot a generar propuestas para estos campos.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
          <span class="s2">"Can you look up when LangGraph was released? "</span>
          <span class="s2">"When you have the answer, use the human_assistance tool for review."</span>
      <span class="p">)</span>
      <span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>
      
      <span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
          <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>
          <span class="n">config</span><span class="p">,</span>
          <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
          <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
              <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>================================<span class="ansi-bold"> Human Message </span>=================================
      
      Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>==================================<span class="ansi-bold"> Ai Message </span>==================================
      
      [{opening_brace}'text': "I'll help you look up when LangGraph was released, and then I'll use the human_assistance tool for review as requested.\n\nFirst, let me search for information about LangGraph\'s release date:", 'type': 'text'{closing_brace}, {opening_brace}'id': 'toolu_011KHWFxYbFnUvGEF6MPt3dE', 'input': {opening_brace}'query': 'LangGraph release date when was LangGraph released'{closing_brace}, 'name': 'tavily_search_results_json', 'type': 'tool_use'{closing_brace}&rbrack;
      Tool Calls:
        tavily_search_results_json (toolu_011KHWFxYbFnUvGEF6MPt3dE)
       Call ID: toolu_011KHWFxYbFnUvGEF6MPt3dE
        Args:
          query: LangGraph release date when was LangGraph released
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>=================================<span class="ansi-bold"> Tool Message </span>=================================
      Name: tavily_search_results_json
      
      [{opening_brace}"title": "LangGraph Studio: The first agent IDE | by Bhavik Jikadara - Medium", "url": "https://bhavikjikadara.medium.com/langgraph-studio-the-first-agent-ide-468132628274", "content": "LangGraph, launched in January 2023, is a low-level orchestration framework designed for building controllable and complex agentic applications.", "score": 0.80405265{closing_brace}, {opening_brace}"title": "langgraph - PyPI", "url": "https://pypi.org/project/langgraph/", "content": "langgraph 路 PyPI\nSkip to main content Switch to mobile version\n\nSearch PyPI  Search\n\nHelp\nSponsors\nLog in\nRegister\n\nMenu\n\nHelp\nSponsors\nLog in\nRegister\n\nSearch PyPI  Search\nlanggraph 0.2.70\npip install langgraph Copy PIP instructions\nLatest versionReleased: Feb 6, 2025\nBuilding stateful, multi-actor applications with LLMs\nNavigation\n\nProject description\nRelease history\nDownload files [...] 0.2.20 Sep 13, 2024\n\n0.2.19 Sep 6, 2024\n\n0.2.18 Sep 6, 2024\n\n0.2.17 Sep 5, 2024\n\n0.2.16 Sep 1, 2024\n\n0.2.15 Aug 30, 2024\n\n0.2.14 Aug 24, 2024\n\n0.2.13 Aug 23, 2024\n\n0.2.12 Aug 22, 2024\n\n0.2.11 Aug 22, 2024\n\n0.2.10 Aug 21, 2024\n\n0.2.9 Aug 21, 2024\n\n0.2.8 Aug 21, 2024\n\n0.2.7 Aug 21, 2024\n\n0.2.7a0 pre-release Aug 21, 2024\n\n0.2.6 Aug 21, 2024\n\n0.2.5 Aug 21, 2024\n\n0.2.5a0 pre-release Aug 20, 2024\n\n0.2.4 Aug 15, 2024\n\n0.2.3 Aug 8, 2024\n\n0.2.2 Aug 7, 2024\n\n0.2.1 Aug 7, 2024\n\n0.2.0 Aug 7, 2024 [...] Download URL: langgraph-0.2.70.tar.gz\nUpload date: Feb 6, 2025\nSize: 129.7 kB\nTags: Source\nUploaded using Trusted Publishing? Yes\nUploaded via: twine/6.1.0 CPython/3.12.8", "score": 0.75659186{closing_brace}]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>==================================<span class="ansi-bold"> Ai Message </span>==================================
      
      [{opening_brace}'text': 'Based on my search, I found that LangGraph was launched in January 2023. However, I noticed some inconsistencies in the information, as one source mentions it was launched in January 2023, while the PyPI page shows a version history starting from 2024.\n\nLet me request human assistance to verify this information:', 'type': 'text'{closing_brace}, {opening_brace}'id': 'toolu_019EopKn8bLi3ksvUVY2Mt5p', 'input': {opening_brace}'name': 'LangGraph', 'birthday': 'January 2023'{closing_brace}, 'name': 'human_assistance', 'type': 'tool_use'{closing_brace}&rbrack;
      Tool Calls:
        human_assistance (toolu_019EopKn8bLi3ksvUVY2Mt5p)
       Call ID: toolu_019EopKn8bLi3ksvUVY2Mt5p
        Args:
          name: LangGraph
          birthday: January 2023
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se ha parado por el <code>interrupt</code> en la herramienta <code>human_assistance</code>. En este caso, el chatbot, con la herramienta de b煤squeda, ha determinado que la fecha de LangGraph es enero de 2023, pero no es la fecha exacta, es el 17 de enero de 2024, por lo que podemos introducirla nosotros mismos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">human_command</span> <span class="o">=</span> <span class="n">Command</span><span class="p">(</span>',
          '    <span class="n">resume</span><span class="o">=</span><span class="p">{opening_brace}</span>',
          '        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"LangGraph"</span><span class="p">,</span>',
          '        <span class="s2">"birthday"</span><span class="p">:</span> <span class="s2">"Jan 17, 2024"</span><span class="p">,</span>',
          '    <span class="p">},</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">human_command</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          '[{\'text\': \'Based on my search, I found that LangGraph was launched in January 2023. However, I noticed some inconsistencies in the information, as one source mentions it was launched in January 2023, while the PyPI page shows a version history starting from 2024.\n\nLet me request human assistance to verify this information:\', \'type\': \'text\'{closing_brace}, {opening_brace}\'id\': \'toolu_019EopKn8bLi3ksvUVY2Mt5p\', \'input\': {opening_brace}\'name\': \'LangGraph\', \'birthday\': \'January 2023\'{closing_brace}, \'name\': \'human_assistance\', \'type\': \'tool_use\'{closing_brace}&rbrack;',
          'Tool Calls:',
          '  human_assistance (toolu_019EopKn8bLi3ksvUVY2Mt5p)',
          ' Call ID: toolu_019EopKn8bLi3ksvUVY2Mt5p',
          '  Args:',
          '    name: LangGraph',
          '    birthday: January 2023',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: human_assistance',
          'Made a correction: {opening_brace}\'name\': \'LangGraph\', \'birthday\': \'Jan 17, 2024\'}',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Thank you for the expert review and correction! Based on the human expert\'s feedback, I can now provide you with the accurate information:',
          'LangGraph was released on January 17, 2024, not January 2023 as one of the search results incorrectly stated. ',
          'This is an important correction, as it means LangGraph is a relatively recent framework in the LLM orchestration space, having been available for less than a year at this point. LangGraph is developed by LangChain and is designed for building stateful, multi-actor applications with LLMs.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '',
          '<span class="p">{opening_brace}</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"name"</span><span class="p">,</span> <span class="s2">"birthday"</span><span class="p">)}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'name\': \'LangGraph\', \'birthday\': \'Jan 17, 2024\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora la fecha es la correcta gracias a la intervenci贸n humana para modificar los valores del estado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vuelvo a escribir todo el c贸digo para que sea m谩s f谩cil de entender</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">os</span>',
          '<span class="kn">import</span> <span class="nn">dotenv</span>',
          '',
          '<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
          '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.types</span> <span class="kn">import</span> <span class="n">Command</span><span class="p">,</span> <span class="n">interrupt</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
          '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
          '<span class="kn">from</span> <span class="nn">langchain_core.messages</span> <span class="kn">import</span> <span class="n">ToolMessage</span>',
          '<span class="kn">from</span> <span class="nn">langchain_core.tools</span> <span class="kn">import</span> <span class="n">InjectedToolCallId</span><span class="p">,</span> <span class="n">tool</span>',
          '<span class="kn">from</span> <span class="nn">langchain_anthropic</span> <span class="kn">import</span> <span class="n">ChatAnthropic</span>',
          '',
          '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
          '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
          '<span class="n">ANTHROPIC_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"ANTHROPIC_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
          '',
          '<span class="c1"># State</span>',
          '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
          '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
          '    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>',
          '    <span class="n">birthday</span><span class="p">:</span> <span class="nb">str</span>',
          '',
          '<span class="c1"># Tools</span>',
          '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
          '<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '',
          '<span class="nd">@tool</span>',
          '<span class="c1"># Note that because we are generating a ToolMessage for a state update, we</span>',
          '<span class="c1"># generally require the ID of the corresponding tool call. We can use</span>',
          '<span class="c1"># LangChain\'s InjectedToolCallId to signal that this argument should not</span>',
          '<span class="c1"># be revealed to the model in the tool\'s schema.</span>',
          '<span class="k">def</span> <span class="nf">human_assistance</span><span class="p">(</span>',
          '    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">birthday</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">InjectedToolCallId</span><span class="p">]</span>',
          '<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>',
          '<span class="w">    </span><span class="sd">"""</span>',
          '<span class="sd">    Request assistance from a human expert. Use this tool ONLY ONCE per conversation.</span>',
          '<span class="sd">    After receiving the expert\'s response, you should provide an elaborated response to the user based on the information received</span>',
          '<span class="sd">    based on the information received, without calling this tool again.</span>',
          '',
          '<span class="sd">    Args:</span>',
          '<span class="sd">        query: The query to ask the human expert.</span>',
          '',
          '<span class="sd">    Returns:</span>',
          '<span class="sd">        The response from the human expert.</span>',
          '<span class="sd">    """</span>',
          '    <span class="n">human_response</span> <span class="o">=</span> <span class="n">interrupt</span><span class="p">(</span>',
          '        <span class="p">{opening_brace}</span>',
          '            <span class="s2">"question"</span><span class="p">:</span> <span class="s2">"Is this correct?"</span><span class="p">,</span>',
          '            <span class="s2">"name"</span><span class="p">:</span> <span class="n">name</span><span class="p">,</span>',
          '            <span class="s2">"birthday"</span><span class="p">:</span> <span class="n">birthday</span><span class="p">,</span>',
          '        <span class="p">},</span>',
          '    <span class="p">)</span>',
          '',
          '    <span class="c1"># If the information is correct, update the state as-is.</span>',
          '    <span class="k">if</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"correct"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"y"</span><span class="p">):</span>',
          '        <span class="n">verified_name</span> <span class="o">=</span> <span class="n">name</span>',
          '        <span class="n">verified_birthday</span> <span class="o">=</span> <span class="n">birthday</span>',
          '        <span class="n">response</span> <span class="o">=</span> <span class="s2">"Correct"</span>',
          '',
          '    <span class="c1"># Otherwise, receive information from the human reviewer.</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="n">verified_name</span> <span class="o">=</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"name"</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>',
          '        <span class="n">verified_birthday</span> <span class="o">=</span> <span class="n">human_response</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"birthday"</span><span class="p">,</span> <span class="n">birthday</span><span class="p">)</span>',
          '        <span class="n">response</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Made a correction: </span><span class="si">{</span><span class="n">human_response</span><span class="si">}</span><span class="s2">"</span>',
          '',
          '    <span class="c1"># This time we explicitly update the state with a ToolMessage inside</span>',
          '    <span class="c1"># the tool.</span>',
          '    <span class="n">state_update</span> <span class="o">=</span> <span class="p">{opening_brace}</span>',
          '        <span class="s2">"name"</span><span class="p">:</span> <span class="n">verified_name</span><span class="p">,</span>',
          '        <span class="s2">"birthday"</span><span class="p">:</span> <span class="n">verified_birthday</span><span class="p">,</span>',
          '        <span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">ToolMessage</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">tool_call_id</span><span class="o">=</span><span class="n">tool_call_id</span><span class="p">)],</span>',
          '    <span class="p">{closing_brace}</span>',
          '    ',
          '    <span class="c1"># We return a Command object in the tool to update our state.</span>',
          '    <span class="k">return</span> <span class="n">Command</span><span class="p">(</span><span class="n">update</span><span class="o">=</span><span class="n">state_update</span><span class="p">)</span>',
          '',
          '<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">,</span> <span class="n">human_assistance</span><span class="p">]</span>',
          '<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Create the LLM</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatAnthropic</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">"claude-3-7-sonnet-20250219"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="n">ANTHROPIC_TOKEN</span><span class="p">)</span>',
          '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Define the chatbot function</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>',
          '    <span class="c1"># Because we will be interrupting during tool execution,</span>',
          '    <span class="c1"># we disable parallel tool calling to avoid repeating any</span>',
          '    <span class="c1"># tool invocations when we resume.</span>',
          '    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>',
          '',
          '<span class="c1"># Graph</span>',
          '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
          '',
          '<span class="c1"># Nodes</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
          '',
          '<span class="c1"># Edges</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '',
          '<span class="c1"># Checkpointer</span>',
          '<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>',
          '',
          '<span class="c1"># Compile</span>',
          '<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>',
          '',
          '<span class="c1"># Visualize</span>',
          '<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>',
          '',
          '<span class="k">try</span><span class="p">:</span>',
          '    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>',
          '<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:',
          '1. Check your internet connection and try again',
          '2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`',
          '3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a pedir a nuestro chatbot que busque el "cumplea帽os" de la biblioteca de <code>LangGraph</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>
          <span class="s2">"Can you look up when LangGraph was released? "</span>
          <span class="s2">"When you have the answer, use the human_assistance tool for review."</span>
      <span class="p">)</span>
      <span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>
      
      <span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>
          <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{opening_brace}</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">{closing_brace}&rbrack;{closing_brace},</span>
          <span class="n">config</span><span class="p">,</span>
          <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>
          <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>
              <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>================================<span class="ansi-bold"> Human Message </span>=================================
      
      Can you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>==================================<span class="ansi-bold"> Ai Message </span>==================================
      
      [{opening_brace}'text': "I'll look up when LangGraph was released and then get human verification of the information.", 'type': 'text'{closing_brace}, {opening_brace}'id': 'toolu_017SLLSEnFQZVdBpj85BKHyy', 'input': {opening_brace}'query': 'when was LangGraph released launch date'{closing_brace}, 'name': 'tavily_search_results_json', 'type': 'tool_use'{closing_brace}&rbrack;
      Tool Calls:
        tavily_search_results_json (toolu_017SLLSEnFQZVdBpj85BKHyy)
       Call ID: toolu_017SLLSEnFQZVdBpj85BKHyy
        Args:
          query: when was LangGraph released launch date
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>=================================<span class="ansi-bold"> Tool Message </span>=================================
      Name: tavily_search_results_json
      
      [{opening_brace}"title": "LangChain Introduces LangGraph Studio: The First Agent IDE for ...", "url": "https://www.marktechpost.com/2024/08/03/langchain-introduces-langgraph-studio-the-first-agent-ide-for-visualizing-interacting-with-and-debugging-complex-agentic-applications/", "content": "LangGraph, launched in January 2023, is a highly controllable, low-level orchestration framework for building agentic applications. Since its inception, it has undergone significant improvements, leading to a stable 0.1 release in June. LangGraph features a persistence layer enabling human-in-the-loop interactions and excels at building complex applications requiring domain-specific cognitive architecture.", "score": 0.83742094}, {opening_brace}"title": "LangGraph Studio: The first agent IDE | by Bhavik Jikadara - Medium", "url": "https://bhavikjikadara.medium.com/langgraph-studio-the-first-agent-ide-468132628274", "content": "LangGraph, launched in January 2023, is a low-level orchestration framework designed for building controllable and complex agentic applications. Its beneficial for creating applications requiring highly domain-specific cognitive architecture and human-in-the-loop interactions. LangGraph is open source, available in Python and JavaScript, and integrates seamlessly with LangSmith, whether or not you use LangChain.\n\nLangGraph: A Comprehensive Guide for Beginners", "score": 0.79369855}]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>==================================<span class="ansi-bold"> Ai Message </span>==================================
      
      [{opening_brace}'text': "Based on my search, I found that LangGraph was launched in January 2023. It's described as a low-level orchestration framework for building agentic applications. Since its release, it has seen significant improvements, including a stable 0.1 release in June (presumably 2024).\n\nLet me now get human verification of this information:", 'type': 'text'{closing_brace}, {opening_brace}'id': 'toolu_016h3391yFhtPDhQvwjNgs7W', 'input': {opening_brace}'name': 'Information Verification', 'birthday': 'January 2023'{closing_brace}, 'name': 'human_assistance', 'type': 'tool_use'{closing_brace}&rbrack;
      Tool Calls:
        human_assistance (toolu_016h3391yFhtPDhQvwjNgs7W)
       Call ID: toolu_016h3391yFhtPDhQvwjNgs7W
        Args:
          name: Information Verification
          birthday: January 2023
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://eu.api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://eu.api.smith.langchain.com/runs/multipart', '{opening_brace}"error":"Forbidden"{closing_brace}\n')
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se ha parado por el <code>interrupt</code> en la herramienta <code>human_assistance</code>. En este caso, el chatbot, con la herramienta de b煤squeda, ha determinado que la fecha de LangGraph es en enero de 2023, pero no es la fecha exacta, es el 17 de enero de 2024, por lo que podemos introducirla nosotros mismos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">human_command</span> <span class="o">=</span> <span class="n">Command</span><span class="p">(</span>',
          '    <span class="n">resume</span><span class="o">=</span><span class="p">{opening_brace}</span>',
          '        <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"LangGraph"</span><span class="p">,</span>',
          '        <span class="s2">"birthday"</span><span class="p">:</span> <span class="s2">"Jan 17, 2024"</span><span class="p">,</span>',
          '    <span class="p">},</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">human_command</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          '[{\'text\': "Based on my search, I found that LangGraph was launched in January 2023. It\'s described as a low-level orchestration framework for building agentic applications. Since its release, it has seen significant improvements, including a stable 0.1 release in June (presumably 2024).\n\nLet me now get human verification of this information:", \'type\': \'text\'{closing_brace}, {opening_brace}\'id\': \'toolu_016h3391yFhtPDhQvwjNgs7W\', \'input\': {opening_brace}\'name\': \'Information Verification\', \'birthday\': \'January 2023\'{closing_brace}, \'name\': \'human_assistance\', \'type\': \'tool_use\'{closing_brace}&rbrack;',
          'Tool Calls:',
          '  human_assistance (toolu_016h3391yFhtPDhQvwjNgs7W)',
          ' Call ID: toolu_016h3391yFhtPDhQvwjNgs7W',
          '  Args:',
          '    name: Information Verification',
          '    birthday: January 2023',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: human_assistance',
          'Made a correction: {opening_brace}\'name\': \'LangGraph\', \'birthday\': \'Jan 17, 2024\'}',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Thank you for the expert correction! I need to update my response with the accurate information.',
          'LangGraph was actually released on January 17, 2024 - not January 2023 as I initially found in my search results. This is a significant correction, as it means LangGraph is a much more recent framework than the search results indicated. ',
          'The expert has provided the specific date (January 17, 2024) for LangGraph\'s release, making it a fairly new tool in the AI orchestration ecosystem. This timing aligns better with the mention of its stable 0.1 release in June 2024, as this would be about 5 months after its initial launch.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '',
          '<span class="p">{opening_brace}</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"name"</span><span class="p">,</span> <span class="s2">"birthday"</span><span class="p">)}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'name\': \'LangGraph\', \'birthday\': \'Jan 17, 2024\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora la fecha es la correcta gracias a la intervenci贸n humana para modificar los valores del estado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Actualizaci%C3%B3n-manual-del-estado">Actualizaci贸n manual del estado<a class="anchor-link" href="#Actualizaci%C3%B3n-manual-del-estado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 53" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>LangGraph proporciona un alto grado de control sobre el estado de aplicaci贸n. Por ejemplo, en cualquier punto (incluso cuando se interrumpe), podemos soreescribir manualmente una key del estado usando <code>graph.update_state</code>:</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a actualizar el <code>name</code> del estado a <code>LangGraph (library)</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph</span><span class="o">.</span><span class="n">update_state</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="p">{opening_brace}</span><span class="s2">"name"</span><span class="p">:</span> <span class="s2">"LangGraph (library)"</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'configurable\': {\'thread_id\': \'1\',',
          '  \'checkpoint_ns\': \'\',',
          '  \'checkpoint_id\': \'1f010a5a-8a70-618e-8006-89107653db68\'{closing_brace}{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si ahora vemos el estado con <code>graph.get_state(config)</code> veremos que el <code>name</code> se ha actualizado.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">snapshot</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>',
          '',
          '<span class="p">{opening_brace}</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">"name"</span><span class="p">,</span> <span class="s2">"birthday"</span><span class="p">)}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'name\': \'LangGraph (library)\', \'birthday\': \'Jan 17, 2024\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Las actualizaciones de estado manuales generar谩n una traza en <code>LangSmith</code>. Se pueden usar para controlar flujos de trabajo de <code>human in the loop</code>, como se puede ver en esta <a href="https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/" target="_blank" rel="nofollow noreferrer">gu铆a</a>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Checkpoints">Checkpoints<a class="anchor-link" href="#Checkpoints"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 54" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En un flujo de trabajo t铆pico de un chatbot, el usuario interact煤a con el chatbot una o m谩s veces para realizar una tarea. En las secciones anteriores, vimos c贸mo agregar memoria y un <code>human in the loop</code> para poder verificar nuestro estado de gr谩fico y controlar las respuestas futuras.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero, a lo mejor un usuario quiere comenzar desde una respuesta anterior y o quiere <code>ramificar</code> para explorar un resultado separado. Esto es 煤til para aplicaciones de agentes, cuando un flujo falla pueden volver a un checkpoint anterior y probar otra estrategia.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>LangGraph</code> da esta posibilidad mediante los <code>checkpoints</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero cargamos los valores de las API KEYs</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      ' ',
      '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
      '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>












      
      <section class="section-block-markdown-cell">
      <p>Creamos el nuevo estado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
      ' ',
      '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
      ' ',
      '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
      '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos el grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
      ' ',
      '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Definimos la <code>tool</code> de b煤squeda</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
      '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
      ' ',
      '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
      '<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>Creamos una lista de <code>tools</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>A continuaci贸n, el <code>LLM</code> con las <code>bind_tools</code> y lo a帽adimos al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
          '<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>',
          '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
          '',
          '',
          '<span class="c1"># Create the LLM</span>',
          '<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>',
          '<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
          '    <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>',
          '    <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>',
          '    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
          '    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="c1"># Create the chat model</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>',
          '',
          '<span class="c1"># Modification: tell the LLM which tools it can call</span>',
          '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Define the chatbot function</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>',
          '',
          '<span class="c1"># Add the chatbot node</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x10d8ce7b0&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos la <code>tool</code> al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>',
          '',
          '<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x10d8ce7b0&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A帽adimos el nodo de <code>START</code> al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;langgraph.graph.state.StateGraph at 0x10d8ce7b0&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un <code>checkpointer</code> <a href="https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver" target="_blank" rel="nofollow noreferrer">MemorySaver</a>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>',
      ' ',
      '<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Compilamos el grafo con el <code>checkpointer</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Lo representamos gr谩ficamente</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
      
      <span class="k">try</span><span class="p">:</span>
          <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>
      <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{opening_brace}</span><span class="n">e</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/uv37.webp" width="216" height="249" alt="image uv 38" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hagamos que nuestro grafo d茅 un par de pasos. Cada paso ser谩 guardado en el historial del estado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hacemos la primera llamada al modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"I\'m learning LangGraph. "</span>',
          '    <span class="s2">"Could you do some research on it for me?"</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'I\'m learning LangGraph. Could you do some research on it for me?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangGraph Quickstart - GitHub Pages", "url": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "content": "[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It\'s particularly useful for developing more complex, [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-6)   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It\'s particularly useful for creating agent and multi-agent workflows.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-7)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-8)2. Developer: [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-19)LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-20)", "score": 0.9328032}, {opening_brace}"title": "langchain-ai/langgraph: Build resilient language agents as graphs.", "url": "https://github.com/langchain-ai/langgraph", "content": "LangGraph  used by Replit, Uber, LinkedIn, GitLab and more  is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables agent orchestration  offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.\n\n```\npip install -U langgraph\n```", "score": 0.8884594}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangGraph Quickstart - GitHub Pages", "url": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "content": "[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It\'s particularly useful for developing more complex, [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-6)   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It\'s particularly useful for creating agent and multi-agent workflows.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-7)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-8)2. Developer: [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-19)LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-20)", "score": 0.9328032}, {opening_brace}"title": "langchain-ai/langgraph: Build resilient language agents as graphs.", "url": "https://github.com/langchain-ai/langgraph", "content": "LangGraph  used by Replit, Uber, LinkedIn, GitLab and more  is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables agent orchestration  offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.\n\n```\npip install -U langgraph\n```", "score": 0.8884594}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph tutorial and documentation',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangGraph Quickstart - GitHub Pages", "url": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "content": "[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-66-36)5. Documentation: The LangGraph documentation has been revamped, which should make it easier for learners like yourself to understand and use the tool.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-66-37) [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-28)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-29)1. Search for the official LangGraph documentation or website to learn more about its features and how to use it.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-30)2. Look for tutorials or guides specifically focused on building AI agents with LangGraph. [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It\'s particularly useful for developing more complex,", "score": 0.8775715}, {opening_brace}"title": "Tutorial 1-Getting Started With LangGraph- Building Stateful Multi AI ...", "url": "https://www.youtube.com/watch?v=gqvFmK7LpDo", "content": "and we will also use Lang Smith so let\'s go ahead and execute this lsmith I hope everybody knows what exactly is so till then I will also go ahead and show you the documentation page of Lang graph so this is what langra is all about right it has python it has it supports JavaScript and all but uh uh if I just go ahead and show you Lang graph tutorials right so here you can see this is the tutorial right and uh not this sorry uh let\'s see yes yes here you go right in the Lang graph page it", "score": 0.80405265}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'LangGraph is a powerful library designed for building stateful, multi-agent applications using Large Language Models (LLMs). Here are some key points about LangGraph:',
          '### Overview',
          '- **Purpose**: LangGraph is specifically designed to create complex workflows and state machines to coordinate multiple AI agents or language model interactions. It is particularly useful for developing sophisticated multi-agent systems.',
          '- **Framework**: It is built on top of LangChain, leveraging its components and adding graph-based coordination capabilities.',
          '- **Features**:',
          '  - **Customizable Architectures**: Allows you to design and implement custom workflows and state machines.',
          '  - **Long-Term Memory**: Supports long-term memory for agents, enabling them to maintain context over time.',
          '  - **Human-in-the-Loop**: Facilitates human interaction in the workflow, making it easier to handle complex tasks that require human oversight.',
          '### Getting Started',
          '- **Installation**:',
          '  ```bash',
          '  pip install -U langgraph',
          '  ```',
          '- **Documentation**: The LangGraph documentation has been revamped to make it easier for learners to understand and use the tool. You can find the official documentation [here](https://langchain-ai.github.io/langgraph/tutorials/introduction/).',
          '### Use Cases',
          '- **Multi-Agent Systems**: Ideal for building systems where multiple AI agents need to interact and coordinate their actions.',
          '- **Complex Task Handling**: Suitable for tasks that require multiple steps and decision-making processes.',
          '- **Custom Workflows**: Enables the creation of custom workflows tailored to specific use cases.',
          '### Tutorials and Resources',
          '- **Official Documentation**: The official LangGraph documentation is a comprehensive resource for learning about its features and usage.',
          '- **Tutorials**: Look for tutorials and guides specifically focused on building AI agents with LangGraph. You can find a tutorial video [here](https://www.youtube.com/watch?v=gqvFmK7LpDo).',
          '### Companies Using LangGraph',
          '- **Replit, Uber, LinkedIn, GitLab, and more**: These companies are using LangGraph to build resilient and controllable language agents.',
          '### Next Steps',
          '1. **Review the Documentation**: Start by going through the official LangGraph documentation to get a deeper understanding of its features and capabilities.',
          '2. **Follow Tutorials**: Watch tutorials and follow step-by-step guides to build your first multi-agent application.',
          '3. **Experiment with Examples**: Try out the examples provided in the documentation to get hands-on experience with LangGraph.',
          'If you have any specific questions or need further assistance, feel free to ask!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Y ahora la segunda llamada</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"Ya that\'s helpful. Maybe I\'ll "</span>',
          '    <span class="s2">"build an autonomous agent with it!"</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Ya that\'s helpful. Maybe I\'ll build an autonomous agent with it!',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph tutorial build autonomous agent',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangGraph Tutorial: Building LLM Agents with LangChain\'s ... - Zep", "url": "https://www.getzep.com/ai-agents/langgraph-tutorial", "content": "This article focuses on building agents with LangGraph rather than LangChain. It provides a tutorial for building LangGraph agents, beginning with a discussion of LangGraph and its components. These concepts are reinforced by building a LangGraph agent from scratch and managing conversation memory with LangGraph agents. Finally, we use Zep\'s long-term memory for egents to create an agent that remembers previous conversations and user facts.\n\n芒\n\nSummary of key LangGraph tutorial concepts [...] human intervention, and the ability to handle complex workflows with\n      cycles and branches.\nBuilding a LangGraph agent | Creating a LangGraph agent is the best way to understand the core concepts\n      of nodes, edges, and state. The LangGraph Python libraries are modular and\n      provide the functionality to build a stateful graph by incrementally\n      adding nodes and edges.Incorporating tools enables an agent to perform specific tasks and access", "score": 0.8338803}, {opening_brace}"title": "Build Autonomous AI Agents with ReAct and LangGraph Tools", "url": "https://www.youtube.com/watch?v=ZfjaIshGkmk", "content": "LangGraph Intro - Build Autonomous AI Agents with ReAct and LangGraph Tools\nGrabDuck!\n4110 subscribers\n18 likes\n535 views\n21 Jan 2025\nIn this video, LangGraph Intro: Build Autonomous AI Agents with ReAct and LangGraph Tools, we dive into creating a powerful agentic system where the LLM decides when to trigger tools and when to finalize results. Youll see how to build a generic agent architecture using the ReAct principle, applying it to real-world examples like analyzing Tesla stock data. [...] reasoning like what they\'re doing so uh it\'s this way you\'re using tool and this is another thing from longchain core library and here you define the function and then you have to Define name description there are other parameters like for example you can provide very specific description of all the parameters like why you need them which one are those Etc but it\'s a bit over complicated for this tutorial I\'m skipping it and uh interesting thing this one return direct is false and this is uh [...] Whether youre wondering how to create AI agents, looking for a LangGraph tutorial, or eager to explore the power of LangChain agents, this video is packed with valuable insights to help you get started.\n\nSupport the channel while you shop on Amazon! \nUse my affiliate link https://amzn.to/4hssSvT\n\nEvery purchase via this Amazon link helps keep our content free for you!\n\n Related Courses &amp; Tutorials", "score": 0.8286204}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph tutorial build autonomous agent',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangGraph Tutorial: Building LLM Agents with LangChain\'s ... - Zep", "url": "https://www.getzep.com/ai-agents/langgraph-tutorial", "content": "This article focuses on building agents with LangGraph rather than LangChain. It provides a tutorial for building LangGraph agents, beginning with a discussion of LangGraph and its components. These concepts are reinforced by building a LangGraph agent from scratch and managing conversation memory with LangGraph agents. Finally, we use Zep\'s long-term memory for egents to create an agent that remembers previous conversations and user facts.\n\n芒\n\nSummary of key LangGraph tutorial concepts [...] human intervention, and the ability to handle complex workflows with\n      cycles and branches.\nBuilding a LangGraph agent | Creating a LangGraph agent is the best way to understand the core concepts\n      of nodes, edges, and state. The LangGraph Python libraries are modular and\n      provide the functionality to build a stateful graph by incrementally\n      adding nodes and edges.Incorporating tools enables an agent to perform specific tasks and access", "score": 0.8338803}, {opening_brace}"title": "Build Autonomous AI Agents with ReAct and LangGraph Tools", "url": "https://www.youtube.com/watch?v=ZfjaIshGkmk", "content": "LangGraph Intro - Build Autonomous AI Agents with ReAct and LangGraph Tools\nGrabDuck!\n4110 subscribers\n18 likes\n535 views\n21 Jan 2025\nIn this video, LangGraph Intro: Build Autonomous AI Agents with ReAct and LangGraph Tools, we dive into creating a powerful agentic system where the LLM decides when to trigger tools and when to finalize results. Youll see how to build a generic agent architecture using the ReAct principle, applying it to real-world examples like analyzing Tesla stock data. [...] reasoning like what they\'re doing so uh it\'s this way you\'re using tool and this is another thing from longchain core library and here you define the function and then you have to Define name description there are other parameters like for example you can provide very specific description of all the parameters like why you need them which one are those Etc but it\'s a bit over complicated for this tutorial I\'m skipping it and uh interesting thing this one return direct is false and this is uh [...] Whether youre wondering how to create AI agents, looking for a LangGraph tutorial, or eager to explore the power of LangChain agents, this video is packed with valuable insights to help you get started.\n\nSupport the channel while you shop on Amazon! \nUse my affiliate link https://amzn.to/4hssSvT\n\nEvery purchase via this Amazon link helps keep our content free for you!\n\n Related Courses &amp; Tutorials", "score": 0.8286204}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph tutorial build autonomous agent',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "LangGraph Tutorial: Building LLM Agents with LangChain\'s ... - Zep", "url": "https://www.getzep.com/ai-agents/langgraph-tutorial", "content": "This article focuses on building agents with LangGraph rather than LangChain. It provides a tutorial for building LangGraph agents, beginning with a discussion of LangGraph and its components. These concepts are reinforced by building a LangGraph agent from scratch and managing conversation memory with LangGraph agents. Finally, we use Zep\'s long-term memory for egents to create an agent that remembers previous conversations and user facts.\n\n芒\n\nSummary of key LangGraph tutorial concepts [...] human intervention, and the ability to handle complex workflows with\n      cycles and branches.\nBuilding a LangGraph agent | Creating a LangGraph agent is the best way to understand the core concepts\n      of nodes, edges, and state. The LangGraph Python libraries are modular and\n      provide the functionality to build a stateful graph by incrementally\n      adding nodes and edges.Incorporating tools enables an agent to perform specific tasks and access", "score": 0.8338803}, {opening_brace}"title": "Build Autonomous AI Agents with ReAct and LangGraph Tools", "url": "https://www.youtube.com/watch?v=ZfjaIshGkmk", "content": "LangGraph Intro - Build Autonomous AI Agents with ReAct and LangGraph Tools\nGrabDuck!\n4110 subscribers\n18 likes\n535 views\n21 Jan 2025\nIn this video, LangGraph Intro: Build Autonomous AI Agents with ReAct and LangGraph Tools, we dive into creating a powerful agentic system where the LLM decides when to trigger tools and when to finalize results. Youll see how to build a generic agent architecture using the ReAct principle, applying it to real-world examples like analyzing Tesla stock data. [...] reasoning like what they\'re doing so uh it\'s this way you\'re using tool and this is another thing from longchain core library and here you define the function and then you have to Define name description there are other parameters like for example you can provide very specific description of all the parameters like why you need them which one are those Etc but it\'s a bit over complicated for this tutorial I\'m skipping it and uh interesting thing this one return direct is false and this is uh [...] Whether youre wondering how to create AI agents, looking for a LangGraph tutorial, or eager to explore the power of LangChain agents, this video is packed with valuable insights to help you get started.\n\nSupport the channel while you shop on Amazon! \nUse my affiliate link https://amzn.to/4hssSvT\n\nEvery purchase via this Amazon link helps keep our content free for you!\n\n Related Courses &amp; Tutorials", "score": 0.8286204}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph tutorial for building autonomous AI agents',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "How to Build AI Agents with LangGraph: A Step-by-Step Guide", "url": "https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832", "content": "By following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs. This tutorial demonstrates the power of LangGraph in managing complex, multi-step processes and highlights how to leverage advanced AI tools to solve real-world challenges efficiently. Whether youre developing AI agents for customer support, energy management, or other applications, LangGraph provides the flexibility, scalability, and [...] Step 7: Build the Graph Structure\nIn this step, we construct the graph structure for the AI assistant using LangGraph, which controls how the assistant processes user input, triggers tools, and moves between stages. The graph defines nodes for the core actions (like invoking the assistant and tool) and edges that dictate the flow between these nodes. [...] Now that we have a solid understanding of what LangGraph is and how it enhances AI development, lets dive into a practical example. In this scenario, well build an AI agent designed to calculate potential energy savings for solar panels based on user input. This agent can be implemented as a lead generation tool on a solar panel sellers website, where it interacts with potential customers, offering personalized savings estimates. By gathering key data such as monthly electricity costs, this", "score": 0.8576849}, {opening_brace}"title": "Building AI Agents with LangGraph: A Beginner\'s Guide - YouTube", "url": "https://www.youtube.com/watch?v=assrhPxNdSk", "content": "In this tutorial, we\'ll break down the fundamentals of building AI agents using LangGraph! Whether you\'re new to AI development or looking", "score": 0.834852}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph tutorial step-by-step',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "How to Build AI Agents with LangGraph: A Step-by-Step Guide", "url": "https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832", "content": "By following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs. This tutorial demonstrates the power of LangGraph in managing complex, multi-step processes and highlights how to leverage advanced AI tools to solve real-world challenges efficiently. Whether youre developing AI agents for customer support, energy management, or other applications, LangGraph provides the flexibility, scalability, and [...] Step 7: Build the Graph Structure\nIn this step, we construct the graph structure for the AI assistant using LangGraph, which controls how the assistant processes user input, triggers tools, and moves between stages. The graph defines nodes for the core actions (like invoking the assistant and tool) and edges that dictate the flow between these nodes. [...] In this article, well explore how LangGraph transforms AI development and provide a step-by-step guide on how to build your own AI agent using an example that computes energy savings for solar panels. This example will showcase how LangGraphs unique features can create intelligent, adaptable, and real-world-ready AI systems.\nWhat is LangGraph?", "score": 0.86441374}, {opening_brace}"title": "What Is LangGraph and How to Use It? - DataCamp", "url": "https://www.datacamp.com/tutorial/langgraph-tutorial", "content": "Building a Simple LangGraph Application\nHeres a step-by-step example of creating a basic chatbot application using LangGraph.\nStep 1: Define the StateGraph\nDefine a StateGraph object to structure the chatbot as a state machine. The State is a class object defined with a single key messages of type List and uses the add_messages() function to append new messages rather than overwrite them.\nfrom typing import Annotated\nfrom typing_extensions import TypedDict [...] Getting Started With LangGraph\n\nInstallation\n\nBasic Concepts\n\n\nBuilding a Simple LangGraph Application\n\nStep 1: Define the StateGraph\nStep 2: Initialize an LLM and add it as a Chatbot node\nStep 3: Set edges\n\nStep 5: Run the chatbot\n\n\nAdvanced LangGraph Features\n\nCustom node types\nEdge types\n\nError handling\n\n\nReal-World Applications of LangGraph\n\nChatbots\nAutonomous agents\nMulti-Agent systems\nWorkflow automation tools\nRecommendation systems\n\nPersonalized learning environments\n\n\nConclusion", "score": 0.82492816}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph tutorial for beginners',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Introduction to LangGraph: A Beginner\'s Guide - Medium", "url": "https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141", "content": "Conclusion\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding! [...] LangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, well introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\nWhat is LangGraph?", "score": 0.8793233}, {opening_brace}"title": "LangGraph Tutorial: A Comprehensive Guide for Beginners", "url": "https://blog.futuresmart.ai/langgraph-tutorial-for-beginners", "content": "These examples highlight how LangGraph helps bridge the gap between AI capabilities and the complexities of real-world situations.\nConclusion\nThis concludes our LangGraph tutorial! As you\'ve learned, LangGraph enables the creation of AI applications that go beyond simple input-output loops by offering a framework for building stateful, agent-driven systems. You\'ve gained hands-on experience defining graphs, managing state, and incorporating tools. [...] LangGraph, a powerful library within the LangChain ecosystem, provides an elegant solution for building and managing multi-agent LLM applications. By representing workflows as cyclical graphs, LangGraph allows developers to orchestrate the interactions of multiple LLM agents, ensuring smooth communication and efficient execution of complex tasks. [...] LangGraph Tutorial: A Comprehensive Guide for Beginners\n\nFutureSmart AI Blog\nFollow\nFutureSmart AI Blog\nFollow\n\nLangGraph Tutorial: A Comprehensive Guide for Beginners\n\n\n+1\nRounak Show\nwith 1 co-author\n路Oct 1, 2024路12 min read\n\nTable of contents\n\nIntroduction\nUnderstanding LangGraph\nKey Concepts\nGraph Structures\nState Management\n\n\nGetting Started with LangGraph\nInstallation\nCreating a Basic Chatbot in LangGraph", "score": 0.8684817}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph tutorial for beginners',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "Introduction to LangGraph: A Beginner\'s Guide - Medium", "url": "https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141", "content": "Conclusion\nLangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph. Happy coding! [...] LangGraph is a powerful tool for building stateful, multi-actor applications with Large Language Models (LLMs). It extends the LangChain library, allowing you to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. In this article, well introduce LangGraph, walk you through its basic concepts, and share some insights and common points of confusion for beginners.\nWhat is LangGraph?", "score": 0.8793233}, {opening_brace}"title": "LangGraph Tutorial: A Comprehensive Guide for Beginners", "url": "https://blog.futuresmart.ai/langgraph-tutorial-for-beginners", "content": "These examples highlight how LangGraph helps bridge the gap between AI capabilities and the complexities of real-world situations.\nConclusion\nThis concludes our LangGraph tutorial! As you\'ve learned, LangGraph enables the creation of AI applications that go beyond simple input-output loops by offering a framework for building stateful, agent-driven systems. You\'ve gained hands-on experience defining graphs, managing state, and incorporating tools. [...] LangGraph, a powerful library within the LangChain ecosystem, provides an elegant solution for building and managing multi-agent LLM applications. By representing workflows as cyclical graphs, LangGraph allows developers to orchestrate the interactions of multiple LLM agents, ensuring smooth communication and efficient execution of complex tasks. [...] LangGraph Tutorial: A Comprehensive Guide for Beginners\n\nFutureSmart AI Blog\nFollow\nFutureSmart AI Blog\nFollow\n\nLangGraph Tutorial: A Comprehensive Guide for Beginners\n\n\n+1\nRounak Show\nwith 1 co-author\n路Oct 1, 2024路12 min read\n\nTable of contents\n\nIntroduction\nUnderstanding LangGraph\nKey Concepts\nGraph Structures\nState Management\n\n\nGetting Started with LangGraph\nInstallation\nCreating a Basic Chatbot in LangGraph", "score": 0.8684817}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Building an autonomous AI agent with LangGraph can be a rewarding and exciting project! Here\'s a step-by-step guide to get you started, based on the tutorials and resources available:',
          '### Step 1: Understand the Basics of LangGraph',
          'LangGraph is a library that extends the capabilities of LangChain, focusing on building stateful, multi-actor applications using Large Language Models (LLMs). It allows you to coordinate multiple LLM agents in complex workflows, using a graph-based approach.',
          '### Step 2: Install LangGraph',
          'First, you need to install the LangGraph library. You can do this using pip:',
          '```bash',
          'pip install -U langgraph',
          '```',
          '### Step 3: Define the Graph Structure',
          'The core of LangGraph is the graph structure, which defines the nodes (actions or states) and edges (transitions between nodes).',
          '#### Example: Solar Panel Energy Savings Calculator',
          'Let\'s build a simple AI agent that calculates potential energy savings for solar panels based on user input.',
          '1. **Define the StateGraph**:',
          '   - **Nodes**: These represent actions or states in your application.',
          '   - **Edges**: These define the transitions between nodes.',
          '```python',
          'from langgraph import StateGraph, State, Edge',
          '# Define the nodes',
          'start_node = State(key="start", action="greet_user")',
          'input_node = State(key="input", action="get_user_input")',
          'calculate_node = State(key="calculate", action="calculate_savings")',
          'result_node = State(key="result", action="show_results")',
          '# Define the edges',
          'start_to_input = Edge(from_node=start_node, to_node=input_node)',
          'input_to_calculate = Edge(from_node=input_node, to_node=calculate_node)',
          'calculate_to_result = Edge(from_node=calculate_node, to_node=result_node)',
          '# Create the graph',
          'graph = StateGraph()',
          'graph.add_state(start_node)',
          'graph.add_state(input_node)',
          'graph.add_state(calculate_node)',
          'graph.add_state(result_node)',
          'graph.add_edge(start_to_input)',
          'graph.add_edge(input_to_calculate)',
          'graph.add_edge(calculate_to_result)',
          '```',
          '### Step 4: Define the Actions',
          'Each node in the graph has an associated action. These actions are Python functions that perform specific tasks.',
          '```python',
          'def greet_user(state, context):',
          '    return {"message": "Hello! I can help you calculate energy savings for solar panels."}',
          'def get_user_input(state, context):',
          '    return {"message": "Please provide your monthly electricity cost in dollars."}',
          'def calculate_savings(state, context):',
          '    # Example calculation (you can use real data and more complex logic here)',
          '    monthly_cost = float(input("Enter your monthly electricity cost: "))',
          '    savings_per_kWh = 0.10  # Example savings rate',
          '    annual_savings = monthly_cost * 12 * savings_per_kWh',
          '    return {"savings": annual_savings}',
          'def show_results(state, context):',
          '    annual_savings = context.get("savings")',
          '    return {"message": f"Your annual savings with solar panels could be ${annual_savings:.2f}."}',
          '```',
          '### Step 5: Run the Graph',
          'Finally, you can run the graph to see how the agent processes user input and performs the calculations.',
          '```python',
          '# Initialize the graph and run it',
          'context = {opening_brace}{closing_brace}',
          'current_node = start_node',
          'while current_node:',
          '    action_result = current_node.action(current_node, context)',
          '    print(action_result["message"])',
          '    ',
          '    if "savings" in action_result:',
          '        context["savings"] = action_result["savings"]',
          '    ',
          '    current_node = graph.get_next_node(current_node, action_result)',
          '```',
          '### Step 6: Enhance with Advanced Features',
          'Once you have the basic structure in place, you can enhance your agent with advanced features such as:',
          '- **Long-term Memory**: Use external storage (e.g., Zep) to remember user conversations and preferences.',
          '- **Conditional Edges**: Define conditions for transitions between nodes to handle different scenarios.',
          '- **Human-in-the-Loop**: Allow human intervention for complex tasks or error handling.',
          '### Additional Resources',
          '- **Official Documentation**: [LangGraph Documentation](https://langchain-ai.github.io/langgraph/tutorials/introduction/)',
          '- **Comprehensive Guide**: [LangGraph Tutorial for Beginners](https://blog.futuresmart.ai/langgraph-tutorial-for-beginners)',
          '- **Example Project**: [Building AI Agents with LangGraph](https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832)',
          '### Conclusion',
          'By following these steps, you can build a robust and flexible AI agent using LangGraph. Start with simple examples and gradually add more complex features to create powerful, stateful, and multi-actor applications. Happy coding!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que hemos hecho dos llamadas al modelo, vamos a ver el historial del estado.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">to_replay</span> <span class="o">=</span> <span class="kc">None</span>',
          '<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state_history</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Num Messages: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span><span class="si">}</span><span class="s2">, Next: </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">, checkpoint id = </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s1">\'checkpoint_id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>',
          '',
          '    <span class="c1"># Get state when first iteracction us done</span>',
          '    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>',
          '        <span class="n">to_replay</span> <span class="o">=</span> <span class="n">state</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Num Messages: 24, Next: (), checkpoint id = 1f027f2f-e5b4-6c84-8018-9fcb33b5f397',
          '--------------------------------------------------------------------------------',
          'Num Messages: 23, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-e414-6b0e-8017-3ad465b70767',
          '--------------------------------------------------------------------------------',
          'Num Messages: 22, Next: (\'tools\',), checkpoint id = 1f027f2f-d382-6692-8016-fcfaf9c9a9f7',
          '--------------------------------------------------------------------------------',
          'Num Messages: 21, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-d1cf-6930-8015-f64aa0e6f750',
          '--------------------------------------------------------------------------------',
          'Num Messages: 20, Next: (\'tools\',), checkpoint id = 1f027f2f-bca9-6164-8014-86452cb10d83',
          '--------------------------------------------------------------------------------',
          'Num Messages: 19, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-bac1-6d24-8013-b539f3e4cedb',
          '--------------------------------------------------------------------------------',
          'Num Messages: 18, Next: (\'tools\',), checkpoint id = 1f027f2f-aa0e-69fa-8012-4ca2d9109f4e',
          '--------------------------------------------------------------------------------',
          'Num Messages: 17, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-a861-62c4-8011-5707badab130',
          '--------------------------------------------------------------------------------',
          'Num Messages: 16, Next: (\'tools\',), checkpoint id = 1f027f2f-93cf-6112-8010-ee536e76cdf7',
          '--------------------------------------------------------------------------------',
          'Num Messages: 15, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-91f5-63fa-800f-6ff45b0ebf86',
          '--------------------------------------------------------------------------------',
          'Num Messages: 14, Next: (\'tools\',), checkpoint id = 1f027f2f-7e07-6190-800e-e0269b0cb0f4',
          '--------------------------------------------------------------------------------',
          'Num Messages: 13, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-7bf9-62a4-800d-bd2bf25381ac',
          '--------------------------------------------------------------------------------',
          'Num Messages: 12, Next: (\'tools\',), checkpoint id = 1f027f2f-639f-6172-800c-e54c8b1b1f4a',
          '--------------------------------------------------------------------------------',
          'Num Messages: 11, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-621b-6972-800b-184a824ce9cb',
          '--------------------------------------------------------------------------------',
          'Num Messages: 10, Next: (\'tools\',), checkpoint id = 1f027f2f-56df-66a8-800a-d56ee9317382',
          '--------------------------------------------------------------------------------',
          'Num Messages: 9, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-5546-60d0-8009-41ee7c932b49',
          '--------------------------------------------------------------------------------',
          'Num Messages: 8, Next: (\'__start__\',), checkpoint id = 1f027f2f-5542-6ff2-8008-e2f4e8278c23',
          '--------------------------------------------------------------------------------',
          'Num Messages: 8, Next: (), checkpoint id = 1f027f2c-8873-61d6-8007-8a1c60438002',
          '--------------------------------------------------------------------------------',
          'Num Messages: 7, Next: (\'chatbot_node\',), checkpoint id = 1f027f2c-8504-663a-8006-517227b123b6',
          '--------------------------------------------------------------------------------',
          'Num Messages: 6, Next: (\'tools\',), checkpoint id = 1f027f2c-75dc-6248-8005-e198dd299848',
          '--------------------------------------------------------------------------------',
          'Num Messages: 5, Next: (\'chatbot_node\',), checkpoint id = 1f027f2c-7448-69d6-8004-e3c6d5c4c5a4',
          '--------------------------------------------------------------------------------',
          'Num Messages: 4, Next: (\'tools\',), checkpoint id = 1f027f2c-627b-6f6e-8003-22208fac7c89',
          '--------------------------------------------------------------------------------',
          'Num Messages: 3, Next: (\'chatbot_node\',), checkpoint id = 1f027f2c-6122-6190-8002-b745c42a724e',
          '--------------------------------------------------------------------------------',
          'Num Messages: 2, Next: (\'tools\',), checkpoint id = 1f027f2c-4c4c-6720-8001-8a1c73b894c1',
          '--------------------------------------------------------------------------------',
          'Num Messages: 1, Next: (\'chatbot_node\',), checkpoint id = 1f027f2c-4a91-6278-8000-56b65f6d77cd',
          '--------------------------------------------------------------------------------',
          'Num Messages: 0, Next: (\'__start__\',), checkpoint id = 1f027f2c-4a8d-6a1a-bfff-2f7cbde97290',
          '--------------------------------------------------------------------------------',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hemos guardado en <code>to_replay</code> el estado del grafo cuando nos ha dado la primera respuesta, justo antes de introducir el segundo mensaje. Podemos volver a un estado pasado y continuar el flujo desde all铆.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La configuraci贸n del checkpoint contiene el <code>checkpoint_id</code>, que es un timestamp del flujo. Podemos verlo para comprobar que estamos en el estado que queremos estar.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">to_replay</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1f027f2c-8873-61d6-8007-8a1c60438002\'{closing_brace}{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si nos fijamos en la lista de estados de antes, vemos que el ID coincide con el momento de introducir el segundo mensaje</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Dando este <code>checkpoint_id</code> a <code>LangGraph</code> carga el estado en ese momento del flujo. As铆 que creamos un nuevo mensaje y se lo pasamos al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"Thanks"</span>',
          '<span class="p">)</span>',
          '',
          '<span class="c1"># The `checkpoint_id` in the `to_replay.config` corresponds to a state we\'ve persisted to our checkpointer.</span>',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>',
          '    <span class="n">to_replay</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Thanks',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'You\'re welcome! If you have any more questions about LangGraph or any other topics, feel free to ask. Happy learning! ',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state_history</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Num Messages: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span><span class="si">}</span><span class="s2">, Next: </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">, checkpoint id = </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s1">\'checkpoint_id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Num Messages: 10, Next: (), checkpoint id = 1f027f43-71ae-67e0-800a-d84a557441fc',
          '--------------------------------------------------------------------------------',
          'Num Messages: 9, Next: (\'chatbot_node\',), checkpoint id = 1f027f43-5b1f-6ad8-8009-34f409789bc4',
          '--------------------------------------------------------------------------------',
          'Num Messages: 8, Next: (\'__start__\',), checkpoint id = 1f027f43-5b1b-68a2-8008-fbbcbd1c175e',
          '--------------------------------------------------------------------------------',
          'Num Messages: 24, Next: (), checkpoint id = 1f027f2f-e5b4-6c84-8018-9fcb33b5f397',
          '--------------------------------------------------------------------------------',
          'Num Messages: 23, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-e414-6b0e-8017-3ad465b70767',
          '--------------------------------------------------------------------------------',
          'Num Messages: 22, Next: (\'tools\',), checkpoint id = 1f027f2f-d382-6692-8016-fcfaf9c9a9f7',
          '--------------------------------------------------------------------------------',
          'Num Messages: 21, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-d1cf-6930-8015-f64aa0e6f750',
          '--------------------------------------------------------------------------------',
          'Num Messages: 20, Next: (\'tools\',), checkpoint id = 1f027f2f-bca9-6164-8014-86452cb10d83',
          '--------------------------------------------------------------------------------',
          'Num Messages: 19, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-bac1-6d24-8013-b539f3e4cedb',
          '--------------------------------------------------------------------------------',
          'Num Messages: 18, Next: (\'tools\',), checkpoint id = 1f027f2f-aa0e-69fa-8012-4ca2d9109f4e',
          '--------------------------------------------------------------------------------',
          'Num Messages: 17, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-a861-62c4-8011-5707badab130',
          '--------------------------------------------------------------------------------',
          'Num Messages: 16, Next: (\'tools\',), checkpoint id = 1f027f2f-93cf-6112-8010-ee536e76cdf7',
          '--------------------------------------------------------------------------------',
          'Num Messages: 15, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-91f5-63fa-800f-6ff45b0ebf86',
          '--------------------------------------------------------------------------------',
          'Num Messages: 14, Next: (\'tools\',), checkpoint id = 1f027f2f-7e07-6190-800e-e0269b0cb0f4',
          '--------------------------------------------------------------------------------',
          'Num Messages: 13, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-7bf9-62a4-800d-bd2bf25381ac',
          '--------------------------------------------------------------------------------',
          'Num Messages: 12, Next: (\'tools\',), checkpoint id = 1f027f2f-639f-6172-800c-e54c8b1b1f4a',
          '--------------------------------------------------------------------------------',
          'Num Messages: 11, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-621b-6972-800b-184a824ce9cb',
          '--------------------------------------------------------------------------------',
          'Num Messages: 10, Next: (\'tools\',), checkpoint id = 1f027f2f-56df-66a8-800a-d56ee9317382',
          '--------------------------------------------------------------------------------',
          'Num Messages: 9, Next: (\'chatbot_node\',), checkpoint id = 1f027f2f-5546-60d0-8009-41ee7c932b49',
          '--------------------------------------------------------------------------------',
          'Num Messages: 8, Next: (\'__start__\',), checkpoint id = 1f027f2f-5542-6ff2-8008-e2f4e8278c23',
          '--------------------------------------------------------------------------------',
          'Num Messages: 8, Next: (), checkpoint id = 1f027f2c-8873-61d6-8007-8a1c60438002',
          '--------------------------------------------------------------------------------',
          'Num Messages: 7, Next: (\'chatbot_node\',), checkpoint id = 1f027f2c-8504-663a-8006-517227b123b6',
          '--------------------------------------------------------------------------------',
          'Num Messages: 6, Next: (\'tools\',), checkpoint id = 1f027f2c-75dc-6248-8005-e198dd299848',
          '--------------------------------------------------------------------------------',
          'Num Messages: 5, Next: (\'chatbot_node\',), checkpoint id = 1f027f2c-7448-69d6-8004-e3c6d5c4c5a4',
          '--------------------------------------------------------------------------------',
          'Num Messages: 4, Next: (\'tools\',), checkpoint id = 1f027f2c-627b-6f6e-8003-22208fac7c89',
          '--------------------------------------------------------------------------------',
          'Num Messages: 3, Next: (\'chatbot_node\',), checkpoint id = 1f027f2c-6122-6190-8002-b745c42a724e',
          '--------------------------------------------------------------------------------',
          'Num Messages: 2, Next: (\'tools\',), checkpoint id = 1f027f2c-4c4c-6720-8001-8a1c73b894c1',
          '--------------------------------------------------------------------------------',
          'Num Messages: 1, Next: (\'chatbot_node\',), checkpoint id = 1f027f2c-4a91-6278-8000-56b65f6d77cd',
          '--------------------------------------------------------------------------------',
          'Num Messages: 0, Next: (\'__start__\',), checkpoint id = 1f027f2c-4a8d-6a1a-bfff-2f7cbde97290',
          '--------------------------------------------------------------------------------',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver en el historial que el grafo ejecut贸 todo lo primero que hicimos, pero que luego piso el historial y volvi贸 a ejecutar desde un punto anterior</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vuelvo a escribir todo el grafo junto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">os</span>',
          '<span class="kn">import</span> <span class="nn">dotenv</span>',
          '',
          '<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Annotated</span>',
          '<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">TypedDict</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.graph.message</span> <span class="kn">import</span> <span class="n">add_messages</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.graph</span> <span class="kn">import</span> <span class="n">StateGraph</span><span class="p">,</span> <span class="n">START</span><span class="p">,</span> <span class="n">END</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.prebuilt</span> <span class="kn">import</span> <span class="n">ToolNode</span><span class="p">,</span> <span class="n">tools_condition</span>',
          '<span class="kn">from</span> <span class="nn">langgraph.checkpoint.memory</span> <span class="kn">import</span> <span class="n">MemorySaver</span>',
          '',
          '<span class="kn">from</span> <span class="nn">langchain_community.utilities.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchAPIWrapper</span>',
          '<span class="kn">from</span> <span class="nn">langchain_community.tools.tavily_search</span> <span class="kn">import</span> <span class="n">TavilySearchResults</span>',
          '<span class="kn">from</span> <span class="nn">langchain_huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
          '<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>',
          '<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"LANGCHAIN_TRACING_V2"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"false"</span>    <span class="c1"># Disable LangSmith tracing</span>',
          '',
          '<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>',
          '',
          '<span class="k">class</span> <span class="nc">State</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">):</span>',
          '    <span class="n">messages</span><span class="p">:</span> <span class="n">Annotated</span><span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="n">add_messages</span><span class="p">]</span>',
          '',
          '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
          '<span class="n">HUGGINGFACE_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_LANGGRAPH"</span><span class="p">)</span>',
          '<span class="n">TAVILY_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"TAVILY_LANGGRAPH_API_KEY"</span><span class="p">)</span>',
          '',
          '<span class="c1"># Tools</span>',
          '<span class="n">wrapper</span> <span class="o">=</span> <span class="n">TavilySearchAPIWrapper</span><span class="p">(</span><span class="n">tavily_api_key</span><span class="o">=</span><span class="n">TAVILY_API_KEY</span><span class="p">)</span>',
          '<span class="n">search_tool</span> <span class="o">=</span> <span class="n">TavilySearchResults</span><span class="p">(</span><span class="n">api_wrapper</span><span class="o">=</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">max_results</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '<span class="n">tools_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">search_tool</span><span class="p">]</span>',
          '<span class="n">tool_node</span> <span class="o">=</span> <span class="n">ToolNode</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Create the LLM</span>',
          '<span class="n">login</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">HUGGINGFACE_TOKEN</span><span class="p">)</span>',
          '<span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
          '    <span class="n">repo_id</span><span class="o">=</span><span class="n">MODEL</span><span class="p">,</span>',
          '    <span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span>',
          '    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
          '    <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
          '    <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="c1"># Create the chat model</span>',
          '<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>',
          '<span class="c1"># Modification: tell the LLM which tools it can call</span>',
          '<span class="n">llm_with_tools</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">bind_tools</span><span class="p">(</span><span class="n">tools_list</span><span class="p">)</span>',
          '',
          '<span class="c1"># Define the chatbot function</span>',
          '<span class="k">def</span> <span class="nf">chatbot_function</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">State</span><span class="p">):</span>',
          '    <span class="n">message</span> <span class="o">=</span> <span class="n">llm_with_tools</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span>',
          '    <span class="k">return</span> <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[</span><span class="n">message</span><span class="p">]}</span>',
          '',
          '<span class="c1"># Create the graph</span>',
          '<span class="n">graph_builder</span> <span class="o">=</span> <span class="n">StateGraph</span><span class="p">(</span><span class="n">State</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add nodes</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">chatbot_function</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="n">tool_node</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"tools"</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add edges</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">START</span><span class="p">,</span> <span class="s2">"chatbot_node"</span><span class="p">)</span>',
          '<span class="n">graph_builder</span><span class="o">.</span><span class="n">add_conditional_edges</span><span class="p">(</span><span class="s2">"chatbot_node"</span><span class="p">,</span> <span class="n">tools_condition</span><span class="p">)</span>',
          '',
          '<span class="c1"># Add checkpointer</span>',
          '<span class="n">memory</span> <span class="o">=</span> <span class="n">MemorySaver</span><span class="p">()</span>',
          '',
          '<span class="c1"># Compile</span>',
          '<span class="n">graph</span> <span class="o">=</span> <span class="n">graph_builder</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">checkpointer</span><span class="o">=</span><span class="n">memory</span><span class="p">)</span>',
          '',
          '<span class="c1"># Visualize</span>',
          '<span class="k">try</span><span class="p">:</span>',
          '    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span><span class="o">.</span><span class="n">draw_mermaid_png</span><span class="p">()))</span>',
          '<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error al visualizar el grafo: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Error al visualizar el grafo: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:',
          '1. Check your internet connection and try again',
          '2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`',
          '3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hacemos la primera llamada al modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{opening_brace}</span><span class="s2">"configurable"</span><span class="p">:</span> <span class="p">{opening_brace}</span><span class="s2">"thread_id"</span><span class="p">:</span> <span class="s2">"1"</span><span class="p">{closing_brace}{closing_brace}</span>',
          '',
          '<span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"I\'m learning LangGraph. "</span>',
          '    <span class="s2">"Could you do some research on it for me?"</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'I\'m learning LangGraph. Could you do some research on it for me?',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "What is LangGraph? - IBM", "url": "https://www.ibm.com/think/topics/langgraph", "content": "LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize large language models (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. [...] Agent systems: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\n\nLLM applications: By using LangGraphs capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences. [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.", "score": 0.9353998}, {opening_brace}"title": "LangGraph Quickstart - GitHub Pages", "url": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "content": "[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It\'s particularly useful for developing more complex, [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-6)   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It\'s particularly useful for creating agent and multi-agent workflows.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-7)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-8)2. Developer: [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-19)LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-20)", "score": 0.9328032}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'Tool Calls:',
          '  tavily_search_results_json (0)',
          ' Call ID: 0',
          '  Args:',
          '    query: LangGraph',
          '=================================<span class="ansi-bold"> Tool Message </span>=================================',
          'Name: tavily_search_results_json',
          '[{opening_brace}"title": "What is LangGraph? - IBM", "url": "https://www.ibm.com/think/topics/langgraph", "content": "LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize large language models (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. [...] Agent systems: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\n\nLLM applications: By using LangGraphs capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences. [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.", "score": 0.9353998}, {opening_brace}"title": "LangGraph Quickstart - GitHub Pages", "url": "https://langchain-ai.github.io/langgraph/tutorials/introduction/", "content": "[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-9-1)Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It\'s particularly useful for developing more complex, [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-6)   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It\'s particularly useful for creating agent and multi-agent workflows.\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-7)\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-21-8)2. Developer: [...] [](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-19)LangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n[](https://langchain-ai.github.io/langgraph/tutorials/introduction/#__codelineno-48-20)", "score": 0.9328032}]',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'LangGraph is an open-source AI agent framework developed by LangChain, designed to build, deploy, and manage complex generative AI agent workflows. Here are some key points about LangGraph:',
          '### Overview',
          '- **Purpose**: LangGraph is aimed at creating, running, and optimizing large language models (LLMs) in a scalable and efficient manner.',
          '- **Graph-Based Architecture**: It uses graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow.',
          '### Features',
          '- **Agent Systems**: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles, or video games.',
          '- **LLM Applications**: Developers can build more sophisticated AI models that learn and improve over time. For example, Norwegian Cruise Line uses LangGraph to compile, construct, and refine guest-facing AI solutions, enhancing personalized guest experiences.',
          '- **Scalability**: By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without sacrificing efficiency.',
          '- **Enhanced Decision-Making**: LangGraph uses AI agents to analyze their past actions and feedback, a process referred to as "reflection" in the context of LLMs.',
          '### Developer Resources',
          '- **Quickstart Guide**: The LangGraph Quickstart guide on GitHub provides a detailed introduction to building stateful multi-agent applications using language models. It covers tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions.',
          '- **Built on LangChain**: LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. This makes it particularly useful for developing more complex, stateful, multi-actor applications with LLMs.',
          '### Further Reading',
          '- **What is LangGraph? - IBM**: [Link](https://www.ibm.com/think/topics/langgraph)',
          '- **LangGraph Quickstart - GitHub Pages**: [Link](https://langchain-ai.github.io/langgraph/tutorials/introduction/)',
          'These resources should provide a solid foundation for understanding and getting started with LangGraph. If you have any specific questions or need further details, feel free to ask!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Y ahora la segunda llamada</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"Ya that\'s helpful. Maybe I\'ll "</span>',
          '    <span class="s2">"build an autonomous agent with it!"</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span>',
          '    <span class="p">{opening_brace}</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>',
          '    <span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Ya that\'s helpful. Maybe I\'ll build an autonomous agent with it!',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'That sounds like an exciting project! Building an autonomous agent using LangGraph can be a rewarding experience. Here are some steps and tips to help you get started:',
          '### 1. **Understand the Basics of LangGraph**',
          '- **Read the Documentation**: Start with the official LangGraph documentation and quickstart guide. This will give you a solid understanding of the framework\'s capabilities and how to use its tools.',
          '  - **Quickstart Guide**: [LangGraph Quickstart - GitHub Pages](https://langchain-ai.github.io/langgraph/tutorials/introduction/)',
          '### 2. **Set Up Your Development Environment**',
          '- **Install LangChain and LangGraph**: Ensure you have the necessary dependencies installed. LangGraph is built on top of LangChain, so you\'ll need to set up both.',
          '  ```bash',
          '  pip install langchain langgraph',
          '  ```',
          '### 3. **Define Your Agent\'s Objectives**',
          '- **Identify the Use Case**: What specific tasks do you want your autonomous agent to perform? This could be anything from navigating a virtual environment, responding to user queries, or managing a robotic system.',
          '- **Define the State and Actions**: Determine the states your agent can be in and the actions it can take. This will help you design the state machine and workflows.',
          '### 4. **Design the Graph-Based Workflow**',
          '- **Create Nodes and Edges**: In LangGraph, you\'ll define nodes (agents or components) and edges (interactions or transitions). Each node can represent a different part of your agent\'s functionality.',
          '- **Define State Transitions**: Use the graph-based architecture to define how the agent transitions between different states based on actions and events.',
          '### 5. **Implement the Agent**',
          '- **Write the Code**: Start coding your agent using the LangGraph library. You can use the provided tools to create and manage the agent\'s workflows.',
          '  - **Example**: Heres a simple example to get you started:',
          '    ```python',
          '    from langgraph import AgentGraph, Node, Edge',
          '    # Define nodes',
          '    node1 = Node("Sensor", process=sensor_process)',
          '    node2 = Node("Decision", process=decision_process)',
          '    node3 = Node("Actuator", process=actuator_process)',
          '    # Define edges',
          '    edge1 = Edge(node1, node2)',
          '    edge2 = Edge(node2, node3)',
          '    # Create the agent graph',
          '    agent_graph = AgentGraph()',
          '    agent_graph.add_node(node1)',
          '    agent_graph.add_node(node2)',
          '    agent_graph.add_node(node3)',
          '    agent_graph.add_edge(edge1)',
          '    agent_graph.add_edge(edge2)',
          '    # Run the graph',
          '    agent_graph.run()',
          '    ```',
          '### 6. **Test and Iterate**',
          '- **Run Simulations**: Test your agent in a simulated environment to see how it performs. Use this to identify and fix any issues.',
          '- **Refine the Model**: Based on the test results, refine your agent\'s model and workflows. You can add more nodes, edges, or improve the decision-making processes.',
          '### 7. **Deploy and Monitor**',
          '- **Deploy the Agent**: Once you are satisfied with the performance, you can deploy your agent in the real world or a production environment.',
          '- **Monitor and Maintain**: Continuously monitor the agent\'s performance and make adjustments as needed. Use feedback loops to improve the agent over time.',
          '### 8. **Community and Support**',
          '- **Join the Community**: Engage with the LangChain and LangGraph community. You can find support, share ideas, and get feedback from other developers.',
          '  - **GitHub**: [LangGraph GitHub](https://github.com/langchain-ai/langgraph)',
          '  - **Forums and Discussion Boards**: Check out forums and discussion boards related to LangGraph and LangChain.',
          '### Additional Resources',
          '- **Tutorials and Examples**: Look for tutorials and example projects to get more hands-on experience.',
          '- **Research Papers and Articles**: Read research papers and articles to deepen your understanding of AI agent design and graph-based architectures.',
          'Good luck with your project! If you have any specific questions or need further guidance, feel free to ask.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos el historial del estado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">to_replay</span> <span class="o">=</span> <span class="kc">None</span>',
          '<span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state_history</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Num Messages: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span><span class="si">}</span><span class="s2">, Next: </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">, checkpoint id = </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s1">\'checkpoint_id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>',
          '',
          '    <span class="c1"># Get state when first iteracction us done</span>',
          '    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>',
          '        <span class="n">to_replay</span> <span class="o">=</span> <span class="n">state</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Num Messages: 8, Next: (), checkpoint id = 1f03263e-a96c-6446-8008-d2c11df0b6cb',
          '--------------------------------------------------------------------------------',
          'Num Messages: 7, Next: (\'chatbot_node\',), checkpoint id = 1f03263d-7a35-6660-8007-a37d4b584c88',
          '--------------------------------------------------------------------------------',
          'Num Messages: 6, Next: (\'__start__\',), checkpoint id = 1f03263d-7a32-624e-8006-6509bbf32ebe',
          '--------------------------------------------------------------------------------',
          'Num Messages: 6, Next: (), checkpoint id = 1f03263d-7a1a-6f36-8005-f10b5d83f22c',
          '--------------------------------------------------------------------------------',
          'Num Messages: 5, Next: (\'chatbot_node\',), checkpoint id = 1f03263c-c53f-6666-8004-c6d35868dd73',
          '--------------------------------------------------------------------------------',
          'Num Messages: 4, Next: (\'tools\',), checkpoint id = 1f03263c-b14b-68f8-8003-28558fa38dbc',
          '--------------------------------------------------------------------------------',
          'Num Messages: 3, Next: (\'chatbot_node\',), checkpoint id = 1f03263c-a66b-6276-8002-2dc89fca4d99',
          '--------------------------------------------------------------------------------',
          'Num Messages: 2, Next: (\'tools\',), checkpoint id = 1f03263c-8c7c-68ec-8001-fb8a9aa300b0',
          '--------------------------------------------------------------------------------',
          'Num Messages: 1, Next: (\'chatbot_node\',), checkpoint id = 1f03263c-6d06-68d2-8000-ced2e7b8538f',
          '--------------------------------------------------------------------------------',
          'Num Messages: 0, Next: (\'__start__\',), checkpoint id = 1f03263c-6cdb-63e4-bfff-c644b57cee28',
          '--------------------------------------------------------------------------------',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">to_replay</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{opening_brace}\'configurable\': {\'thread_id\': \'1\', \'checkpoint_ns\': \'\', \'checkpoint_id\': \'1f03263d-7a1a-6f36-8005-f10b5d83f22c\'{closing_brace}{closing_brace}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Dando este <code>checkpoint_id</code> a <code>LangGraph</code> carga el estado en ese momento del flujo. As铆 que creamos un nuevo mensaje y se lo pasamos al grafo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">user_input</span> <span class="o">=</span> <span class="p">(</span>',
          '    <span class="s2">"Thanks"</span>',
          '<span class="p">)</span>',
          '',
          '<span class="c1"># The `checkpoint_id` in the `to_replay.config` corresponds to a state we\'ve persisted to our checkpointer.</span>',
          '<span class="n">events</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">stream</span><span class="p">({</span><span class="s2">"messages"</span><span class="p">:</span> <span class="p">[{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span><span class="s2">"content"</span><span class="p">:</span> <span class="n">user_input</span><span class="p">},],},</span>',
          '    <span class="n">to_replay</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>',
          '    <span class="n">stream_mode</span><span class="o">=</span><span class="s2">"values"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">events</span><span class="p">:</span>',
          '    <span class="k">if</span> <span class="s2">"messages"</span> <span class="ow">in</span> <span class="n">event</span><span class="p">:</span>',
          '        <span class="n">event</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '================================<span class="ansi-bold"> Human Message </span>=================================',
          'Thanks',
          '==================================<span class="ansi-bold"> Ai Message </span>==================================',
          'You\'re welcome! If you have any more questions about LangGraph or any other topic, feel free to reach out. Happy learning! ',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">get_state_history</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Num Messages: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="s2">"messages"</span><span class="p">])</span><span class="si">}</span><span class="s2">, Next: </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">next</span><span class="si">}</span><span class="s2">, checkpoint id = </span><span class="si">{</span><span class="n">state</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">"configurable"</span><span class="p">][</span><span class="s1">\'checkpoint_id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="s2">"-"</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Num Messages: 8, Next: (), checkpoint id = 1f03263f-fcb9-63a0-8008-e8c4a3fb44f9',
          '--------------------------------------------------------------------------------',
          'Num Messages: 7, Next: (\'chatbot_node\',), checkpoint id = 1f03263f-eb3b-663c-8007-72da4d16bf64',
          '--------------------------------------------------------------------------------',
          'Num Messages: 6, Next: (\'__start__\',), checkpoint id = 1f03263f-eb36-6ac4-8006-a2333805d5d6',
          '--------------------------------------------------------------------------------',
          'Num Messages: 8, Next: (), checkpoint id = 1f03263e-a96c-6446-8008-d2c11df0b6cb',
          '--------------------------------------------------------------------------------',
          'Num Messages: 7, Next: (\'chatbot_node\',), checkpoint id = 1f03263d-7a35-6660-8007-a37d4b584c88',
          '--------------------------------------------------------------------------------',
          'Num Messages: 6, Next: (\'__start__\',), checkpoint id = 1f03263d-7a32-624e-8006-6509bbf32ebe',
          '--------------------------------------------------------------------------------',
          'Num Messages: 6, Next: (), checkpoint id = 1f03263d-7a1a-6f36-8005-f10b5d83f22c',
          '--------------------------------------------------------------------------------',
          'Num Messages: 5, Next: (\'chatbot_node\',), checkpoint id = 1f03263c-c53f-6666-8004-c6d35868dd73',
          '--------------------------------------------------------------------------------',
          'Num Messages: 4, Next: (\'tools\',), checkpoint id = 1f03263c-b14b-68f8-8003-28558fa38dbc',
          '--------------------------------------------------------------------------------',
          'Num Messages: 3, Next: (\'chatbot_node\',), checkpoint id = 1f03263c-a66b-6276-8002-2dc89fca4d99',
          '--------------------------------------------------------------------------------',
          'Num Messages: 2, Next: (\'tools\',), checkpoint id = 1f03263c-8c7c-68ec-8001-fb8a9aa300b0',
          '--------------------------------------------------------------------------------',
          'Num Messages: 1, Next: (\'chatbot_node\',), checkpoint id = 1f03263c-6d06-68d2-8000-ced2e7b8538f',
          '--------------------------------------------------------------------------------',
          'Num Messages: 0, Next: (\'__start__\',), checkpoint id = 1f03263c-6cdb-63e4-bfff-c644b57cee28',
          '--------------------------------------------------------------------------------',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      







    </div>

  </section>

</PostLayout>
