---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'LLMs quantization';
const end_url = 'llms-quantization';
const description = '¬°Imagina que tienes un modelo de lenguaje gigante que puede responder a cualquier pregunta, desde la capital de Francia hasta la receta perfecta para hacer brownies! üçûÔ∏èüá´üá∑ Pero, ¬øqu√© pasa cuando ese modelo tiene que caber en un dispositivo m√≥vil? üì± ¬°Eso es donde entra en juego la cuantizaci√≥n! üéâ Esta t√©cnica nos permite reducir el tama√±o de los modelos sin sacrificar su precisi√≥n, lo que significa que podemos disfrutar de inteligencia artificial en nuestros dispositivos m√≥viles sin necesidad de un supercomputador. üíª ¬°Es como comprimir un elefante en una caja de zapatos, pero sin aplastar al elefante! üêòüòÇ';
const keywords = 'cuantizaci√≥n, LLMs, GPT, IA, aprendizaje autom√°tico, aprendizaje profundo, FP16, FP32, TF32, BF16, INT8, UINT8, FP8, FP4, post-entrenamiento, durante-entrenamiento, punto-cero, afin, QLoRA, AWQ, QuIP, GGUF, HQQ, AQLM, FBGEMM FP8';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/quantization-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1080
    image_height=794
    image_extension=webp
    article_date=2024-07-21+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Formato-de-los-par%C3%A1metros"><h2>Formato de los par√°metros</h2></a>
      <a class="anchor-link" href="#Tipos-de-cuantizaci%C3%B3n"><h2>Tipos de cuantizaci√≥n</h2></a>
      <a class="anchor-link" href="#Cuantizaci%C3%B3n-de-punto-cero"><h3>Cuantizaci√≥n de punto cero</h3></a>
      <a class="anchor-link" href="#Cuantizazi%C3%B3n-afin"><h3>Cuantizazi√≥n afin</h3></a>
      <a class="anchor-link" href="#Momentos-de-cuantizaci%C3%B3n"><h2>Momentos de cuantizaci√≥n</h2></a>
      <a class="anchor-link" href="#Cuantizaci%C3%B3n-post-entrenamiento"><h3>Cuantizaci√≥n post entrenamiento</h3></a>
      <a class="anchor-link" href="#Cuantizaci%C3%B3n-durante-el-entrenamiento"><h3>Cuantizaci√≥n durante el entrenamiento</h3></a>
      <a class="anchor-link" href="#M%C3%A9todos-de-cuantizaci%C3%B3n"><h2>M√©todos de cuantizaci√≥n</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="LLMs-quantization">LLMs quantization<a class="anchor-link" href="#LLMs-quantization"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los modelos de lenguaje son cada vez m√°s grandes, lo que hace que cada vez sean m√°s costosos y caros de ejecutar.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="LLMs-size-evolution" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LLMs-size-evolution.webp" width="1108" height="1200"/></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Llama-size-evolution" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Llama-size-evolution.webp" width="1080" height="1143"/></p>
      <p>Por ejemplo, el modelo llama 3 400B, si sus par√°metros est√°n almacenados es formato FP32, cada par√°metros ocupa por tanto 4 bytes, lo que supone que solo para almacenar el modelo hace falta 400<em>(10e9)</em>4 bytes = 1.6 TB de memoria VRAM. Esto supone 20 GPUs de 80GB de memoria VRAM cada una, las cuales adem√°s no son baratas.</p>
      <p>Pero si dejamos a un lado modelos gigantes y nos vamos a modelos con tama√±os m√°s comunes, por ejemplo, 70B de par√°metros, solo almacenar el modelo supone 70<em>(10e9)</em>4 bytes = 280 GB de memoria VRAM, lo que supone 4 GPUs de 80GB de memoria VRAM cada una.</p>
      <p>Esto es porque almacenamos los pesos en formato FP32, es decir, que cada par√°metro ocupa 4 bytes. Pero qu√© pasa si conseguimos que cada par√°metro ocupe menos bytes? A esto se le llama cuantizaci√≥n.</p>
      <p>Por ejemplo, si conseguimos que un modelo de 70B de par√°metros, sus par√°metros ocupen medio byte, entonces solo necesitar√≠amos 70<em>(10e9)</em>0.5 bytes = 35 GB de memoria VRAM, lo que supone 2 GPUs de 24GB de memoria VRAM cada una, las cuales ya se pueden considerar GPUs de usuarios normales.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Necesitamos por tanto maneras de poder resudir el tama√±o de estos modelos. Existen tres formas de hacer eso, la destilaci√≥n, la poda y la cuantizaci√≥n.</p>
      <p>La destilaci√≥n consiste en entrenar un modelo m√°s peque√±o a partir de las salidas del grande. Es decir, una entrada se le mete al modelo peque√±o y al grande, se considera que la salida correcta es la del modelo grande, por lo que se realiza el entrenamiento del modelo peque√±o de acuerdo con la salida del modelo grande. Pero esto requiere tener almacenado el modelo grande, que no es lo que queremos o podemos hacer.</p>
      <p>La poda consiste en eliminar par√°metros del modelo haci√©ndolo cada vez m√°s peque√±o. Este m√©todo se basa en la idea de que los modelos de lenguaje actuales est√°n sobredimensionados y solo unos pocos par√°metros son los que realmente aportan informaci√≥n. Por ello, si conseguimos eliminar los par√°metros que no aportan informaci√≥n, conseguiremos un modelo m√°s peque√±o. Pero esto no es sencillo a d√≠a de hoy, porque no tenemos manera de saber bien qu√© par√°metros son los importantes y cuales no.</p>
      <p>Por otro lado, la cuantizaci√≥n consiste en reducir el tama√±o de cada uno de los par√°metros del modelo. Y es lo que vamos a explicar en este post.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Formato-de-los-par%C3%A1metros">Formato de los par√°metros<a class="anchor-link" href="#Formato-de-los-par%C3%A1metros"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los par√°metros de los pesos se pueden almacenar en varios tipos de formatos</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="numbers-representation" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/numbers-representation.webp" width="1200" height="379"/></p>
      <p>Originalmente se usaba FP32 para almacenar los par√°metros, pero debido a que empezamos a quedarnos sin memoria para almacenar los modelos, se empezaron a pasar a FP16, lo cual no daba malos resultados.</p>
      <p>Sin embargo el problema de FP16 es que no alcanza valores tan altos como FP32, por lo que puede darse el caso de desbordamiento de valores, es decir, al realizarse c√°lculos internos en la red, el resultado sea tan alto que no se pueda representar en FP16, lo que produce errores. Esto ocurre porque el modelo fue entrenado en FP32, lo que hace que todos los posibles c√°lculos internos sean posibles, pero al pasarse despu√©s a FP16 para poder hacer inferencias, algunos c√°lculos internos pueden producir desbordamientos.</p>
      <p>Debido a estos errores de desbordamiento se crearon TF32 y BF16, los cuales tienen la misma cantidad de bits de exponente, lo que hace que puedan llegar a valores tan altos como FP32, pero con la ventaja de ocupar menos memoria por tener menos bits. Sin embargo, ambos al tener menos bits de mantisa, no pueden representar n√∫meros con tanta precisi√≥n como FP32, lo cual puede dar errores de redondeo, pero al menos no obtendremos un error al ejecutar la red. TF32 tiene en total 19 bits, mientras que BF16 tiene 16 bits. Se suele usar m√°s BF16 porque se ahorra m√°s memoria.</p>
      <p>Hist√≥ricamente han existido los formatos INT8 y UINT8, que pueden representar n√∫meros desde -128 a 127 y de 0 a 255 respectivamente. Aunque son formatos buenos porque permiten ahorrar menos memoria, ya que cada par√°metro ocupa 1 byte en comparaci√≥n de los 4 bytes de FP32, el problema que tienen es que solo pueden representar un rango peque√±o de n√∫meros y adem√°s solo enteros, por lo que pueden darse los dos problemas vistos antes, desbordamiento y falta de precisi√≥n.</p>
      <p>Para solucionar el problema de que los formatos INT8 y UINT8 solo representan n√∫mero enteros se han creado los formatos FP8 y FP4, pero a√∫n no est√°n muy consolidados, ni tienen un formato muy extandardizado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aunque tengamos manera de poder almacenar los par√°metros de los modelos en formatos m√°s peque√±os, y aunque consigamos resolver los problemas de desbordamiento y redondeo, tenemos otro problema, y es que no todas las GPUs son capaces de representar todos los formatos. Esto es porque estos problemas de memoria son relativamente nuevos, por lo que las GPUs m√°s antiguas no se dise√±aron para poder resolver estos problemas y por tanto no son capaces de representar todos los formatos.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="GPUs-data-formating" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPUs-data-formating.webp" width="1200" height="308"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como √∫ltimo detalle, como curiosidad, durante el entrenamiento de los modelos se utiliza lo que se llama precisi√≥n mixta. Los pesos del modelo se alamcenan en formato FP32, sin embargo el <code>forward pass</code> y el <code>backward pass</code> se realizan en FP16 para que sea m√°s r√°pido. Los gradientes resultantes del <code>backward pass</code> se almacenan en FP16 y se usan para modificar los valores FP32 de los pesos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Tipos-de-cuantizaci%C3%B3n">Tipos de cuantizaci√≥n<a class="anchor-link" href="#Tipos-de-cuantizaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Cuantizaci%C3%B3n-de-punto-cero">Cuantizaci√≥n de punto cero<a class="anchor-link" href="#Cuantizaci%C3%B3n-de-punto-cero"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este es el tipo de cuantizaci√≥n m√°s sencilla. Consiste en reducir el rango de valores de manera lineal, el m√≠nimo valor de FP32 corresponde al m√≠nimo valor del nuevo formato, el cero de FP32 corresponde al cero del nuevo formato y el m√°ximo valor de FP32 corresponde al m√°ximo valor del nuevo formato.</p>
      <p>Por ejemplo, si queremos pasar el n√∫meros representados desde -1 hasta 1 en formato UINT8, como los l√≠mites de UINT8 son -127 y 127, si queremos representar el valor 0.3 lo que hacemos es multiplicar 0.3 por 127, que da 38.1 y redondearlo a 38, que es el valor que se almacenar√≠a en UINT8.</p>
      <p>Si queremos hacer el paso contrario, para pasar 38 a formato de entre -1 y 1, lo que hacemos es dividir 38 entre 127, que da 0.2992, que es aproximadamente 0.3, y podemos ver que tenemos un error de 0.008</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="quantization-zero-point" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/quantization-zero-point.webp" width="836" height="286"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Cuantizazi%C3%B3n-afin">Cuantizazi√≥n afin<a class="anchor-link" href="#Cuantizazi%C3%B3n-afin"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este tipo de cuantizaci√≥n, si se tiene un array de valores en un formato y se quiere pasar a otro, primero se divide el array entero por el m√°ximo valor del array y luego se multiplica el array entero por el m√°ximo valor del nuevo formato.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="quantization-affine" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/quantization-affine.webp" width="1200" height="669"/></p>
      <p>Por ejemplo, en la imagen anterior tenemos el array</p>
      <pre><code>[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]</code></pre>
      <p>Como su valor m√°ximo es <code>5.4</code> dividimos el array por ese valor y obtenemos</p>
      <pre><code>[0.2222222222, -0.09259259259, -0.7962962963, 0.2222222222, -0.5740740741, 0.1481481481, 0.4444444444, 1]</code></pre>
      <p>Si ahora multiplicamos todos los valores por <code>127</code>, que es el m√°ximo valor de UINT8, obtenemos</p>
      <pre><code>[28,22222222, -11.75925926, -101.1296296, 28.22222222, -72.90740741, 18.81481481, 56.44444444, 127]</code></pre>
      <p>Que redondeando ser√≠a</p>
      <pre><code>[28, -12, -101, 28, -73, 19, 56, 127]</code></pre>
      <p>Si ahora quisi√©semos realizar el paso inverso tendr√≠amos que dividir el array resultante por <code>127</code>, que dar√≠a</p>
      <pre><code>[0,2204724409, -0.09448818898, -0.7952755906, 0.2204724409, -0.5748031496, 0.1496062992, 0.4409448819, 1]</code></pre>
      <p>Y volver a multiplicar por <code>5.4</code>, con lo que obtendr√≠amos</p>
      <pre><code>[1,190551181, -0.5102362205, -4.294488189, 1.190551181, -3.103937008, 0.8078740157, 2.381102362, 5.4]</code></pre>
      <p>Si lo comparamos con el array original, vemos que tenemos error</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Momentos-de-cuantizaci%C3%B3n">Momentos de cuantizaci√≥n<a class="anchor-link" href="#Momentos-de-cuantizaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Cuantizaci%C3%B3n-post-entrenamiento">Cuantizaci√≥n post entrenamiento<a class="anchor-link" href="#Cuantizaci%C3%B3n-post-entrenamiento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como su nombre indica, la cuantizaci√≥n se produce despu√©s del entrenamiento. Se entrena el modelo en FP32 y despu√©s se cuantiza a otro formato. Este m√©todo es el m√°s sencillo, pero puede dar lugar a errores de precisi√≥n en la cuantizaci√≥n</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Cuantizaci%C3%B3n-durante-el-entrenamiento">Cuantizaci√≥n durante el entrenamiento<a class="anchor-link" href="#Cuantizaci%C3%B3n-durante-el-entrenamiento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Durante el entrenamiento se realiza el <code>forward pass</code> en el modelo original y en un modelo cuantizado y se ven los posibles errores derivados de la cuantizaci√≥n para poder mitigarlos. Este proceso hace que el entrenamiento sea m√°s costoso, porque tienes que tener almacenado en memoria el modelo original y el cuantizado, y m√°s lento, porque tienes que realizar el <code>forward pass</code> en dos modelos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="M%C3%A9todos-de-cuantizaci%C3%B3n">M√©todos de cuantizaci√≥n<a class="anchor-link" href="#M%C3%A9todos-de-cuantizaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A continuaci√≥n muestro los enlaces a los posts donde explico cada uno de los m√©todos para que este post no se haga muy largo</p>
      <ul>
      <li><a href="/llm-int8">LLM.int8()</a></li>
      <li><a href="/gptq">GPTQ</a></li>
      <li><a href="/qlora">QLoRA</a></li>
      <li>AWQ</li>
      <li>QuIP</li>
      <li>GGUF</li>
      <li>HQQ</li>
      <li>AQLM</li>
      <li>FBGEMM FP8</li>
      </ul>
      </section>
      






    </div>

  </section>

</PostLayout>
