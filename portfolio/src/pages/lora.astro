---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'LoRA ‚Äì low rank adaptation of large language models';
const end_url = 'lora';
const description = '¬°Prep√°rate para llevar la adaptaci√≥n de tus modelos al siguiente nivel con LoRA! üöÄ Esta t√©cnica de adaptaci√≥n de baja rango es como una capa de superh√©roe para tus redes neuronales - les ayuda a aprender nuevos trucos sin olvidar los antiguos ü§Ø. Y lo mejor de todo? Puedes implementarla en solo unas pocas l√≠neas de c√≥digo PyTorch üíª. ¬°Y si eres como yo, un pobre de GPU que lucha con recursos limitados üí∏, LoRA es como un regalo del cielo: te permite adaptar tus modelos sin necesidad de entrenarlos desde cero ni gastar una fortuna en hardware üôè. ¬°Revisa el post para obtener una gu√≠a paso a paso y un ejemplo pr√°ctico!';
const keywords = 'lora, adaptaci√≥n de baja clasificaci√≥n, redes neuronales, pytorch, gpu, hardware';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_thumbnail_ES.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1080
    image_height=607
    image_extension=webp
    article_date=2024-07-20+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Explicaci%C3%B3n-de-LoRA"><h2>Explicaci√≥n de LoRA</h2></a>
      <a class="anchor-link" href="#Actualizaci%C3%B3n-de-pesos-en-una-red-neuronal"><h3>Actualizaci√≥n de pesos en una red neuronal</h3></a>
      <a class="anchor-link" href="#LoRA"><h3>LoRA</h3></a>
      <a class="anchor-link" href="#Implementaci%C3%B3n-de-LoRA-en-transformers"><h3>Implementaci√≥n de LoRA en transformers</h3></a>
      <a class="anchor-link" href="#Tama%C3%B1o-del-rango-r"><h3>Tama√±o del rango r</h3></a>
      <a class="anchor-link" href="#Inicializaci%C3%B3n-de-las-matrices-A-y-B"><h3>Inicializaci√≥n de las matrices A y B</h3></a>
      <a class="anchor-link" href="#Influencia-de-LoRA-mediante-el-par%C3%A1metro-$%5Calpha$"><h3>Influencia de LoRA mediante el par√°metro $\alpha$</h3></a>
      <a class="anchor-link" href="#Ventajas-de-LoRA"><h2>Ventajas de LoRA</h2></a>
      <a class="anchor-link" href="#Implementaci%C3%B3n-de-LoRA-en-un-LLM"><h2>Implementaci√≥n de LoRA en un LLM</h2></a>
      <a class="anchor-link" href="#Login-en-el-Hub"><h3>Login en el Hub</h3></a>
      <a class="anchor-link" href="#Dataset"><h3>Dataset</h3></a>
      <a class="anchor-link" href="#Tokenizador"><h3>Tokenizador</h3></a>
      <a class="anchor-link" href="#Modelo"><h3>Modelo</h3></a>
      <a class="anchor-link" href="#LoRA"><h3>LoRA</h3></a>
      <a class="anchor-link" href="#Training"><h3>Training</h3></a>
      <a class="anchor-link" href="#Evaluaci%C3%B3n"><h3>Evaluaci√≥n</h3></a>
      <a class="anchor-link" href="#Publicar-el-modelo"><h3>Publicar el modelo</h3></a>
      <a class="anchor-link" href="#Prueba-del-modelo"><h2>Prueba del modelo</h2></a>
      <a class="anchor-link" href="#Implementaci%C3%B3n-de-LoRA-en-un-LLM-con-PEFT-de-Hugging-Face"><h2>Implementaci√≥n de LoRA en un LLM con PEFT de Hugging Face</h2></a>
      <a class="anchor-link" href="#Login-en-el-Hub"><h3>Login en el Hub</h3></a>
      <a class="anchor-link" href="#Dataset"><h3>Dataset</h3></a>
      <a class="anchor-link" href="#Tokenizador"><h3>Tokenizador</h3></a>
      <a class="anchor-link" href="#Modelo"><h3>Modelo</h3></a>
      <a class="anchor-link" href="#LoRA-con-PEFT"><h3>LoRA con PEFT</h3></a>
      <a class="anchor-link" href="#Training"><h3>Training</h3></a>
      <a class="anchor-link" href="#Evaluaci%C3%B3n"><h3>Evaluaci√≥n</h3></a>
      <a class="anchor-link" href="#Publicar-el-modelo"><h3>Publicar el modelo</h3></a>
      <a class="anchor-link" href="#Prueba-del-modelo-entrenado-con-PEFT"><h2>Prueba del modelo entrenado con PEFT</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="LoRA---low-rank-adaptation-of-large-language-models">LoRA - low rank adaptation of large language models<a class="anchor-link" href="#LoRA---low-rank-adaptation-of-large-language-models"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El aumento de tama√±o de los modelos de lenguaje hace que sea cada vez m√°s caros entrenarlos debido a que cada vez hace falta m√°s VRAM para almacenar todos sus par√°metros y los gradientes derivados del entrenamiento</p>
      <p>En el paper <a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="nofollow noreferrer">LoRA - Low rank adaption of large language models</a> proponen congelar los pesos del modelo y entrenar dos matrices llamadas A y B reduciendo mucho el n√∫mero de par√°metros que se tienen que entrenar</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="LoRA" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_adapat.webp" width="340" height="299"/></p>
      <p>Vamos a ver c√≥mo se hace esto</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Explicaci%C3%B3n-de-LoRA">Explicaci√≥n de LoRA<a class="anchor-link" href="#Explicaci%C3%B3n-de-LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Actualizaci%C3%B3n-de-pesos-en-una-red-neuronal">Actualizaci√≥n de pesos en una red neuronal<a class="anchor-link" href="#Actualizaci%C3%B3n-de-pesos-en-una-red-neuronal"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para entender c√≥mo funciona LoRA, primero tenemos que recordar qu√© ocurre cuando entrenamos un modelo. Volvamos a la parte m√°s b√°sica del deep learning, tenemos una capa densa de una red neuronal que se define como:</p>
      $$
      y = Wx + b
      $$<p>Donde $W$ es la matriz de pesos y $b$ es el vector de sesgos.</p>
      <p>Para simplificar vamos a suponer que no hay sesgo, por lo que quedar√≠a as√≠</p>
      $$
      y = Wx
      $$<p>Supongamos que para una entrada $x$ queremos que tenga una salida $≈∑$</p>
      <ul>
      <li>Primero lo que hacemos es calcular la salida que obtenemos con nuestro valor actual de pesos $W$, es decir obtenemos el valor $y$</li>
      <li>A continuaci√≥n calculamos el error que existe entre el valor de $y$ que hemos obtenido y el valor que quer√≠amos obtener $≈∑$. A ese error lo llamamos $loss$, y lo calculamos con alguna funci√≥n matem√°tica, ahora no importa cual</li>
      <li>Calculamos el gardiente (la derivada) del error $loss$ con respecto a la matriz de pesos $W$, es decir $\Delta W = \frac{opening_brace}dloss{closing_brace}{opening_brace}dW{closing_brace}$</li>
      <li>Actualizamos los pesos $W$ restando a cada uno de sus valores el valor del gradiente multiplicado por un factor de aprendizaje $\alpha$, es decir $W = W - \alpha \Delta W$</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="LoRA">LoRA<a class="anchor-link" href="#LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los autores de LoRA lo que proponen es que la matriz de pesos $W$ se puede descomponer en</p>
      $$
      W \sim W + \Delta W
      $$<p>De manera que congelando la matriz $W$ y entrenando solo la matriz $\Delta W$ se puede obtener un modelo que se adapte a nuevos datos sin tener que reentrenar todo el modelo</p>
      <p>Pero podr√°s pensar que $\Delta W$ es una matriz de tama√±o igual a $W$ por lo que no se ha ganado nada, pero aqu√≠ los autores se basan en <code>Aghajanyan et al. (2020)</code>, un paper en el que demostraron que aunque los modelos de lenguaje son grandes y que sus par√°metros son matrices con dimensiones muy grandes, para adaptarlos a nuevas tareas no es necesario cambiar todos los valores de las matrices, sino que cambiando unos pocos valores es suficiente, que en t√©rminos t√©cnicos, se llama adaptaci√≥n de bajo rango. De ah√≠ el nombre de LoRA (Low Rank Adaptation)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos congelado el modelo y ahora queremos entrenar la matriz $\Delta W$, supongamos que tanto $W$ como $\Delta W$ son matrices de tama√±o $20 \times 10$, por lo que tenemos 200 par√°metros entrenables</p>
      <p>Ahora supongamos que la matriz $\Delta W$ se puede descomponer en el producto de dos matrices $A$ y $B$, es decir</p>
      $$
      \Delta W = A \cdot B
      $$<p>Para que esta multiplicaci√≥n se produzca los tama√±os de las matrices $A$ y $B$ tienen que ser $20 \times n$ y $n \times 10$ respectivamente. Supongamos que $n = 5$, por lo que $A$ ser√≠a de tama√±o $20 \times 5$, es decir 100 par√°metros, y $B$ de tama√±o $5 \times 10$, es decir 50 par√°metros, por lo que tendr√≠amos 100+50=150 par√°metros entrenables. Ya tenemos menos par√°metros entrenables que antes</p>
      <p>Ahora supongamos que $W$ en realidad es una matriz de tama√±o $10.000 \times 10.000$, por lo que tendr√≠amos 100.000.000 par√°metros entrenables, pero si descomponemos $\Delta W$ en $A$ y $B$ con $n = 5$, tendr√≠amos una matriz de tama√±o $10.000 \times 5$ y otra de tama√±o $5 \times 10.000$, por lo que tendr√≠amos 50.000 par√°metros de una y otros 50.000 par√°metros de otra, en total 100.000 par√°metros entrenables, es decir hemos reducido el n√∫mero de par√°metros 1000 veces</p>
      <p>Ya puedes ir viendo el poder de LoRA, cuando se tienen modelos muy grandes, el n√∫mero de par√°metros entrenables se puede reducir much√≠simo</p>
      <p>Si volvemos a ver la imagen de la arquitectura de LoRA, la entenderemos mejor</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="LoRA adapt" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_adapat.webp" width="340" height="299"/></p>
      <p>Pero se ve mejor aun, el ahorro en n√∫mero de par√°metros entrenables con esta imagen</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="LoRA matmul" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Lora_matmul.webp" width="523" height="414"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Implementaci%C3%B3n-de-LoRA-en-transformers">Implementaci√≥n de LoRA en transformers<a class="anchor-link" href="#Implementaci%C3%B3n-de-LoRA-en-transformers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como los modelos de lenguaje son implementaciones de transformers, vamos a ver c√≥mo se implementa LoRA en transformers. En la arquitectura transformer hay capas lineales en las matrices de atenci√≥n $Q$, $K$ y $V$, y en las capas feedforward, por lo que se puede aplicar LoRA a todas estas capas lineales. En el paper hablan que por simplicidad lo aplican solo a las capas lineales de las matrices de atenci√≥n $Q$, $K$ y $V$</p>
      <p>Estas capas tienen un tama√±o $d_{opening_brace}model{closing_brace} \times d_{opening_brace}model{closing_brace}$, donde $d_{opening_brace}model{closing_brace}$ es la dimensi√≥n de embedding del modelo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tama%C3%B1o-del-rango-r">Tama√±o del rango r<a class="anchor-link" href="#Tama%C3%B1o-del-rango-r"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder tener estos beneficios, el tama√±o del rango $r$ tienen que ser menor que el tama√±o de las capas lineales. Como hemos dicho que solo lo implementaban en las capas lineales de atenci√≥n, que tienen un tama√±o $d_{opening_brace}model{closing_brace} \times d_{opening_brace}model{closing_brace}$, el tama√±o del rango $r$ tiene que ser menor que $d_{opening_brace}model{closing_brace}$</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Inicializaci%C3%B3n-de-las-matrices-A-y-B">Inicializaci√≥n de las matrices A y B<a class="anchor-link" href="#Inicializaci%C3%B3n-de-las-matrices-A-y-B"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Las matrices $A$ y $B$ se inicializan con una distribuci√≥n gaussiana aleatoria para $A$ y cero para $B$, as√≠ el producto de ambas matrices ser√° cero al principio, es decir</p>
      $$
      \Delta W = A \cdot B = 0
      $$
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Influencia-de-LoRA-mediante-el-par%C3%A1metro-$%5Calpha$">Influencia de LoRA mediante el par√°metro $\alpha$<a class="anchor-link" href="#Influencia-de-LoRA-mediante-el-par%C3%A1metro-$%5Calpha$"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, en la implementaci√≥n de LoRA, se a√±ade un par√°metro $\alpha$ para establecer el grado de influencia de LoRA en el entrenamiento. Es similar al learning rate en el fine tuning normal, pero en este caso se usa para establecer la influencia de LoRA en el entrenamiento. De esta manera la f√≥rmula de LoRA quedar√≠a as√≠</p>
      $$
      W = W + \alpha \Delta W = W + \alpha A \cdot B
      $$
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Ventajas-de-LoRA">Ventajas de LoRA<a class="anchor-link" href="#Ventajas-de-LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={opening_brace}svg_paths.link_svg_path{closing_brace}/></a></h2><p>Ahora que hemos entendido c√≥mo funciona, vamos a ver las ventajas que tiene este m√©todo</p>
      <ul>
      <li>Reducci√≥n del n√∫mero de par√°metros entrenables. Como hemos visto, el n√∫mero de par√°metros entrenables se reduce dr√°sticamente, lo que hace que el entrenamiento sea mucho m√°s r√°pido y que se necesite menos VRAM, por lo que se ahorran muchos costes</li>
      <li>Adaptadores en producci√≥n. Podemos tener en producci√≥n un √∫nico modelo de lenguaje y varios adaptadores, cada uno para una tarea diferente, en vez de tener varios modelos entrenados para cada tarea, por lo que se ahorran costes de almacenamiento y de computaci√≥n. Adem√°s este m√©todo no tiene por qu√© a√±adir latencia en la inferencia porque se puede fusionar la matriz de pesos original con el adaptador, ya que hemos visto que $W \sim W + \Delta W = W + A \cdot B$, por lo que el tiempo de inferencia ser√≠a la misma que usar el modelo de lenguaje original</li>
      <li>Compratir adaptadores. Si entrenamos un adaptador, podemos compartir solo el adaptador. Es decir, en producci√≥n, todo el mundo puede tener el modelo original y cada vez que entrenamos un adaptador compartir solo el adaptador, por lo que como se compartir√≠an matrices mucho m√°s peque√±as, el tama√±o de los archivos que se comparte ser√≠a mucho m√°s peque√±o</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Implementaci%C3%B3n-de-LoRA-en-un-LLM">Implementaci√≥n de LoRA en un LLM<a class="anchor-link" href="#Implementaci%C3%B3n-de-LoRA-en-un-LLM"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a repetir el c√≥digo de entrenamiento del post <a href="https://www.maximofn.com/fine-tuning-sml/">Fine tuning SLMs</a>, en concreto el entrenamiento para clasificaci√≥n de texto con las librer√≠as de Hugging Face, pero esta vez vamos a hacerlo con LoRA. En el anterior post usamos un batch size de 28 para el bucle de entrenamiento y de 40 para el de evaluaci√≥n, sin embargo, como ahora no vamos a entrenar todos los pesos del modelo, sino solo las matrices de LoRA, vamos a poder usar un batch size mayor</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Login-en-el-Hub">Login en el Hub<a class="anchor-link" href="#Login-en-el-Hub"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Nos logueamos para subir el modelo al Hub</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
      '      <span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Descargamos el dataset que vamos a usar, que es un dataset de reviews de <a href="https://huggingface.co/datasets/mteb/amazon_reviews_multi" target="_blank" rel="nofollow noreferrer">Amazon</a></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
          '<span class="n">notebook_login</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
          '',
          '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"mteb/amazon_reviews_multi"</span><span class="p">,</span> <span class="s2">"en"</span><span class="p">)</span>',
          '<span class="n">dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'DatasetDict({',
          '    train: Dataset({',
          '        features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '        num_rows: 200000',
          '    })',
          '    validation: Dataset({',
          '        features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '        num_rows: 5000',
          '    })',
          '    test: Dataset({',
          '        features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '        num_rows: 5000',
          '    })',
          '})',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un subset por si quieres probar el c√≥digo con un dataset m√°s peque√±o. En mi caso usar√© el 100% del dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">percentage</span> <span class="o">=</span> <span class="mi">1</span>',
          '',
          '<span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">\'train\'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">\'train\'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
          '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">\'validation\'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">\'validation\'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
          '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">\'test\'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">\'test\'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
          '',
          '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '     num_rows: 200000',
          ' }),',
          ' Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '     num_rows: 5000',
          ' }),',
          ' Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '     num_rows: 5000',
          ' }))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos una muestra</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>',
          '',
          '<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset_dataset_train</span><span class="p">))</span>',
          '<span class="n">subset_dataset_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'id\': \'en_0388304\',',
          ' \'text\': \'The N was missing from on\n\nThe N was missing from on\',',
          ' \'label\': 0,',
          ' \'label_text\': \'0\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos el n√∫mero de clases, para obtener el n√∫mero de clases usamos <code>dataset['train']</code> y no <code>subset_dataset_train</code> porque si el subset lo hamos muy peque√±o es posible que no haya ejemplos con todas las posibles clases del dataset original</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">\'train\'</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">\'label\'</span><span class="p">))</span>',
          '<span class="n">num_classes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '5',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una funci√≥n para crear el campo <code>label</code> en el dataset. El dataset descargado tiene el campo <code>labels</code> pero la librer√≠a <code>transformers</code> necesita que el campo se llame <code>label</code> y no <code>labels</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">set_labels</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>',
      '          <span class="n">example</span><span class="p">[</span><span class="s1">\'labels\'</span><span class="p">]</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">\'label\'</span><span class="p">]</span>',
      '          <span class="k">return</span> <span class="n">example</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Aplicamos la funci√≥n al dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">def</span> <span class="nf">set_labels</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>',
          '    <span class="n">example</span><span class="p">[</span><span class="s1">\'labels\'</span><span class="p">]</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">\'label\'</span><span class="p">]</span>',
          '    <span class="k">return</span> <span class="n">example</span>',
          '</span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
          '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
          '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
          '',
          '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\', \'labels\'],',
          '     num_rows: 200000',
          ' }),',
          ' Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\', \'labels\'],',
          '     num_rows: 5000',
          ' }),',
          ' Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\', \'labels\'],',
          '     num_rows: 5000',
          ' }))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver una muestra</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">subset_dataset_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'id\': \'en_0388304\',',
          ' \'text\': \'The N was missing from on\n\nThe N was missing from on\',',
          ' \'label\': 0,',
          ' \'label_text\': \'0\',',
          ' \'labels\': 0}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizador">Tokenizador<a class="anchor-link" href="#Tokenizador"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Implementamos el tokenizador. Para que no nos de error, asignamos el token de end of string al token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>Creamos una funci√≥n para tokenizar el dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '          <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Aplicamos la funci√≥n al dataset y de paso eliminamos las columnas que no necesitamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '</span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '</span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">\'text\'</span><span class="p">,</span> <span class="s1">\'label\'</span><span class="p">,</span> <span class="s1">\'id\'</span><span class="p">,</span> <span class="s1">\'label_text\'</span><span class="p">])</span>',
          '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">\'text\'</span><span class="p">,</span> <span class="s1">\'label\'</span><span class="p">,</span> <span class="s1">\'id\'</span><span class="p">,</span> <span class="s1">\'label_text\'</span><span class="p">])</span>',
          '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">\'text\'</span><span class="p">,</span> <span class="s1">\'label\'</span><span class="p">,</span> <span class="s1">\'id\'</span><span class="p">,</span> <span class="s1">\'label_text\'</span><span class="p">])</span>',
          '',
          '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(Dataset({',
          '     features: [\'labels\', \'input_ids\', \'attention_mask\'],',
          '     num_rows: 200000',
          ' }),',
          ' Dataset({',
          '     features: [\'labels\', \'input_ids\', \'attention_mask\'],',
          '     num_rows: 5000',
          ' }),',
          ' Dataset({',
          '     features: [\'labels\', \'input_ids\', \'attention_mask\'],',
          '     num_rows: 5000',
          ' }))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver una muestra, pero en este caso solo vemos las <code>keys</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">subset_dataset_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'dict_keys([\'labels\', \'input_ids\', \'attention_mask\'])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos el modelo. Tambi√©n, para que no nos de error, asignamos el token de end of string al token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>',
          '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [\'score.weight\']',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como ya vimos en el post <a href="https://www.maximofn.com/fine-tuning-sml/">Fine tuning SLMs</a> obtenemos un warning en el que dice que algunas capas no se han inicializado. Esto es porque en este caso, como es un problema de clasificaci√≥n y cuando hemos instanciado el modelo le hemos dicho que queremos que sea un modelo de clasificaci√≥n con 5 clases, la librer√≠a ha eliminado la √∫ltima capa y la ha sustituido por una de 5 neuronas a la salida. Si no entiendes bien esto ve al post que cito que est√° mejor eplicado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="LoRA">LoRA<a class="anchor-link" href="#LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de implementar LoRA, vemos el n√∫mero de par√°metros entrenables que tiene el modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total trainable parameters before: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Total trainable parameters before: 124,443,648',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que tiene 124M de par√°metros entrenables. Ahora vamos a congelarlos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>',
          '    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>',
          '',
          '<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total trainable parameters after: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Total trainable parameters after: 0',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Tras congelar ya no hay par√°metros entrenables</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver c√≥mo es el modelo antes de aplicar LoRA</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '  (transformer): GPT2Model(',
          '    (wte): Embedding(50257, 768)',
          '    (wpe): Embedding(1024, 768)',
          '    (drop): Dropout(p=0.1, inplace=False)',
          '    (h): ModuleList(',
          '      (0-11): 12 x GPT2Block(',
          '        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '        (attn): GPT2Attention(',
          '          (c_attn): Conv1D()',
          '          (c_proj): Conv1D()',
          '          (attn_dropout): Dropout(p=0.1, inplace=False)',
          '          (resid_dropout): Dropout(p=0.1, inplace=False)',
          '        )',
          '        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '        (mlp): GPT2MLP(',
          '          (c_fc): Conv1D()',
          '          (c_proj): Conv1D()',
          '          (act): NewGELUActivation()',
          '          (dropout): Dropout(p=0.1, inplace=False)',
          '        )',
          '      )',
          '    )',
          '    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '  )',
          '  (score): Linear(in_features=768, out_features=5, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Primero creamos la capa LoRA layer.</p>
      <p>Tiene que heredar de <code>torch.nn.Module</code> para que pueda actuar como una capa de una red neuronal</p>
      <p>En el m√©todo <code>_init_</code> creamos las matrices <code>A</code> y <code>B</code> inicializadas como hemos explicado antes, la matriz <code>A</code> con una distribuci√≥n gaussiana aleatoria y la matriz <code>B</code> con ceros. Tambi√©n creamos los par√°metros <code>rank</code> y <code>alpha</code>.</p>
      <p>En el m√©todo <code>forward</code> calculamos LoRA como hemos explicado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '              <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>',
      '              <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># similar to standard weight initialization</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>',
      '      ',
      '          <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
      '              <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">x</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>


















      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos una clase lineal con LoRA.</p>
      <p>Al igual que antes hereda de <code>torch.nn.Module</code> para que pueda actuar como una capa de una red neuronal.</p>
      <p>En el m√©todo <code>_init_</code> creamos una variable con la capa lineal original de la red y creamos otra variable con la nueva capa LoRA que hab√≠amos implementado antes</p>
      <p>En el m√©todo <code>forward</code> sumamos las salidas de la capa lineal original y la capa LoRA</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '              <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>',
      '              <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># similar to standard weight initialization</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>',
      '      ',
      '          <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
      '              <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">x</span>',
      '<span></span><span class="k">class</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '              <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">lora</span> <span class="o">=</span> <span class="n">LoRALayer</span><span class="p">(</span>',
      '                  <span class="n">linear</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">linear</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span>',
      '              <span class="p">)</span>',
      '      ',
      '          <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo creamos una funci√≥n que sustituya las capas lineales por la nueva capa linear con LoRA que hemos creado. Lo que hace es que si encuentra una capa lineal en el modelo, la sustituye por la capa lineal con LoRA, si no, aplica la funci√≥n dentro de las subcapas de la capa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '              <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>',
      '              <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># similar to standard weight initialization</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>',
      '      ',
      '          <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
      '              <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">x</span>',
      '<span></span><span class="k">class</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '              <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">lora</span> <span class="o">=</span> <span class="n">LoRALayer</span><span class="p">(</span>',
      '                  <span class="n">linear</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">linear</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span>',
      '              <span class="p">)</span>',
      '      ',
      '          <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>',
      '<span></span><span class="k">def</span> <span class="nf">replace_linear_with_lora</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '          <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>',
      '              <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>',
      '                  <span class="c1"># Replace the Linear layer with LinearWithLoRA</span>',
      '                  <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">LoRALinear</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>',
      '              <span class="k">else</span><span class="p">:</span>',
      '                  <span class="c1"># Recursively apply the same function to child modules</span>',
      '                  <span class="n">replace_linear_with_lora</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>Aplicamos la funci√≥n al modelo para sustituir las capas lineales del modelo por la nueva capa lineal con LoRA</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '              <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>',
      '              <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># similar to standard weight initialization</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>',
      '      ',
      '          <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
      '              <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">x</span>',
      '<span></span><span class="k">class</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '              <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">lora</span> <span class="o">=</span> <span class="n">LoRALayer</span><span class="p">(</span>',
      '                  <span class="n">linear</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">linear</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span>',
      '              <span class="p">)</span>',
      '      ',
      '          <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>',
      '<span></span><span class="k">def</span> <span class="nf">replace_linear_with_lora</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '          <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>',
      '              <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>',
      '                  <span class="c1"># Replace the Linear layer with LinearWithLoRA</span>',
      '                  <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">LoRALinear</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>',
      '              <span class="k">else</span><span class="p">:</span>',
      '                  <span class="c1"># Recursively apply the same function to child modules</span>',
      '                  <span class="n">replace_linear_with_lora</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>',
      '<span></span><span class="n">rank</span> <span class="o">=</span> <span class="mi">16</span>',
      '      <span class="n">alpha</span> <span class="o">=</span> <span class="mi">16</span>',
      '      ',
      '      <span class="n">replace_linear_with_lora</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Vemos ahora el n√∫mero de par√°metros entrenables</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="k">class</span> <span class="nc">LoRALayer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
          '    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
          '        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>',
          '        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># similar to standard weight initialization</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>',
          '',
          '    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
          '        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>',
          '        <span class="k">return</span> <span class="n">x</span>',
          '</span><span class="k">class</span> <span class="nc">LoRALinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
          '    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
          '        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">lora</span> <span class="o">=</span> <span class="n">LoRALayer</span><span class="p">(</span>',
          '            <span class="n">linear</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">linear</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span>',
          '        <span class="p">)</span>',
          '',
          '    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
          '        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>',
          '</span><span class="k">def</span> <span class="nf">replace_linear_with_lora</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
          '    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>',
          '        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>',
          '            <span class="c1"># Replace the Linear layer with LinearWithLoRA</span>',
          '            <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">LoRALinear</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>',
          '        <span class="k">else</span><span class="p">:</span>',
          '            <span class="c1"># Recursively apply the same function to child modules</span>',
          '            <span class="n">replace_linear_with_lora</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>',
          '</span><span class="n">rank</span> <span class="o">=</span> <span class="mi">16</span>',
          '<span class="n">alpha</span> <span class="o">=</span> <span class="mi">16</span>',
          '',
          '<span class="n">replace_linear_with_lora</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>',
          '</span><span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total trainable LoRA parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Total trainable LoRA parameters: 12,368',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hemos pasado de 124M de par√°metros entrenables a 12k par√°metros entrenables, es decir hemos reducido el n√∫mero de par√°metros entrenables 10.000 veces!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver el modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '  (transformer): GPT2Model(',
          '    (wte): Embedding(50257, 768)',
          '    (wpe): Embedding(1024, 768)',
          '    (drop): Dropout(p=0.1, inplace=False)',
          '    (h): ModuleList(',
          '      (0-11): 12 x GPT2Block(',
          '        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '        (attn): GPT2Attention(',
          '          (c_attn): Conv1D()',
          '          (c_proj): Conv1D()',
          '          (attn_dropout): Dropout(p=0.1, inplace=False)',
          '          (resid_dropout): Dropout(p=0.1, inplace=False)',
          '        )',
          '        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '        (mlp): GPT2MLP(',
          '          (c_fc): Conv1D()',
          '          (c_proj): Conv1D()',
          '          (act): NewGELUActivation()',
          '          (dropout): Dropout(p=0.1, inplace=False)',
          '        )',
          '      )',
          '    )',
          '    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '  )',
          '  (score): LoRALinear(',
          '    (linear): Linear(in_features=768, out_features=5, bias=False)',
          '    (lora): LoRALayer()',
          '  )',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a compararlos capa por capa</p>
      <table>
      <thead>
      <tr>
      <th>Modelo original</th>
      <th>Modelo con LoRA</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>GPT2ForSequenceClassification(</td>
      <td>GPT2ForSequenceClassification(</td>
      </tr>
      <tr>
      <td>(transformer): GPT2Model(</td>
      <td>(transformer): GPT2Model(</td>
      </tr>
      <tr>
      <td>(wte): Embedding(50257, 768)</td>
      <td>(wte): Embedding(50257, 768)</td>
      </tr>
      <tr>
      <td>(wpe): Embedding(1024, 768)</td>
      <td>(wpe): Embedding(1024, 768)</td>
      </tr>
      <tr>
      <td>(drop): Dropout(p=0.1, inplace=False)</td>
      <td>(drop): Dropout(p=0.1, inplace=False)</td>
      </tr>
      <tr>
      <td>(h): ModuleList(</td>
      <td>(h): ModuleList(</td>
      </tr>
      <tr>
      <td>(0-11): 12 x GPT2Block(</td>
      <td>(0-11): 12 x GPT2Block(</td>
      </tr>
      <tr>
      <td>(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
      <td>(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
      </tr>
      <tr>
      <td>(attn): GPT2Attention(</td>
      <td>(attn): GPT2Attention(</td>
      </tr>
      <tr>
      <td>(c_attn): Conv1D()</td>
      <td>(c_attn): Conv1D()</td>
      </tr>
      <tr>
      <td>(c_proj): Conv1D()</td>
      <td>(c_proj): Conv1D()</td>
      </tr>
      <tr>
      <td>(attn_dropout): Dropout(p=0.1, inplace=False)</td>
      <td>(attn_dropout): Dropout(p=0.1, inplace=False)</td>
      </tr>
      <tr>
      <td>(resid_dropout): Dropout(p=0.1, inplace=False)</td>
      <td>(resid_dropout): Dropout(p=0.1, inplace=False)</td>
      </tr>
      <tr>
      <td>)</td>
      <td>)</td>
      </tr>
      <tr>
      <td>(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
      <td>(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
      </tr>
      <tr>
      <td>(mlp): GPT2MLP(</td>
      <td>(mlp): GPT2MLP(</td>
      </tr>
      <tr>
      <td>(c_fc): Conv1D()</td>
      <td>(c_fc): Conv1D()</td>
      </tr>
      <tr>
      <td>(c_proj): Conv1D()</td>
      <td>(c_proj): Conv1D()</td>
      </tr>
      <tr>
      <td>(act): NewGELUActivation()</td>
      <td>(act): NewGELUActivation()</td>
      </tr>
      <tr>
      <td>(dropout): Dropout(p=0.1, inplace=False)</td>
      <td>(dropout): Dropout(p=0.1, inplace=False)</td>
      </tr>
      <tr>
      <td>)</td>
      <td>)</td>
      </tr>
      <tr>
      <td>)</td>
      <td>)</td>
      </tr>
      <tr>
      <td>)</td>
      <td>)</td>
      </tr>
      <tr>
      <td>(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
      <td>(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
      </tr>
      <tr>
      <td>)</td>
      <td>)</td>
      </tr>
      <tr>
      <td></td>
      <td>(score): LoRALinear()</td>
      </tr>
      <tr>
      <td>(score): Linear(in_features=768, out_features=5, bias=False)</td>
      <td>(linear): Linear(in_features=768, out_features=5, bias=False)</td>
      </tr>
      <tr>
      <td></td>
      <td>(lora): LoRALayer()</td>
      </tr>
      <tr>
      <td></td>
      <td>)</td>
      </tr>
      <tr>
      <td>)</td>
      <td>)</td>
      </tr>
      </tbody>
      </table>
      <p>Vemos que son iguales menos al final, donde en el modelo original hab√≠a una capa lineal normal y en el modelo con LoRA hay una capa <code>LoRALinear</code> que dentro tiene la capa lineal del modelo original y una capa <code>LoRALayer</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Training">Training<a class="anchor-link" href="#Training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez instanciado el modelo con LoRA, vamos a entrenarlo como siempre</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como hemos dicho, en el post <a href="https://www.maximofn.com/fine-tuning-sml/">Fine tuning SLMs</a> usamos un batch size de 28 para el bucle de entrenamiento y de 40 para el de evaluaci√≥n, mientras que ahora que hay menos par√°metros entrenables podemos usar un batch size mayor.</p>
      <p>¬øEsto por qu√© pasa? Cuando se entrena un modelo hay que guardar en la meoria de la GPU el modelo y los gradientes de este, por lo que tanto con LoRA como sin LoRA el modelo hay que guardarlo igualmente, pero en el caso de LoRA solo se guardan los gradientes de 12k par√°metros, mientras que con LoRA se guardan los gradientes de 128M de par√°metros, por lo que con LoRA se necesita menos memoria de la GPU, por lo que se puede usar un batch size mayor</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
































      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '          <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '          <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="p">,</span>',
      '          <span class="n">training_args</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '          <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-html-rendered-html-output-subarea">
      <div>
      <progress max="1500" style="width:300px; height:20px; vertical-align: middle;" value="1500"></progress>
            [1500/1500 42:41, Epoch 3/3]
          </div>
      <table border="1" class="dataframe">
      <thead>
      <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>1</td>
      <td>2.396400</td>
      <td>1.602937</td>
      <td>0.269400</td>
      </tr>
      <tr>
      <td>2</td>
      <td>1.572700</td>
      <td>1.531719</td>
      <td>0.320800</td>
      </tr>
      <tr>
      <td>3</td>
      <td>1.534400</td>
      <td>1.511815</td>
      <td>0.335800</td>
      </tr>
      </tbody>
      </table><p></p></div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>&lt;transformers.trainer_utils.EvalPrediction object at 0x7cd07be46440&gt;
      &lt;transformers.trainer_utils.EvalPrediction object at 0x7cd07be45c30&gt;
      &lt;transformers.trainer_utils.EvalPrediction object at 0x7cd07be8b970&gt;
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[27]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>TrainOutput(global_step=1500, training_loss=1.8345018310546874, metrics={opening_brace}'train_runtime': 2565.4667, 'train_samples_per_second': 233.876, 'train_steps_per_second': 0.585, 'total_flos': 2.352076406784e+17, 'train_loss': 1.8345018310546874, 'epoch': 3.0{closing_brace})</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Evaluaci%C3%B3n">Evaluaci√≥n<a class="anchor-link" href="#Evaluaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez entrenado evaluamos sobre el dataset de test</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-html-rendered-html-output-subarea">
      <div>
      <progress max="13" style="width:300px; height:20px; vertical-align: middle;" value="13"></progress>
            [13/13 00:17]
          </div>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>&lt;transformers.trainer_utils.EvalPrediction object at 0x7cd07be8bbe0&gt;
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[28]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>{opening_brace}'eval_loss': 1.5203168392181396,
       'eval_accuracy': 0.3374,
       'eval_runtime': 19.3843,
       'eval_samples_per_second': 257.94,
       'eval_steps_per_second': 0.671,
       'epoch': 3.0}</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Publicar-el-modelo">Publicar el modelo<a class="anchor-link" href="#Publicar-el-modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya tenemos nuestro modelo entrenado, ya podemos compartirlo con el mundo, as√≠ que primero creamos una model card</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '          <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="p">,</span>',
      '          <span class="n">training_args</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '          <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Y ya lo podemos publicar. Como lo primero que hemos hecho ha sido loguearnos con el hub de huggingface, lo podremos subir a nuestro hub sin ning√∫n problema</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '          <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="p">,</span>',
      '          <span class="n">training_args</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '          <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <h2 id="Prueba-del-modelo">Prueba del modelo<a class="anchor-link" href="#Prueba-del-modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Limpiamos todo lo posible</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '          <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="p">,</span>',
      '          <span class="n">training_args</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '          <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>',
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">import</span> <span class="nn">gc</span>',
      '      ',
      '      ',
      '      <span class="k">def</span> <span class="nf">clear_hardwares</span><span class="p">():</span>',
      '          <span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>',
      '          <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>',
      '          <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>',
      '          <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>',
      '      ',
      '      ',
      '      <span class="n">clear_hardwares</span><span class="p">()</span>',
      '      <span class="n">clear_hardwares</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>


















      
      <section class="section-block-markdown-cell">
      <p>Como hemos subido el modelo a nuestro hub podemos descargarlo y usarlo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '          <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="p">,</span>',
      '          <span class="n">training_args</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '          <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>',
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">import</span> <span class="nn">gc</span>',
      '      ',
      '      ',
      '      <span class="k">def</span> <span class="nf">clear_hardwares</span><span class="p">():</span>',
      '          <span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>',
      '          <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>',
      '          <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>',
      '          <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>',
      '      ',
      '      ',
      '      <span class="n">clear_hardwares</span><span class="p">()</span>',
      '      <span class="n">clear_hardwares</span><span class="p">()</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>',
      '      <span class="n">checkpoints</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>',
      '      <span class="n">task</span> <span class="o">=</span> <span class="s2">"text-classification"</span>',
      '      <span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Ahora si queremos que nos devuelva la probabilidad de todas las clases, simplemente usamos el clasificador que acabamos de instanciar, con el par√°metro <code>top_k=None</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
          '',
          '<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
          '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-LoRA-finetuned-amazon-reviews-en-classification"</span>',
          '<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
          '<span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
          '<span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
          '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
          '<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
          '',
          '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
          '    <span class="n">model_name</span><span class="p">,</span>',
          '    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
          '    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
          '    <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
          '    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
          '    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
          '    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
          '    <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
          '    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
          '    <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
          '    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
          '    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
          '    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
          '    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
          '    <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
          '<span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
          '',
          '<span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
          '',
          '<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
          '    <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
          '    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
          '    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
          '',
          '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
          '    <span class="n">model</span><span class="p">,</span>',
          '    <span class="n">training_args</span><span class="p">,</span>',
          '    <span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
          '    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
          '    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
          '    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>',
          '</span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">import</span> <span class="nn">gc</span>',
          '',
          '',
          '<span class="k">def</span> <span class="nf">clear_hardwares</span><span class="p">():</span>',
          '    <span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>',
          '    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>',
          '    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>',
          '    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>',
          '',
          '',
          '<span class="n">clear_hardwares</span><span class="p">()</span>',
          '<span class="n">clear_hardwares</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>',
          '<span class="n">checkpoints</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>',
          '<span class="n">task</span> <span class="o">=</span> <span class="s2">"text-classification"</span>',
          '<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>',
          '</span><span class="n">labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>',
          '<span class="n">labels</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[{\'label\': \'LABEL_0\', \'score\': 0.8419149518013},',
          ' {\'label\': \'LABEL_1\', \'score\': 0.09386005252599716},',
          ' {\'label\': \'LABEL_3\', \'score\': 0.03624210134148598},',
          ' {\'label\': \'LABEL_2\', \'score\': 0.02049318142235279},',
          ' {\'label\': \'LABEL_4\', \'score\': 0.0074898069724440575}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si solo queremos la clase con la mayor probabilidad hacemos lo mismo pero con el par√°metro <code>top_k=1</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">label</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
          '<span class="n">label</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[{\'label\': \'LABEL_0\', \'score\': 0.8419149518013}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Y si queremos n clases hacemos lo mismo pero con el par√°metro <code>top_k=n</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">two_labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '<span class="n">two_labels</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[{\'label\': \'LABEL_0\', \'score\': 0.8419149518013},',
          ' {\'label\': \'LABEL_1\', \'score\': 0.09386005252599716}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Tambi√©n podemo probar el modelo con Automodel y AutoTokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
      '      <span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>',
      '      <span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
          '<span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-finetuned-amazon-reviews-en-classification"</span>',
          '<span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>',
          '<span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
          '<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
          '    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>',
          '<span class="n">logits</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">logits</span>',
          '<span class="n">lables</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>',
          '<span class="n">lables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[0.003940582275390625,',
          ' 0.00266265869140625,',
          ' 0.013946533203125,',
          ' 0.1544189453125,',
          ' 0.8251953125]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si quires probar m√°s el modelo puedes verlo en <a href="https://huggingface.co/Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification" target="_blank" rel="nofollow noreferrer">Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Implementaci%C3%B3n-de-LoRA-en-un-LLM-con-PEFT-de-Hugging-Face">Implementaci√≥n de LoRA en un LLM con PEFT de Hugging Face<a class="anchor-link" href="#Implementaci%C3%B3n-de-LoRA-en-un-LLM-con-PEFT-de-Hugging-Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos hacer lo mismo con la librer√≠a <code>PEFT</code> de Hugging Face. Vamos a verlo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Login-en-el-Hub">Login en el Hub<a class="anchor-link" href="#Login-en-el-Hub"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Nos logueamos para subir el modelo al Hub</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
      '      <span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a descargar el dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
          '<span class="n">notebook_login</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
          '',
          '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"mteb/amazon_reviews_multi"</span><span class="p">,</span> <span class="s2">"en"</span><span class="p">)</span>',
          '<span class="n">dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'DatasetDict({',
          '    train: Dataset({',
          '        features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '        num_rows: 200000',
          '    })',
          '    validation: Dataset({',
          '        features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '        num_rows: 5000',
          '    })',
          '    test: Dataset({',
          '        features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '        num_rows: 5000',
          '    })',
          '})',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un subset por si quieres probar el c√≥digo con un dataset m√°s peque√±o. En mi caso usar√© el 100% del dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">percentage</span> <span class="o">=</span> <span class="mi">1</span>',
          '',
          '<span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">\'train\'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">\'train\'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
          '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">\'validation\'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">\'validation\'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
          '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">\'test\'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">\'test\'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
          '',
          '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '     num_rows: 200000',
          ' }),',
          ' Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '     num_rows: 5000',
          ' }),',
          ' Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\'],',
          '     num_rows: 5000',
          ' }))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos el n√∫mero de clases, para obtener el n√∫mero de clases usamos <code>dataset['train']</code> y no <code>subset_dataset_train</code> porque si el subset lo hamos muy peque√±o es posible que no haya ejemplos con todas las posibles clases del dataset original</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">\'train\'</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">\'label\'</span><span class="p">))</span>',
          '<span class="n">num_classes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '5',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una funci√≥n para crear el campo <code>label</code> en el dataset. El dataset descargado tiene el campo <code>labels</code> pero la librer√≠a <code>transformers</code> necesita que el campo se llame <code>label</code> y no <code>labels</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">set_labels</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>',
      '          <span class="n">example</span><span class="p">[</span><span class="s1">\'labels\'</span><span class="p">]</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">\'label\'</span><span class="p">]</span>',
      '          <span class="k">return</span> <span class="n">example</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Aplicamos la funci√≥n al dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">def</span> <span class="nf">set_labels</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>',
          '    <span class="n">example</span><span class="p">[</span><span class="s1">\'labels\'</span><span class="p">]</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">\'label\'</span><span class="p">]</span>',
          '    <span class="k">return</span> <span class="n">example</span>',
          '</span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
          '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
          '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
          '',
          '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\', \'labels\'],',
          '     num_rows: 200000',
          ' }),',
          ' Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\', \'labels\'],',
          '     num_rows: 5000',
          ' }),',
          ' Dataset({',
          '     features: [\'id\', \'text\', \'label\', \'label_text\', \'labels\'],',
          '     num_rows: 5000',
          ' }))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizador">Tokenizador<a class="anchor-link" href="#Tokenizador"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos el tokenizador. Para que no nos de error, asignamos el token de end of string al token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>Creamos una funci√≥n para tokenizar el dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '          <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Aplicamos la funci√≥n al dataset y de paso eliminamos las columnas que no necesitamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '</span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '</span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">\'text\'</span><span class="p">,</span> <span class="s1">\'label\'</span><span class="p">,</span> <span class="s1">\'id\'</span><span class="p">,</span> <span class="s1">\'label_text\'</span><span class="p">])</span>',
          '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">\'text\'</span><span class="p">,</span> <span class="s1">\'label\'</span><span class="p">,</span> <span class="s1">\'id\'</span><span class="p">,</span> <span class="s1">\'label_text\'</span><span class="p">])</span>',
          '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">\'text\'</span><span class="p">,</span> <span class="s1">\'label\'</span><span class="p">,</span> <span class="s1">\'id\'</span><span class="p">,</span> <span class="s1">\'label_text\'</span><span class="p">])</span>',
          '',
          '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(Dataset({',
          '     features: [\'labels\', \'input_ids\', \'attention_mask\'],',
          '     num_rows: 200000',
          ' }),',
          ' Dataset({',
          '     features: [\'labels\', \'input_ids\', \'attention_mask\'],',
          '     num_rows: 5000',
          ' }),',
          ' Dataset({',
          '     features: [\'labels\', \'input_ids\', \'attention_mask\'],',
          '     num_rows: 5000',
          ' }))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos el modelo. Tambi√©n, para que no nos de error, asignamos el token de end of string al token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>',
          '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [\'score.weight\']',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="LoRA-con-PEFT">LoRA con PEFT<a class="anchor-link" href="#LoRA-con-PEFT"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de crear el modelo con LoRA, vamos a ver sus capas</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '  (transformer): GPT2Model(',
          '    (wte): Embedding(50257, 768)',
          '    (wpe): Embedding(1024, 768)',
          '    (drop): Dropout(p=0.1, inplace=False)',
          '    (h): ModuleList(',
          '      (0-11): 12 x GPT2Block(',
          '        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '        (attn): GPT2Attention(',
          '          (c_attn): Conv1D()',
          '          (c_proj): Conv1D()',
          '          (attn_dropout): Dropout(p=0.1, inplace=False)',
          '          (resid_dropout): Dropout(p=0.1, inplace=False)',
          '        )',
          '        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '        (mlp): GPT2MLP(',
          '          (c_fc): Conv1D()',
          '          (c_proj): Conv1D()',
          '          (act): NewGELUActivation()',
          '          (dropout): Dropout(p=0.1, inplace=False)',
          '        )',
          '      )',
          '    )',
          '    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '  )',
          '  (score): Linear(in_features=768, out_features=5, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos solo hay una capa <code>Linear</code>, que es <code>score</code> y que es la que vamos a sustituir</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos crear una configuraci√≥n de LoRA con la librer√≠a PEFT y luego aplicar LoRA al mo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">TaskType</span>',
      '      ',
      '      <span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>',
      '          <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
      '          <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '          <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>',
      '          <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">"score"</span><span class="p">],</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <p>Con esta configuraci√≥n hemos configurado un rank de 16 y un alpha de 32. Adem√°s hemos a√±adido un dropout a las capas de lora de 0.1. Le tenemos que indicar la tarea a la configuraci√≥n de LoRA, en este caso es una tarea de sequence clasification. Por √∫ltimo le indicamos qu√© capas queremos sustituir, en este caso la capa <code>score</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora aplicamos LoRA al modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">TaskType</span>',
      '      ',
      '      <span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>',
      '          <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
      '          <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '          <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>',
      '          <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">"score"</span><span class="p">],</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">get_peft_model</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver cuantos par√°metros entrenables tiene ahora el modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">TaskType</span>',
          '',
          '<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>',
          '    <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
          '    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
          '    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>',
          '    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>',
          '    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">"score"</span><span class="p">],</span>',
          '<span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">get_peft_model</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>',
          '</span><span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'trainable params: 12,368 || all params: 124,456,016 || trainable%: 0.0099',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos los mismos par√°metros entrenables que antes</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Training">Training<a class="anchor-link" href="#Training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez instanciado el modelo con LoRA, vamos a entrenarlo como siempre</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
































      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '          <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '          <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="p">,</span>',
      '          <span class="n">training_args</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '          <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-html-rendered-html-output-subarea">
      <div>
      <progress max="1500" style="width:300px; height:20px; vertical-align: middle;" value="811"></progress>
            [ 811/1500 22:43 &lt; 19:20, 0.59 it/s, Epoch 1.62/3]
          </div>
      <table border="1" class="dataframe">
      <thead>
      <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>1</td>
      <td>2.275100</td>
      <td>1.512476</td>
      <td>0.318200</td>
      </tr>
      </tbody>
      </table><p></p></div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>&lt;transformers.trainer_utils.EvalPrediction object at 0x7f774a50bbe0&gt;
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-html-rendered-html-output-subarea">
      <div>
      <progress max="1500" style="width:300px; height:20px; vertical-align: middle;" value="1500"></progress>
            [1500/1500 42:28, Epoch 3/3]
          </div>
      <table border="1" class="dataframe">
      <thead>
      <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>1</td>
      <td>2.275100</td>
      <td>1.512476</td>
      <td>0.318200</td>
      </tr>
      <tr>
      <td>2</td>
      <td>1.515900</td>
      <td>1.417553</td>
      <td>0.373800</td>
      </tr>
      <tr>
      <td>3</td>
      <td>1.463500</td>
      <td>1.405058</td>
      <td>0.381400</td>
      </tr>
      </tbody>
      </table><p></p></div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>&lt;transformers.trainer_utils.EvalPrediction object at 0x7f77486a7c40&gt;
      &lt;transformers.trainer_utils.EvalPrediction object at 0x7f7749eb5690&gt;
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>TrainOutput(global_step=1500, training_loss=1.751504597981771, metrics={opening_brace}'train_runtime': 2551.7753, 'train_samples_per_second': 235.13, 'train_steps_per_second': 0.588, 'total_flos': 2.352524525568e+17, 'train_loss': 1.751504597981771, 'epoch': 3.0{closing_brace})</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Evaluaci%C3%B3n">Evaluaci√≥n<a class="anchor-link" href="#Evaluaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez entrenado evaluamos sobre el dataset de test</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-html-rendered-html-output-subarea">
      <div>
      <progress max="13" style="width:300px; height:20px; vertical-align: middle;" value="13"></progress>
            [13/13 00:17]
          </div>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>&lt;transformers.trainer_utils.EvalPrediction object at 0x7f77a1d1f7c0&gt;
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>{opening_brace}'eval_loss': 1.4127237796783447,
       'eval_accuracy': 0.3862,
       'eval_runtime': 19.3275,
       'eval_samples_per_second': 258.699,
       'eval_steps_per_second': 0.673,
       'epoch': 3.0}</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Publicar-el-modelo">Publicar el modelo<a class="anchor-link" href="#Publicar-el-modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 27" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una model card</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
      '      <span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification"</span>',
      '      <span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '      <span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '      <span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">model_name</span><span class="p">,</span>',
      '          <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '          <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
      '          <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '          <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '          <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '          <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '          <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="p">,</span>',
      '          <span class="n">training_args</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '          <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Lo publicamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>',
          '',
          '<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>',
          '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification"</span>',
          '<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
          '<span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
          '<span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
          '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
          '<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
          '',
          '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
          '    <span class="n">model_name</span><span class="p">,</span>',
          '    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
          '    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>',
          '    <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
          '    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
          '    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
          '    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
          '    <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
          '    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>',
          '    <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
          '    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
          '    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
          '    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
          '    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
          '    <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
          '<span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>',
          '',
          '<span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
          '',
          '<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
          '    <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
          '    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
          '    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>',
          '',
          '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
          '    <span class="n">model</span><span class="p">,</span>',
          '    <span class="n">training_args</span><span class="p">,</span>',
          '    <span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
          '    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
          '    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
          '    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CommitInfo(commit_url=\'https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification/commit/839066c2bde02689a6b3f5624ac25f89c4de217d\', commit_message=\'End of training\', commit_description=\'\', oid=\'839066c2bde02689a6b3f5624ac25f89c4de217d\', pr_url=None, pr_revision=None, pr_num=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Prueba-del-modelo-entrenado-con-PEFT">Prueba del modelo entrenado con PEFT<a class="anchor-link" href="#Prueba-del-modelo-entrenado-con-PEFT"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 28" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Limpiamos todo lo posible</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">import</span> <span class="nn">gc</span>',
      '      ',
      '      ',
      '      <span class="k">def</span> <span class="nf">clear_hardwares</span><span class="p">():</span>',
      '          <span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>',
      '          <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>',
      '          <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>',
      '          <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>',
      '      ',
      '      ',
      '      <span class="n">clear_hardwares</span><span class="p">()</span>',
      '      <span class="n">clear_hardwares</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>


















      
      <section class="section-block-markdown-cell">
      <p>Como hemos subido el modelo a nuestro hub podemos descargarlo y usarlo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">import</span> <span class="nn">gc</span>',
          '',
          '',
          '<span class="k">def</span> <span class="nf">clear_hardwares</span><span class="p">():</span>',
          '    <span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>',
          '    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>',
          '    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>',
          '    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>',
          '',
          '',
          '<span class="n">clear_hardwares</span><span class="p">()</span>',
          '<span class="n">clear_hardwares</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>',
          '<span class="n">checkpoints</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>',
          '<span class="n">task</span> <span class="o">=</span> <span class="s2">"text-classification"</span>',
          '<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [\'score.weight\']',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora si queremos que nos devuelva la probabilidad de todas las clases, simplemente usamos el clasificador que acabamos de instanciar, con el par√°metro <code>top_k=None</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>',
          '<span class="n">labels</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[{\'label\': \'LABEL_1\', \'score\': 0.9979197382926941},',
          ' {\'label\': \'LABEL_0\', \'score\': 0.002080311067402363}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si solo queremos la clase con la mayor probabilidad hacemos lo mismo pero con el par√°metro <code>top_k=1</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">label</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
          '<span class="n">label</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[{\'label\': \'LABEL_1\', \'score\': 0.9979197382926941}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Y si queremos n clases hacemos lo mismo pero con el par√°metro <code>top_k=n</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">two_labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
          '<span class="n">two_labels</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '[{\'label\': \'LABEL_1\', \'score\': 0.9979197382926941},',
          ' {\'label\': \'LABEL_0\', \'score\': 0.002080311067402363}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Si quires probar m√°s el modelo puedes verlo en <a href="https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification" target="_blank" rel="nofollow noreferrer">Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification</a></p>
      </section>
      






    </div>

  </section>

</PostLayout>
