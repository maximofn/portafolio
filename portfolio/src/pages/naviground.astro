---
import Layout from '@layouts/Layout.astro';

const { social_links } = await import('@portfolio/consts.json');
const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { color_palette } = await import('@portfolio/consts.json');
const { sticky_top_positions } = await import('@portfolio/consts.json');

const page_title = metadata_page.title + " - Naviground";
const url = social_links.portfolio_link_external + "/naviground"
const description = "Sistema de percepción para vehículo autónomo";
const keywords = "Percepción, vehículo autónomo";

const images_width = 1920*2/3;
const images_height =1080*2/3;
const buyMeACoffee_width = 305;
const buyMeACoffee_height = 28;

const inline_code_background_color = color_palette.color_950;

const sticky_top_default=sticky_top_positions.sticky_top_default;
const sticky_top_649px=sticky_top_positions.sticky_top_649px;
const sticky_top_418px=sticky_top_positions.sticky_top_418px;
const sticky_top_334px=sticky_top_positions.sticky_top_334px;
const sticky_top_315px=sticky_top_positions.sticky_top_315px;
const sticky_top_229px=sticky_top_positions.sticky_top_229px;
const sticky_top_h3_increment = "50px"
---

<Layout 
	title={page_title}
	languaje={metadata_page.languaje_es}
	description={description}
	keywords={keywords}
	author={metadata_page.author}
	theme_color={colors.background_color}
	url={url}
	icon={metadata_page.icon}
>
	<div class="project">
		<h1>Naviground</h1>
		<h4>Sistema de percepción para vehículo autónomo</h4>
		<figure class="video-container">
			<video 
				preload="auto"
				controls
				width={images_width}
				height={images_height}
			>
				<source src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground.webm" type="video/webm"/>
				<source src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground.mp4" type="video/mp4"/>
			</video>
		</figure>

        <p>Naviground es un sistema de navegación implementable en vehículos terrestres tripulados y no tripulados. Permite navegar en entornos estructurados y no estructurados. Participé en el desarrollo del sistema de percepción, especialmente en el de detección del entorno mediante cámaras.</p>

        <section>
            <h2>Sistema de visión</h2>
            <p>Aunque el sistema de navegación contaba con sensores LIDAR y RADAR, por varios motivos se quería tener un sistema de percepción formado únicamente por cámaras.</p>
            <ul>
                <li>Auque el precio de los LIDAR y RADAR ha disminuido mucho en los últimos años, sigue siendo más caro que el de las cámaras.</li>
                <li>Los sensores LIDAR y RADAR son sensores activos (emiten una onda electromagnética y miden la reflexión), por lo que en un entorno de guerra hacen que el vehículo pueda ser detectado.</li>
                <li>Al ser un vehículo autónomo, el procesamiento no se puede hacer en una máquina potentísima, por lo que si se puede eliminar el procesamiento de la cantidad de datos que generan los LIDAR y RADAR, mejor.</li>
            </ul>
            <p>Para poder realizar la detección del entorno, utilizamos tres tipos de redes neuronales:</p>
            <ul>
                <li>
                    <p>Redes de segmentación semántica</p>
                    <p>Clasifican a qué clase pertenece cada pixel de la imagen, obteniendo una máscara de segmentación.</p>
                    <figure>
                        <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/semantic_segmentation.webp" alt="Segmentación semántica" width={images_width} height={images_height} loading="lazy">
                    </figure>
                </li>
                <li>
                    <p>Redes de clasificación de objetos</p>
                    <p>Mediante una YOLO, se pueden detectar objetos en la imagen</p>
                    <figure>
                        <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/yolo.webp" alt="Clasificación de objetos con YOLO" width={images_width/2} height={images_height/2} loading="lazy">
                    </figure>
                </li>
                <li>
                    <p>Profundidad</p>
                    <p>Mediante una red neuronal, se puede estimar la profundidad de cada pixel de la imagen, con lo que se puede obtener a qué distancia se encuentra cada objeto.</p>
                    <figure>
                        <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/depth.webp" alt="Profundidad" width={images_width/2} height={images_height/2} loading="lazy">
                    </figure>
                </li>
            </ul>
        </section>

        <section>
            <h2>Entrenamiento</h2>
            <p>Nuestro problema era que al ser un vehículo para entornos estructurados y no estructurados, no nos valían las redes preentrenadas, por lo que tivimos que hacer entrenamientos de las redes de segmentación y de clasificación de objetos.</p>
        </section>
        
        <section>
            <h3>Dataset</h3>
            <p>Como teníamos horas de videos grabados durante pruebas en entornos como este, creamos un dataset</p>
            <figure>
                <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground-dataset.webp" alt="Captura de videos de Naviground" width={images_width/2} height={images_height/2} loading="lazy">
            </figure>
            <p>Creamos un algoritmo que mediante un clasificador no supervisado, creó varios clusteres de imágenes, donde las imágenes de cada cluster eran similares entre sí. De esta manera, nos quedábamos con unas pocas imágenes de cada cluster, para así tener un dataset con imágenes heterogéneas.</p>
        </section>

        <section>
            <h3>Etiquetador</h3>
            <p>Etiquetar objetos para la YOLO, aunque es pesado, es un proceso más o menos rápido y fácil</p>
            <figure>
                <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/yolo-labeling.gif" alt="YOLO labeling" width={images_width/2} height={images_height/2} loading="lazy">
            </figure>
            <p>Sin embargo, etiquetar las imágenes para la segmentación semántica, donde hay que etiquetar cada pixel, es un proceso lento y tedioso. Como no nos convencía ninguna herramienta de etiquetado para segmentación, construimos nuestra propia herramienta de etiquetado. Fue tan buena que se reutilizó en otros proyectos e incluso se habló de comercializarla.</p>
        </section>

        <section>
            <h3>Generación de imágenes de entrenamiento</h3>
            <p>Uno de los problemas que teníamos es que todas las imágenes de entrenamiento eran de día, con sol, sin lluvia, etc. Por lo que para poder hacer las redes más robustas neecsitábamos más imágenes. Pero eso supone que alguien tenga que salir de noche, esperar a que llueva para tener imágenes con lluvia, esperar a que nieve, que es más complicado, etc.</p>
            <p>En aquel momento ya había muchas redes de generación de imágenes bastante buenas, por lo que podíamos generar imágenes con nuevas condiciones ambientales, pero el problema era que había que etiquetarlas, y para la segmentación requería mucho tiempo.</p>
            <p>Así que realicé un pipeline que mediante IA generativa, modificaba las condiciones ambientales de las imágenes que ya teníamos etiquetadas, teniendo imágenes en diferentes condiciones ambientales, pero sin tener que perder tiempo etiquetándolas.</p>
        </section>
        
        <section>
            <h2>Optimización con TensorRT</h2>
            <p>Como esto tenía que funcionar en un vehículo, no se podía utilizar un ordenador con una GPU potente. Por lo que se utilizaba un dispositivo embebido, una Jetson Orin. Por lo que era importante poder optimizar las redes neuronales para que hicieran la inferencia lo más rápido posible.</p>
            <p>Me encargué de optimizarlas con TendorRT, haciendo que en algunos casos se ejecutaran hasta un 40% más rápido.</p>
            <figure>
                <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground-ascod.webp" alt="ASCOD" width={images_width/2} height={images_height/2} loading="lazy">
            </figure>
            <figure>
                <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground-system.webp" alt="Naviground system" width={images_width/2} height={images_height/2} loading="lazy">
            </figure>
        </section>
	</div>

</Layout>

<style define:vars={{
	inline_code_background_color,
	sticky_top_default,
	sticky_top_649px,
	sticky_top_418px,
	sticky_top_334px,
	sticky_top_315px,
	sticky_top_229px,
	sticky_top_h3_increment,
}}>
	.project {
		display: flex;
		flex-direction: column;
	}

	.inline-code {
		background-color: var(--inline_code_background_color);
		border-radius: 7px;
		padding: 0.2em 0.4em;
		font-style: oblique;
		font-family: 'Courier New', Courier, monospace;
	}

	section {
		position: relative;
	}

	h1 {
		margin-bottom: 0px;
	}

	h4 {
		padding-left: 0px;
	}

    figure {
        display: flex;
        justify-content: center;
        align-items: center;
        padding-bottom: 2em;
        padding-top: 2em;
    }
</style>
