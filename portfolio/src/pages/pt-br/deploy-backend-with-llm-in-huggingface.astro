---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Desplegar backend com LLM no HuggingFace';
const end_url = 'deploy-backend-with-llm-in-huggingface';
const description = 'Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.';
const keywords = 'hugging face, fastapi, langchain, docker, backend, llm';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=960
    image_height=720
    image_extension=webp
    article_date=2025-03-02+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Desplegar backend com Gradio"><h2>Desplegar backend com Gradio</h2></a>
      <a class="anchor-link" href="#Criar espaco"><h3>Criar espa√ßo</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#LEIA-ME.md"><h4>LEIA-ME.md</h4></a>
      <a class="anchor-link" href="#Implantacao"><h3>Implanta√ß√£o</h3></a>
      <a class="anchor-link" href="#Backend"><h3>Backend</h3></a>
      <a class="anchor-link" href="#Teste da API"><h3>Teste da API</h3></a>
      <a class="anchor-link" href="#Deploy do backend com FastAPI, Langchain e Docker"><h2>Deploy do backend com FastAPI, Langchain e Docker</h2></a>
      <a class="anchor-link" href="#Criar espaco"><h3>Criar espa√ßo</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#Dockerfile"><h4>Dockerfile</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#README.md"><h4>README.md</h4></a>
      <a class="anchor-link" href="#Token do HuggingFace"><h3>Token do HuggingFace</h3></a>
      <a class="anchor-link" href="#Adicionar o token aos secrets do espaco"><h3>Adicionar o token aos secrets do espa√ßo</h3></a>
      <a class="anchor-link" href="#Implantacao"><h3>Implanta√ß√£o</h3></a>
      <a class="anchor-link" href="#URL do backend"><h3>URL do backend</h3></a>
      <a class="anchor-link" href="#Documentacao"><h3>Documenta√ß√£o</h3></a>
      <a class="anchor-link" href="#Teste da API"><h3>Teste da API</h3></a>
      <a class="anchor-link" href="#Deploy do backend com Gradio e modelo rodando no servidor"><h2>Deploy do backend com Gradio e modelo rodando no servidor</h2></a>
      <a class="anchor-link" href="#Criar Espaco"><h3>Criar Espa√ßo</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#Teste da API"><h4>Teste da API</h4></a>
      <a class="anchor-link" href="#Deploy de backend com FastAPI, Langchain e Docker e modelo rodando no servidor"><h2>Deploy de backend com FastAPI, Langchain e Docker e modelo rodando no servidor</h2></a>
      <a class="anchor-link" href="#Criar Espaco"><h3>Criar Espa√ßo</h3></a>
      <a class="anchor-link" href="#Codigo"><h3>C√≥digo</h3></a>
      <a class="anchor-link" href="#app.py"><h4>app.py</h4></a>
      <a class="anchor-link" href="#requirements.txt"><h4>requirements.txt</h4></a>
      <a class="anchor-link" href="#Dockerfile"><h4>Dockerfile</h4></a>
      <a class="anchor-link" href="#Teste da API"><h3>Teste da API</h3></a>
      <a class="anchor-link" href="#Conclusoes"><h2>Conclus√µes</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Neste post, vamos ver como deployar um backend no HuggingFace. Vamos ver como fazer isso de duas maneiras, atrav√©s da forma comum, criando uma aplica√ß√£o com Gradio, e atrav√©s de uma op√ß√£o diferente usando FastAPI, Langchain e Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para ambos casos ser√° necess√°rio ter uma conta no HuggingFace, j√° que vamos implantar o backend em um espa√ßo do HuggingFace.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desplegar backend com Gradio">Desplegar backend com Gradio<a class="anchor-link" href="#Desplegar backend com Gradio"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 79" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Criar espaco">Criar espa√ßo<a class="anchor-link" href="#Criar espaco"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 80" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro de tudo, criamos um novo espa√ßo na Hugging Face.</p>
      <ul>
        <li>Colocamos um nome, uma descri√ß√£o e escolhemos a licen√ßa.</li>
        <li>Escolhemos o Gradio como o tipo de SDK. Ao escolher o Gradio, ser√£o exibidas algumas templates, ent√£o escolhemos a template do chatbot.</li>
        <li>Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc√™ escolha o que melhor considerar.* E por √∫ltimo, temos que escolher se queremos criar o espa√ßo p√∫blico ou privado.</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp" alt="backend gradio - criar espa√ßo">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">C√≥digo<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 81" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ao criar o space, podemos clon√°-lo ou podemos ver os arquivos na pr√≥pria p√°gina do HuggingFace. Podemos ver que foram criados 3 arquivos, <code>app.py</code>, <code>requirements.txt</code> e <code>README.md</code>. Ent√£o, vamos ver o que colocar em cada um.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 82" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aqui est√° o c√≥digo do aplicativo. Como escolhemos o template de chatbot, j√° temos muito feito, mas vamos ter que mudar 2 coisas: primeiro, o modelo de linguagem e o system prompt.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como modelo de linguagem, vejo <code>HuggingFaceH4/zephyr-7b-beta</code>, mas vamos utilizar <code>Qwen/Qwen2.5-72B-Instruct</code>, que √© um modelo muito capaz.</p>
      <p>Ent√£o, procure pelo texto <code>client = InferenceClient(&quot;HuggingFaceH4/zephyr-7b-beta&quot;)</code> e substitua-o por <code>client = InferenceClient(&quot;Qwen/Qwen2.5-72B-Instruct&quot;)</code>, ou espere que colocarei todo o c√≥digo mais tarde.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m vamos alterar o system prompt, que por padr√£o √© <code>You are a friendly Chatbot.</code>, mas como o modelo foi treinado principalmente em ingl√™s, √© prov√°vel que se voc√™ falar com ele em outro idioma, ele responda em ingl√™s. Ent√£o, vamos mud√°-lo para <code>You are a friendly Chatbot. Always reply in the language in which the user is writing to you.</code>.</p>
      <p>Ent√£o, procure pelo texto <code>gr.Textbox(value=&quot;You are a friendly Chatbot.&quot;, label=&quot;System message&quot;),</code> e substitua-o por <code>gr.Textbox(value=&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;, label=&quot;System message&quot;),</code>, ou espere at√© eu colocar todo o c√≥digo agora.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">import gradio as gr<br>do huggingface_hub import InferenceClient<br><br>""""""<br>Para mais informa√ß√µes sobre o suporte da API de Infer√™ncia do `huggingface_hub`, consulte a documenta√ß√£o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference<br>""""""<br>client = InferenceClient("Qwen/Qwen2.5-72B-Instruct")<br><br><br>def responder(<br>mensagem,<br>hist√≥ria: list[tuple[str, str]],<br>Mensagem do sistema,<br>max_tokens,<br>temperatura,<br>top_p,<br>):<br>mensagens = [{opening_brace}"papel": "sistema", "conte√∫do": system_message{closing_brace}]<br><br>for val in history:<br>if val[0]:<br>messages.append({opening_brace}"role": "user", "content": val[0]{closing_brace})<br>if val[1]:<br>messages.append({opening_brace}"role": "assistant", "content": val[1]{closing_brace})<br><br>messages.append({opening_brace}"role": "user", "content": message{closing_brace})<br><br>response = ""<br><br>para mensagem em client.chat_completion(<br>mensagens,<br>max_tokens=max_tokens,<br>stream=True,<br>temperature=temperature,<br>top_p=top_p,<br>):<br>token = message.choices[0].delta.content<br><br>response += token<br>yield response<br><br><br>""""""<br>Para informa√ß√µes sobre como personalizar a ChatInterface, consulte a documenta√ß√£o do gradio: https://www.gradio.app/docs/gradio/chatinterface<br>""""""<br>demo = gr.ChatInterface(<br>responda,</code></pre></div>
            </section>
      <p>markdown</p>
      <p>additional_inputs=[</p>
      <div class='highlight'><pre><code>gr.Textbox(value="Voc√™ √© um chatbot amig√°vel. Sempre responda na l√≠ngua em que o usu√°rio est√° escrevendo para voc√™.", label="Mensagem do sistema"),
      gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="M√°ximo de novos tokens"),
      gr.Slider(m√≠nimo=0,1, m√°ximo=4,0, valor=0,7, passo=0,1, r√≥tulo="Temperatura"),
      gr.Slider(
      m√≠nimo=0.1,
      m√°ximo=1.0,
      value=0.95,
      passo=0.05,
      label="Top-p (amostragem do n√∫cleo)"
      ),
      ],
      )
      
      
      if __name__ == "__main__":
      demo.launch()</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 83" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este √© o arquivo onde ser√£o escritas as depend√™ncias, mas para este caso vai ser muito simples:</p>
      <p>``` txt</p>
      <p>huggingface_hub==0.25.2```</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="LEIA-ME.md">LEIA-ME.md<a class="anchor-link" href="#LEIA-ME.md"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 84" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este √© o arquivo no qual vamos colocar as informa√ß√µes do espa√ßo. Nos spaces da HuggingFace, no in√≠cio dos readmes, coloca-se um c√≥digo para que a HuggingFace saiba como exibir a miniatura do espa√ßo, qual arquivo deve ser usado para executar o c√≥digo, vers√£o do sdk, etc.</p>
      <div class='highlight'><pre><code class="language-md">---
      t√≠tulo: SmolLM2
      emoji: üí¨
      colorFrom: amarelo
      colorTo: roxo
      sdk: gradio
      sdk_version: 5.0.1
      app_file: app.py
      pinned: false
      licen√ßa: apache-2.0
      short_description: Bate-papo com o Gradio SmolLM2
      ---
      
      Um exemplo de chatbot usando [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index) e a [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Implantacao">Implanta√ß√£o<a class="anchor-link" href="#Implantacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 85" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se n√≥s clonamos o espa√ßo, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv√°-los.</p>
      <p>Ent√£o, quando as altera√ß√µes estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa√ßo seja constru√≠do e possamos us√°-lo.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp" alt="backend gradio - chatbot">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Backend">Backend<a class="anchor-link" href="#Backend"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 86" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Muito bem, fizemos um chatbot, mas n√£o era essa a inten√ß√£o, aqui t√≠nhamos vindo fazer um backend! P√°ra, p√°ra, olha o que diz abaixo do chatbot</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp" alt="backend gradio - Use via API">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver um texto <code>Use via API</code>, onde se clicarmos, se abrir√° um menu com uma API para poder usar o chatbot.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp" alt="backend gradio - API">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que nos d√° uma documenta√ß√£o de como usar a API, tanto com Python, com JavaScript, quanto com bash.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Teste da API">Teste da API<a class="anchor-link" href="#Teste da API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 87" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usamos o c√≥digo de exemplo de Python.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2&quot;</span><span class="p">)</span>',
      '<span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2.hf.space ‚úî',
          '¬°Hola M√°ximo! Mucho gusto, estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Estamos fazendo chamadas √† API do <code>InferenceClient</code> da HuggingFace, ent√£o poder√≠amos pensar, Para que fizemos um backend, se podemos chamar diretamente a API da HuggingFace? Bem, voc√™ vai ver isso abaixo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;¬øC√≥mo me llamo?&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Tu nombre es M√°ximo. ¬øEs correcto?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>O modelo de bate-papo do Gradio gerencia o hist√≥rico para n√≥s, de forma que cada vez que criamos um novo <code>cliente</code>, uma nova thread de conversa √© criada.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a tentar criar um novo cliente e ver se uma nova thread de conversa √© criada.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">new_client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2&quot;</span><span class="p">)</span>',
      '<span class="n">result</span> <span class="o">=</span> <span class="n">new_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;Hola, ¬øc√≥mo est√°s? Me llamo Luis&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2.hf.space ‚úî',
          'Hola Luis, estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? Es un gusto conocerte. ¬øEn qu√© puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora vamos perguntar novamente como me chamo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">result</span> <span class="o">=</span> <span class="n">new_client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;¬øC√≥mo me llamo?&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, temos dois clientes, cada um com seu pr√≥prio fio de conversa.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Deploy do backend com FastAPI, Langchain e Docker">Deploy do backend com FastAPI, Langchain e Docker<a class="anchor-link" href="#Deploy do backend com FastAPI, Langchain e Docker"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 88" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora vamos a fazer o mesmo, criar um backend de um chatbot, com o mesmo modelo, mas nesse caso usando FastAPI, Langchain e Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Criar espaco">Criar espa√ßo<a class="anchor-link" href="#Criar espaco"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 89" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Temos que criar um novo espa√ßo, mas nesse caso faremos de outra maneira</p>
      <ul>
        <li>Colocamos um nome, uma descri√ß√£o e escolhemos a licen√ßa.</li>
        <li>Escolhemos Docker como o tipo de SDK. Ao escolher Docker, aparecer√£o modelos, ent√£o escolhemos um modelo em branco.</li>
        <li>Selecionamos o HW no qual vamos a desdobrar o backend, eu vou escolher a CPU gratuita, mas voc√™ escolha o que melhor considerar.* E por fim, √© preciso escolher se queremos criar o espa√ßo p√∫blico ou privado.</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp" alt="backend docker - criar espa√ßo">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">C√≥digo<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 90" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, ao criar o space, vemos que temos apenas um arquivo, o <code>README.md</code>. Ent√£o vamos ter que criar todo o c√≥digo n√≥s mesmos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 91" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a criar o c√≥digo do aplicativo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Come√ßamos com as bibliotecas necess√°rias</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from fastapi import FastAPI, HTTPException<br>from pydantic import BaseModel<br>do huggingface_hub import InferenceClient<br></code></pre></div>
            </section>
      <p>markdown</p>
      <p>from langchain_core.messages import HumanMessage, AIMessage</p>
      <div class='highlight'><pre><code>from langgraph.checkpoint.memory import MemorySaver
      from langgraph.graph import START, MessagesState, StateGraph
      
      import osfrom dotenv import load_dotenv
      load_dotenv()</code></pre></div>
      <p>Carregamos <code>fastapi</code> para poder criar as rotas da API, <code>pydantic</code> para criar o template das queries, <code>huggingface_hub</code> para poder criar um modelo de linguagem, <code>langchain</code> para indicar ao modelo se as mensagens s√£o do chatbot ou do usu√°rio e <code>langgraph</code> para criar o chatbot.</p>
      <p>Al√©m disso, carregamos <code>os</code> e <code>dotenv</code> para poder carregar as vari√°veis de ambiente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Carregamos o token do HuggingFace</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Token da HuggingFace<br>HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o modelo de linguagem</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Inicializar o modelo da HuggingFace<br>model = InferenceClient(<br>model="Qwen/Qwen2.5-72B-Instruct",api_key=os.getenv("HUGGINGFACE_TOKEN")<br>)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos agora uma fun√ß√£o para chamar o modelo</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Defina a fun√ß√£o que chama o modelo<br>def chamar_modelo(estado: MessagesState):<br>""""""<br>Chame o modelo com as mensagens fornecidas<br><br>Argumentos:<br>estado: EstadoMensagens<br><br>Retorna:<br>um dicion√°rio contendo o texto gerado e o ID do thread<br>""""""<br># Converter mensagens do LangChain para o formato do HuggingFace<br>hf_messages = []<br>for msg in state["messages"]:<br>Se `isinstance(msg, HumanMessage)`:<br>hf_messages.append({opening_brace}"role": "user", "content": msg.content{closing_brace})<br>elif isinstance(msg, AIMessage):<br>hf_messages.append({opening_brace}"role": "assistant", "content": msg.content{closing_brace})<br>&#x20;&#x20;<br># Chamar a API<br>resposta = modelo.completar_chat(<br>mensagens=hf_mensagens,<br>temperature=0.5,<br>max_tokens=64,<br>top_p=0,7<br>)<br>&#x20;&#x20;<br># Converter a resposta para o formato LangChain</code></pre></div>
            </section>
      <p>python</p>
      <p>ai_message = AIMessage(content=response.choices[0].message.content)</p>
      <div class='highlight'><pre><code>return {opening_brace}"messages": state["messages"] + [ai_message]{closing_brace}</code></pre></div>
      <p>Convertemos as mensagens do formato LangChain para o formato HuggingFace, assim podemos usar o modelo de linguagem.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Definimos uma template para as queries</p>
      <p>``` python</p>
      <p>class QueryRequest(BaseModel):</p>
      <p>query: str</p>
      <p>thread_id: str = "padr√£o"```</p>
      <p>As consultas ter√£o um <code>query</code>, a mensagem do usu√°rio, e um <code>thread_id</code>, que √© o identificador do fio da conversa√ß√£o e mais adiante explicaremos para que o utilizamos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos um grafo de LangGraph</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Definir o gr√°fico<br>workflow = StateGraph(state_schema=MessagesState)<br><br># Defina o n√≥do na gr√°fico<br>workflow.add_edge(START, "model")<br>workflow.add_node("modelo", call_model)<br><br># Adicionar mem√≥ria<br>memory = MemorySaver()<br>graph_app = workflow.compile(checkpointer=memory)</code></pre></div>
            </section>
      <p>Com isso, criamos um grafo de LangGraph, que √© uma estrutura de dados que nos permite criar um chatbot e gerenciar o estado do chatbot para n√≥s, ou seja, entre outras coisas, o hist√≥rico de mensagens. Dessa forma, n√£o precisamos fazer isso n√≥s mesmos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos a aplica√ß√£o de FastAPI</p>
      <p>``` python</p>
      <p>app = FastAPI(title="LangChain FastAPI", description="API para gerar texto usando LangChain e LangGraph")```</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos os endpoints da API</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Ponto de entrada Bem-vindo<br>@app.get("/")<br>async def api_home():<br>"""Ponto de entrada Welcome"""<br>return {opening_brace}"detail": "Bem-vindo ao tutorial de FastAPI, Langchain, Docker"{closing_brace}<br><br># Gerar ponto final<br>@app.post("/generate")<br>async def gerar(request: QueryRequest):<br>""""""<br>Ponto final para gerar texto usando o modelo de linguagem<br>&#x20;&#x20;<br>Argumentos:<br>solicita√ß√£o: QueryRequest<br>query: str<br>thread_id: str = "padr√£o"<br><br>Retorna:<br>um dicion√°rio contendo o texto gerado e o ID do thread<br>""""""<br>tente:<br># Configurar o ID da thread<br>config = {opening_brace}"configurable": {opening_brace}"thread_id": request.thread_id{closing_brace}{closing_brace}<br>&#x20;&#x20;&#x20;&#x20;<br># Crie a mensagem de entrada<br>input_messages = [HumanMessage(content=request.query)]<br>&#x20;&#x20;&#x20;&#x20;<br># Invocar o gr√°fico<br>output = graph_app.invoke({opening_brace}"messages": input_messages{closing_brace}, config)<br>&#x20;&#x20;&#x20;&#x20;<br># Obter a resposta do modelo<br>resposta = output["messages"][-1].conte√∫do<br>&#x20;&#x20;&#x20;&#x20;<br>return {opening_brace}<br>"generated_text": resposta,<br>"thread_id": request.thread_id<br>{closing_brace}<br>except Exception as e:<br>raise HTTPException(status_code=500, detail=f"Erro ao gerar texto: {opening_brace}str(e){closing_brace}")</code></pre></div>
            </section>
      <p>Criamos o endpoint <code>/</code> que nos retornar√° um texto quando acessarmos a API, e o endpoint <code>/generate</code> que √© o que usaremos para gerar o texto.</p>
      <p>Se n√≥s olharmos para a fun√ß√£o <code>generate</code>, temos a vari√°vel <code>config</code>, que √© um dicion√°rio que cont√©m o <code>thread_id</code>. Este <code>thread_id</code> √© o que nos permite ter um hist√≥rico de mensagens de cada usu√°rio, desta forma, diferentes usu√°rios podem usar o mesmo endpoint e ter seu pr√≥prio hist√≥rico de mensagens.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, temos o c√≥digo para que se possa executar a aplica√ß√£o</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">if __name__ == "__main__":<br>import uvicornuvicorn.run(app, host="0.0.0.0", port=7860)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos escrever todo o c√≥digo juntos</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from fastapi import FastAPI, HTTPException<br>from pydantic import BaseModel<br>do huggingface_hub import InferenceClient<br></code></pre></div>
            </section>
      <p>markdown</p>
      <p>from langchain_core.messages import HumanMessage, AIMessage</p>
      <div class='highlight'><pre><code>from langgraph.checkpoint.memory import MemorySaver
      from langgraph.graph import START, MessagesState, StateGraph
      
      import os
      from dotenv import load_dotenv
      load_dotenv()
      
      # Token da HuggingFace
      HUGGINGFACE_TOKEN = os.environ.get("HUGGINGFACE_TOKEN", os.getenv("HUGGINGFACE_TOKEN"))
      
      # Inicialize o modelo do HuggingFace
      model = InferenceClient(
      model="Qwen/Qwen2.5-72B-Instruct",
      api_key=os.getenv("HUGGINGFACE_TOKEN")
      )
      
      # Defina a fun√ß√£o que chama o modelo
      def chamar_modelo(estado: EstadoMensagens):
      """"""
      Chame o modelo com as mensagens fornecidas
      
      Argumentos:
      estado: MensagensState
      
      Retorna:
      um dicion√°rio contendo o texto gerado e o ID do thread
      """"""
      # Converter mensagens do LangChain para o formato do HuggingFace
      hf_messages = []
      for msg in state["messages"]:
      if isinstance(msg, HumanMessage):
      hf_messages.append({opening_brace}"role": "user", "content": msg.content{closing_brace})
      elif isinstance(msg, AIMessage):
      hf_messages.append({opening_brace}"role": "assistant", "content": msg.content{closing_brace})
          
      # Chame a API
      resposta = modelo.completar_chat(
      mensagens=hf_mensagens,
      temperature=0.5,
      max_tokens=64,
      top_p=0,7
      )
          
      # Converter a resposta para o formato LangChain</code></pre></div>
      <p>markdown</p>
      <p>ai_message = AIMessage(content=response.choices[0].message.content)</p>
      <div class='highlight'><pre><code>return {opening_brace}"messages": state["messages"] + [ai_message]{closing_brace}
      
      # Definir o gr√°fico
      workflow = StateGraph(state_schema=MessagesState)
      
      # Defina o n√≥ no grafo
      workflow.add_edge(START, "model")
      workflow.add_node("modelo", call_model)
      
      # Adicionar mem√≥ria
      memory = MemorySaver()
      graph_app = workflow.compile(checkpointer=memory)
      
      # Defina o modelo de dados para o pedidoclass QueryRequest(BaseModel):
      query: str
      thread_id: str = "padr√£o"
      
      # Criar a aplica√ß√£o FastAPI
      app = FastAPI(title="LangChain FastAPI", description="API para gerar texto usando LangChain e LangGraph")
      
      # Ponto de entrada Bem-vindo
      @app.get("/")
      async def api_home():
      """Ponto de entrada Welcome"""
      return {opening_brace}"detalhe": "Bem-vindo ao tutorial de FastAPI, Langchain, Docker"{closing_brace}
      
      # Gerar ponto final
      @app.post("/gerar")
      async def generate(request: QueryRequest):
      """
      Ponto final para gerar texto usando o modelo de linguagem
          
      Argumentos:
      solicita√ß√£o: QueryRequest
      query: str
      thread_id: str = "padr√£o"
      
      Retorna:
      um dicion√°rio contendo o texto gerado e o ID do fio
      """
      tente:
      # Configurar o ID da thread
      config = {opening_brace}"configurable": {opening_brace}"thread_id": request.thread_id{closing_brace}{closing_brace}
              
      # Criar a mensagem de entrada
      input_messages = [HumanMessage(content=request.query)]
              
      # Invocar o gr√°fico
      output = graph_app.invoke({opening_brace}"messages": input_messages{closing_brace}, config)
              
      # Obter a resposta do modelo
      resposta = output["messages"][-1].conte√∫do
              
      return {opening_brace}
      "generated_text": resposta,
      "thread_id": request.thread_id
      {closing_brace}
      except Exception as e:
      raise HTTPException(status_code=500, detail=f"Erro ao gerar texto: {opening_brace}str(e){closing_brace}")
      
      if __name__ == "__main__":
      import uvicorn
      uvicorn.run(app, host="0.0.0.0", port=7860)</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dockerfile">Dockerfile<a class="anchor-link" href="#Dockerfile"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 92" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora vemos como criar o Dockerfile</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro indicamos a partir de qual imagem vamos come√ßar</p>
      <p>``` dockerfile</p>
      <p>FROM python:3.13-slim```</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos o diret√≥rio de trabalho</p>
      <div class='highlight'><pre><code class="language-dockerfile">RUN useradd -m -u 1000 user
      WORKDIR /app</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Copiamos o arquivo com as depend√™ncias e instalamos</p>
      <div class='highlight'><pre><code class="language-dockerfile">COPY --chown=user ./requirements.txt requirements.txt
      RUN pip install --no-cache-dir --upgrade -r requirements.txt</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Copiamos o resto do c√≥digo</p>
      <p>``` dockerfile</p>
      <p>COPY --chown=user . /app```</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Exponhamos o porto 7860</p>
      <p>``` dockerfile</p>
      <p>EXPOSE 7860```</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos as vari√°veis de ambiente</p>
      <div class='highlight'><pre><code class="language-dockerfile">RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \
      test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Segredo existe!"</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, indicamos o comando para executar a aplica√ß√£o</p>
      <p>``` dockerfile</p>
      <p>CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]```</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora colocamos tudo junto</p>
      <div class='highlight'><pre><code class="language-dockerfile">FROM python:3.13-slim
      
      RUN useradd -m -u 1000 user
      WORKDIR /app
      
      COPY --chown=user ./requirements.txt requirements.txt
      RUN pip install --no-cache-dir --upgrade -r requirements.txt
      
      COPY --chown=user . /app
      
      EXPOSE 7860
      
      RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \
      test -f /run/secrets/HUGGINGFACE_TOKEN &amp;&amp; echo "Segredo existe!"
      
      CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 93" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o arquivo com as depend√™ncias</p>
      <div class='highlight'><pre><code class="language-txt">fastapi
      uvicorn
      pedidos
      pydantic&gt;=2.0.0
      langchainlangchain-huggingface
      langchain-core
      langgraph &gt; 0.2.27
      python-dotenv.2.11</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="README.md">README.md<a class="anchor-link" href="#README.md"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 94" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por fim, criamos o arquivo README.md com informa√ß√µes sobre o espa√ßo e as instru√ß√µes para o HuggingFace.</p>
      <div class='highlight'><pre><code class="language-md">---
      t√≠tulo: Backend do SmolLM2
      emoji: üìä
      colorFrom: amarelo
      colorTo: vermelho
      sdk: docker
      pinned: false
      licen√ßa: apache-2.0
      short_description: Backend do chat SmolLM2
      app_port: 7860
      ---
      
      # Backend do SmolLM2
      
      Este projeto implementa uma API FastAPI que usa LangChain e LangGraph para gerar texto com o modelo Qwen2.5-72B-Instruct do HuggingFace.
      
      ## Configura√ß√£o
      
      ### No HuggingFace Spaces
      
      Este projeto est√° projetado para ser executado em HuggingFace Spaces. Para configur√°-lo:
      
      1. Crie um novo Espa√ßo no HuggingFace com o SDK Docker
      2. Configure a vari√°vel de ambiente `HUGGINGFACE_TOKEN` ou `HF_TOKEN` na configura√ß√£o do Space:
      - V√° para a aba "Configura√ß√µes" do seu Espa√ßo
      - Role para a se√ß√£o "Secrets do reposit√≥rio"
      - Adicione uma nova vari√°vel com o nome `HUGGINGFACE_TOKEN` e seu token como valor
      - Salve as altera√ß√µes
      
      ### Desenvolvimento local
      
      Para o desenvolvimento local:
      
      1. Clone este reposit√≥rio
      2. Crie um arquivo `.env` na raiz do projeto com seu token do HuggingFace:</code></pre></div>
      <div class='highlight'><pre><code>HUGGINGFACE_TOKEN=seu_token_aqui</code></pre></div>
      <div class='highlight'><pre><code>3. Instale as depend√™ncias:</code></pre></div>
      <div class='highlight'><pre><code>pip install -r requirements.txt</code></pre></div>
      <p>Por favor, forne√ßa o texto em Markdown que voc√™ gostaria de traduzir para o portugu√™s.</p>
      <div class='highlight'><pre><code>## Execu√ß√£o local
      
      ``bash
      uvicorn app:app --reload</code></pre></div>
      <p>Por favor, forne√ßa o texto em Markdown que voc√™ gostaria de traduzir para o portugu√™s.</p>
      <div class='highlight'><pre><code>A API estar√° dispon√≠vel em `http://localhost:8000`.
      
      ## Endpoints
      
      ### GET `/`
      
      Ponto final de boas-vindas que retorna uma mensagem de sauda√ß√£o.
      
      ### POST `/gerar`
      
      Ponto final para gerar texto usando o modelo de linguagem.
      
      **Par√¢metros da solicita√ß√£o:**
      ``json
      {opening_brace}
      "query": "Sua pergunta aqui",
      "thread_id": "identificador_de_thread_opcional"
      {closing_brace}</code></pre></div>
      <p>Por favor, forne√ßa o texto em markdown que voc√™ gostaria de traduzir para o portugu√™s.</p>
      <div class='highlight'><pre><code>**Resposta:**
      ``json```
      {opening_brace}
      "Texto gerado pelo modelo"
      "thread_id": "identificador do thread"
      {closing_brace}</code></pre></div>
      <p>Por favor, forne√ßa o texto em Markdown que deseja traduzir para o portugu√™s.</p>
      <div class='highlight'><pre><code>## Docker
      
      Para executar a aplica√ß√£o em um cont√™iner Docker:
      
      ``bash
      # Construa a imagem
      docker build -t smollm2-backend .
      
      # Executar o cont√™iner
      docker run -p 8000:8000 --env-file .env smollm2-backend</code></pre></div>
      <div class='highlight'><pre><code>## Documenta√ß√£o da API
      
      A documenta√ß√£o interativa da API est√° dispon√≠vel em:
      - Swagger UI: `http://localhost:8000/docs`
      - ReDoc: `http://localhost:8000/redoc`</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Token do HuggingFace">Token do HuggingFace<a class="anchor-link" href="#Token do HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 95" src={svg_paths.link_svg_path}/></a></h3>
      <p>Se voc√™ notou no c√≥digo e no Dockerfile, usamos um token do HuggingFace, ent√£o vamos ter que criar um. Em nossa conta do HuggingFace, criamos um <a href="https://huggingface.co/settings/tokens/new?tokenType=fineGrained" target="_blank" rel="nofollow noreferrer">novo token</a>, damos um nome a ele e concedemos as seguintes permiss√µes:</p>
      <ul>
        <li>Acesso de leitura aos conte√∫dos de todos os reposit√≥rios sob o seu namespace pessoal</li>
        <li>Acesso de leitura aos conte√∫dos de todos os reposit√≥rios sob seu namespace pessoal</li>
        <li>Fazer chamadas para provedores de infer√™ncia</li>
        <li>Fazer chamadas para Pontos de Extremidade de Infer√™ncia</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-token.webp" alt="backend docker - token">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Adicionar o token aos secrets do espaco">Adicionar o token aos secrets do espa√ßo<a class="anchor-link" href="#Adicionar o token aos secrets do espaco"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 96" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que j√° temos o token, precisamos adicion√°-lo ao espa√ßo. Na parte superior do aplicativo, poderemos ver um bot√£o chamado <code>Settings</code>, clicamos nele e poderemos ver a se√ß√£o de configura√ß√£o do espa√ßo.</p>
      <p>Se formos para baixo, poderemos ver uma se√ß√£o onde podemos adicionar <code>Variables</code> e <code>Secrets</code>. Neste caso, como estamos adicionando um token, vamos adicion√°-lo aos <code>Secrets</code>.</p>
      <p>Damos o nome <code>HUGGINGFACE_TOKEN</code> e o valor do token.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Implantacao">Implanta√ß√£o<a class="anchor-link" href="#Implantacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 97" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se n√≥s clonamos o espa√ßo, temos que fazer um commit e um push. Se modificamos os arquivos no HuggingFace, basta salv√°-los.</p>
      <p>Ent√£o, quando as altera√ß√µes estiverem no HuggingFace, teremos que esperar alguns segundos para que o espa√ßo seja constru√≠do e possamos us√°-lo.</p>
      <p>Neste caso, constru√≠mos apenas um backend, portanto o que vamos ver ao entrar no espa√ßo √© o que definimos no endpoint <code>/</code></p>
      <p>!<a href="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-space.webp" target="_blank" rel="nofollow noreferrer">backend docker - espa√ßo</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="URL do backend">URL do backend<a class="anchor-link" href="#URL do backend"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 98" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Precisamos saber a URL do backend para poder fazer chamadas √† API. Para isso, temos que clicar nos tr√™s pontos no canto superior direito para ver as op√ß√µes.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-options.webp" alt="backend docker - op√ß√µes">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No menu suspenso, clicamos em <code>Embed this Spade</code>. Ser√° aberta uma janela indicando como incorporar o espa√ßo com um iframe e tamb√©m fornecer√° a URL do espa√ßo.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-embed.webp" alt="backend docker - embed">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se agora formos para essa URL, veremos o mesmo que no espa√ßo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Documentacao">Documenta√ß√£o<a class="anchor-link" href="#Documentacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 99" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>FastAPI, al√©m de ser uma API extremamente r√°pida, tem outra grande vantagem: gera documenta√ß√£o automaticamente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se adicionarmos <code>/docs</code> √† URL que vimos anteriormente, poderemos visualizar a documenta√ß√£o da API com o <code>Swagger UI</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-swagger-doc.webp" alt="backend docker - swagger doc">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m podemos adicionar <code>/redoc</code> √† URL para ver a documenta√ß√£o com <code>ReDoc</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-redoc.webp" alt="backend docker - redoc doc">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Teste da API">Teste da API<a class="anchor-link" href="#Teste da API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 100" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O bom da documenta√ß√£o <code>Swagger UI</code> √© que nos permite testar a API diretamente do navegador.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Adicionamos <code>/docs</code> √† URL que obtivemos, abrimos o menu suspenso do endpoint <code>/generate</code> e clicamos em <code>Try it out</code>, modificamos o valor da <code>query</code> e do <code>thread_id</code> e clicamos em <code>Execute</code>.</p>
      <p>No primeiro caso vou colocar</p>
      <ul>
        <li><strong>query</strong>: Ol√°, como voc√™ est√°? Sou M√°ximo</li>
        <li><strong>thread_id</strong>: user1</li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-test-API.webp" alt="backend docker - test API">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Recebemos a seguinte resposta <code>Ol√° M√°ximo! Estou muito bem, obrigado por perguntar. Como voc√™ est√°? Em que posso ajudar hoje?</code></p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user1.webp" alt="backend docker -response 1 - user1">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos testar agora a mesma pergunta, mas com um <code>thread_id</code> diferente, neste caso <code>user2</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-query1-user2.webp" alt="backend docker - query 1 - user2">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>E nos responde isso <code>Ol√° Luis! Estou muito bem, obrigado por perguntar. Como voc√™ est√°? No que posso ajudar hoje?</code></p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user2.webp" alt="backend docker - response 1 - user2">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora pedimos nosso nome com os dois usu√°rios e obtemos isso</p>
      <ul>
        <li>Para o usu√°rio <strong>user1</strong>: <code>Voc√™ se chama M√°ximo. H√° algo mais em que eu possa ajudar voc√™?</code></li>
        <li>Para o usu√°rio <strong>user2</strong>: <code>Voc√™ se chama Luis. H√° mais alguma coisa em que eu possa ajud√°-lo hoje, Luis?</code></li>
      </ul>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user1.webp" alt="backend docker - response 2 - user1">
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user2.webp" alt="backend docker - response 2 - user2">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Deploy do backend com Gradio e modelo rodando no servidor">Deploy do backend com Gradio e modelo rodando no servidor<a class="anchor-link" href="#Deploy do backend com Gradio e modelo rodando no servidor"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 101" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os dois backends que criamos na verdade n√£o est√£o executando um modelo, mas sim fazendo chamadas para Inference Endpoints da HuggingFace. Mas pode ser que queiramos que tudo rode no servidor, inclusive o modelo. Pode ser que voc√™ tenha feito um fine-tuning de um LLM para seu caso de uso, por isso j√° n√£o pode fazer chamadas para Inference Endpoints.</p>
      <p>Ent√£o vamos ver como modificar o c√≥digo dos dois backends para executar um modelo no servidor e n√£o fazer chamadas para Inference Endpoints.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Criar Espaco">Criar Espa√ßo<a class="anchor-link" href="#Criar Espaco"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 102" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Na hora de criar o space no HuggingFace fazemos o mesmo que antes, criamos um novo space, colocamos um nome e uma descri√ß√£o, selecionamos Gradio como SDK, selecionamos o HW em que vamos deployar, no meu caso selecionei o HW mais b√°sico e gratuito, e selecionamos se o faremos privado ou p√∫blico.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">C√≥digo<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 103" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Temos que fazer altera√ß√µes em <code>app.py</code> e em <code>requirements.txt</code> para que, em vez de fazer chamadas a Inference Endpoints, o modelo seja executado localmente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 104" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As mudan√ßas que temos que fazer s√£o</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importar <code>AutoModelForCausalLM</code> e <code>AutoTokenizer</code> da biblioteca <code>transformers</code> e importar <code>torch</code></p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer<br>import torch</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Em vez de criar um modelo com <code>InferenceClient</code>, criamos com <code>AutoModelForCausalLM</code> e <code>AutoTokenizer</code>.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Carregar o modelo e o tokenizer<br>model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"<br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>model = AutoModelForCausalLM.from_pretrained(<br>nome_do_modelo,<br>torch_dtype=torch.float16,<br>device_map="auto"<br>)</code></pre></div>
            </section>
      <p>Utilizo <code>HuggingFaceTB/SmolLM2-1.7B-Instruct</code> porque √© um modelo bastante capaz com apenas 1.7B de par√¢metros. Como escolhi o hardware mais b√°sico, n√£o posso usar modelos muito grandes. Voc√™, se quiser usar um modelo maior, tem duas op√ß√µes: usar o hardware gratuito e aceitar que a infer√™ncia ser√° mais lenta, ou usar um hardware mais potente, mas pago.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Modificar a fun√ß√£o <code>respond</code> para que construa o prompt com a estrutura necess√°ria pela biblioteca <code>transformers</code>, tokenizar o prompt, fazer a infer√™ncia e destokenizar a resposta.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">def responder(<br>mensagem,<br>hist√≥rico: list[tuple[str, str]],<br>Mensagem do sistema,<br>max_tokens,<br>temperatura,<br>top_p,<br>):<br># Construir o prompt com o formato correto<br>prompt = f"&lt;|system|&gt;\n{opening_brace}system_message{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>for val in history:<br>if val[0]:<br>prompt += f"&lt;|user|&gt;\n{opening_brace}val[0]{closing_brace}&lt;/s&gt;\n"<br>if val[1]:<br>prompt += f"&lt;|assistant|&gt;\n{opening_brace}val[1]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>prompt += f"&lt;|user|&gt;\n{opening_brace}message{closing_brace}&lt;/s&gt;\n&lt;|assistant|&gt;\n"<br>&#x20;&#x20;<br># Tokenizar o prompt<br>inputs = tokenizer(prompt, return_tensors="pt").to(model.device)<br>&#x20;&#x20;<br># Gerar a resposta<br>outputs = model.generate(<br>**entradas,**<br>max_new_tokens=max_tokens,<br>temperature=temperature,<br>top_p=top_p,<br>do_sample=True,<br>pad_token_id=tokenizer.eos_token_id<br>)<br>&#x20;&#x20;<br># Decodificar a resposta<br>response = tokenizer.decode(outputs[0], skip_special_tokens=True)<br>&#x20;&#x20;<br># Extrair apenas a parte da resposta do assistente<br>response = response.split("&lt;|assistant|&gt;\n")[-1].strip()<br>&#x20;&#x20;<br>yield response</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A seguir deixo todo o c√≥digo</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">import gradio as gr<br>from transformers import AutoModelForCausalLM, AutoTokenizer<br>import torch<br><br>""""""<br>Para mais informa√ß√µes sobre o suporte √† API de Infer√™ncia do `huggingface_hub`, consulte a documenta√ß√£o: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference<br>""""""<br><br># Carregar o modelo e o tokenizer<br>model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"<br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>model = AutoModelForCausalLM.from_pretrained(<br>nome_do_modelo,<br>torch_dtype=torch.float16,<br>device_map="auto"<br>)<br><br>def responder(<br>mensagem,<br>hist√≥ria: list[tuple[str, str]],<br>Mensagem do sistema,<br>max_tokens,<br>temperatura,<br>top_p,<br>):<br># Construir o prompt com o formato correto<br>prompt = f"&lt;|system|&gt;\n{opening_brace}system_message{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>for val in history:<br>if val[0]:<br>prompt += f"&lt;|user|&gt;\n{opening_brace}val[0]{closing_brace}&lt;/s&gt;\n"<br>if val[1]:<br>prompt += f"&lt;|assistant|&gt;\n{opening_brace}val[1]{closing_brace}&lt;/s&gt;\n"<br>&#x20;&#x20;<br>prompt += f"&lt;|user|&gt;\n{opening_brace}message{closing_brace}&lt;/s&gt;\n&lt;|assistant|&gt;\n"<br>&#x20;&#x20;<br># Tokenizar o prompt<br>inputs = tokenizer(prompt, return_tensors="pt").to(model.device)<br>&#x20;&#x20;<br># Gerar a resposta<br>sa√≠das = modelo.gerar(<br>**entradas,**<br>max_new_tokens=max_tokens,<br>temperature=temperature,<br>top_p=top_p,<br>do_sample=True,<br>pad_token_id=tokenizer.eos_token_id<br>)<br>&#x20;&#x20;<br># Decodificar a resposta<br>response = tokenizer.decode(outputs[0], skip_special_tokens=True)<br>&#x20;&#x20;<br># Extrair apenas a parte da resposta do assistente<br>response = response.split("&lt;|assistant|&gt;\n")[-1].strip()<br>&#x20;&#x20;<br>yield response<br><br><br>""""""<br>Para informa√ß√µes sobre como personalizar o ChatInterface, consulte a documenta√ß√£o do Gradio: https://www.gradio.app/docs/gradio/chatinterface<br>""""""<br>demo = gr.ChatInterface(<br>responda,</code></pre></div>
            </section>
      <p>markdown</p>
      <p>additional_inputs=[</p>
      <div class='highlight'><pre><code>gr.Textbox(
      value="Voc√™ √© um Chatbot amig√°vel. Sempre responda na l√≠ngua em que o usu√°rio est√° escrevendo para voc√™."
      r√≥tulo="Mensagem do sistema"
      ),
      gr.Slider(minimum=1, maximum=2048, value=512, step=1, label="M√°ximo de novos tokens"),
      gr.Slider(m√≠nimo=0,1, m√°ximo=4,0, valor=0,7, passo=0,1, r√≥tulo="Temperatura"),
      gr.Slider()
      m√≠nimo=0.1,
      m√°ximo=1.0,
      value=0.95,
      passo=0.05,
      label="Top-p (amostragem do n√∫cleo)"
      ),
      ],
      )
      
      
      if __name__ == "__main__":
      demo.launch()</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 105" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Neste arquivo, devemos adicionar as novas bibliotecas que vamos utilizar, neste caso <code>transformers</code>, <code>accelerate</code> e <code>torch</code>. O arquivo completo ficaria:</p>
      <div class='highlight'><pre><code class="language-txt">huggingface_hub==0.25.2
      gradio&gt;=4.0.0
      transformers&gt;=4.36.0torch&gt;=2.0.0
      accelerate&gt;=0.25.0</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Teste da API">Teste da API<a class="anchor-link" href="#Teste da API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 106" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Desplegamos o space e testamos diretamente a API.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gradio_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span>',
      '<span class="w"> </span>',
      '<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;Maximofn/SmolLM2_localModel&quot;</span><span class="p">)</span>',
      '<span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">message</span><span class="o">=</span><span class="s2">&quot;Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">system_message</span><span class="o">=</span><span class="s2">&quot;You are a friendly Chatbot. Always reply in the language in which the user is writing to you.&quot;</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>',
      '<span class="w">	</span><span class="w">	</span><span class="n">api_name</span><span class="o">=</span><span class="s2">&quot;/chat&quot;</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî',
          'Hola M√°ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en d√≠a. ¬øC√≥mo puedo servirte?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Surpreende-me o qu√£o r√°pido o modelo responde, mesmo estando em um servidor sem GPU.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Deploy de backend com FastAPI, Langchain e Docker e modelo rodando no servidor">Deploy de backend com FastAPI, Langchain e Docker e modelo rodando no servidor<a class="anchor-link" href="#Deploy de backend com FastAPI, Langchain e Docker e modelo rodando no servidor"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 107" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora fazemos o mesmo que antes, mas com FastAPI, LangChain e Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Criar Espaco">Criar Espa√ßo<a class="anchor-link" href="#Criar Espaco"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 108" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ao criar o space no HuggingFace, fazemos o mesmo que antes: criamos um novo espa√ßo, colocamos um nome e uma descri√ß√£o, selecionamos Docker como SDK, escolhemos o HW em que vamos implant√°-lo, no meu caso, escolho o HW mais b√°sico e gratuito, e decidimos se o faremos privado ou p√∫blico.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codigo">C√≥digo<a class="anchor-link" href="#Codigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 109" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="app.py">app.py<a class="anchor-link" href="#app.py"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 110" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>J√° n√£o importamos <code>InferenceClient</code> e agora importamos <code>AutoModelForCausalLM</code> e <code>AutoTokenizer</code> da biblioteca <code>transformers</code> e importamos <code>torch</code>.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer<br>import torch</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos o modelo e o tokenizer com <code>AutoModelForCausalLM</code> e <code>AutoTokenizer</code>.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Inicialize o modelo e o tokenizador<br>print("Carregando modelo e tokenizer...")<br>dispositivo = "cuda" if torch.cuda.is_available() else "cpu"<br>model_name = "HuggingFaceTB/SmolLM2-1.7B-Instruct"<br><br>tente:<br># Carregar o modelo no formato BF16 para melhor desempenho e menor uso de mem√≥ria<br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>&#x20;&#x20;<br>if device == "cuda":<br>print("Usando GPU para o modelo...")<br>model = AutoModelForCausalLM.from_pretrained(<br>nome_do_modelo,<br>torch_dtype=torch.bfloat16,<br>device_map="auto",<br>low_cpu_mem_usage=True)<br>else:<br>print("Usando CPU para o modelo...")<br>model = AutoModelForCausalLM.from_pretrained(<br>nome_do_modelo,<br>device_map={opening_brace}"": device{closing_brace},<br>torch_dtype=torch.float32<br>)<br><br>print(f"Modelo carregado com sucesso em: {opening_brace}device{closing_brace}")<br>except Exception as e:<br>print(f"Erro ao carregar o modelo: {opening_brace}str(e){closing_brace}")<br>aumentar</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Re definimos a fun√ß√£o <code>call_model</code> para que fa√ßa a infer√™ncia com o modelo local.</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python"># Defina a fun√ß√£o que chama o modelo<br>def chamar_modelo(estado: EstadoMensagens):<br>"""<br>Chame o modelo com as mensagens fornecidas<br><br>Argumentos:<br>estado: EstadoMensagens<br><br>Retorna:<br>dicion√°rio: Um dicion√°rio contendo o texto gerado e o ID do thread<br>""""""<br># Converter mensagens LangChain para formato de bate-papo<br>mensagens = []<br>for msg in state["messages"]:<br>if isinstance(msg, HumanMessage):<br>messages.append({opening_brace}"role": "user", "content": msg.content{closing_brace})<br>elif isinstance(msg, AIMessage):<br>messages.append({opening_brace}"role": "assistant", "content": msg.content{closing_brace})<br>&#x20;&#x20;<br># Prepare o input usando o modelo de bate-papo<br>input_text = tokenizer.apply_chat_template(messages, tokenize=False)<br>inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)<br>&#x20;&#x20;<br># Gerar resposta<br>sa√≠das = modelo.gerar(<br>entradas,<br>max_new_tokens=512,  # Aumente o n√∫mero de tokens para respostas mais longas<br>temperature=0.7,<br>top_p=0,9,<br>do_sample=True,<br>pad_token_id=tokenizer.eos_token_id<br>)<br>&#x20;&#x20;<br># Decodificar e limpar a resposta<br>response = tokenizer.decode(outputs[0], skip_special_tokens=True)<br># Extrair apenas a resposta do assistente (ap√≥s a √∫ltima mensagem do usu√°rio)<br>response = response.split("Assistant:")[-1].strip()<br>&#x20;&#x20;<br># Converter a resposta para o formato LangChain</code></pre></div>
            </section>
      <p>markdown</p>
      <p>ai_message = AIMessage(content=response)</p>
      <div class='highlight'><pre><code>return {opening_brace}"messages": state["messages"] + [ai_message]{closing_brace}</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="requirements.txt">requirements.txt<a class="anchor-link" href="#requirements.txt"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 111" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Temos que remover <code>langchain-huggingface</code> e adicionar <code>transformers</code>, <code>accelerate</code> e <code>torch</code> no arquivo <code>requirements.txt</code>. O arquivo ficaria:</p>
      <div class='highlight'><pre><code class="language-txt">fastapi
      uvicorn
      solicita√ß√µes
      pydantic&gt;=2.0.0
      langchain&gt;=0.1.0
      langchain-core&gt;=0.1.10langgraph&gt;=0.2.27
      python-dotenv&gt;=1.0.0
      transformers&gt;=4.36.0
      torch&gt;=2.0.0
      accelerate&gt;=0.26.0</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dockerfile">Dockerfile<a class="anchor-link" href="#Dockerfile"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 112" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>J√° n√£o precisamos ter <code>RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true</code> porque como o modelo vai estar no servidor e n√£o vamos fazer chamadas para Inference Endpoints, n√£o precisamos do token. O arquivo ficaria:</p>
      <div class='highlight'><pre><code class="language-dockerfile">FROM python:3.13-slim
      
      RUN useradd -m -u 1000 user
      WORKDIR /app
      
      COPY --chown=user ./requirements.txt requirements.txt
      RUN pip install --no-cache-dir --upgrade -r requirements.txt
      
      COPY --chown=user . /app
      
      EXPOSE 7860
      
      CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "7860"]</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Teste da API">Teste da API<a class="anchor-link" href="#Teste da API"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 113" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Deployamos o space e testamos a API. Neste caso, vou testar diretamente do Python.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>',
      '<span class="w"> </span>',
      '<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://maximofn-smollm2-backend-localmodel.hf.space/generate&quot;</span>',
      '<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>',
      '<span class="w">    </span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;Hola, ¬øc√≥mo est√°s?&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;thread_id&quot;</span><span class="p">:</span> <span class="s2">&quot;user1&quot;</span>',
      '<span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>',
      '<span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>',
      '<span class="w">    </span><span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Respuesta:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;generated_text&quot;</span><span class="p">])</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Thread ID:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;thread_id&quot;</span><span class="p">])</span>',
      '<span class="k">else</span><span class="p">:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error:&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Respuesta: system',
          'You are a friendly Chatbot. Always reply in the language in which the user is writing to you.',
          'user',
          'Hola, ¬øc√≥mo est√°s?',
          'assistant',
          'Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.',
          'Thread ID: user1',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Este demora um pouco mais que o anterior. Na verdade, demora o normal para um modelo sendo executado em um servidor sem GPU. O estranho √© quando o deployamos no Gradio. N√£o sei o que a HuggingFace faz por tr√°s, ou talvez tenha sido coincid√™ncia.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Conclusoes">Conclus√µes<a class="anchor-link" href="#Conclusoes"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 114" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vimos como criar um backend com um LLM, tanto fazendo chamadas ao Inference Endpoint da HuggingFace, quanto fazendo chamadas a um modelo rodando localmente. Vimos como fazer isso com Gradio ou com FastAPI, Langchain e Docker.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A partir daqui voc√™ tem o conhecimento para poder implantar seus pr√≥prios modelos, mesmo que n√£o sejam LLMs, podem ser modelos multimodais. A partir daqui voc√™ pode fazer o que quiser.</p>
      </section>







    </div>

  </section>

</PostLayout>
