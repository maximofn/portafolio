---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'DoLa ‚Äì Decoding by Contrasting Layers Improves Factuality in Large Language Models';
const end_url = 'dola';
const description = 'Voc√™ j√° conversou com um LLM e ele lhe respondeu algo que parece ter bebido caf√© de m√°quina a noite toda? üòÇ Isso √© o que chamamos de alucina√ß√£o no mundo dos LLMs! Mas n√£o se preocupe, pois n√£o √© que seu modelo de linguagem esteja louco (embora √†s vezes possa parecer isso ü§™). A verdade √© que os LLMs podem ser um pouco... criativos quando se trata de gerar texto. Mas gra√ßas ao DoLa, um m√©todo que usa camadas de contraste para melhorar a viabilidade dos LLMs, podemos evitar que nossos modelos de linguagem se transformem em escritores de fic√ß√£o cient√≠fica üòÇ. Nesta publica√ß√£o, explicarei como o DoLa funciona e mostrarei um exemplo de c√≥digo para que voc√™ possa entender melhor como tornar seus LLMs mais confi√°veis e menos propensos a inventar hist√≥rias. Vamos salvar nossos LLMs da loucura e torn√°-los mais √∫teis! üöÄ';
const keywords = 'dola, decodifica√ß√£o por camadas contrastantes, factibilidade, grandes modelos de linguagem, transformers, hugging face, nlp, processamento de linguagem natural, aprendizado de m√°quina, intelig√™ncia artificial';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-08-01+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Metodo"><h2>M√©todo</h2></a>
      <a class="anchor-link" href="#Selecao dinamica da camada prematura"><h2>Sele√ß√£o din√¢mica da camada prematura</h2></a>
      <a class="anchor-link" href="#Contraste das previsoes"><h2>Contraste das previs√µes</h2></a>
      <a class="anchor-link" href="#Penalizacao por repeticao"><h2>Penaliza√ß√£o por repeti√ß√£o</h2></a>
      <a class="anchor-link" href="#Implementacao com transformers"><h2>Implementa√ß√£o com transformers</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A medida que os LLMs aumentam de tamanho e surgem novas capacidades, temos um problema: as alucina√ß√µes. Os autores do artigo <a href="https://arxiv.org/abs/2309.03883" target="_blank" rel="nofollow noreferrer">DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</a> prop√µem um m√©todo para evitar esse problema.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Prop√µem uma abordagem de decodifica√ß√£o contrastiva, onde a probabilidade de sa√≠da da pr√≥xima palavra √© obtida a partir da diferen√ßa nos logits entre uma camada superior e uma inferior. Ao enfatizar o conhecimento das camadas superiores e diminuir a import√¢ncia das inferiores, podemos fazer com que os LM sejam mais f√°ticos e, portanto, reduzir as alucina√ß√µes.</p>
      <p>Na figura a seguir, essa ideia √© ilustrada. Enquanto <code>Seattle</code> mant√©m uma alta probabilidade em todas as camadas, a probabilidade da resposta correta <code>Olympia</code> aumenta ap√≥s as camadas superiores injetarem mais conhecimento factual. Contrastar as diferen√ßas entre as diferentes camadas pode revelar a resposta correta neste caso.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure1.webp" alt="DoLa-figure1">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Metodo">M√©todo<a class="anchor-link" href="#Metodo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Um LLM consiste em uma camada de embedding, v√°rios transformers sequenciais e, em seguida, uma camada de sa√≠da. O que prop√µem √© medir a sa√≠da de cada transformer por meio da diverg√™ncia de Jensen-Shannon (JSD).</p>
      <p>Na figura seguinte, pode-se ver essa medida na sa√≠da de cada transformer para uma frase de entrada no LLM. Cada coluna corresponde a um token da frase.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure2.webp" alt="DoLa-figure2">
      <p>Podem observar dois padr√µes</p>
      <ul>
        <li>O primeiro ocorre quando s√£o previstas entidades de nome ou datas importantes, como <code>Wole Soyinka</code> e <code>1986</code>, que requerem conhecimento f√°tico. Pode-se ver que a JSD calculada continua extremamente alta nas camadas superiores. Este padr√£o indica que o modelo ainda est√° mudando suas previs√µes nas √∫ltimas camadas, e potencialmente injetando mais conhecimento f√°tico nas previs√µes</li>
      </ul>
      <ul>
        <li>O segundo ocorre quando se preveem palavras funcionais, como <code>was</code>, <code>the</code>, <code>to</code>, <code>in</code>, e os tokens copiados da pergunta de entrada, como <code>first Nigerian</code>, <code>Nobel Prize</code>. Quando esses tokens "f√°ceis" s√£o previstos, podemos observar que a JSD torna-se muito pequena a partir das camadas intermedi√°rias. Essa descoberta indica que o modelo j√° decidiu qual token gerar nas camadas intermedi√°rias e mant√©m as distribui√ß√µes de sa√≠da quase inalteradas nas camadas superiores. Essa descoberta tamb√©m √© consistente com as suposi√ß√µes em LLMs de sa√≠da precoce <code>Schuster et al., 2022</code></li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando a previs√£o da pr√≥xima palavra requer conhecimento f√°tico, o LLM parece alterar as previs√µes nas camadas superiores. Contrastar as camadas antes e depois de uma mudan√ßa s√∫bita pode, portanto, amplificar o conhecimento que emerge das camadas superiores e fazer com que o modelo se baseie mais em seu conhecimento interno f√°tico. Al√©m disso, essa evolu√ß√£o da informa√ß√£o parece variar de um token para outro</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Seu m√©todo requer selecionar com precis√£o a camada prematura que cont√©m informa√ß√µes plaus√≠veis mas menos factuais, que pode n√£o estar sempre na mesma camada inicial. Portanto, eles prop√µem encontrar essa camada prematura atrav√©s de uma sele√ß√£o din√¢mica da camada prematura, como visto na imagem a seguir.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure3.webp" alt="DoLa-figure3">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Selecao dinamica da camada prematura">Sele√ß√£o din√¢mica da camada prematura<a class="anchor-link" href="#Selecao dinamica da camada prematura"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para selecionar a camada prematura, eles calculam a diverg√™ncia de Jensen-Shannon (JSD) entre as camadas intermedi√°rias e a final. A camada prematura √© selecionada como a camada com o JSD mais alto.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No entanto, como esse processo pode ser um pouco lento, o que fazem √© agrupar v√°rias camadas para fazer menos c√°lculos</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Contraste das previsoes">Contraste das previs√µes<a class="anchor-link" href="#Contraste das previsoes"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos a √∫ltima camada (camada madura) e a camada prematura, podemos contrastar as previs√µes de ambas camadas. Para isso, calculam a probabilidade logar√≠tmica do pr√≥ximo token na camada madura e na camada prematura. Em seguida, subtraem a probabilidade logar√≠tmica da camada prematura da da camada madura, assim dando mais import√¢ncia ao conhecimento da camada madura.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Penalizacao por repeticao">Penaliza√ß√£o por repeti√ß√£o<a class="anchor-link" href="#Penalizacao por repeticao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A motiva√ß√£o do DoLa √© diminuir a import√¢ncia do conhecimento lingu√≠stico das camadas inferiores e amplificar o conhecimento f√°tico do mundo real. No entanto, isso pode levar o modelo a gerar par√°grafos gramaticalmente incorretos.</p>
      <p>Empiricamente, eles n√£o observaram esse problema, mas encontraram que a distribui√ß√£o DoLa resultante √†s vezes tem uma maior tend√™ncia a repetir frases geradas anteriormente, especialmente durante a gera√ß√£o de longas sequ√™ncias de racioc√≠nio na cadeia de pensamento.</p>
      <p>Ent√£o, eles incluem uma penaliza√ß√£o por repeti√ß√£o introduzida em <code>Keskar et al. (2019)</code> com <code>Œ∏ = 1.2</code> durante a decodifica√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Implementacao com transformers">Implementa√ß√£o com transformers<a class="anchor-link" href="#Implementacao com transformers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como implementar DoLa com a biblioteca <code>transformers</code> do Hugging Face. Para obter mais informa√ß√µes sobre como aplicar DoLa com a biblioteca <code>transformers</code>, voc√™ pode consultar o seguinte <a href="https://huggingface.co/docs/transformers/main/en/generation_strategies#dola-decoding" target="_blank" rel="nofollow noreferrer">enlace</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro nos logamos no Hub, pois vamos usar o Llama 3 8B, para poder us√°-lo √© necess√°rio solicitar permiss√£o √† Meta, ent√£o para poder baix√°-lo √© preciso estar logado para que saibam quem est√° fazendo o download.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora instanciamos o tokenizador e o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Atribu√≠mos um valor fixo de semente para a reprodutibilidade do exemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Geramos os tokens de entrada para o LLM</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;What does Darth Vader say to Luke in &quot;The Empire Strikes Back&quot;?&#39;</span>',
      '<span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Answer with a short answer.</span><span class="se">\n\n</span><span class="s2">Question: </span><span class="si">{</span><span class="n">question</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Answer: &quot;</span>',
      '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Geramos agora a entrada vanilla, ou seja, sem aplicar DoLa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generate_kwargs</span><span class="o">=</span><span class="p">{</span>',
      '<span class="w">    </span><span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="kc">None</span>',
      '<span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">vanilla_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">vanilla_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&quot;No, I am your father.&quot; (Note: This is a famous misquote. The actual quote is &quot;No, I am your father&quot; is not in the movie. The correct quote is &quot;No, I am your father.&quot; is not',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que sabe que h√° um erro famoso, mas n√£o consegue dizer a frase correta.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora aplicando DoLa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dola_high_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span> <span class="n">dola_layers</span><span class="o">=</span><span class="s1">&#39;high&#39;</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">dola_high_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&quot;No, I am your father.&quot; (Note: This is one of the most famous lines in movie history, and it&#x27;s often misquoted as &quot;Luke, I am your father.&quot;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora ele consegue dizer a frase correta e o <a href="https://www.bbc.co.uk/bitesize/articles/zc38kty" target="_blank" rel="nofollow noreferrer">erro famoso</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos fazer outro teste com outro exemplo, reinicio o notebook e uso outro modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">&quot;huggyllama/llama-7b&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Atribu√≠mos um valor fixo de semente para a reprodu√ß√£o do exemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Claro, por favor, proporciona el texto markdown que deseas que traduzca al portug√©s.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;On what date was the Declaration of Independence officially signed?&quot;</span>',
      '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Geramos a sa√≠da vanilla</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generate_kwargs</span><span class="o">=</span><span class="p">{</span>',
      '<span class="w">    </span><span class="s2">&quot;do_sample&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;top_p&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="kc">None</span>',
      '<span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">vanilla_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">vanilla_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The Declaration of Independence was signed on July 4, 1776.',
          'What was the date of the signing of the Declaration of Independence?',
          'The Declaration of Independence was signed on July 4,',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, gera a sa√≠da incorretamente, pois embora seja celebrado em 4 de julho, na verdade foi assinada no <a href="https://www.nps.gov/inde/learn/historyculture/resources-declarationofindependence.htm" target="_blank" rel="nofollow noreferrer">2 de agosto</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a provar agora com DoLa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dola_high_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span> <span class="n">dola_layers</span><span class="o">=</span><span class="s1">&#39;high&#39;</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">dola_high_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'July 4, 1776. This is the most well-known date in U.S. history. The day has been celebrated with parades, barbeques, fireworks and festivals for hundreds of years.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Siga sem gerar uma sa√≠da correta, ent√£o vamos indicar que apenas contraste a camada final com as camadas 28 e 30</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dola_high_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span> <span class="n">dola_layers</span><span class="o">=</span><span class="p">[</span><span class="mi">28</span><span class="p">,</span><span class="mi">30</span><span class="p">],</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">dola_high_output</span><span class="p">[:,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'It was officially signed on 2 August 1776, when 56 members of the Second Continental Congress put their John Hancocks to the Declaration. The 2-page document had been written in 17',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora sim, consigo gerar a resposta correta.</p>
      </section>







    </div>

  </section>

</PostLayout>
