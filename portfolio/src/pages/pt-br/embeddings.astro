---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Embeddings';
const end_url = 'embeddings';
const description = 'Descubra o poder dos embeddings';
const keywords = 'embeddings, nlp, processamento de linguagem natural, transformers, huggingface, bert, word2vec, glove';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/embeddings.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2023-12-09+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Codificacao ordinal"><h2>Codificação ordinal</h2></a>
      <a class="anchor-link" href="#Codificacao one-hot"><h2>Codificação one-hot</h2></a>
      <a class="anchor-link" href="#Embeddings de palavras"><h2>Embeddings de palavras</h2></a>
      <a class="anchor-link" href="#Similaridade entre palavras"><h3>Similaridade entre palavras</h3></a>
      <a class="anchor-link" href="#Exemplo com embeddings da OpenAI"><h3>Exemplo com embeddings da OpenAI</h3></a>
      <a class="anchor-link" href="#Exemplo com embeddings do HuggingFace"><h3>Exemplo com embeddings do HuggingFace</h3></a>
      <a class="anchor-link" href="#Operacoes com palavras"><h3>Operações com palavras</h3></a>
      <a class="anchor-link" href="#Tipos de Word Embeddings"><h3>Tipos de Word Embeddings</h3></a>
      <a class="anchor-link" href="#Word2Vec"><h4>Word2Vec</h4></a>
      <a class="anchor-link" href="#CBOW"><h5>CBOW</h5></a>
      <a class="anchor-link" href="#Skip-gram"><h5>Skip-gram</h5></a>
      <a class="anchor-link" href="#GloVe"><h4>GloVe</h4></a>
      <a class="anchor-link" href="#FastText"><h4>FastText</h4></a>
      <a class="anchor-link" href="#Limitacoes dos word embeddings"><h4>Limitações dos word embeddings</h4></a>
      <a class="anchor-link" href="#Embutimentos de frases"><h2>Embutimentos de frases</h2></a>
      <a class="anchor-link" href="#ELMo"><h3>ELMo</h3></a>
      <a class="anchor-link" href="#InferSent"><h3>InferSent</h3></a>
      <a class="anchor-link" href="#Sentence-BERT"><h3>Sentence-BERT</h3></a>
      <a class="anchor-link" href="#Treinamento de um modelo word2vec com gensim"><h2>Treinamento de um modelo word2vec com gensim</h2></a>
      <a class="anchor-link" href="#Download do dataset"><h3>Download do dataset</h3></a>
      <a class="anchor-link" href="#Limpeza do conjunto de dados"><h3>Limpeza do conjunto de dados</h3></a>
      <a class="anchor-link" href="#Treinamento do modelo word2vec"><h3>Treinamento do modelo word2vec</h3></a>
      <a class="anchor-link" href="#Avaliacao do modelo word2vec"><h3>Avaliação do modelo word2vec</h3></a>
      <a class="anchor-link" href="#Visualizacao dos embeddings"><h3>Visualização dos embeddings</h3></a>
      <a class="anchor-link" href="#Uso de modelos pre-treinados com HuggingFace"><h2>Uso de modelos pré-treinados com HuggingFace</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o português usando um modelo de tradução automática. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Em um post anterior sobre <a href="https://maximofn.com/tokens/">tokens</a>, já vimos a representação mínima de cada palavra. Que corresponde a dar um número à menor divisão de cada palavra.</p>
      <p>No entanto, os transformers e, portanto, os LLMs, não representam assim a informação das palavras, mas sim através de <code>embeddings</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver primeiro duas formas de representar as palavras, o <code>ordinal encoding</code> e o <code>one hot encoding</code>. E vendo os problemas desses dois tipos de representações poderemos chegar até os <code>word embeddings</code> e os <code>sentence embeddings</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Além disso, vamos ver um exemplo de como treinar um modelo de <code>word embeddings</code> com a biblioteca <code>gensim</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>E por último veremos como usar modelos pré-treinados de <code>embeddings</code> com a biblioteca <code>transformers</code> do HuggingFace.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Codificacao ordinal">Codificação ordinal<a class="anchor-link" href="#Codificacao ordinal"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 53" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Esta é a maneira mais básica de representar as palavras dentro dos transformers. Consiste em dar um número a cada palavra, ou ficarmos com os números que já estão atribuídos aos tokens.</p>
      <p>No entanto, este tipo de representação tem dois problemas</p>
      <ul>
        <li>Imaginemos que mesa corresponde ao token 3, gato ao token 1 e cachorro ao token 2. Se poderia chegar a supor que <code>mesa = gato + cachorro</code>, mas não é assim. Não existe essa relação entre essas palavras. Até mesmo poderíamos pensar que atribuindo os tokens corretos sim poderia chegar a dar esse tipo de relações. No entanto, este pensamento desmorona com as palavras que têm mais de um significado, como por exemplo a palavra <code>banco</code></li>
      </ul>
      <ul>
        <li>O segundo problema é que as redes neurais internamente realizam muitos cálculos numéricos, pelo que poderia acontecer que se a mesa tiver o token 3, tenha internamente mais importância que a palavra gato que tem o token 1.</li>
      </ul>
      <p>Portanto, esse tipo de representação das palavras pode ser descartado muito rapidamente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Codificacao one-hot">Codificação one-hot<a class="anchor-link" href="#Codificacao one-hot"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 54" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aqui o que se faz é usar vetores de <code>N</code> dimensões. Por exemplo, vimos que a OpenAI tem um vocabulário de <code>100277</code> tokens distintos. Portanto, se usarmos <code>one hot encoding</code>, cada palavra seria representada por um vetor de <code>100277</code> dimensões.</p>
      <p>No entanto, o one hot encoding tem outros dois grandes problemas</p>
      <ul>
        <li>Não leva em conta a relação entre as palavras. Portanto, se tivermos duas palavras que são sinônimos, como por exemplo <code>gato</code> e <code>felino</code>, teríamos dois vetores diferentes para representá-los.</li>
      </ul>
      <p>Na linguagem, a relação entre as palavras é muito importante, e não levar em conta essa relação é um grande problema.</p>
      <ul>
        <li>O segundo problema é que os vetores são muito grandes. Se tivermos um vocabulário de <code>100277</code> tokens, cada palavra seria representada por um vetor de <code>100277</code> dimensões. Isso faz com que os vetores sejam muito grandes e que os cálculos sejam muito custosos. Além disso, esses vetores serão todos zeros, exceto na posição correspondente ao token da palavra. Portanto, a maioria dos cálculos será multiplicação por zero, que são cálculos que não contribuem com nada. Assim, vamos ter uma grande quantidade de memória alocada para vetores nos quais há apenas um <code>1</code> em uma posição específica.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Embeddings de palavras">Embeddings de palavras<a class="anchor-link" href="#Embeddings de palavras"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 55" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Com os word embeddings tenta-se resolver os problemas dos dois tipos de representações anteriores. Para isso, utilizam-se vetores de <code>N</code> dimensões, mas, neste caso, não se usam vetores de 100277 dimensões, e sim vetores com muito menos dimensões. Por exemplo, veremos que a OpenAI usa <code>1536</code> dimensões.</p>
      <p>Cada uma das dimensões desses vetores representa uma característica da palavra. Por exemplo, uma das dimensões poderia representar se a palavra é um verbo ou um substantivo. Outra dimensão poderia representar se a palavra é um animal ou não. Outra dimensão poderia representar se a palavra é um nome próprio ou não. E assim sucessivamente.</p>
      <p>No entanto, essas características não são definidas manualmente, mas sim aprendidas automaticamente. Durante o treinamento dos transformers, os valores de cada uma das dimensões dos vetores são ajustados, de modo que se aprendam as características de cada palavra.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ao fazer com que cada uma das dimensões das palavras represente uma característica da palavra, consegue-se que as palavras que tenham características semelhantes, tenham vetores semelhantes. Por exemplo, as palavras <code>gato</code> e <code>felino</code> terão vetores muito semelhantes, já que ambas são animais. E as palavras <code>mesa</code> e <code>silla</code> terão vetores semelhantes, já que ambas são móveis.</p>
      <p>Na imagem a seguir, podemos ver uma representação tridimensional de palavras, e observamos que todas as palavras relacionadas com <code>school</code> estão próximas, todas as palavras relacionadas com <code>food</code> estão próximas e todas as palavras relacionadas com <code>ball</code> estão próximas.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/word_embedding_3_dimmension.webp" alt="word_embedding_3_dimmension">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O fato de cada uma das dimensões dos vetores representar uma característica da palavra permite que possamos realizar operações com palavras. Por exemplo, se subtrairmos a palavra <code>homem</code> da palavra <code>rei</code> e somarmos a palavra <code>mulher</code>, obtemos uma palavra muito semelhante à palavra <code>rainha</code>. Verificaremos isso com um exemplo mais adiante.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Similaridade entre palavras">Similaridade entre palavras<a class="anchor-link" href="#Similaridade entre palavras"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 56" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como cada uma das palavras é representada por um vetor de <code>N</code> dimensões, podemos calcular a similaridade entre duas palavras. Para isso, usa-se a função de similaridade do cosseno ou <code>cosine similarity</code>.</p>
      <p>Se duas palavras estão próximas no espaço vetorial, isso significa que o ângulo entre seus vetores é pequeno, portanto seu cosseno é próximo de 1. Se há um ângulo de 90 graus entre os vetores, o cosseno é 0, ou seja, não há similaridade entre as palavras. E se há um ângulo de 180 graus entre os vetores, o cosseno é -1, ou seja, as palavras são opostas.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/cosine_similarity.webp" alt="cosine similarity">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Exemplo com embeddings da OpenAI">Exemplo com embeddings da OpenAI<a class="anchor-link" href="#Exemplo com embeddings da OpenAI"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 57" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que sabemos o que são os <code>embeddings</code>, vejamos alguns exemplos com os <code>embeddings</code> que nos proporciona a <code>API</code> da <code>OpenAI</code>.</p>
      <p>Para isso, primeiro temos que ter o pacote de <code>OpenAI</code> instalado.</p>
      <div class='highlight'><pre><code class="language-bash">pip install openai</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importamos as bibliotecas necessárias</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Usamos uma <code>API key</code> da OpenAI. Para isso, nos dirigimos à página de <a href="https://openai.com/" target="_blank" rel="nofollow noreferrer">OpenAI</a>, e nos registramos. Uma vez registrados, nos dirigimos à seção de <a href="https://platform.openai.com/api-keys">API Keys</a>, e criamos uma nova <code>API Key</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif" alt="open ai api key">
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">api_key</span> <span class="o">=</span> <span class="s2">&quot;Pon aquí tu API key&quot;</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Selecionamos qual modelo de embeddings queremos usar. Neste caso, vamos a usar <code>text-embedding-ada-002</code>, que é o recomendado pela <code>OpenAI</code> em sua documentação de <a href="https://platform.openai.com/docs/guides/embeddings/" target="_blank" rel="nofollow noreferrer">embeddings</a>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_openai</span> <span class="o">=</span> <span class="s2">&quot;text-embedding-ada-002&quot;</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos um cliente da <code>API</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">client_openai</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span> <span class="n">organization</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como são os <code>embeddings</code> da palavra <code>Rei</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;Rey&quot;</span>',
      '<span class="n">embedding_openai</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding_openai</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_openai</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(torch.Size([1536]),',
          'tensor([-0.0103, -0.0005, -0.0189,  ..., -0.0009, -0.0226,  0.0045]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, obtemos um vetor de <code>1536</code> dimensões.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Exemplo com embeddings do HuggingFace">Exemplo com embeddings do HuggingFace<a class="anchor-link" href="#Exemplo com embeddings do HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 58" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como a geração de embeddings da OpenAI é paga, vamos ver como usar os embeddings do HuggingFace, que são gratuitos. Para isso, primeiro temos que nos certificar de ter a biblioteca <code>sentence-transformers</code> instalada.</p>
      <div class='highlight'><pre><code class="language-bash">pip install -U sentence-transformers</code></pre></div>
      <p>E agora começamos a gerar os embeddings das palavras</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro importamos a biblioteca</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos um modelo de <code>embeddings</code> do <code>HuggingFace</code>. Usamos <code>paraphrase-MiniLM-L6-v2</code> porque é um modelo pequeno e rápido, mas que dá bons resultados, e agora para nosso exemplo nos basta.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;paraphrase-MiniLM-L6-v2&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>E já podemos gerar os <code>embeddings</code> das palavras</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Rey&#39;</span><span class="p">]</span>',
      '<span class="n">embedding_huggingface</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding_huggingface</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_huggingface</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '((1, 384),',
          'array([ 4.99837071e-01, -7.60397986e-02,  5.47384083e-01,  1.89465046e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.21713984e-01, -1.01025246e-01,  6.44087136e-01,  4.91398573e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.73571329e-02, -2.77234882e-01,  4.34713453e-01, -1.06284058e+00,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.44114518e-01,  8.98794234e-01,  4.74923879e-01, -7.48904228e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.84665376e-01, -1.75070837e-01,  5.92192829e-01, -1.02512836e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;9.45721626e-01,  2.43777707e-01,  3.91995460e-01,  3.35530996e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.58333105e-01,  1.18869759e-01,  5.31717360e-01, -1.21750660e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.45580745e-01, -7.63889611e-01, -3.19075316e-01,  2.55386919e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.06407446e-01, -8.99556637e-01,  6.34190366e-02, -2.96231866e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.22994244e-01,  7.44934231e-02, -4.49327320e-01, -2.71379113e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.88012260e-01, -2.82730222e-01,  2.50365853e-01,  3.06314558e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5.01561277e-02, -5.73592126e-01, -4.93096076e-02, -2.54629493e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.45663840e-01, -1.54654181e-03,  1.85357735e-01,  2.49421135e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.80077875e-01, -2.99735814e-01,  7.34686375e-01,  9.35385004e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-8.64403173e-02,  5.90056717e-01,  9.62065995e-01, -3.89911681e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.52635378e-01,  1.10802782e+00, -4.28262979e-01,  8.98583114e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.79768258e-01, -7.25559890e-01,  4.38431054e-01,  6.08255446e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.06222546e+00,  1.86217821e-03,  5.23232877e-01, -5.59782684e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.08870542e+00, -1.29855171e-01, -1.34669527e-01,  4.24595959e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.99118191e-01, -2.53481418e-01, -1.82368979e-01,  9.74772453e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-7.66527832e-01,  2.02146843e-01, -9.27186012e-01, -3.72025579e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.51360565e-01,  3.66043419e-01,  3.58169287e-01, -5.50914466e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.87659878e-01,  2.67650932e-01, -1.30100116e-01, -9.08647776e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.58671075e-01, -4.44935560e-01, -1.43231079e-01, -2.83272982e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.21463636e-02,  1.98998764e-01, -9.47986841e-02,  1.74529219e+00,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.71559617e-01,  5.96294463e-01,  1.38505893e-02,  3.90956283e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.46427560e-01,  2.63105750e-01,  2.64972121e-01, -2.67196923e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.54366294e-02,  9.39224422e-01,  3.35206270e-01, -1.99105024e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.06340271e-01,  3.83643419e-01,  4.37904626e-01,  8.92579079e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.86432815e-01, -2.59302586e-01, -6.39415443e-01,  1.21703267e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;6.44594133e-01,  2.56335083e-02,  5.53315282e-02,  5.85618019e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.03075497e-01, -4.17360187e-01,  5.00189543e-01,  4.23062295e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-7.62073815e-01, -4.36184794e-01, -4.13090199e-01, -2.14746520e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.76077414e-01, -1.51846036e-02, -6.51694953e-01,  2.05930993e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.73996288e-01,  1.14034235e-01, -7.40544260e-01,  1.98710993e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-6.66027904e-01,  3.00016254e-01, -4.03109461e-01,  1.85078502e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.27183425e-01,  4.19003010e-01,  1.16863050e-01, -4.33366179e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.62291127e-01,  6.25310719e-01, -3.34749371e-01,  3.18448655e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-9.09660235e-02,  3.58690947e-01,  1.23402506e-01, -5.08333087e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.18513209e-01,  5.83032072e-01, -8.37822199e-01, -1.52947128e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5.07765234e-01, -2.90990144e-01, -2.56464798e-02,  5.69117546e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.43118417e-01, -3.27799052e-01, -1.70862004e-01,  4.14014012e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.74694878e-01,  5.15708327e-01,  3.21234539e-02,  1.55380607e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.21141332e-01, -1.72114551e-01,  6.43211603e-01, -3.89207341e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.29103401e-01,  4.13877398e-01, -9.22305062e-02, -4.54976231e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.50242126e+00, -2.81573564e-01,  1.70057654e-01,  4.53076512e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.25060362e-01, -1.33391351e-01,  5.40394569e-03,  3.71117502e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.29107875e-01,  1.35897202e-02,  2.44936779e-01,  1.04574718e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.65612388e-01,  4.33572650e-01, -4.09719855e-01, -2.95067448e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.26362443e-02, -7.43583977e-01, -7.35885441e-01, -1.35508239e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.12558493e-01, -5.46157181e-01,  7.55161867e-02, -3.57991695e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.20607555e-01,  5.53125329e-02, -3.23110700e-01,  4.88573104e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.07487953e+00,  1.72190830e-01,  8.48749802e-02,  5.73584400e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.06147277e-01,  3.26699704e-01,  5.09487510e-01, -2.60940105e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.85459042e-01,  3.15197736e-01, -8.84049162e-02, -2.14854136e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.04228538e-01, -3.53874594e-01,  3.30587216e-02, -2.04278827e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.45132256e-01, -4.05272096e-01,  9.07981098e-01, -1.70708492e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.62848401e-01, -3.17223936e-01,  1.53909430e-01,  7.24429131e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.27339968e-01, -1.16330147e+00, -9.58504915e-01,  4.87008452e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.30886355e-01, -1.40117988e-01,  7.84571916e-02, -2.93157458e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.00778294e+00,  1.34625390e-01, -4.66320179e-02,  6.51122704e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.50451362e-02, -2.15500608e-01, -2.42915586e-01, -3.21900517e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.94186682e-01,  4.71027017e-01,  1.56058431e-01,  1.30854800e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.84257025e-01, -1.44421116e-01, -7.09840000e-01, -1.80235609e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-8.30230191e-02,  9.08326149e-01, -8.22497830e-02,  1.46948382e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.41326815e-01,  3.81170362e-01, -6.37023628e-01,  1.70148894e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.00046806e-01,  5.70729785e-02, -1.09820545e+00, -1.03613675e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-6.21219516e-01,  4.55532551e-01,  1.86942443e-01, -2.04409719e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.81394243e-01, -7.88963258e-01,  2.19068691e-01, -3.62780124e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.41522694e-01, -1.73794985e-01, -4.00943428e-01,  5.01900315e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.53949839e-01,  1.03774257e-01, -1.66873619e-01, -4.63893116e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.78147718e-01,  4.85655308e-01, -3.02978605e-02, -5.67060888e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.68107373e-01, -6.57559693e-01, -5.02855539e-01, -1.94635347e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-9.58659649e-01, -4.97986436e-01,  1.33874401e-01,  3.09395105e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.52993363e-01,  7.43827343e-01, -1.87271550e-01, -6.11483693e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.08927953e+00, -2.30332208e-03,  2.11169615e-01, -3.46892715e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.32458824e-01,  2.07640216e-01, -4.10387546e-01,  3.12181324e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.69687408e-01,  8.62928331e-01,  2.40735337e-01, -3.65841389e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;6.84210837e-01,  3.45884450e-02,  5.63964128e-01,  2.39361122e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.10872793e-01, -6.34638309e-01, -9.07931089e-01, -6.35836497e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.20288679e-01,  2.59186536e-01, -4.45540816e-01,  6.33085072e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.97424471e-01,  7.51152515e-01, -2.68558711e-01, -4.39288855e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.13556695e-01, -1.89288303e-01,  5.81856608e-01,  4.75860722e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.60344616e-01, -2.96180040e-01,  2.91323394e-01,  1.34404674e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.22037649e-01,  4.19363379e-02, -3.87936801e-01, -9.25336123e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.28307915e-01, -1.74257740e-01, -1.52818128e-01,  4.31716293e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.12064430e-01,  2.98252910e-01,  9.86064151e-02,  3.84781063e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;6.68018535e-02, -2.29525566e-01, -8.20755959e-03,  5.17108142e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-6.66776478e-01, -1.38897672e-01,  4.68370765e-01, -2.14766636e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.43549764e-01,  2.25854263e-01, -1.92763060e-02,  2.78505355e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.39088053e-01, -9.69757214e-02, -2.71263003e-01,  1.05703615e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.14365645e-01,  4.16649908e-01,  4.18699026e-01, -1.76222697e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.08620593e-01, -5.79392374e-01, -1.68948188e-01, -1.77841976e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5.69338985e-02,  2.12916449e-01,  4.24367547e-01, -7.13860095e-02,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;8.28932896e-02, -2.40542665e-01, -5.94049037e-01,  4.09415931e-01,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.01326215e+00, -5.71239054e-01,  4.35258061e-01, -3.64619821e-01],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;dtype=float32))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, obtemos um vetor de <code>384</code> dimensões. Neste caso, obtemos um vetor desta dimensão porque foi usado o modelo <code>paraphrase-MiniLM-L6-v2</code>. Se usarmos outro modelo, obteremos vetores de outra dimensão.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Operacoes com palavras">Operações com palavras<a class="anchor-link" href="#Operacoes com palavras"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 59" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a obter os embeddings das palavras <code>rei</code>, <code>homem</code>, <code>mulher</code> e <code>rainha</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai_rey</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;rey&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_hombre</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;hombre&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_mujer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;mujer&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;reina&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai_reina</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_openai_reina</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(torch.Size([1536]),',
          'tensor([-0.0110, -0.0084, -0.0115,  ...,  0.0082, -0.0096, -0.0024]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a obter o embedding resultante de subtrair o embedding de <code>homem</code> do embedding de <code>rei</code> e adicionar o embedding de <code>mulher</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai</span> <span class="o">=</span> <span class="n">embedding_openai_rey</span> <span class="o">-</span> <span class="n">embedding_openai_hombre</span> <span class="o">+</span> <span class="n">embedding_openai_mujer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_openai</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(torch.Size([1536]),',
          'tensor([-0.0226, -0.0323,  0.0017,  ...,  0.0014, -0.0290, -0.0188]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Por último, comparamos o resultado obtido com o embedding de <code>rainha</code>. Para isso, usamos a função <code>cosine_similarity</code> fornecida pela biblioteca <code>pytorch</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">similarity_openai</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding_openai</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">embedding_openai_reina</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;similarity_openai: </span><span class="si">{</span><span class="n">similarity_openai</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'similarity_openai: 0.7564167976379395',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos é um valor muito próximo de 1, portanto podemos dizer que o resultado obtido é muito semelhante ao embedding de <code>reina</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se usarmos palavras em inglês, obtemos um resultado mais próximo de 1</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai_rey</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_hombre</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_mujer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
      '<span class="n">embedding_openai_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;queen&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_openai</span> <span class="o">=</span> <span class="n">embedding_openai_rey</span> <span class="o">-</span> <span class="n">embedding_openai_hombre</span> <span class="o">+</span> <span class="n">embedding_openai_mujer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">similarity_openai</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding_openai</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">embedding_openai_reina</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;similarity_openai: </span><span class="si">{</span><span class="n">similarity_openai</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'similarity_openai: tensor([0.8849])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Isto é normal, pois o modelo da OpenAI foi treinado com mais textos em inglês do que em português.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tipos de Word Embeddings">Tipos de Word Embeddings<a class="anchor-link" href="#Tipos de Word Embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 60" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Existem vários tipos de word embeddings, e cada um deles tem suas vantagens e desvantagens. Vamos ver os mais importantes.</p>
      <ul>
        <li>Word2Vec</li>
        <li>GloVe</li>
        <li>FastText</li>
        <li>BERT</li>
        <li>GPT-2</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Word2Vec">Word2Vec<a class="anchor-link" href="#Word2Vec"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 61" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Word2Vec é um algoritmo usado para criar word embeddings. Este algoritmo foi criado pelo Google em 2013 e é um dos algoritmos mais utilizados para criar word embeddings.</p>
      <p>Ele tem duas variantes, <code>CBOW</code> e <code>Skip-gram</code>. O <code>CBOW</code> é mais rápido de treinar, enquanto o <code>Skip-gram</code> é mais preciso. Vamos ver como cada um deles funciona.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="CBOW">CBOW<a class="anchor-link" href="#CBOW"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 62" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>CBOW</code> ou <code>Continuous Bag of Words</code> é um algoritmo que é usado para prever uma palavra com base nas palavras ao seu redor. Por exemplo, se tivermos a frase <code>O gato é um animal</code>, o algoritmo tentará prever a palavra <code>gato</code> com base nas palavras ao seu redor, neste caso <code>O</code>, <code>é</code>, <code>um</code> e <code>animal</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/cbow.webp" alt="CBOW">
      <p>Nesta arquitetura, o modelo prevê qual é a palavra mais provável no contexto dado. Portanto, as palavras que têm a mesma probabilidade de aparecer são consideradas semelhantes e, por isso, ficam mais próximas no espaço dimensional.</p>
      <p>Suponhamos que em uma frase substituímos <code>barco</code> por <code>bote</code>, então o modelo prevê a probabilidade para ambos e, se for semelhante, podemos considerar que as palavras são similares.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Skip-gram">Skip-gram<a class="anchor-link" href="#Skip-gram"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 63" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Skip-gram</code> ou <code>Skip-gram with Negative Sampling</code> é um algoritmo que é usado para prever as palavras que cercam uma palavra. Por exemplo, se temos a frase <code>O gato é um animal</code>, o algoritmo tentará prever as palavras <code>O</code>, <code>é</code>, <code>um</code> e <code>animal</code> a partir da palavra <code>gato</code>.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Skip-gram.webp" alt="Skip-gram">
      <p>Esta arquitetura é semelhante à de CBOW, mas em vez disso o modelo funciona ao contrário. O modelo prevê o contexto usando a palavra dada. Portanto, as palavras que têm o mesmo contexto são consideradas similares e, portanto, se aproximam mais no espaço dimensional.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="GloVe">GloVe<a class="anchor-link" href="#GloVe"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 64" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>GloVe</code> ou <code>Global Vectors for Word Representation</code> é um algoritmo usado para criar word embeddings. Este algoritmo foi criado pela Universidade de Stanford em 2014.</p>
      <p>Word2Vec ignora o fato de que algumas palavras de contexto ocorrem com mais frequência do que outras e também só levam em conta o contexto local, portanto, não capturam o contexto global.</p>
      <p>Este algoritmo usa uma matriz de co-ocorrência para criar os word embeddings. Esta matriz de co-ocorrência é uma matriz que contém o número de vezes que cada palavra aparece junto com cada uma das outras palavras do vocabulário.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="FastText">FastText<a class="anchor-link" href="#FastText"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 65" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>FastText</code> é um algoritmo usado para criar word embeddings. Este algoritmo foi criado pelo Facebook em 2016.</p>
      <p>Uma das principais desvantagens do <code>Word2Vec</code> e <code>GloVe</code> é que eles não podem codificar palavras desconhecidas ou fora do vocabulário.</p>
      <p>Então, para lidar com esse problema, o Facebook propôs um modelo <code>FastText</code>. É uma extensão de <code>Word2Vec</code> e segue o mesmo modelo <code>Skip-gram</code> e <code>CBOW</code>. Mas, ao contrário do <code>Word2Vec</code>, que alimenta palavras inteiras na rede neural, o <code>FastText</code> primeiro divide as palavras em várias subpalavras (ou <code>n-grams</code>) e depois as alimenta à rede neural.</p>
      <p>Por exemplo, se o valor de <code>n</code> for 3 e a palavra for <code>maçã</code>, então seu tri-gram será [<code>&#x3C;ma</code>, <code>maç</code>, <code>açã</code>, <code>çã&#x3E;</code>, <code>ã&#x3E;</code>] e seu embedding de palavras será a soma da representação vetorial desses tri-grams. Aqui, os hiperparâmetros <code>min_n</code> e <code>max_n</code> são considerados como 3 e os caracteres <code>&#x3C;</code> e <code>&#x3E;</code> representam o início e o fim da palavra.</p>
      <p>Portanto, utilizando esta metodologia, as palavras desconhecidas podem ser representadas em forma vetorial, pois há uma alta probabilidade de que seus <code>n-grams</code> também estejam presentes em outras palavras.</p>
      <p>Este algoritmo é uma melhoria de <code>Word2Vec</code>, pois além de levar em conta as palavras que cercam uma palavra, também leva em conta os <code>n-grams</code> da palavra. Por exemplo, se temos a palavra <code>gato</code>, também leva em conta os <code>n-gramas</code> da palavra, neste caso <code>ga</code>, <code>at</code> e <code>to</code>, para <code>n = 2</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Limitacoes dos word embeddings">Limitações dos word embeddings<a class="anchor-link" href="#Limitacoes dos word embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 66" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As técnicas de word embedding têm dado um resultado decente, mas o problema é que a abordagem não é precisa o suficiente. Elas não levam em conta a ordem das palavras em que aparecem, o que leva à perda da compreensão sintática e semântica da frase.</p>
      <p>Por exemplo, <code>Você vai lá para ensinar, não para jogar</code> e <code>Você vai lá jogar, não para ensinar</code> Ambas as frases terão a mesma representação no espaço vetorial, mas não significam a mesma coisa.</p>
      <p>Além disso, o modelo de word embedding não pode fornecer resultados satisfatórios em uma grande quantidade de dados de texto, pois a mesma palavra pode ter um significado diferente em uma frase diferente de acordo com o contexto da frase.</p>
      <p>Por exemplo, <code>Vou me sentar no banco</code> e <code>Vou fazer trâmites no banco</code>. Nas duas frases, a palavra <code>banco</code> tem significados diferentes.</p>
      <p>Portanto, precisamos de um tipo de representação que possa reter o significado contextual da palavra presente em uma frase.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Embutimentos de frases">Embutimentos de frases<a class="anchor-link" href="#Embutimentos de frases"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 67" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O sentence embedding é semelhante ao word embedding, mas em vez de palavras, codifica toda a frase na representação vetorial.</p>
      <p>Uma forma simples de obter embeddings de sentenças é fazendo a média dos embeddings das palavras presentes na sentença. Mas eles não são suficientemente precisos.</p>
      <p>Alguns dos modelos mais avançados para embedings de sentenças são <code>ELMo</code>, <code>InferSent</code> e <code>Sentence-BERT</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="ELMo">ELMo<a class="anchor-link" href="#ELMo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 68" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>ELMo</code> ou <code>Embeddings from Language Models</code> é um modelo de sentence embedding que foi criado pela Universidade de Allen em 2018. Utiliza uma rede LSTM profunda bidirecional para produzir representação vetorial. <code>ELMo</code> pode representar palavras desconhecidas ou fora do vocabulário de forma vetorial, pois está baseado em caracteres.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="InferSent">InferSent<a class="anchor-link" href="#InferSent"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 69" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>InferSent</code> é um modelo de embedding de sentenças que foi criado pelo Facebook em 2017. Utiliza uma rede LSTM profunda bidirecional para produzir representação vetorial. <code>InferSent</code> pode representar palavras desconhecidas ou fora do vocabulário de forma vetorial, pois é baseado em caracteres. As sentenças são codificadas em uma representação vetorial de 4096 dimensões.</p>
      <p>O treinamento do modelo é realizado no conjunto de dados Stanford Natural Language Inference (<code>SNLI</code>). Este conjunto de dados está anotado e escrito por humanos para cerca de 500K pares de sentenças.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sentence-BERT">Sentence-BERT<a class="anchor-link" href="#Sentence-BERT"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 70" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Sentence-BERT</code> é um modelo de embedding de sentenças criado pela Universidade de Londres em 2019. Utiliza uma rede LSTM profunda bidirecional para produzir representação vetorial. <code>Sentence-BERT</code> pode representar palavras desconhecidas ou fora do vocabulário de forma vetorial, pois está baseado em caracteres. As sentenças são codificadas em uma representação vetorial de 768 dimensões.</p>
      <p>O modelo de NLP de última geração <code>BERT</code> é excelente nas tarefas de Similaridade Textual Semântica, mas o problema é que levaria muito tempo para um corpus enorme (65 horas para 10.000 frases), já que requer que ambas as frases sejam introduzidas na rede e isso aumenta o cálculo por um fator enorme.</p>
      <p>Portanto, <code>Sentence-BERT</code> é uma modificação do modelo <code>BERT</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Treinamento de um modelo word2vec com gensim">Treinamento de um modelo word2vec com gensim<a class="anchor-link" href="#Treinamento de um modelo word2vec com gensim"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 71" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para baixar o dataset que vamos usar, é necessário instalar a biblioteca <code>dataset</code> do huggingface:</p>
      <div class='highlight'><pre><code class="language-bash">pip install datasets</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para treinar o modelo de embeddings vamos a usar a biblioteca <code>gensim</code>. Para instalá-la com Conda usamos</p>
      <div class='highlight'><pre><code class="language-bash">conda install -c conda-forge gensim</code></pre></div>
      <p>E para instalá-la com pip usamos</p>
      <div class='highlight'><pre><code class="language-bash">pip install gensim</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para limpar o dataset que baixamos, vamos usar expressões regulares, que geralmente já estão instaladas no Python, e <code>nltk</code>, que é uma biblioteca de processamento de linguagem natural. Para instalá-la com Conda, usamos</p>
      <div class='highlight'><pre><code class="language-bash">conda install -c anaconda nltk</code></pre></div>
      <p>E para instalá-lo com pip usamos</p>
      <div class='highlight'><pre><code class="language-bash">pip install nltk</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos tudo instalado, podemos importar as bibliotecas que vamos usar:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gensim.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Word2Vec</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">gensim.parsing.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">strip_punctuation</span><span class="p">,</span> <span class="n">strip_numeric</span><span class="p">,</span> <span class="n">strip_short</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.corpus</span><span class="w"> </span><span class="kn">import</span> <span class="n">stopwords</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">word_tokenize</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Download do dataset">Download do dataset<a class="anchor-link" href="#Download do dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 72" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a baixar um conjunto de dados de textos provenientes da Wikipedia em espanhol, para isso executamos o seguinte:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>',
      '<span class="w"> </span>',
      '<span class="n">dataset_corpus</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;large_spanish_corpus&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;all_wikis&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver como é</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset_corpus</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'DatasetDict(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;train: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 28109484',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, o conjunto de dados tem mais de 28 milhões de textos. Vamos dar uma olhada em alguns deles:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset_corpus</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x27;¡Bienvenidos!&#x27;,',
          '&#x27;Ir a los contenidos»&#x27;,',
          '&#x27;= Contenidos =&#x27;,',
          '&#x27;&#x27;,',
          '&#x27;Portada&#x27;,',
          '&#x27;Tercera Lengua más hablada en el mundo.&#x27;,',
          '&#x27;La segunda en número de habitantes en el mundo occidental.&#x27;,',
          '&#x27;La de mayor proyección y crecimiento día a día.&#x27;,',
          '&#x27;El español es, hoy en día, nombrado en cada vez más contextos, tomando realce internacional como lengua de cultura y civilización siempre de mayor envergadura.&#x27;,',
          '&#x27;Ejemplo de ello es que la comunidad minoritaria más hablada en los Estados Unidos es precisamente la que habla idioma español.&#x27;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como há muitos exemplos, vamos criar um subconjunto de 10 milhões de exemplos para poder trabalhar mais rapidamente:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset</span> <span class="o">=</span> <span class="n">dataset_corpus</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10000000</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Limpeza do conjunto de dados">Limpeza do conjunto de dados<a class="anchor-link" href="#Limpeza do conjunto de dados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 73" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora baixamos as <code>stopwords</code> do <code>nltk</code>, que são palavras que não trazem informações e que vamos eliminar dos textos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>',
      '<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[nltk_data] Downloading package stopwords to',
          '[nltk_data]     /home/wallabot/nltk_data...',
          '[nltk_data]   Package stopwords is already up-to-date!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'True',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora vamos a baixar os <code>punkt</code> do <code>nltk</code>, que é um <code>tokenizer</code> que nos vai permitir separar os textos em frases.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[nltk_data] Downloading package punkt to /home/wallabot/nltk_data...',
          '[nltk_data]   Package punkt is already up-to-date!',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'True',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos uma função para limpar os dados. Esta função vai a:</p>
      <ul>
        <li>passar o texto para minúsculas</li>
        <li>Eliminar as URLs</li>
        <li>Remover as menções a redes sociais como <code>@twitter</code> e <code>#hashtag</code></li>
        <li>Eliminar os sinais de pontuação</li>
        <li>Eliminar os números</li>
        <li>Eliminar as palavras curtas</li>
        <li>Eliminar as palavras de parada</li>
      </ul>
      <p>Como estamos usando um dataset do huggingface, os textos estão no formato <code>dict</code>, então retornamos um dicionário.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">clean_text</span><span class="p">(</span><span class="n">sentence_batch</span><span class="p">):</span>',
      '<span class="w">    </span><span class="c1"># extrae el texto de la entrada</span>',
      '<span class="w">    </span><span class="n">text_list</span> <span class="o">=</span> <span class="n">sentence_batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="n">cleaned_text_list</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">text_list</span><span class="p">:</span>',
      '<span class="w">        </span><span class="c1"># Convierte el texto a minúsculas</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina URLs</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;http\S+|www\S+|https\S+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">MULTILINE</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina las menciones @ y &#39;#&#39; de las redes sociales</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\@\w+|\#\w+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina los caracteres de puntuación</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">strip_punctuation</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina los números</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">strip_numeric</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina las palabras cortas</span>',
      '<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">strip_short</span><span class="p">(</span><span class="n">text</span><span class="p">,</span><span class="n">minsize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Elimina las palabras comunes (stop words)</span>',
      '<span class="w">        </span><span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;spanish&#39;</span><span class="p">))</span>',
      '<span class="w">        </span><span class="n">word_tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
      '<span class="w">        </span><span class="n">filtered_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="n">cleaned_text_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filtered_text</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="c1"># Devuelve el texto limpio</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">cleaned_text_list</span><span class="p">}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aplicamos a função aos dados</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentences_corpus</span> <span class="o">=</span> <span class="n">subset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">clean_text</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Map:   0%|          | 0/10000000 [00:00&amp;lt;?, ? examples/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a salvar o conjunto de dados filtrado em um arquivo para não ter que executar novamente o processo de limpeza</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentences_corpus</span><span class="o">.</span><span class="n">save_to_disk</span><span class="p">(</span><span class="s2">&quot;sentences_corpus&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Saving the dataset (0/4 shards):   0%|          | 0/15000000 [00:00&amp;lt;?, ? examples/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Para carregá-lo podemos fazer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_from_disk</span>',
      '<span class="n">sentences_corpus</span> <span class="o">=</span> <span class="n">load_from_disk</span><span class="p">(</span><span class="s1">&#39;sentences_corpus&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora teremos uma lista de listas, onde cada lista é uma frase tokenizada e sem stopwords. Isso significa que temos uma lista de frases, e cada frase é uma lista de palavras. Vamos ver como é:</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;La frase &quot;</span><span class="si">{</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&quot; se convierte en la lista de palabras &quot;</span><span class="si">{</span><span class="n">sentences_corpus</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'La frase &quot;¡Bienvenidos!&quot; se convierte en la lista de palabras &quot;[&#x27;¡bienvenidos&#x27;]&quot;',
          'La frase &quot;Ir a los contenidos»&quot; se convierte en la lista de palabras &quot;[&#x27;ir&#x27;, &#x27;contenidos&#x27;, &#x27;»&#x27;]&quot;',
          'La frase &quot;= Contenidos =&quot; se convierte en la lista de palabras &quot;[&#x27;contenidos&#x27;]&quot;',
          'La frase &quot;&quot; se convierte en la lista de palabras &quot;[]&quot;',
          'La frase &quot;Portada&quot; se convierte en la lista de palabras &quot;[&#x27;portada&#x27;]&quot;',
          'La frase &quot;Tercera Lengua más hablada en el mundo.&quot; se convierte en la lista de palabras &quot;[&#x27;tercera&#x27;, &#x27;lengua&#x27;, &#x27;hablada&#x27;, &#x27;mundo&#x27;]&quot;',
          'La frase &quot;La segunda en número de habitantes en el mundo occidental.&quot; se convierte en la lista de palabras &quot;[&#x27;segunda&#x27;, &#x27;número&#x27;, &#x27;habitantes&#x27;, &#x27;mundo&#x27;, &#x27;occidental&#x27;]&quot;',
          'La frase &quot;La de mayor proyección y crecimiento día a día.&quot; se convierte en la lista de palabras &quot;[&#x27;mayor&#x27;, &#x27;proyección&#x27;, &#x27;crecimiento&#x27;, &#x27;día&#x27;, &#x27;día&#x27;]&quot;',
          'La frase &quot;El español es, hoy en día, nombrado en cada vez más contextos, tomando realce internacional como lengua de cultura y civilización siempre de mayor envergadura.&quot; se convierte en la lista de palabras &quot;[&#x27;español&#x27;, &#x27;hoy&#x27;, &#x27;día&#x27;, &#x27;nombrado&#x27;, &#x27;cada&#x27;, &#x27;vez&#x27;, &#x27;contextos&#x27;, &#x27;tomando&#x27;, &#x27;realce&#x27;, &#x27;internacional&#x27;, &#x27;lengua&#x27;, &#x27;cultura&#x27;, &#x27;civilización&#x27;, &#x27;siempre&#x27;, &#x27;mayor&#x27;, &#x27;envergadura&#x27;]&quot;',
          'La frase &quot;Ejemplo de ello es que la comunidad minoritaria más hablada en los Estados Unidos es precisamente la que habla idioma español.&quot; se convierte en la lista de palabras &quot;[&#x27;ejemplo&#x27;, &#x27;ello&#x27;, &#x27;comunidad&#x27;, &#x27;minoritaria&#x27;, &#x27;hablada&#x27;, &#x27;unidos&#x27;, &#x27;precisamente&#x27;, &#x27;habla&#x27;, &#x27;idioma&#x27;, &#x27;español&#x27;]&quot;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Treinamento do modelo word2vec">Treinamento do modelo word2vec<a class="anchor-link" href="#Treinamento do modelo word2vec"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 74" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos treinar um modelo de embeddings que converterá palavras em vetores. Para isso, vamos usar a biblioteca <code>gensim</code> e seu modelo <code>Word2Vec</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">sentences_corpus</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>',
      '<span class="n">dim_embedding</span> <span class="o">=</span> <span class="mi">100</span>',
      '<span class="n">window_size</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># 5 palabras a la izquierda y 5 palabras a la derecha</span>',
      '<span class="n">min_count</span> <span class="o">=</span> <span class="mi">5</span>   <span class="c1"># Ignora las palabras con frecuencia menor a 5</span>',
      '<span class="n">workers</span> <span class="o">=</span> <span class="mi">4</span>    <span class="c1"># Número de hilos de ejecución</span>',
      '<span class="n">sg</span> <span class="o">=</span> <span class="mi">1</span>        <span class="c1"># 0 para CBOW, 1 para Skip-gram</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="n">dim_embedding</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_count</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="n">sg</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Este modelo foi treinado na CPU, já que <code>gensim</code> não tem opção de realizar o treinamento na GPU e mesmo assim no meu computador levou X minutos para treinar o modelo. Embora a dimensão do embedding que escolhemos seja de apenas 100 (em comparação com o tamanho dos embeddings da OpenAI, que é de 1536), não é um tempo muito grande, já que o dataset tem 10 milhões de frases.</p>
      <p>Os grandes modelos de linguagem são treinados com conjuntos de dados de bilhões de frases, portanto é normal que o treinamento de um modelo de embeddings com um conjunto de dados de 10 milhões de frases demore alguns minutos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez treinado o modelo, o salvamos em um arquivo para poder usá-lo no futuro</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;word2vec.model&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos carregá-lo no futuro, podemos fazer isso com</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;word2vec.model&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Avaliacao do modelo word2vec">Avaliação do modelo word2vec<a class="anchor-link" href="#Avaliacao do modelo word2vec"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 75" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver as palavras mais semelhantes de algumas palavras</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;perro&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[(&#x27;gato&#x27;, 0.7948548197746277),',
          '(&#x27;perros&#x27;, 0.77247554063797),',
          '(&#x27;cachorro&#x27;, 0.7638891339302063),',
          '(&#x27;hámster&#x27;, 0.7540281414985657),',
          '(&#x27;caniche&#x27;, 0.7514827251434326),',
          '(&#x27;bobtail&#x27;, 0.7492328882217407),',
          '(&#x27;mastín&#x27;, 0.7491254210472107),',
          '(&#x27;lobo&#x27;, 0.7312178611755371),',
          '(&#x27;semental&#x27;, 0.7292628288269043),',
          '(&#x27;sabueso&#x27;, 0.7290207147598267)]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;gato&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[(&#x27;conejo&#x27;, 0.8148329854011536),',
          '(&#x27;zorro&#x27;, 0.8109457492828369),',
          '(&#x27;perro&#x27;, 0.7948548793792725),',
          '(&#x27;lobo&#x27;, 0.7878773808479309),',
          '(&#x27;ardilla&#x27;, 0.7860757112503052),',
          '(&#x27;mapache&#x27;, 0.7817519307136536),',
          '(&#x27;huiña&#x27;, 0.766639232635498),',
          '(&#x27;oso&#x27;, 0.7656188011169434),',
          '(&#x27;mono&#x27;, 0.7633568644523621),',
          '(&#x27;camaleón&#x27;, 0.7623056769371033)]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora vamos ver o exemplo no qual verificamos a similaridade da palavra <code>rainha</code> com o resultado de subtrair a palavra <code>homem</code> da palavra <code>rei</code> e adicionar a palavra <code>mulher</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_hombre</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;hombre&#39;</span><span class="p">]</span>',
      '<span class="n">embedding_mujer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;mujer&#39;</span><span class="p">]</span>',
      '<span class="n">embedding_rey</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;rey&#39;</span><span class="p">]</span>',
      '<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;reina&#39;</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_rey</span> <span class="o">-</span> <span class="n">embedding_hombre</span> <span class="o">+</span> <span class="n">embedding_mujer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding_reina</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">embedding_reina</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="n">similarity</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'tensor([0.8156])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, há bastante similaridade</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Visualizacao dos embeddings">Visualização dos embeddings<a class="anchor-link" href="#Visualizacao dos embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 76" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos visualizar os embeddings, para isso primeiro obtemos os vetores e as palavras do modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vectors</span>',
      '<span class="n">words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como a dimensão dos embeddings é 100, para poder visualizá-los em 2 ou 3 dimensões temos que reduzir a dimensão. Para isso vamos usar <code>PCA</code> (mais rápido) ou <code>TSNE</code> (mais preciso) de <code>sklearn</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>',
      '<span class="w"> </span>',
      '<span class="n">dimmesions</span> <span class="o">=</span> <span class="mi">2</span>',
      '<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">dimmesions</span><span class="p">)</span>',
      '<span class="n">reduced_embeddings_PCA</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">TSNE</span>',
      '<span class="w"> </span>',
      '<span class="n">dimmesions</span> <span class="o">=</span> <span class="mi">2</span>',
      '<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">dimmesions</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>',
      '<span class="n">reduced_embeddings_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[t-SNE] Computing 121 nearest neighbors...',
          '[t-SNE] Indexed 493923 samples in 0.013s...',
          '[t-SNE] Computed neighbors for 493923 samples in 377.143s...',
          '[t-SNE] Computed conditional probabilities for sample 1000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 2000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 3000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 4000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 5000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 6000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 7000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 8000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 9000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 10000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 11000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 12000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 13000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 14000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 15000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 16000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 17000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 18000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 19000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 20000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 21000 / 493923',
          '[t-SNE] Computed conditional probabilities for sample 22000 / 493923',
          '...',
          '[t-SNE] Computed conditional probabilities for sample 493923 / 493923',
          '[t-SNE] Mean sigma: 0.275311',
          '[t-SNE] KL divergence after 250 iterations with early exaggeration: 117.413788',
          '[t-SNE] KL divergence after 300 iterations: 5.774648',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora os visualizamos em 2 dimensões com <code>matplotlib</code>. Vamos visualizar a redução de dimensionalidade que fizemos com <code>PCA</code> e com <code>TSNE</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>',
      '<span class="w"> </span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>',
      '<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">200</span><span class="p">]):</span> <span class="c1"># Limitar a las primeras 200 palabras</span>',
      '<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>',
      '<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Embeddings (PCA)&#39;</span><span class="p">)</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;Figure size 1000x1000 with 1 Axes&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>',
      '<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">200</span><span class="p">]):</span> <span class="c1"># Limitar a las primeras 200 palabras</span>',
      '<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>',
      '<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>',
      '<span class="w">                 </span><span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>',
      '<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;Figure size 1000x1000 with 1 Axes&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Uso de modelos pre-treinados com HuggingFace">Uso de modelos pré-treinados com HuggingFace<a class="anchor-link" href="#Uso de modelos pre-treinados com HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 77" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para usar modelos pré-treinados de <code>embeddings</code> vamos a usar a biblioteca <code>transformers</code> do <code>huggingface</code>. Para instalá-la com Conda usamos</p>
      <div class='highlight'><pre><code class="language-bash">conda install -c conda-forge transformers</code></pre></div>
      <p>E para instalá-lo com pip usamos</p>
      <div class='highlight'><pre><code class="language-bash">pip install transformers</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Com a tarefa <code>feature-extraction</code> do <code>huggingface</code> podemos usar modelos pré-treinados para obter os embeddings das palavras. Para isso, primeiro importamos a biblioteca necessária.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a obter os <code>embeddings</code> de <code>BERT</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>',
      '<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;feature-extraction&quot;</span><span class="p">,</span><span class="n">framework</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span><span class="n">model</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver os <code>embeddings</code> da palavra <code>rei</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;rey&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">embedding</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([3, 768])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, obtemos um vetor de <code>768</code> dimensões, ou seja, os <code>embeddings</code> do <code>BERT</code> têm <code>768</code> dimensões. Por outro lado, vemos que tem 3 vetores de <code>embeddings</code>, isso ocorre porque o <code>BERT</code> adiciona um token no início e outro no final da frase, portanto, apenas nos interessa o vetor do meio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a fazer o exemplo novamente no qual verificamos a similaridade da palavra <code>rainha</code> com o resultado de subtrair da palavra <code>rei</code> a palavra <code>homem</code> e adicionar a palavra <code>mulher</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding_hombre</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="n">embedding_mujer</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="n">embedding_rey</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;queen&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_rey</span> <span class="o">-</span> <span class="n">embedding_hombre</span> <span class="o">+</span> <span class="n">embedding_mujer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver a semelhança</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding_reina</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">embedding_reina</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="n">similarity</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '/tmp/ipykernel_33343/4248442045.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).',
          '&#x20;&#x20;embedding = torch.tensor(embedding).unsqueeze(0)',
          '/tmp/ipykernel_33343/4248442045.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).',
          '&#x20;&#x20;embedding_reina = torch.tensor(embedding_reina).unsqueeze(0)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '0.742547333240509',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Usando os <code>embeddings</code> do <code>BERT</code> também obtemos um resultado muito próximo de 1</p>
      </section>







    </div>

  </section>

</PostLayout>
