---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'GPT1 ‚Äì Improving Language Understanding by Generative Pre-Training';
const end_url = 'gpt1';
const description = 'Desbloqueie o poder da linguagem!!!! üí• Em minha √∫ltima postagem, apresentei o artigo GPT-1, explicando de forma clara e concisa como funciona esse modelo pioneiro no processamento de linguagem natural. E n√£o √© s√≥ isso! Tamb√©m mostro como fazer o ajuste fino do modelo para que voc√™ possa adapt√°-lo √†s suas necessidades espec√≠ficas üìä N√£o perca a oportunidade de conhecer um dos modelos mais influentes da hist√≥ria! üöÄ Leia minha postagem e descubra como voc√™ pode melhorar suas habilidades de intelig√™ncia artificial! üìÑ';
const keywords = 'gpt1, nlp, transformers, ajuste fino, modelo de linguagem, hugging face, pytorch';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_thumnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-06-12+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Papel"><h2>Papel</h2></a>
      <a class="anchor-link" href="#Arquitetura"><h2>Arquitetura</h2></a>
      <a class="anchor-link" href="#Resumo-do-artigo"><h2>Resumo do artigo</h2></a>
      <a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto"><h2>Gera√ß√£o de texto</h2></a>
      <a class="anchor-link" href="#Gerar-token-para-o-texto-do-token"><h3>Gerar token para o texto do token</h3></a>
      <a class="anchor-link" href="#Greedy-search"><h4>Greedy search</h4></a>
      <a class="anchor-link" href="#Ajuste-fino-do-GPT"><h2>Ajuste fino do GPT</h2></a>
      <a class="anchor-link" href="#C%C3%A1lculo-da-perda"><h3>C√°lculo da perda</h3></a>
      <a class="anchor-link" href="#Conjunto-de-dados"><h3>Conjunto de dados</h3></a>
      <a class="anchor-link" href="#Treinamento-Pytorch"><h3>Treinamento Pytorch</h3></a>
      <a class="anchor-link" href="#Conjunto-de-dados-Pytorch"><h4>Conjunto de dados Pytorch</h4></a>
      <a class="anchor-link" href="#Dataloader"><h4>Dataloader</h4></a>
      <a class="anchor-link" href="#Treinamento"><h4>Treinamento</h4></a>
      <a class="anchor-link" href="#Infer%C3%AAncia"><h4>Infer√™ncia</h4></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="GPT1---Aprimorando-a-compreens%C3%A3o-da-linguagem-por-meio-de-pr%C3%A9-treinamento-generativo">GPT1 - Aprimorando a compreens√£o da linguagem por meio de pr√©-treinamento generativo<a class="anchor-link" href="#GPT1---Aprimorando-a-compreens%C3%A3o-da-linguagem-por-meio-de-pr%C3%A9-treinamento-generativo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Papel">Papel<a class="anchor-link" href="#Papel"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
      <p>[Improving Language Understanding by Generative Pre-Training] (<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="nofollow noreferrer">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a>) √© o artigo GPT1. Antes de ler a postagem, √© necess√°rio se colocar na situa√ß√£o: antes do GPT, os modelos de linguagem eram baseados em redes recorrentes (RNN), que eram redes que funcionavam relativamente bem para tarefas espec√≠ficas, mas com as quais n√£o era poss√≠vel reutilizar o pr√©-treinamento para fazer um ajuste fino para outras tarefas. Elas tamb√©m n√£o tinham muita mem√≥ria, portanto, se voc√™ colocasse frases muito longas nelas, elas n√£o se lembravam muito bem do in√≠cio da frase.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Arquitetura">Arquitetura<a class="anchor-link" href="#Arquitetura"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 32" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de falarmos sobre a arquitetura do GPT1, vamos nos lembrar de como era a arquitetura dos Transformers.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="arquitetura do transformador" src="https://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp" width="852" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O GPT1 √© um modelo baseado nos decodificadores de transformador, portanto, como n√£o temos um codificador, a arquitetura de um √∫nico decodificador √© a seguinte</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="arquitetura do decodificador" src="https://maximofn.com/wp-content/uploads/2024/06/transformer_decoder_only-scaled.webp" width="1200" height="1069"/></p>
      <p>O mecanismo de aten√ß√£o entre a senten√ßa do codificador e do decodificador √© eliminado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No documento GPT1, eles prop√µem a seguinte arquitetura</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="arquitetura gpt1" src="https://maximofn.com/wp-content/uploads/2024/06/GPT1_architecture.webp" width="310" height="604"/></p>
      <p>O que corresponde ao decodificador de um transformador, como vimos anteriormente, executado 12 vezes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resumo-do-artigo">Resumo do artigo<a class="anchor-link" href="#Resumo-do-artigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As ideias mais interessantes do artigo s√£o:</p>
      <ul>
      <li>O modelo √© treinado em um grande corpus de texto n√£o supervisionado. Isso cria um modelo de linguagem. Um modelo de linguagem de alta capacidade √© criado em um grande corpus de texto.</li>
      <li>O ajuste fino √© ent√£o realizado em tarefas supervisionadas de NLP com conjuntos de dados rotulados. O ajuste fino √© realizado em uma tarefa-alvo supervisionada. Al√©m disso, quando o modelo √© avaliado na tarefa supervisionada, ele n√£o √© avaliado apenas nessa tarefa, mas em qu√£o bem ele prev√™ o pr√≥ximo token, o que ajuda a aprimorar a generaliza√ß√£o do modelo supervisionado e faz com que o modelo converse mais rapidamente.</li>
      <li>Embora j√° tenhamos mencionado isso, o documento diz que a arquitetura do transformador √© usada, j√° que at√© aquele momento os RNNs eram usados para os modelos de linguagem. Isso levou a uma melhoria no sentido de que o que foi aprendido no primeiro treinamento (treinamento no corpus de texto n√£o supervisionado) √© mais f√°cil de transferir para tarefas supervisionadas. Ou seja, gra√ßas ao uso de transformadores, foi poss√≠vel treinar em um corpus inteiro de texto e depois fazer o ajuste fino em tarefas supervisionadas.</li>
      <li>Eles testaram o modelo em quatro tipos de tarefas de compreens√£o de linguagem:<ul>
      <li>Infer√™ncia de linguagem natural</li>
      <li>Resposta √†s perguntas</li>
      <li>Similaridade sem√¢ntica</li>
      <li>Classifica√ß√£o de textos.</li>
      </ul>
      </li>
      <li>O modelo geral (aquele treinado em todo o corpus de texto n√£o supervisionado) supera os modelos RNN treinados de forma discriminat√≥ria que empregam arquiteturas espec√≠ficas de tarefas, melhorando significativamente o estado da arte em 9 das 12 tarefas estudadas. Eles tamb√©m analisaram os comportamentos de "disparo zero" do modelo pr√©-treinado em quatro ambientes diferentes e mostraram que ele adquire conhecimento lingu√≠stico √∫til para tarefas subsequentes.</li>
      <li>Nos √∫ltimos anos, os pesquisadores demonstraram os benef√≠cios do uso de embeddings, que s√£o treinados em corpora n√£o rotulados, para melhorar o desempenho em v√°rias tarefas. No entanto, essas abordagens transferem informa√ß√µes principalmente no n√≠vel da palavra, enquanto o uso de transformadores treinados em grandes corpora de texto n√£o supervisionados captura a sem√¢ntica de n√≠vel superior, no n√≠vel da frase.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Gera%C3%A7%C3%A3o-de-texto">Gera√ß√£o de texto<a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como gerar texto com um GPT1 pr√©-treinado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, voc√™ precisa instalar o <code>ftfy</code> e o <code>spacy</code> via</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ftfy<span class="w"> </span>spacy
      <span class="sb">```</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Depois de instalado, voc√™ deve fazer o download do modelo de idioma spacy que deseja usar. Por exemplo, para fazer o download do modelo em ingl√™s, voc√™ pode executar:</p>
      <div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>spacy<span class="w"> </span>download<span class="w"> </span>en_core_web_sm
      <span class="sb">```</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para gerar texto, usaremos o modelo do reposit√≥rio <a href="https://huggingface.co/openai-community/openai-gpt" target="_blank" rel="nofollow noreferrer">GPT1</a> do Hugging Face.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importamos as bibliotecas</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTTokenizer</span><span class="p">,</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Se voc√™ notar, importamos o <code>OpenAIGPTTokenizer</code> e o <code>AutoTokenizer</code>. Isso ocorre porque no <a href="https://huggingface.co/openai-community/openai-gpt" target="_blank" rel="nofollow noreferrer">model card</a> do GPT1 diz para usar o <code>OpenAIGPTTokenizer</code>, mas na postagem da biblioteca <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> explicamos que voc√™ deve usar o <code>AutoTokenizer</code> para carregar o tokenizador. Ent√£o, vamos tentar os dois</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTTokenizer</span><span class="p">,</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
          '</span><span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">OpenAIGPTTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello, my dog is cute and"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="n">input_auto_tokens</span> <span class="o">=</span> <span class="n">auto_tokenizer</span><span class="p">(</span><span class="s2">"Hello, my dog is cute and"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input auto tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_auto_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'input tokens: ',
          '{\'input_ids\': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1, 1]])}',
          'input auto tokens: ',
          '{\'input_ids\': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1, 1]])}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como voc√™ pode ver, com os dois tokenizadores voc√™ obt√©m os mesmos tokens. Portanto, para tornar o c√≥digo mais geral, de modo que, se voc√™ alterar os pontos de verifica√ß√£o, n√£o precisar√° alterar o c√≥digo, vamos usar o <code>AutoTokenizer</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Em seguida, criamos o dispositivo, o tokenizador e o modelo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Como instanciamos o modelo, vamos ver quantos par√¢metros ele tem</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '</span><span class="n">params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">params</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Number of parameters: 117M',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Na era de bilh√µes de par√¢metros, podemos ver que o GPT1 tinha apenas 117 milh√µes de par√¢metros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos os tokens de entrada para o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_sentence</span> <span class="o">=</span> <span class="s2">"Hello, my dog is cute and"</span>',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '',
          '<span class="n">input_tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'input_ids\': tensor([[3570,  240,  547, 2585,  544, 4957,  488]], device=\'cuda:0\'), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1, 1]], device=\'cuda:0\')}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>N√≥s os passamos para o modelo para gerar os tokens de sa√≠da.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>
      
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">output_tokens</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>output tokens: 
      tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,
                 485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],
             device='cuda:0')
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
        warnings.warn(
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Decodificamos os tokens para obter a declara√ß√£o de sa√≠da</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">output_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded output: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'output tokens: ',
          'tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,',
          '           485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],',
          '       device=\'cuda:0\')',
          'decoded output: ',
          'hello, my dog is cute and i\'m going to take him for a walk. " ',
          ' "',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>J√° conseguimos gerar texto com o GPT1</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Gerar-token-para-o-texto-do-token">Gerar token para o texto do token<a class="anchor-link" href="#Gerar-token-para-o-texto-do-token"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Greedy-search">Greedy search<a class="anchor-link" href="#Greedy-search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usamos o <code>model.generate</code> para gerar os tokens de sa√≠da de uma s√≥ vez, mas vamos ver como ger√°-los um a um. Para fazer isso, em vez de usar <code>model.generate</code>, usaremos <code>model</code>, que, na verdade, chama o m√©todo <code>model.forward</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '',
          '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],',
          '         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],',
          '         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],',
          '         ...,',
          '         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],',
          '         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],',
          '         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],',
          '       device=\'cuda:0\', grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que isso gera muitos dados, mas primeiro vamos dar uma olhada nas chaves de sa√≠da.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'odict_keys([\'logits\'])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Nesse caso, temos apenas os logits do modelo, vamos ver seu tamanho.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
          '',
          '<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 7, 40478])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver quantos tokens t√≠nhamos na entrada.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 7])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Bem, temos o mesmo n√∫mero de logits na sa√≠da e na entrada. Isso √© normal</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos os logits da √∫ltima posi√ß√£o da sa√≠da</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
          '',
          '<span class="n">nex_token_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([40478])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>H√° um total de 40478 logits, ou seja, h√° um vocabul√°rio de 40478 tokens e temos que ver qual token tem a maior probabilidade.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">softmax_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([40478])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(tensor(0.1898, device=\'cuda:0\', grad_fn=&lt;MaxBackward0&gt;),',
          ' tensor(249, device=\'cuda:0\'))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtivemos o seguinte token, agora vamos decodific√°-lo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'i\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtivemos o seguinte token usando o m√©todo guloso, ou seja, o token com a maior probabilidade. Mas j√° vimos na postagem sobre a biblioteca de transformadores, as [formas de gerar textos] (<a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto</a>) que podem ser feitas por amostragem, top-k, top-p, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos colocar tudo em uma fun√ß√£o e ver o que acontece se gerarmos alguns tokens.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
      '          <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '          <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '          <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '          <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
      '          <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '          <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '          <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '          <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
      '<span></span><span class="k">def</span> <span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
      '          <span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>',
      '          <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>',
      '              <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
      '              <span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
      '          <span class="k">return</span> <span class="n">generated_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Agora vamos gerar o texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
          '    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
          '    <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
          '    <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '    <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '    <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
          '</span><span class="k">def</span> <span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
          '    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>',
          '    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>',
          '        <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
          '        <span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
          '    <span class="k">return</span> <span class="n">generated_text</span>',
          '</span><span class="n">generate_greedy_text</span><span class="p">(</span><span class="s2">"Hello, my dog is cute and"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'Hello, my dog is cute andi."\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>O resultado √© bastante repetitivo, como j√° foi visto em [ways to generate text] (<a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto</a>).</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Ajuste-fino-do-GPT">Ajuste fino do GPT<a class="anchor-link" href="#Ajuste-fino-do-GPT"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 37" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="C%C3%A1lculo-da-perda">C√°lculo da perda<a class="anchor-link" href="#C%C3%A1lculo-da-perda"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 38" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de come√ßarmos a fazer o ajuste fino do GPT1, vamos dar uma olhada em um aspecto. Antes, quando costum√°vamos obter a sa√≠da do modelo, faz√≠amos o seguinte</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '',
          '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],',
          '         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],',
          '         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],',
          '         ...,',
          '         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],',
          '         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],',
          '         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],',
          '       device=\'cuda:0\', grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Voc√™ pode ver que obtemos <code>loss=None</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'None',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como precisaremos da perda para fazer o ajuste fino, vamos ver como obt√™-la.</p>
      <p>Se consultarmos a documenta√ß√£o do m√©todo <a href="https://huggingface.co/docs/transformers/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel.forward" target="_blank" rel="nofollow noreferrer">forward</a> de <code>OpenAIGPTLMHeadModel</code>, veremos que ele diz que a sa√≠da retorna um objeto do tipo <code>transformers.modeling_outputs.CausalLMOutput</code>, portanto, se consultarmos a documenta√ß√£o de <a href="https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput">transformers.modeling_outputs.CausalLMOutput</a>, veremos que ele diz que retorna <code>loss</code> se <code>labels</code> for passado para o m√©todo <code>forward</code>.</p>
      <p>Se acessarmos o c√≥digo-fonte do m√©todo <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai/modeling_openai.py#L544" target="_blank" rel="nofollow noreferrer">forward</a>, veremos este bloco de c√≥digo</p>
      <div class="highlight"><pre><span></span><span class="n">perda</span> <span class="o">=</span> <span class="n">Nenhuma</span>
              <span class="n">se</span> <span class="n">labels</span> <span class="n">n√£o</span> <span class="k">for</span> <span class="kc">None</span><span class="p">:</span>
                  <span class="c1"># Deslocamento de modo que os tokens &lt; n prevejam n</span>
                  <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">lm_logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
                  <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
                  <span class="c1"># Achatar os tokens</span>
                  <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
                  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
      <span class="err">```</span>
      
      <span class="n">Em</span> <span class="n">outras</span> <span class="n">palavras</span><span class="p">,</span> <span class="n">a</span> <span class="s2">"perda"</span> <span class="n">√©</span> <span class="n">calculada</span> <span class="n">da</span> <span class="n">seguinte</span> <span class="n">forma</span>
      
       <span class="o">*</span> <span class="n">Deslocamento</span> <span class="n">de</span> <span class="n">logits</span> <span class="n">e</span> <span class="n">r√≥tulos</span><span class="p">:</span> <span class="n">a</span> <span class="n">primeira</span> <span class="n">parte</span> <span class="n">√©</span> <span class="n">deslocar</span> <span class="n">logits</span> <span class="p">(</span><span class="err">`</span><span class="n">lm_logits</span><span class="err">`</span><span class="p">)</span> <span class="n">e</span> <span class="n">r√≥tulos</span> <span class="p">(</span><span class="err">`</span><span class="n">labels</span><span class="err">`</span><span class="p">)</span> <span class="n">para</span> <span class="n">que</span> <span class="err">`</span><span class="n">tokens</span> <span class="o">&lt;</span> <span class="n">n</span><span class="err">`</span> <span class="n">prevejam</span> <span class="err">`</span><span class="n">n</span><span class="err">`</span><span class="p">,</span> <span class="n">ou</span> <span class="n">seja</span><span class="p">,</span> <span class="n">a</span> <span class="n">partir</span> <span class="n">de</span> <span class="n">uma</span> <span class="n">posi√ß√£o</span> <span class="err">`</span><span class="n">n</span><span class="err">`</span><span class="p">,</span> <span class="n">o</span> <span class="n">pr√≥ximo</span> <span class="n">token</span> <span class="n">√©</span> <span class="n">previsto</span> <span class="n">a</span> <span class="n">partir</span> <span class="n">dos</span> <span class="n">anteriores</span><span class="o">.</span>
       <span class="o">*</span> <span class="n">CrossEntropyLoss</span><span class="p">:</span> <span class="n">√©</span> <span class="n">criada</span> <span class="n">uma</span> <span class="n">inst√¢ncia</span> <span class="n">da</span> <span class="n">fun√ß√£o</span> <span class="n">de</span> <span class="n">perda</span> <span class="err">`</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="err">`</span><span class="o">.</span>
       <span class="o">*</span> <span class="n">Achatar</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">e</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">s√£o</span> <span class="n">achatados</span> <span class="n">usando</span> <span class="err">`</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="err">`</span> <span class="n">e</span> <span class="err">`</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="err">`</span><span class="p">,</span> <span class="n">respectivamente</span><span class="o">.</span> <span class="n">Isso</span> <span class="n">√©</span> <span class="n">feito</span> <span class="n">para</span> <span class="n">que</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">e</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">tenham</span> <span class="n">a</span> <span class="n">mesma</span> <span class="n">forma</span> <span class="n">para</span> <span class="n">a</span> <span class="n">fun√ß√£o</span> <span class="n">de</span> <span class="n">perda</span><span class="o">.</span>
       <span class="o">*</span> <span class="n">C√°lculo</span> <span class="n">da</span> <span class="n">perda</span><span class="p">:</span> <span class="n">finalmente</span><span class="p">,</span> <span class="n">a</span> <span class="n">perda</span> <span class="n">√©</span> <span class="n">calculada</span> <span class="n">usando</span> <span class="n">a</span> <span class="n">fun√ß√£o</span> <span class="n">de</span> <span class="n">perda</span> <span class="err">`</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="err">`</span> <span class="n">com</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">achatados</span> <span class="n">e</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">achatados</span> <span class="n">como</span> <span class="n">entradas</span><span class="o">.</span>
      
      <span class="n">Em</span> <span class="n">resumo</span><span class="p">,</span> <span class="n">a</span> <span class="s2">"perda"</span> <span class="n">√©</span> <span class="n">calculada</span> <span class="n">como</span> <span class="n">a</span> <span class="n">perda</span> <span class="n">de</span> <span class="n">entropia</span> <span class="n">cruzada</span> <span class="n">entre</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">deslocados</span> <span class="n">e</span> <span class="n">achatados</span> <span class="n">e</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">deslocados</span> <span class="n">e</span> <span class="n">achatados</span><span class="o">.</span>
      
      <span class="n">Portanto</span><span class="p">,</span> <span class="n">se</span> <span class="n">passarmos</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">para</span> <span class="n">o</span> <span class="n">m√©todo</span> <span class="err">`</span><span class="n">forward</span><span class="err">`</span><span class="p">,</span> <span class="n">ele</span> <span class="n">retornar√°</span> <span class="n">a</span> <span class="err">`</span><span class="n">perda</span><span class="err">`</span><span class="o">.</span>
      </pre></div>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>',
          '',
          '<span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'tensor(4.2607, device=\'cuda:0\', grad_fn=&lt;NllLossBackward0&gt;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Conjunto-de-dados">Conjunto de dados<a class="anchor-link" href="#Conjunto-de-dados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 39" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para o treinamento, usaremos um conjunto de dados de piadas em ingl√™s [short-jokes-dataset] (<a href="https://huggingface.co/datasets/Maximofn/short-jokes-dataset" target="_blank" rel="nofollow noreferrer">https://huggingface.co/datasets/Maximofn/short-jokes-dataset</a>), que √© um conjunto de dados com 231 mil piadas em ingl√™s.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Baixamos o conjunto de dados</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
          '',
          '<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"Maximofn/short-jokes-dataset"</span><span class="p">)</span>',
          '<span class="n">jokes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'DatasetDict({',
          '    train: Dataset({',
          '        features: [\'ID\', \'Joke\'],',
          '        num_rows: 231657',
          '    })',
          '})',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada nisso</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'ID\': 1,',
          ' \'Joke\': \'[me narrating a documentary about narrators] "I can\'t hear what they\'re saying cuz I\'m talking"\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Treinamento-Pytorch">Treinamento Pytorch<a class="anchor-link" href="#Treinamento-Pytorch"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 40" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, vamos dar uma olhada em como o treinamento puro do Pytorch seria feito.</p>
      <blockquote>
      <p>Reinicie o notebook para que n√£o haja problemas de mem√≥ria da GPU</p>
      </blockquote>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <h4 id="Conjunto-de-dados-Pytorch">Conjunto de dados Pytorch<a class="anchor-link" href="#Conjunto-de-dados-Pytorch"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 41" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criar uma classe de conjunto de dados do Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '              ',
      '          <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '              <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '              <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>





















      
      <section class="section-block-markdown-cell">
      <p>N√≥s o instanciamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '              ',
      '          <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '              <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '              <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Aqui est√° um exemplo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
      <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&lt;|endoftext|&gt;
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[27]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>(torch.Size([1, 30]), torch.Size([1, 30]))</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dataloader">Dataloader<a class="anchor-link" href="#Dataloader"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 42" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, criamos um carregador de dados do Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '              ',
      '          <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '              <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '              <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
      '<span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
      '      <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>',
      '      ',
      '      <span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>',
      '      <span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Vemos um lote</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
          '',
          '<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
          '',
          '<span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
          '    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
          '        ',
          '    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
          '        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
          '',
          '    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
          '        <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
          '        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '        <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
          '</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
          '</span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
          '<span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
          '</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>',
          '',
          '<span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>',
          '<span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '</span><span class="n">sentences</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">))</span>',
          '<span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'JOKE: Why can\'t Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&lt;|endoftext|&gt;',
          '(1, torch.Size([1, 1, 29]), torch.Size([1, 1, 29]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 43" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>
      <span class="kn">import</span> <span class="nn">tqdm</span>
      
      <span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>
      <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-5</span>
      <span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>
      <span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>
      
      <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      <span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
      <span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>
      
      <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>
      
      <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
          
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"EPOCH </span><span class="si">{opening_brace}</span><span class="n">epoch</span><span class="si">{closing_brace}</span><span class="s2"> started"</span> <span class="o">+</span> <span class="s1">'='</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
          <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>
          
          <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
      
              <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>
              
              <span class="c1">#################### "Fit as many joke sequences into MAX_SEQ_LEN sequence as possible" logic start ####</span>
              <span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
      
              <span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>
              <span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
                  <span class="k">continue</span>
              
              <span class="c1"># The first joke sequence in the sequence</span>
              <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>
                  <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
                  <span class="k">continue</span>
              <span class="k">else</span><span class="p">:</span>
                  <span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>
                  <span class="c1"># as the start for next sequence </span>
                  <span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
                      <span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>
                      <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="c1">#Add the joke to sequence, continue and try to add more</span>
                      <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                      <span class="k">continue</span>
              <span class="c1">################## Sequence ready, process it trough the model ##################</span>
                  
              <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>
              <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
              <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                             
              <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>
              <span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
                  <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>    
                  <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>
                  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                  <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> 
                  <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                  <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      
              <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({opening_brace}</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">'lr'</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]{closing_brace})</span>
              <span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
                  <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 0 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:31&lt;00:00, 334.88it/s, loss=2.88, lr=2.93e-6]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 1 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:30&lt;00:00, 335.27it/s, loss=2.49, lr=5.87e-6]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 2 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:17&lt;00:00, 341.75it/s, loss=2.57, lr=8.81e-6]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 3 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:18&lt;00:00, 341.27it/s, loss=2.41, lr=1.18e-5]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 4 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:19&lt;00:00, 341.04it/s, loss=2.49, lr=1.47e-5]
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Infer%C3%AAncia">Infer√™ncia<a class="anchor-link" href="#Infer%C3%AAncia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 44" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como o modelo faz piadas.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>',
          '<span class="kn">import</span> <span class="nn">tqdm</span>',
          '',
          '<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>',
          '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>',
          '<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-5</span>',
          '<span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>',
          '<span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>',
          '',
          '<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
          '<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>',
          '<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
          '<span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>',
          '<span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
          '',
          '<span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>',
          '',
          '<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>',
          '    ',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> started"</span> <span class="o">+</span> <span class="s1">\'=\'</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>',
          '    <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>',
          '    ',
          '    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
          '',
          '        <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>',
          '        ',
          '        <span class="c1">#################### "Fit as many joke sequences into MAX_SEQ_LEN sequence as possible" logic start ####</span>',
          '        <span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '',
          '        <span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>',
          '        <span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
          '            <span class="k">continue</span>',
          '        ',
          '        <span class="c1"># The first joke sequence in the sequence</span>',
          '        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>',
          '            <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
          '            <span class="k">continue</span>',
          '        <span class="k">else</span><span class="p">:</span>',
          '            <span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>',
          '            <span class="c1"># as the start for next sequence </span>',
          '            <span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
          '                <span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>',
          '                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
          '            <span class="k">else</span><span class="p">:</span>',
          '                <span class="c1">#Add the joke to sequence, continue and try to add more</span>',
          '                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
          '                <span class="k">continue</span>',
          '        <span class="c1">################## Sequence ready, process it trough the model ##################</span>',
          '            ',
          '        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>',
          '        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
          '        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>',
          '                       ',
          '        <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>',
          '        <span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>',
          '            <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>    ',
          '            <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>',
          '            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>',
          '            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> ',
          '            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
          '            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
          '',
          '        <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">\'loss\'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">\'lr\'</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]})</span>',
          '        <span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>',
          '            <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
          '</span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"JOKE:"</span>',
          '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
          '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning',
          '  warnings.warn(',
          'decoded joke: ',
          'joke : what do you call a group of people who are not afraid of the dark? a group',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Voc√™ pode ver que voc√™ passa uma sequ√™ncia com a palavra <code>joke</code> e ele retorna uma piada. Mas se voc√™ retornar outra string, ela n√£o retornar√°</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"My dog is cute and"</span>',
          '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
          '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'decoded joke: ',
          'my dog is cute and i\'m not sure if i should be offended or not. " ',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      






    </div>

  </section>

</PostLayout>
