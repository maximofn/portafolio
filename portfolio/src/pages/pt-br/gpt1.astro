---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'GPT1 ‚Äì Improving Language Understanding by Generative Pre-Training';
const end_url = 'gpt1';
const description = 'Desbloqueie o poder da linguagem!!!! üí• Em minha √∫ltima postagem, apresentei o artigo GPT-1, explicando de forma clara e concisa como funciona esse modelo pioneiro no processamento de linguagem natural. E n√£o √© s√≥ isso! Tamb√©m mostro como fazer o ajuste fino do modelo para que voc√™ possa adapt√°-lo √†s suas necessidades espec√≠ficas üìä N√£o perca a oportunidade de conhecer um dos modelos mais influentes da hist√≥ria! üöÄ Leia minha postagem e descubra como voc√™ pode melhorar suas habilidades de intelig√™ncia artificial! üìÑ';
const keywords = 'gpt1, nlp, transformers, ajuste fino, modelo de linguagem, hugging face, pytorch';
const languaje = 'PT';
const image_path = 'https://images.maximofn.com/GPT1_thumnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-06-12+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Artigo"><h2>Artigo</h2></a>
      <a class="anchor-link" href="#Arquitetura"><h2>Arquitetura</h2></a>
      <a class="anchor-link" href="#Resumo do artigo"><h2>Resumo do artigo</h2></a>
      <a class="anchor-link" href="#Geracao de texto"><h2>Gera√ß√£o de texto</h2></a>
      <a class="anchor-link" href="#Gerar texto token a token"><h3>Gerar texto token a token</h3></a>
      <a class="anchor-link" href="#Busca gulosa"><h4>Busca gulosa</h4></a>
      <a class="anchor-link" href="#Ajuste fino do GPT"><h2>Ajuste fino do GPT</h2></a>
      <a class="anchor-link" href="#Calculo da perda"><h3>C√°lculo da perda</h3></a>
      <a class="anchor-link" href="#Conjunto de Dados"><h3>Conjunto de Dados</h3></a>
      <a class="anchor-link" href="#Treinamento com Pytorch"><h3>Treinamento com Pytorch</h3></a>
      <a class="anchor-link" href="#Conjunto de dados do Pytorch"><h4>Conjunto de dados do Pytorch</h4></a>
      <a class="anchor-link" href="#Dataloader"><h4>Dataloader</h4></a>
      <a class="anchor-link" href="#Treinamento"><h4>Treinamento</h4></a>
      <a class="anchor-link" href="#Inferencia"><h4>Infer√™ncia</h4></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Artigo">Artigo<a class="anchor-link" href="#Artigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="nofollow noreferrer">Melhorando a Compreens√£o de Linguagem por Pr√©-Treinamento Gerativo</a> √© o paper do GPT1. Antes de ler o post, √© necess√°rio que voc√™ se situe: antes do GPT, os modelos de linguagem estavam baseados em redes recorrentes (RNN), que eram redes que funcionavam relativamente bem para tarefas espec√≠ficas, mas com as quais n√£o era poss√≠vel reutilizar o pr√©-treinamento para fazer um fine tuning para outras tarefas. Al√©m disso, elas n√£o tinham muita mem√≥ria, ent√£o se fossem alimentadas com frases muito longas, n√£o lembravam muito bem o in√≠cio da frase.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Arquitetura">Arquitetura<a class="anchor-link" href="#Arquitetura"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 32" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de falar da arquitetura do GPT1, vamos lembrar como era a arquitetura dos transformers.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://images.maximofn.com/transformer-scaled.webp" alt="arquitetura do transformer">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>GPT1 √© um modelo baseado nos decodificadores dos transformers, ent√£o, como n√£o temos codificador, a arquitetura de um √∫nico decodificador fica da seguinte maneira</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://images.maximofn.com/transformer_decoder_only-scaled.webp" alt="decoder architecture">
      <p>O mecanismo de aten√ß√£o entre a senten√ßa do encoder e do decoder √© removido.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No o artigo do GPT1 prop√µem a seguinte arquitetura</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://images.maximofn.com/GPT1_architecture.webp" alt="arquitetura do gpt1">
      <p>Que corresponde ao decoder de um transformer como vimos anteriormente, executado 12 vezes</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resumo do artigo">Resumo do artigo<a class="anchor-link" href="#Resumo do artigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As ideias mais interessantes do paper s√£o:</p>
      <ul>
        <li>O modelo √© treinado em um grande corpus de texto sem supervis√£o. Com isso, cria-se um modelo de linguagem. Cria-se um modelo de linguagem de alta capacidade em um grande corpus de texto.</li>
        <li>Em seguida, √© feito um fine-tuning em tarefas de NLP supervisionadas com conjuntos de dados rotulados. Realiza-se um ajuste fino em uma tarefa objetivo com supervis√£o. Al√©m disso, quando o modelo √© avaliado na tarefa supervisionada, n√£o √© avaliado apenas por essa tarefa, mas tamb√©m pelo qu√£o bem ele prev√™ o pr√≥ximo token, o que ajuda a melhorar a generaliza√ß√£o do modelo supervisionado e faz com que o modelo convirja mais rapidamente.</li>
        <li>Embora j√° tenhamos mencionado, no artigo diz que √© utilizada a arquitetura transformer, pois at√© aquele momento eram usadas RNN para os modelos de linguagem. Isso resultou em uma melhoria na qual o aprendizado do primeiro treinamento (treinamento no corpus de texto sem supervis√£o) √© mais f√°cil de transferir para tarefas supervisionadas. Ou seja, gra√ßas ao uso de transformers, foi poss√≠vel realizar um treinamento em todo um corpus de texto e, posteriormente, ajustes finos em tarefas supervisionadas.</li>
        <li>Avaliaram o modelo em quatro tipos de tarefas de compreens√£o da linguagem:</li>
        <li>Infer√™ncia da linguagem natural* Resposta a perguntas</li>
        <li>Similaridade sem√¢ntica</li>
        <li>Classifica√ß√£o de textos.</li>
        <li>O modelo geral (treinado em todo o corpus de texto sem supervis√£o) supera os modelos RNN treinados discriminativamente que utilizam arquiteturas projetadas especificamente para cada tarefa, melhorando significativamente o estado da arte em 9 das 12 tarefas estudadas. Eles tamb√©m analisaram os comportamentos de "tiro zero" do modelo pr√©-treinado em quatro ambientes diferentes e demonstraram que ele adquire um conhecimento lingu√≠stico √∫til para as tarefas subsequentes.</li>
        <li>Nos √∫ltimos anos, os pesquisadores haviam demonstrado os benef√≠cios de utilizar embeddings, que s√£o treinados em corpora n√£o anotados, para melhorar o desempenho em uma variedade de tarefas. No entanto, essas abordagens transferem principalmente informa√ß√µes a n√≠vel de palavra, enquanto o uso de transformers treinados em grandes corpora de texto sem supervis√£o captura a sem√¢ntica de n√≠vel superior, a n√≠vel de frase.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Geracao de texto">Gera√ß√£o de texto<a class="anchor-link" href="#Geracao de texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver como gerar texto com um GPT1 pr√©-treinado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro √© necess√°rio instalar <code>ftfy</code> e <code>spacy</code> atrav√©s de</p>
      <div class='highlight'><pre><code class="language-bash">pip install ftfy spacy</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez instaladas, voc√™ deve baixar o modelo de linguagem do spaCy que deseja usar. Por exemplo, para baixar o modelo em ingl√™s, voc√™ pode executar:</p>
      <div class='highlight'><pre><code class="language-bash">python -m spacy download en_core_web_sm</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para gerar texto vamos utilizar o modelo do reposit√≥rio <a href="https://huggingface.co/openai-community/openai-gpt" target="_blank" rel="nofollow noreferrer">GPT1</a> da Hugging Face.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Importamos as bibliotecas</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIGPTTokenizer</span><span class="p">,</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se voc√™ reparar, importamos <code>OpenAIGPTTokenizer</code> e <code>AutoTokenizer</code>. Isso √© porque na <a href="https://huggingface.co/openai-community/openai-gpt" target="_blank" rel="nofollow noreferrer">model card</a> do GPT1 indica-se que se deve usar <code>OpenAIGPTTokenizer</code>, mas no post da biblioteca <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> explicamos que se deve usar <code>AutoTokenizer</code> para carregar o tokenizador. Ent√£o, vamos testar os dois.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">&quot;openai-community/openai-gpt&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">OpenAIGPTTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute and&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="n">input_auto_tokens</span> <span class="o">=</span> <span class="n">auto_tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute and&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input auto tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_auto_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'input tokens:',
          '&#x7B;&#x27;input_ids&#x27;: tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1]])&#x7D;',
          'input auto tokens:',
          '&#x7B;&#x27;input_ids&#x27;: tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1]])&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como pode ser visto com os dois tokenizadores, s√£o obtidos os mesmos tokens. Ent√£o, para que o c√≥digo seja mais geral, de forma que se os checkpoints forem alterados, n√£o seja necess√°rio alterar o c√≥digo, vamos utilizar <code>AutoTokenizer</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos ent√£o o dispositivo, o tokenizador e o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como instanciamos o modelo, vamos a ver quantos par√¢metros ele tem.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of parameters: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">params</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Number of parameters: 117M',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Na era dos trilh√µes de par√¢metros, podemos ver que o GPT1 tinha apenas 117 milh√µes de par√¢metros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos os tokens de entrada para o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_sentence</span> <span class="o">=</span> <span class="s2">&quot;Hello, my dog is cute and&quot;</span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;input_ids&#x27;: tensor([[3570,  240,  547, 2585,  544, 4957,  488]], device=&#x27;cuda:0&#x27;), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1]], device=&#x27;cuda:0&#x27;)&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Passamo-los ao modelo para gerar os tokens de sa√≠da</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">output_tokens</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'output tokens:',
          'tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Decodificamos os tokens para obter a frase de sa√≠da</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decoded output: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'decoded output:',
          'hello, my dog is cute and i&#x27;m going to take him for a walk. &quot;',
          '&quot;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>J√° conseguimos gerar texto com GPT1</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Gerar texto token a token">Gerar texto token a token<a class="anchor-link" href="#Gerar texto token a token"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Busca gulosa">Busca gulosa<a class="anchor-link" href="#Busca gulosa"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>N√≥s usamos <code>model.generate</code> para gerar os tokens de sa√≠da de uma s√≥ vez, mas vamos ver como ger√°-los um a um. Para isso, em vez de usar <code>model.generate</code> vamos usar <code>model</code>, que na verdade chama o m√©todo <code>model.forward</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;UnsafeViewBackward0&amp;gt;), hidden_states=None, attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que sa√≠ muitos dados, primeiro vamos ver as keys da sa√≠da</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'odict_keys([&#x27;logits&#x27;])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Neste caso, temos apenas os logits do modelo, vamos verificar seu tamanho.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="w"> </span>',
      '<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 7, 40478])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver quantos tokens t√≠nhamos na entrada</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 7])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Bem, na sa√≠da temos o mesmo n√∫mero de logits que na entrada. Isso √© normal.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos os logits da √∫ltima posi√ß√£o da sa√≠da</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">nex_token_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([40478])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>H√° um total de 40478 logits, ou seja, h√° um vocabul√°rio de 40478 tokens e temos que ver qual √© o token com a maior probabilidade. Para isso, primeiro calculamos a softmax.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">softmax_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'torch.Size([40478])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(tensor(0.1898, device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;MaxBackward0&amp;gt;),',
          'tensor(249, device=&#x27;cuda:0&#x27;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtivemos o seguinte token, agora vamos decodific√°-lo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;i&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtivemos o seguinte token atrav√©s do m√©todo greedy, ou seja, o token com maior probabilidade. Mas j√° vimos no post da biblioteca transformers as <a href="https://maximofn.com/hugging-face-transformers/#Formas-de-genera%C3%A7%C3%A3o-de-texto">formas de gerar textos</a> que podem ser feitas sampling, top-k, top-p, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a colocar tudo dentro de uma fun√ß√£o e ver o que sai se gerarmos alguns tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="w">    </span><span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '<span class="w">    </span><span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>',
      '<span class="w">        </span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
      '<span class="w">        </span><span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">generated_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora geramos texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generate_greedy_text</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute and&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;Hello, my dog is cute andi.&quot;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A sa√≠da √© bastante repetitiva, como j√° foi visto nas <a href="https://maximofn.com/hugging-face-transformers/#Formas-de-gera%C3%A7%C3%A3o-de-texto">formas de gerar textos</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Ajuste fino do GPT">Ajuste fino do GPT<a class="anchor-link" href="#Ajuste fino do GPT"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 37" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Calculo da perda">C√°lculo da perda<a class="anchor-link" href="#Calculo da perda"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 38" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de come√ßar a fazer o fine tuning do GPT1, vamos ver uma coisa. Antes, quando obt√≠nhamos a sa√≠da do modelo, faz√≠amos isso</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;...,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;[ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;UnsafeViewBackward0&amp;gt;), hidden_states=None, attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Pode-se ver que obtemos <code>loss=None</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'None',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vamos a precisar da loss para fazer o fine tuning, vamos a ver como obt√™-la.</p>
      <p>Se n√≥s formos √† documenta√ß√£o do m√©todo <a href="https://huggingface.co/docs/transformers/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel.forward" target="_blank" rel="nofollow noreferrer">forward</a> de <code>OpenAIGPTLMHeadModel</code>, podemos ver que diz que na sa√≠da retorna um objeto do tipo <code>transformers.modeling_outputs.CausalLMOutput</code>, ent√£o se formos √† documenta√ß√£o de <a href="https://huggingface.co/docs/transformers/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput">transformers.modeling_outputs.CausalLMOutput</a>, podemos ver que diz que retorna <code>loss</code> se for passado <code>labels</code> para o m√©todo <code>forward</code>.</p>
      <p>Se n√≥s formos √† fonte do c√≥digo do m√©todo <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai/modeling_openai.py#L544" target="_blank" rel="nofollow noreferrer">forward</a>, vemos este bloco de c√≥digo</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">perda = None<br>se labels n√£o for None:<br># Deslocar para que os tokens &lt; n prevejam n<br>shift_logits = lm_logits[..., :-1, :].contiguous()<br>shift_labels = labels[..., 1:].contiguous()<br># Aplanar os tokens<br>loss_fct = CrossEntropyLoss()<br>loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))</code></pre></div>
            </section>
      <p>Isso significa que a <code>loss</code> √© calculada da seguinte maneira</p>
      <ul>
        <li>Shift de logits e labels: A primeira parte √© deslocar os logits (<code>lm_logits</code>) e as etiquetas (<code>labels</code>) para que os <code>tokens < n</code> prevejam <code>n</code>, ou seja, a partir de uma posi√ß√£o <code>n</code> se prev√™ o pr√≥ximo token com base nos anteriores.</li>
        <li>CrossEntropyLoss: Cria-se uma inst√¢ncia da fun√ß√£o de perda <code>CrossEntropyLoss()</code>.</li>
        <li>Achatando tokens: A seguir, os logits e as etiquetas s√£o aplanados utilizando <code>view(-1, shift_logits.size(-1))</code> e <code>view(-1)</code>, respectivamente. Isso √© feito para que os logits e as etiquetas tenham a mesma forma para a fun√ß√£o de perda.</li>
        <li>C√°lculo da perda: Finalmente, calcula-se a perda utilizando a fun√ß√£o de perda <code>CrossEntropyLoss()</code> com os logits achatados e as etiquetas achatadas como entradas.</li>
      </ul>
      <p>Em resumo, a <code>loss</code> √© calculada como a perda de entropia cruzada entre os logits deslocados e achatados e as labels deslocadas e achatadas.</p>
      <p>Portanto, se passarmos os labels para o m√©todo <code>forward</code>, ele retornar√° a <code>loss</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'tensor(4.2607, device=&#x27;cuda:0&#x27;, grad_fn=&amp;lt;NllLossBackward0&amp;gt;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Conjunto de Dados">Conjunto de Dados<a class="anchor-link" href="#Conjunto de Dados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 39" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para o treinamento, vamos usar um dataset de piadas em ingl√™s <a href="https://huggingface.co/datasets/Maximofn/short-jokes-dataset" target="_blank" rel="nofollow noreferrer">short-jokes-dataset</a>, que √© um dataset com 231 mil piadas em ingl√™s.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Baixamos o dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>',
      '<span class="w"> </span>',
      '<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Maximofn/short-jokes-dataset&quot;</span><span class="p">)</span>',
      '<span class="n">jokes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'DatasetDict(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;train: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;ID&#x27;, &#x27;Joke&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 231657',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos v√™-lo um pouco</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">jokes</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;ID&#x27;: 1,',
          '&#x27;Joke&#x27;: &#x27;[me narrating a documentary about narrators] &quot;I can\&#x27;t hear what they\&#x27;re saying cuz I\&#x27;m talking&quot;&#x27;&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Treinamento com Pytorch">Treinamento com Pytorch<a class="anchor-link" href="#Treinamento com Pytorch"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 40" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro vamos ver como seria o treinamento com puro Pytorch</p>
      <blockquote>
      <p>Reiniciamos o notebook para que n√£o haja problemas com a mem√≥ria da GPU</p>
      </blockquote>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">&quot;openai-community/openai-gpt&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Conjunto de dados do Pytorch">Conjunto de dados do Pytorch<a class="anchor-link" href="#Conjunto de dados do Pytorch"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 41" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos uma classe Dataset do Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span>',
      '<span class="w"> </span>',
      '<span class="k">class</span><span class="w"> </span><span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">&quot;JOKE: &quot;</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">&quot;&amp;lt;|endoftext|&amp;gt;&quot;</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '<span class="w">        </span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '<span class="w">        </span><span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">])</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '<span class="w">        </span><span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">&quot;Joke&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '<span class="w">        </span><span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="w">        </span><span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>A instanciamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver um exemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
      '<span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'JOKE: Why can&#x27;t Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&amp;lt;|endoftext|&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '(torch.Size([1, 30]), torch.Size([1, 30]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dataloader">Dataloader<a class="anchor-link" href="#Dataloader"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 42" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos agora um dataloader do Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>',
      '<span class="w"> </span>',
      '<span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>',
      '<span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos um lote</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentences</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">))</span>',
      '<span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(1, torch.Size([1, 1, 29]), torch.Size([1, 1, 29]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 43" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>',
      '<span class="w"> </span>',
      '<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>',
      '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>',
      '<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-5</span>',
      '<span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>',
      '<span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
      '<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>',
      '<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>',
      '<span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
      '<span class="w"> </span>',
      '<span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>',
      '<span class="w"> </span>',
      '<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>',
      '<span class="w">    </span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> started&quot;</span> <span class="o">+</span> <span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Training&quot;</span><span class="p">)</span>',
      '<span class="w">    </span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>',
      '<span class="w">        </span>',
      '<span class="w">        </span><span class="c1">#################### &quot;Fit as many joke sequences into MAX_SEQ_LEN sequence as possible&quot; logic start ####</span>',
      '<span class="w">        </span><span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
      '<span class="w">            </span><span class="k">continue</span>',
      '<span class="w">        </span>',
      '<span class="w">        </span><span class="c1"># The first joke sequence in the sequence</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>',
      '<span class="w">            </span><span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
      '<span class="w">            </span><span class="k">continue</span>',
      '<span class="w">        </span><span class="k">else</span><span class="p">:</span>',
      '<span class="w">            </span><span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>',
      '<span class="w">            </span><span class="c1"># as the start for next sequence </span>',
      '<span class="w">            </span><span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
      '<span class="w">                </span><span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>',
      '<span class="w">                </span><span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
      '<span class="w">            </span><span class="k">else</span><span class="p">:</span>',
      '<span class="w">                </span><span class="c1">#Add the joke to sequence, continue and try to add more</span>',
      '<span class="w">                </span><span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="w">                </span><span class="k">continue</span>',
      '<span class="w">        </span><span class="c1">################## Sequence ready, process it trough the model ##################</span>',
      '<span class="w">            </span>',
      '<span class="w">        </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>',
      '<span class="w">        </span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
      '<span class="w">        </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>',
      '<span class="w">                       </span>',
      '<span class="w">        </span><span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>',
      '<span class="w">            </span><span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>',
      '<span class="w">            </span><span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>',
      '<span class="w">            </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>',
      '<span class="w">            </span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>',
      '<span class="w">            </span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
      '<span class="w">            </span><span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="w">        </span><span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]})</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>',
      '<span class="w">            </span><span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 0 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:31&amp;lt;00:00, 334.88it/s, loss=2.88, lr=2.93e-6]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 1 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:30&amp;lt;00:00, 335.27it/s, loss=2.49, lr=5.87e-6]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 2 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:17&amp;lt;00:00, 341.75it/s, loss=2.57, lr=8.81e-6]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 3 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:18&amp;lt;00:00, 341.27it/s, loss=2.41, lr=1.18e-5]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'EPOCH 4 started==============================',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:19&amp;lt;00:00, 341.04it/s, loss=2.49, lr=1.47e-5]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Inferencia">Infer√™ncia<a class="anchor-link" href="#Inferencia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 44" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver como o modelo faz piadas</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">&quot;JOKE:&quot;</span>',
      '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
      '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'decoded joke:',
          'joke : what do you call a group of people who are not afraid of the dark? a group',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Pode-se ver que voc√™ passa uma sequ√™ncia com a palavra <code>joke</code> e ele retorna uma piada. Mas se voc√™ retornar outra sequ√™ncia n√£o</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">&quot;My dog is cute and&quot;</span>',
      '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
      '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'decoded joke:',
          'my dog is cute and i&#x27;m not sure if i should be offended or not. &quot;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>


















    </div>

  </section>

</PostLayout>
