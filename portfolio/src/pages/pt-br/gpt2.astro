---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'GPT-2 ‚Äì Language Models are Unsupervised Multitask Learners';
const end_url = 'gpt2';
const description = 'Libere o poder da gera√ß√£o de texto com o GPT-2, o mais recente modelo aberto da OpenAI üí∏! üöÄ Nesta postagem, eu o conduzo pela arquitetura por tr√°s desse modelo e mostro como ajust√°-lo üòú, incluindo o c√≥digo. Leia mais e descubra como o GPT-2 pode tornar suas palavras mais interessantes do que as de um ser humano üí¨ (ou, pelo menos, de um ser humano entediado) üòâ';
const keywords = 'gpt2, openai, gera√ß√£o de texto, ajuste fino, nlp, processamento de linguagem natural';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT2.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=800
    image_height=436
    image_extension=webp
    article_date=2024-07-09+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Papel"><h2>Papel</h2></a>
      <a class="anchor-link" href="#Arquitetura"><h2>Arquitetura</h2></a>
      <a class="anchor-link" href="#Resumo-do-artigo"><h2>Resumo do artigo</h2></a>
      <a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto"><h2>Gera√ß√£o de texto</h2></a>
      <a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto-com-pipeline"><h3>Gera√ß√£o de texto com pipeline</h3></a>
      <a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto-com-modelo-autom%C3%A1tico"><h3>Gera√ß√£o de texto com modelo autom√°tico</h3></a>
      <a class="anchor-link" href="#Gerar-token-para-o-texto-do-token"><h3>Gerar token para o texto do token</h3></a>
      <a class="anchor-link" href="#Greedy-search"><h4>Greedy search</h4></a>
      <a class="anchor-link" href="#Arquitetura-dos-modelos-dispon%C3%ADveis-no-Hugging-Face"><h2>Arquitetura dos modelos dispon√≠veis no Hugging Face</h2></a>
      <a class="anchor-link" href="#GPT2Model"><h3>GPT2Model</h3></a>
      <a class="anchor-link" href="#GPT2LMHeadModel"><h3>GPT2LMHeadModel</h3></a>
      <a class="anchor-link" href="#GPT2ForSequenceClassification"><h3>GPT2ForSequenceClassification</h3></a>
      <a class="anchor-link" href="#GPT2ForQuestionAnswering"><h3>GPT2ForQuestionAnswering</h3></a>
      <a class="anchor-link" href="#GPT2ForTokenClassification"><h3>GPT2ForTokenClassification</h3></a>
      <a class="anchor-link" href="#Ajuste-fino-do-GPT-2"><h2>Ajuste fino do GPT-2</h2></a>
      <a class="anchor-link" href="#Ajuste-fino-para-gera%C3%A7%C3%A3o-de-texto"><h3>Ajuste fino para gera√ß√£o de texto</h3></a>
      <a class="anchor-link" href="#C%C3%A1lculo-de-perdas"><h4>C√°lculo de perdas</h4></a>
      <a class="anchor-link" href="#Conjunto-de-dados"><h4>Conjunto de dados</h4></a>
      <a class="anchor-link" href="#Inst%C3%A2ncia-do-modelo"><h4>Inst√¢ncia do modelo</h4></a>
      <a class="anchor-link" href="#Conjunto-de-dados-Pytorch"><h4>Conjunto de dados Pytorch</h4></a>
      <a class="anchor-link" href="#Dataloader"><h4>Dataloader</h4></a>
      <a class="anchor-link" href="#Treinamento"><h4>Treinamento</h4></a>
      <a class="anchor-link" href="#Infer%C3%AAncia"><h4>Infer√™ncia</h4></a>
      <a class="anchor-link" href="#Ajuste-fino-do-GPT-2-para-classifica%C3%A7%C3%A3o-de-senten%C3%A7as"><h3>Ajuste fino do GPT-2 para classifica√ß√£o de senten√ßas</h3></a>
      <a class="anchor-link" href="#Conjunto-de-dados"><h4>Conjunto de dados</h4></a>
      <a class="anchor-link" href="#Tokeniser"><h4>Tokeniser</h4></a>
      <a class="anchor-link" href="#Modelo"><h4>Modelo</h4></a>
      <a class="anchor-link" href="#Avalia%C3%A7%C3%A3o"><h4>Avalia√ß√£o</h4></a>
      <a class="anchor-link" href="#Trainer"><h4>Trainer</h4></a>
      <a class="anchor-link" href="#Treinamento"><h4>Treinamento</h4></a>
      <a class="anchor-link" href="#Infer%C3%AAncia"><h4>Infer√™ncia</h4></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="GPT-2---Modelos-de-linguagem-s%C3%A3o-aprendizes-multitarefa-n%C3%A3o-supervisionados">GPT-2 - Modelos de linguagem s√£o aprendizes multitarefa n√£o supervisionados<a class="anchor-link" href="#GPT-2---Modelos-de-linguagem-s%C3%A3o-aprendizes-multitarefa-n%C3%A3o-supervisionados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 64" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Papel">Papel<a class="anchor-link" href="#Papel"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 65" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
      <p>[Language Models are Unsupervised Multitask Learners] (<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="nofollow noreferrer">https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf</a>) √© o documento GPT-2. Essa √© a segunda vers√£o do modelo [GPT-1] (<a href="https://maximofn.com/gpt1/">https://maximofn.com/gpt1/</a>) que j√° vimos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Arquitetura">Arquitetura<a class="anchor-link" href="#Arquitetura"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 66" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de falarmos sobre a arquitetura da GPT-2, vamos nos lembrar de como era a arquitetura da GPT-1.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="arquitetura gpt1" src="https://maximofn.com/wp-content/uploads/2024/06/GPT1_architecture.webp" width="310" height="604"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma arquitetura baseada em transformador √© usada no GPT-2, como no [GPT-1] (<a href="https://maximofn.com/gpt1/">https://maximofn.com/gpt1/</a>), com os seguintes tamanhos</p>
      <table>
      <thead>
      <tr>
      <th>Parameters</th>
      <th>Layers</th>
      <th>d_model</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>117M</td>
      <td>12</td>
      <td>768</td>
      </tr>
      <tr>
      <td>345M</td>
      <td>24</td>
      <td>1024</td>
      </tr>
      <tr>
      <td>762M</td>
      <td>36</td>
      <td>1280</td>
      </tr>
      <tr>
      <td>1542M</td>
      <td>48</td>
      <td>1600</td>
      </tr>
      </tbody>
      </table>
      <p>O modelo menor √© equivalente ao GPT original, e o segundo modelo menor √© equivalente ao modelo BERT maior. O modelo maior tem mais de uma ordem de magnitude de par√¢metros a mais do que o GPT.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al√©m disso, foram feitas as seguintes modifica√ß√µes arquitet√¥nicas</p>
      <ul>
      <li>Uma camada de normaliza√ß√£o √© adicionada antes do bloco de aten√ß√£o. Isso pode ajudar a estabilizar o treinamento do modelo e melhorar a capacidade do modelo de aprender representa√ß√µes mais profundas. Ao normalizar as entradas para cada bloco, a variabilidade nas sa√≠das √© reduzida e o treinamento do modelo √© facilitado.</li>
      <li>Uma normaliza√ß√£o adicional foi adicionada ap√≥s o bloco de autoaten√ß√£o final. Isso pode ajudar a reduzir a variabilidade nos resultados do modelo e melhorar a estabilidade do modelo.</li>
      <li>Na maioria dos modelos, os pesos das camadas s√£o inicializados aleatoriamente, seguindo uma distribui√ß√£o normal ou uniforme. No entanto, no caso do GPT-2, os autores decidiram usar uma inicializa√ß√£o modificada que leva em conta a profundidade do modelo. A ideia por tr√°s dessa inicializa√ß√£o modificada √© que, √† medida que o modelo se torna mais profundo, o sinal que flui pelas camadas residuais fica mais fraco. Isso ocorre porque cada camada residual √© adicionada √† entrada original, o que pode fazer com que o sinal seja atenuado com a profundidade do modelo. Para neutralizar esse efeito, eles decidiram dimensionar os pesos da camada residual na inicializa√ß√£o por um fator de 1/‚àöN, em que N √© o n√∫mero de camadas residuais. Isso significa que, √† medida que o modelo se torna mais profundo, os pesos das camadas residuais ficam menores. Esse truque de inicializa√ß√£o pode ajudar a estabilizar o treinamento do modelo e melhorar sua capacidade de aprender representa√ß√µes mais profundas. Ao dimensionar os pesos da camada residual, a variabilidade nas sa√≠das de cada camada √© reduzida e o fluxo de sinal pelo modelo √© facilitado. Em resumo, a inicializa√ß√£o modificada na GPT-2 √© usada para neutralizar o efeito de atenua√ß√£o do sinal nas camadas residuais, o que ajuda a estabilizar o treinamento do modelo e a melhorar sua capacidade de aprender representa√ß√µes mais profundas.</li>
      <li>O tamanho do vocabul√°rio foi ampliado para 50.257. Isso significa que o modelo pode aprender a representar um conjunto maior de palavras e tokens.</li>
      <li>O tamanho do contexto foi aumentado de 512 para 1024 tokens. Isso permite que o modelo leve em conta um contexto maior ao gerar o texto.</li>
      </ul>
      <p>Arquitetura GPT1 vs. GPT-2](<a href="https://maximofn.com/wp-content/uploads/2024/06/GPT1_vs_GPT2_architecture.webp">https://maximofn.com/wp-content/uploads/2024/06/GPT1_vs_GPT2_architecture.webp</a>)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resumo-do-artigo">Resumo do artigo<a class="anchor-link" href="#Resumo-do-artigo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 67" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As ideias mais interessantes do artigo s√£o:</p>
      <ul>
      <li>Para o pr√©-treinamento do modelo, eles pensaram em usar uma fonte de texto diversificada e quase ilimitada, a raspagem da Web como Common Crawl. No entanto, eles descobriram que o texto era de qualidade quase muito ruim. Por isso, usaram o conjunto de dados WebText, que tamb√©m era proveniente de raspagem da Web, mas com um filtro de qualidade, como a quantidade de links de sa√≠da do reddit etc. Eles tamb√©m removeram o texto proveniente da Wikip√©dia, pois ele poderia ser repetido em outras p√°ginas.</li>
      <li>Eles usaram um tokenizador BPE, que explicamos em uma [postagem] anterior (<a href="https://maximofn.com/bpe/">https://maximofn.com/bpe/</a>).</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Gera%C3%A7%C3%A3o-de-texto">Gera√ß√£o de texto<a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 68" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como gerar texto com um GPT-2 pr√©-treinado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para gerar o texto, usaremos o modelo do reposit√≥rio [GPT-2] (<a href="https://huggingface.co/openai-community/gpt2" target="_blank" rel="nofollow noreferrer">https://huggingface.co/openai-community/gpt2</a>) do Hugging Face.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Gera%C3%A7%C3%A3o-de-texto-com-pipeline">Gera√ß√£o de texto com pipeline<a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto-com-pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 69" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Com esse modelo, agora podemos usar o pipeline de transformadores.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
      
      <span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>
      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">'text-generation'</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Hello, I'm a language model,"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Output </span><span class="si">{opening_brace}</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">{closing_brace}</span><span class="s2">: </span><span class="si">{opening_brace}</span><span class="n">o</span><span class="p">[</span><span class="s1">'generated_text'</span><span class="p">]</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
      Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Output 1: Hello, I'm a language model, and I want to change the way you read
      
      A little in today's post I want to talk about
      Output 2: Hello, I'm a language model, with two roles: the language model and the lexicographer-semantics expert. The language models are going
      Output 3: Hello, I'm a language model, and this is your brain. Here is your brain, and all this data that's stored in there, that
      Output 4: Hello, I'm a language model, and I like to talk... I want to help you talk to your customers
      
      Are you using language model
      Output 5: Hello, I'm a language model, I'm gonna tell you about what type of language you're using. We all know a language like this,
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Gera%C3%A7%C3%A3o-de-texto-com-modelo-autom%C3%A1tico">Gera√ß√£o de texto com modelo autom√°tico<a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto-com-modelo-autom%C3%A1tico"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 70" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mas se quisermos usar o <code>Automodel</code>, podemos fazer o seguinte</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
      '      <span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">\'text-generation\'</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>',
      '      <span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Hello, I\'m a language model,"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
      '      <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>',
      '          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Output </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">o</span><span class="p">[</span><span class="s1">\'generated_text\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      ',
      '      <span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '      <span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Assim como no <a href="https://maximofn.com/gpt1/#Generaci%C3%B3n-de-texto">GPT-1</a>, podemos importar o <code>GPT2Tokenizer</code> e o <code>AutoTokenizer</code>. Isso ocorre porque no <a href="https://huggingface.co/openai-community/gpt2">model card</a> do GPT-2 diz para usar o <code>GPT2Tokenizer</code>, mas na postagem da biblioteca <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> explicamos que voc√™ deve usar o <code>AutoTokenizer</code> para carregar o tokenizer. Portanto, vamos tentar os dois</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
          '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">\'text-generation\'</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>',
          '<span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Hello, I\'m a language model,"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
          '<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Output </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">o</span><span class="p">[</span><span class="s1">\'generated_text\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
          '<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
          '</span><span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
          '<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello, I\'m a language model,"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '<span class="n">input_auto_tokens</span> <span class="o">=</span> <span class="n">auto_tokenizer</span><span class="p">(</span><span class="s2">"Hello, I\'m a language model,"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input auto tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_auto_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to \'longest_first\' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'input tokens: ',
          '{\'input_ids\': tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]]), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}',
          'input auto tokens: ',
          '{\'input_ids\': tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]]), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como voc√™ pode ver, com os dois tokenizadores voc√™ obt√©m os mesmos tokens. Portanto, para tornar o c√≥digo mais geral, de modo que, se voc√™ alterar os pontos de verifica√ß√£o, n√£o precisar√° alterar o c√≥digo, vamos usar o <code>AutoTokenizer</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Em seguida, criamos o dispositivo, o tokenizador e o modelo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>Como instanciamos o modelo, vamos ver quantos par√¢metros ele tem</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>',
          '',
          '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
          '',
          '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '</span><span class="n">params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">params</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Number of parameters: 1558M',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, carregamos o modelo de par√¢metro 1.5B, mas se quis√©ssemos carregar os outros modelos, ter√≠amos que fazer o seguinte</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">checkpoints_small</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
          '<span class="n">model_small</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_small</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters of small model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_small</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>',
          '',
          '<span class="n">checkpoints_medium</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-medium"</span>',
          '<span class="n">model_medium</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_medium</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters of medium model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_medium</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>',
          '',
          '<span class="n">checkpoints_large</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-large"</span>',
          '<span class="n">model_large</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_large</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters of large model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_large</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>',
          '',
          '<span class="n">checkpoints_xl</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
          '<span class="n">model_xl</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_xl</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters of xl model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_xl</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Number of parameters of small model: 124M',
          'Number of parameters of medium model: 355M',
          'Number of parameters of large model: 774M',
          'Number of parameters of xl model: 1558M',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos os tokens de entrada para o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_sentence</span> <span class="o">=</span> <span class="s2">"Hello, I\'m a language model,"</span>',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '',
          '<span class="n">input_tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'input_ids\': tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]],',
          '       device=\'cuda:0\'), \'attention_mask\': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device=\'cuda:0\')}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>N√≥s os passamos para o modelo para gerar os tokens de sa√≠da.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>
      
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">output_tokens</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      /home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>output tokens: 
      tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   314,
                1101,  1016,   284,  1037,   345,   351,   534,  1917,    13,   198]],
             device='cuda:0')
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Decodificamos os tokens para obter a declara√ß√£o de sa√≠da</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">output_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded output: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.',
          '  warnings.warn(',
          'decoded output: ',
          'Hello, I\'m a language model, and I\'m going to help you with your problem.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>J√° conseguimos gerar texto com o GPT-2</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Gerar-token-para-o-texto-do-token">Gerar token para o texto do token<a class="anchor-link" href="#Gerar-token-para-o-texto-do-token"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 71" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Greedy-search">Greedy search<a class="anchor-link" href="#Greedy-search"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 72" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usamos o <code>model.generate</code> para gerar os tokens de sa√≠da de uma s√≥ vez, mas vamos ver como ger√°-los um a um. Para fazer isso, em vez de usar <code>model.generate</code>, usaremos <code>model</code>, que, na verdade, chama o m√©todo <code>model.forward</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '',
          '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ 6.6288,  5.1421, -0.8002,  ..., -6.3998, -4.4113,  1.8240],',
          '         [ 2.7250,  1.9371, -1.2293,  ..., -5.0979, -5.1617,  2.2694],',
          '         [ 2.6891,  4.3089, -1.6074,  ..., -7.6321, -2.0448,  0.4042],',
          '         ...,',
          '         [ 6.0513,  3.8020, -2.8080,  ..., -6.7754, -8.3176,  1.1541],',
          '         [ 6.8402,  5.6952,  0.2002,  ..., -9.1281, -6.7818,  2.7576],',
          '         [ 1.0255, -0.2201, -2.5484,  ..., -6.2137, -7.2322,  0.1665]]],',
          '       device=\'cuda:0\', grad_fn=&lt;UnsafeViewBackward0&gt;), past_key_values=((tensor([[[[ 0.4779,  0.7671, -0.7532,  ..., -0.3551,  0.4590,  0.3073],',
          '          [ 0.2034, -0.6033,  0.2484,  ...,  0.7760, -0.3546,  0.0198],',
          '          [-0.1968, -0.9029,  0.5570,  ...,  0.9985, -0.5028, -0.3508],',
          '          ...,',
          '          [-0.5007, -0.4009,  0.1604,  ..., -0.3693, -0.1158,  0.1320],',
          '          [-0.4854, -0.1369,  0.7377,  ..., -0.8043, -0.1054,  0.0871],',
          '          [ 0.1610, -0.8358, -0.5534,  ...,  0.9951, -0.3085,  0.4574]],',
          '         [[ 0.6288, -0.1374, -0.3467,  ..., -1.0003, -1.1518,  0.3114],',
          '          [-1.7269,  1.2920, -0.0734,  ...,  1.0572,  1.4698, -2.0412],',
          '          [ 0.2714, -0.0670, -0.4769,  ...,  0.6305,  0.6890, -0.8158],',
          '          ...,',
          '          [-0.0499, -0.0721,  0.4580,  ...,  0.6797,  0.2331,  0.0210],',
          '          [-0.1894,  0.2077,  0.6722,  ...,  0.6938,  0.2104, -0.0574],',
          '          [ 0.3661, -0.0218,  0.2618,  ...,  0.8750,  1.2205, -0.6103]],',
          '         [[ 0.5964,  1.1178,  0.3604,  ...,  0.8426,  0.4881, -0.4094],',
          '          [ 0.3186, -0.3953,  0.2687,  ..., -0.1110, -0.5640,  0.5900],',
          '          ...,',
          '          [ 0.2092,  0.3898, -0.6061,  ..., -0.2859, -0.3136, -0.1002],',
          '          [ 0.0539,  0.8941,  0.3423,  ..., -0.6326, -0.1053, -0.6679],',
          '          [ 0.5628,  0.6687, -0.2720,  ..., -0.1073, -0.9792, -0.0302]]]],',
          '       device=\'cuda:0\', grad_fn=&lt;PermuteBackward0&gt;))), hidden_states=None, attentions=None, cross_attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que isso gera muitos dados, mas primeiro vamos dar uma olhada nas chaves de sa√≠da.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'odict_keys([\'logits\', \'past_key_values\'])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Nesse caso, temos apenas os logits do modelo, vamos ver seu tamanho.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
          '',
          '<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 8, 50257])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver quantos tokens t√≠nhamos na entrada.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([1, 8])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Bem, temos o mesmo n√∫mero de logits na sa√≠da e na entrada. Isso √© normal</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos os logits da √∫ltima posi√ß√£o da sa√≠da</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
          '',
          '<span class="n">nex_token_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([50257])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>H√° um total de 50257 logits, ou seja, h√° um vocabul√°rio de 50257 tokens e temos que ver qual token tem a maior probabilidade.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">softmax_logits</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'torch.Size([50257])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Depois de calcularmos o softmax, obtemos o token mais prov√°vel procurando aquele com a maior probabilidade, ou seja, aquele com o maior valor ap√≥s o softmax.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '',
          '<span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(tensor(0.1732, device=\'cuda:0\', grad_fn=&lt;MaxBackward0&gt;),',
          ' tensor(290, device=\'cuda:0\'))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtivemos o seguinte token, agora vamos decodific√°-lo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\' and\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtivemos o seguinte token usando o m√©todo guloso, ou seja, o token com a maior probabilidade. Mas j√° vimos na postagem sobre a biblioteca de transformadores, a [ways to generate texts] (<a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto</a>) que voc√™ pode fazer <code>sampling</code>, <code>top-k</code>, <code>top-p</code>, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos colocar tudo em uma fun√ß√£o e ver o que acontece se gerarmos alguns tokens.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
      '          <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '          <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '          <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '          <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
      '          <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
      '          <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
      '          <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
      '          <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
      '<span></span><span class="k">def</span> <span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
      '          <span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>',
      '          <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>',
      '              <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
      '              <span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
      '          <span class="k">return</span> <span class="n">generated_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Agora vamos gerar o texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>',
          '    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>',
          '    <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>',
          '    <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '    <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
          '    <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>',
          '</span><span class="k">def</span> <span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
          '    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>',
          '    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>',
          '        <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
          '        <span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
          '    <span class="k">return</span> <span class="n">generated_text</span>',
          '</span><span class="n">generate_greedy_text</span><span class="p">(</span><span class="s2">"Hello, I\'m a language model,"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '"Hello, I\'m a language model, and I\'m going to help you with your problem.\n\n\nI\'m going to help you"',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>O resultado √© bastante repetitivo, como j√° visto em [ways to generate text] (<a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto</a>). Mas, ainda assim, √© um resultado melhor do que o que obtivemos com o [GPT-1] (<a href="https://maximofn.com/gpt1/#Generaci%C3%B3n-de-texto">https://maximofn.com/gpt1/#Generaci%C3%B3n-de-texto</a>).</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Arquitetura-dos-modelos-dispon%C3%ADveis-no-Hugging-Face">Arquitetura dos modelos dispon√≠veis no Hugging Face<a class="anchor-link" href="#Arquitetura-dos-modelos-dispon%C3%ADveis-no-Hugging-Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 73" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se acessarmos a documenta√ß√£o do Hugging Face do <a href="https://huggingface.co/docs/transformers/en/model_doc/gpt2" target="_blank" rel="nofollow noreferrer">GPT2</a>, veremos que temos as op√ß√µes <code>GPT2Model</code>, <code>GPT2LMHeadModel</code>, <code>GPT2ForSequenceClassification</code>, <code>GPT2ForQuestionAnswering</code>, <code>GPT2ForTokenClassification</code>. Vamos dar uma olhada neles</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2Model">GPT2Model<a class="anchor-link" href="#GPT2Model"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 74" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Esse √© o modelo b√°sico, ou seja, o decodificador de transformador.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Model</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'GPT2Model(',
          '  (wte): Embedding(50257, 768)',
          '  (wpe): Embedding(1024, 768)',
          '  (drop): Dropout(p=0.1, inplace=False)',
          '  (h): ModuleList(',
          '    (0-11): 12 x GPT2Block(',
          '      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '      (attn): GPT2Attention(',
          '        (c_attn): Conv1D()',
          '        (c_proj): Conv1D()',
          '        (attn_dropout): Dropout(p=0.1, inplace=False)',
          '        (resid_dropout): Dropout(p=0.1, inplace=False)',
          '      )',
          '      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '      (mlp): GPT2MLP(',
          '        (c_fc): Conv1D()',
          '        (c_proj): Conv1D()',
          '        (act): NewGELUActivation()',
          '        (dropout): Dropout(p=0.1, inplace=False)',
          '      )',
          '    )',
          '  )',
          '  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como voc√™ pode ver na sa√≠da, um tensor de dimens√£o 768, que √© a dimens√£o dos embeddings do modelo pequeno. Se tiv√©ssemos usado o modelo <code>openai-community/gpt2-xl</code>, ter√≠amos obtido um resultado de 1600.</p>
      <p>Dependendo da tarefa em quest√£o, seria necess√°rio adicionar mais camadas.</p>
      <p>Podemos adicion√°-las manualmente, mas os pesos dessas camadas seriam inicializados de forma aleat√≥ria. Por outro lado, se usarmos os modelos Hugging Face com essas camadas, os pesos ser√£o pr√©-treinados.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2LMHeadModel">GPT2LMHeadModel<a class="anchor-link" href="#GPT2LMHeadModel"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 75" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>√â o mesmo que usamos anteriormente para gerar texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'GPT2LMHeadModel(',
          '  (transformer): GPT2Model(',
          '    (wte): Embedding(50257, 768)',
          '    (wpe): Embedding(1024, 768)',
          '    (drop): Dropout(p=0.1, inplace=False)',
          '    (h): ModuleList(',
          '      (0-11): 12 x GPT2Block(',
          '        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '        (attn): GPT2Attention(',
          '          (c_attn): Conv1D()',
          '          (c_proj): Conv1D()',
          '          (attn_dropout): Dropout(p=0.1, inplace=False)',
          '          (resid_dropout): Dropout(p=0.1, inplace=False)',
          '        )',
          '        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '        (mlp): GPT2MLP(',
          '          (c_fc): Conv1D()',
          '          (c_proj): Conv1D()',
          '          (act): NewGELUActivation()',
          '          (dropout): Dropout(p=0.1, inplace=False)',
          '        )',
          '      )',
          '    )',
          '    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '  )',
          '  (lm_head): Linear(in_features=768, out_features=50257, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como voc√™ pode ver, √© o mesmo modelo anterior, s√≥ que no final foi adicionada uma camada linear com uma entrada de 768 (os embeddings) e uma sa√≠da de 50257, que corresponde ao tamanho do vocabul√°rio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2ForSequenceClassification">GPT2ForSequenceClassification<a class="anchor-link" href="#GPT2ForSequenceClassification"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 76" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Essa op√ß√£o serve para classificar sequ√™ncias de texto; nesse caso, devemos especificar com <code>num_labels</code> o n√∫mero de classes que queremos classificar.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForSequenceClassification</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
      <span class="n">model</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']
      You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[10]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>GPT2ForSequenceClassification(
        (transformer): GPT2Model(
          (wte): Embedding(50257, 768)
          (wpe): Embedding(1024, 768)
          (drop): Dropout(p=0.1, inplace=False)
          (h): ModuleList(
            (0-11): 12 x GPT2Block(
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): GPT2Attention(
                (c_attn): Conv1D()
                (c_proj): Conv1D()
                (attn_dropout): Dropout(p=0.1, inplace=False)
                (resid_dropout): Dropout(p=0.1, inplace=False)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): GPT2MLP(
                (c_fc): Conv1D()
                (c_proj): Conv1D()
                (act): NewGELUActivation()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (score): Linear(in_features=768, out_features=5, bias=False)
      )</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, em vez de termos uma sa√≠da de 50257, temos uma sa√≠da de 5, que √© o n√∫mero que inserimos em <code>num_labels</code> e √© o n√∫mero de classes que queremos classificar.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2ForQuestionAnswering">GPT2ForQuestionAnswering<a class="anchor-link" href="#GPT2ForQuestionAnswering"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 77" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Na postagem <a href="https://maximofn.com/hugging-face-transformers/">transformers</a>, explicamos que, nesse modo, voc√™ passa um contexto para o modelo e uma pergunta sobre o contexto e ele retorna a resposta.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForQuestionAnswering</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>
      <span class="n">model</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
      You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[13]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>GPT2ForQuestionAnswering(
        (transformer): GPT2Model(
          (wte): Embedding(50257, 768)
          (wpe): Embedding(1024, 768)
          (drop): Dropout(p=0.1, inplace=False)
          (h): ModuleList(
            (0-11): 12 x GPT2Block(
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): GPT2Attention(
                (c_attn): Conv1D()
                (c_proj): Conv1D()
                (attn_dropout): Dropout(p=0.1, inplace=False)
                (resid_dropout): Dropout(p=0.1, inplace=False)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): GPT2MLP(
                (c_fc): Conv1D()
                (c_proj): Conv1D()
                (act): NewGELUActivation()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
      )</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que o resultado nos d√° um tensor bidimensional.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="GPT2ForTokenClassification">GPT2ForTokenClassification<a class="anchor-link" href="#GPT2ForTokenClassification"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 78" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m na postagem <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> dissemos o que era a calsifica√ß√£o de tokens, explicamos que ela classificava a qual categoria cada token pertencia. Temos que passar o n√∫mero de classes que queremos classificar com <code>num_labels</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForTokenClassification</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
      <span class="n">model</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']
      You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[2]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>GPT2ForTokenClassification(
        (transformer): GPT2Model(
          (wte): Embedding(50257, 768)
          (wpe): Embedding(1024, 768)
          (drop): Dropout(p=0.1, inplace=False)
          (h): ModuleList(
            (0-11): 12 x GPT2Block(
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): GPT2Attention(
                (c_attn): Conv1D()
                (c_proj): Conv1D()
                (attn_dropout): Dropout(p=0.1, inplace=False)
                (resid_dropout): Dropout(p=0.1, inplace=False)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): GPT2MLP(
                (c_fc): Conv1D()
                (c_proj): Conv1D()
                (act): NewGELUActivation()
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (classifier): Linear(in_features=768, out_features=5, bias=True)
      )</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Na sa√≠da, obtemos as 5 classes que especificamos com <code>num_labels</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Ajuste-fino-do-GPT-2">Ajuste fino do GPT-2<a class="anchor-link" href="#Ajuste-fino-do-GPT-2"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 79" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Ajuste-fino-para-gera%C3%A7%C3%A3o-de-texto">Ajuste fino para gera√ß√£o de texto<a class="anchor-link" href="#Ajuste-fino-para-gera%C3%A7%C3%A3o-de-texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 80" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, vamos dar uma olhada em como o treinamento puro do Pytorch seria feito.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="C%C3%A1lculo-de-perdas">C√°lculo de perdas<a class="anchor-link" href="#C%C3%A1lculo-de-perdas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 81" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de come√ßarmos a fazer o ajuste fino do GPT-2, vamos dar uma olhada em um aspecto. Antes, quando obtivemos a sa√≠da do modelo, fizemos o seguinte</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForSequenceClassification</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
          '<span class="n">model</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForQuestionAnswering</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">model</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForTokenClassification</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
          '<span class="n">model</span>',
          '</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>',
          '',
          '<span class="n">outputs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [\'score.weight\']',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
          'Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [\'qa_outputs.bias\', \'qa_outputs.weight\']',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
          'Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [\'classifier.bias\', \'classifier.weight\']',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
          'CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ 6.6288,  5.1421, -0.8002,  ..., -6.3998, -4.4113,  1.8240],',
          '         [ 2.7250,  1.9371, -1.2293,  ..., -5.0979, -5.1617,  2.2694],',
          '         [ 2.6891,  4.3089, -1.6074,  ..., -7.6321, -2.0448,  0.4042],',
          '         ...,',
          '         [ 6.0513,  3.8020, -2.8080,  ..., -6.7754, -8.3176,  1.1541],',
          '         [ 6.8402,  5.6952,  0.2002,  ..., -9.1281, -6.7818,  2.7576],',
          '         [ 1.0255, -0.2201, -2.5484,  ..., -6.2137, -7.2322,  0.1665]]],',
          '       device=\'cuda:0\', grad_fn=&lt;UnsafeViewBackward0&gt;), past_key_values=((tensor([[[[ 0.4779,  0.7671, -0.7532,  ..., -0.3551,  0.4590,  0.3073],',
          '          [ 0.2034, -0.6033,  0.2484,  ...,  0.7760, -0.3546,  0.0198],',
          '          [-0.1968, -0.9029,  0.5570,  ...,  0.9985, -0.5028, -0.3508],',
          '          ...,',
          '          [-0.5007, -0.4009,  0.1604,  ..., -0.3693, -0.1158,  0.1320],',
          '          [-0.4854, -0.1369,  0.7377,  ..., -0.8043, -0.1054,  0.0871],',
          '          [ 0.1610, -0.8358, -0.5534,  ...,  0.9951, -0.3085,  0.4574]],',
          '         [[ 0.6288, -0.1374, -0.3467,  ..., -1.0003, -1.1518,  0.3114],',
          '          [-1.7269,  1.2920, -0.0734,  ...,  1.0572,  1.4698, -2.0412],',
          '          [ 0.2714, -0.0670, -0.4769,  ...,  0.6305,  0.6890, -0.8158],',
          '          ...,',
          '          [-0.0499, -0.0721,  0.4580,  ...,  0.6797,  0.2331,  0.0210],',
          '          [-0.1894,  0.2077,  0.6722,  ...,  0.6938,  0.2104, -0.0574],',
          '          [ 0.3661, -0.0218,  0.2618,  ...,  0.8750,  1.2205, -0.6103]],',
          '         [[ 0.5964,  1.1178,  0.3604,  ...,  0.8426,  0.4881, -0.4094],',
          '          [ 0.3186, -0.3953,  0.2687,  ..., -0.1110, -0.5640,  0.5900],',
          '          ...,',
          '          [ 0.2092,  0.3898, -0.6061,  ..., -0.2859, -0.3136, -0.1002],',
          '          [ 0.0539,  0.8941,  0.3423,  ..., -0.6326, -0.1053, -0.6679],',
          '          [ 0.5628,  0.6687, -0.2720,  ..., -0.1073, -0.9792, -0.0302]]]],',
          '       device=\'cuda:0\', grad_fn=&lt;PermuteBackward0&gt;))), hidden_states=None, attentions=None, cross_attentions=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Voc√™ pode ver que obtemos <code>loss=None</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'None',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como precisaremos da perda para fazer o ajuste fino, vamos ver como obt√™-la.</p>
      <p>Se consultarmos a documenta√ß√£o do m√©todo <a href="https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel.forward" target="_blank" rel="nofollow noreferrer">forward</a> de <code>GPT2LMHeadModel</code>, veremos que ele diz que a sa√≠da retorna um objeto do tipo <code>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</code>, portanto, se consultarmos a documenta√ß√£o de <a href="https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions">transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a>, podemos ver que ele diz que retorna <code>loss</code> se <code>labels</code> for passado para o m√©todo <code>forward</code>.</p>
      <p>Se acessarmos o c√≥digo-fonte do m√©todo <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1277" target="_blank" rel="nofollow noreferrer">forward</a>, veremos este bloco de c√≥digo</p>
      <div class="highlight"><pre><span></span><span class="n">perda</span> <span class="o">=</span> <span class="n">Nenhuma</span>
              <span class="n">se</span> <span class="n">labels</span> <span class="n">n√£o</span> <span class="k">for</span> <span class="kc">None</span><span class="p">:</span>
                  <span class="c1"># mova os r√≥tulos para o dispositivo correto para ativar o paralelismo do modelo</span>
                  <span class="n">r√≥tulos</span> <span class="o">=</span> <span class="n">r√≥tulos</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                  <span class="c1"># Deslocamento de modo que os tokens &lt; n prevejam n</span>
                  <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">lm_logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
                  <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
                  <span class="c1"># Achatar os tokens</span>
                  <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
                  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
      <span class="err">```</span>
      
      <span class="n">Em</span> <span class="n">outras</span> <span class="n">palavras</span><span class="p">,</span> <span class="n">a</span> <span class="s2">"perda"</span> <span class="n">√©</span> <span class="n">calculada</span> <span class="n">da</span> <span class="n">seguinte</span> <span class="n">forma</span>
      
       <span class="o">*</span> <span class="n">Deslocamento</span> <span class="n">de</span> <span class="n">logits</span> <span class="n">e</span> <span class="n">r√≥tulos</span><span class="p">:</span> <span class="n">a</span> <span class="n">primeira</span> <span class="n">parte</span> <span class="n">√©</span> <span class="n">deslocar</span> <span class="n">logits</span> <span class="p">(</span><span class="err">`</span><span class="n">lm_logits</span><span class="err">`</span><span class="p">)</span> <span class="n">e</span> <span class="n">r√≥tulos</span> <span class="p">(</span><span class="err">`</span><span class="n">labels</span><span class="err">`</span><span class="p">)</span> <span class="n">para</span> <span class="n">que</span> <span class="err">`</span><span class="n">tokens</span> <span class="o">&lt;</span> <span class="n">n</span><span class="err">`</span> <span class="n">prevejam</span> <span class="err">`</span><span class="n">n</span><span class="err">`</span><span class="p">,</span> <span class="n">ou</span> <span class="n">seja</span><span class="p">,</span> <span class="n">a</span> <span class="n">partir</span> <span class="n">de</span> <span class="n">uma</span> <span class="n">posi√ß√£o</span> <span class="err">`</span><span class="n">n</span><span class="err">`</span><span class="p">,</span> <span class="n">o</span> <span class="n">pr√≥ximo</span> <span class="n">token</span> <span class="n">√©</span> <span class="n">previsto</span> <span class="n">a</span> <span class="n">partir</span> <span class="n">dos</span> <span class="n">anteriores</span><span class="o">.</span>
       <span class="o">*</span> <span class="n">CrossEntropyLoss</span><span class="p">:</span> <span class="n">√©</span> <span class="n">criada</span> <span class="n">uma</span> <span class="n">inst√¢ncia</span> <span class="n">da</span> <span class="n">fun√ß√£o</span> <span class="n">de</span> <span class="n">perda</span> <span class="err">`</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="err">`</span><span class="o">.</span>
       <span class="o">*</span> <span class="n">Achatar</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">e</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">s√£o</span> <span class="n">achatados</span> <span class="n">usando</span> <span class="err">`</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="err">`</span> <span class="n">e</span> <span class="err">`</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="err">`</span><span class="p">,</span> <span class="n">respectivamente</span><span class="o">.</span> <span class="n">Isso</span> <span class="n">√©</span> <span class="n">feito</span> <span class="n">para</span> <span class="n">que</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">e</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">tenham</span> <span class="n">a</span> <span class="n">mesma</span> <span class="n">forma</span> <span class="n">para</span> <span class="n">a</span> <span class="n">fun√ß√£o</span> <span class="n">de</span> <span class="n">perda</span><span class="o">.</span>
       <span class="o">*</span> <span class="n">C√°lculo</span> <span class="n">da</span> <span class="n">perda</span><span class="p">:</span> <span class="n">finalmente</span><span class="p">,</span> <span class="n">a</span> <span class="n">perda</span> <span class="n">√©</span> <span class="n">calculada</span> <span class="n">usando</span> <span class="n">a</span> <span class="n">fun√ß√£o</span> <span class="n">de</span> <span class="n">perda</span> <span class="err">`</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="err">`</span> <span class="n">com</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">achatados</span> <span class="n">e</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">achatados</span> <span class="n">como</span> <span class="n">entradas</span><span class="o">.</span>
      
      <span class="n">Em</span> <span class="n">resumo</span><span class="p">,</span> <span class="n">a</span> <span class="s2">"perda"</span> <span class="n">√©</span> <span class="n">calculada</span> <span class="n">como</span> <span class="n">a</span> <span class="n">perda</span> <span class="n">de</span> <span class="n">entropia</span> <span class="n">cruzada</span> <span class="n">entre</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">deslocados</span> <span class="n">e</span> <span class="n">achatados</span> <span class="n">e</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">deslocados</span> <span class="n">e</span> <span class="n">achatados</span><span class="o">.</span>
      
      <span class="n">Portanto</span><span class="p">,</span> <span class="n">se</span> <span class="n">passarmos</span> <span class="n">os</span> <span class="n">r√≥tulos</span> <span class="n">para</span> <span class="n">o</span> <span class="n">m√©todo</span> <span class="err">`</span><span class="n">forward</span><span class="err">`</span><span class="p">,</span> <span class="n">ele</span> <span class="n">retornar√°</span> <span class="n">a</span> <span class="err">`</span><span class="n">perda</span><span class="err">`</span><span class="o">.</span>
      </pre></div>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>',
          '',
          '<span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'tensor(3.8028, device=\'cuda:0\', grad_fn=&lt;NllLossBackward0&gt;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Conjunto-de-dados">Conjunto de dados<a class="anchor-link" href="#Conjunto-de-dados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 82" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para o treinamento, usaremos um conjunto de dados de piadas em ingl√™s [short-jokes-dataset] (<a href="https://huggingface.co/datasets/Maximofn/short-jokes-dataset" target="_blank" rel="nofollow noreferrer">https://huggingface.co/datasets/Maximofn/short-jokes-dataset</a>), que √© um conjunto de dados com 231 mil piadas em ingl√™s.</p>
      <blockquote>
      <p>Reinicie o notebook para que n√£o haja problemas de mem√≥ria da GPU</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Baixamos o conjunto de dados</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
          '',
          '<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"Maximofn/short-jokes-dataset"</span><span class="p">)</span>',
          '<span class="n">jokes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'DatasetDict({',
          '    train: Dataset({',
          '        features: [\'ID\', \'Joke\'],',
          '        num_rows: 231657',
          '    })',
          '})',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada nisso</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'ID\': 1,',
          ' \'Joke\': \'[me narrating a documentary about narrators] "I can\'t hear what they\'re saying cuz I\'m talking"\'}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Inst%C3%A2ncia-do-modelo">Inst√¢ncia do modelo<a class="anchor-link" href="#Inst%C3%A2ncia-do-modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 83" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para usar o modelo <code>xl</code>, ou seja, aquele com par√¢metros de 1,5B, eu o mudo para FP16 para n√£o ficar sem mem√≥ria.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <h4 id="Conjunto-de-dados-Pytorch">Conjunto de dados Pytorch<a class="anchor-link" href="#Conjunto-de-dados-Pytorch"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 84" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criar uma classe de conjunto de dados do Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '              ',
      '          <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '              <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '              <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>





















      
      <section class="section-block-markdown-cell">
      <p>N√≥s o instanciamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '              ',
      '          <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '              <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '              <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Aqui est√° um exemplo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
      <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&lt;|endoftext|&gt;
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[5]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>(torch.Size([1, 22]), torch.Size([1, 22]))</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Dataloader">Dataloader<a class="anchor-link" href="#Dataloader"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 85" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, criamos um carregador de dados do Pytorch</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
      '      ',
      '      <span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
      '          <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
      '              <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
      '              ',
      '          <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
      '              <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
      '      ',
      '          <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
      '              <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
      '              <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
      '              <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
      '<span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
      '<span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
      '      <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
      '<span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>',
      '      ',
      '      <span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>',
      '      <span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Vemos um lote</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
          '<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>',
          '',
          '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
          '',
          '<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>',
          '',
          '<span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>',
          '    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>',
          '        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>',
          '        ',
          '    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>',
          '        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>',
          '',
          '    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>',
          '        <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>',
          '        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>',
          '        <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>',
          '</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
          '</span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>',
          '<span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
          '</span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>',
          '',
          '<span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>',
          '<span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '</span><span class="n">sentences</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">))</span>',
          '<span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'JOKE: Why can\'t Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&lt;|endoftext|&gt;',
          '(1, torch.Size([1, 1, 36]), torch.Size([1, 1, 36]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 86" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>
      <span class="kn">import</span> <span class="nn">tqdm</span>
      
      <span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
      <span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>
      <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-6</span>
      <span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>
      <span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>
      
      <span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
      <span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>
      
      <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>
      
      <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
      
      <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
          
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"EPOCH </span><span class="si">{opening_brace}</span><span class="n">epoch</span><span class="si">{closing_brace}</span><span class="s2"> started"</span> <span class="o">+</span> <span class="s1">'='</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
          <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>
          
          <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
      
              <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>
              
              <span class="c1">#################### "Fit as many joke sequences into MAX_SEQ_LEN sequence as possible" logic start ####</span>
              <span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
      
              <span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>
              <span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
                  <span class="k">continue</span>
              
              <span class="c1"># The first joke sequence in the sequence</span>
              <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>
                  <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
                  <span class="k">continue</span>
              <span class="k">else</span><span class="p">:</span>
                  <span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>
                  <span class="c1"># as the start for next sequence </span>
                  <span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
                      <span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>
                      <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
                  <span class="k">else</span><span class="p">:</span>
                      <span class="c1">#Add the joke to sequence, continue and try to add more</span>
                      <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                      <span class="k">continue</span>
              <span class="c1">################## Sequence ready, process it trough the model ##################</span>
                  
              <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>
              <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
              <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                          
              <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>
              <span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
                  <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>    
                  <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>
                  <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                  <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> 
                  <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                  <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      
              <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({opening_brace}</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">'lr'</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]{closing_brace})</span>
              <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
              <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
              <span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
                  <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 0 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training:   0%|          | 0/231657 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [32:29&lt;00:00, 118.83it/s, loss=3.1, lr=2.31e-7] 
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 1 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [32:34&lt;00:00, 118.55it/s, loss=2.19, lr=4.62e-7]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 2 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [32:36&lt;00:00, 118.42it/s, loss=2.42, lr=6.93e-7]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 3 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [32:23&lt;00:00, 119.18it/s, loss=2.16, lr=9.25e-7]
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>EPOCH 4 started==============================
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [32:22&lt;00:00, 119.25it/s, loss=2.1, lr=1.16e-6] 
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
      <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
      
      <span class="n">losses_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
      <span class="n">lrs_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lrs</span><span class="p">)</span>
      
      <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_np</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs_np</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'learning rate'</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-png-output-subarea">
      <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/gpt20.webp" width="993" height="505" alt="image gpt2 1" loading="lazy">
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Infer%C3%AAncia">Infer√™ncia<a class="anchor-link" href="#Infer%C3%AAncia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 87" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como o modelo faz piadas.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"JOKE:"</span>
      <span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
      <span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>
      <span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">decoded_output_joke</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      /home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>decoded joke: 
      JOKE:!!!!!!!!!!!!!!!!!
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Voc√™ pode ver que voc√™ passa uma sequ√™ncia com a palavra <code>joke</code> e ele retorna uma piada. Mas se voc√™ retornar outra string, ele n√£o retornar√°</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"My dog is cute and"</span>
      <span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
      <span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>
      <span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{opening_brace}</span><span class="n">decoded_output_joke</span><span class="si">{closing_brace}</span><span class="s2">"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>decoded joke: 
      My dog is cute and!!!!!!!!!!!!!!!
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Ajuste-fino-do-GPT-2-para-classifica%C3%A7%C3%A3o-de-senten%C3%A7as">Ajuste fino do GPT-2 para classifica√ß√£o de senten√ßas<a class="anchor-link" href="#Ajuste-fino-do-GPT-2-para-classifica%C3%A7%C3%A3o-de-senten%C3%A7as"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 88" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora vamos fazer um treinamento com as bibliotecas Hugging Face.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Conjunto-de-dados">Conjunto de dados<a class="anchor-link" href="#Conjunto-de-dados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 89" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usaremos o conjunto de dados <code>imdb</code> para classificar as frases em positivas e negativas.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>',
          '<span class="kn">import</span> <span class="nn">tqdm</span>',
          '',
          '<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>',
          '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>',
          '<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-6</span>',
          '<span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>',
          '<span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>',
          '',
          '<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>',
          '<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
          '<span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>',
          '<span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
          '',
          '<span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>',
          '',
          '<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>',
          '<span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>',
          '',
          '<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>',
          '    ',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> started"</span> <span class="o">+</span> <span class="s1">\'=\'</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>',
          '    <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>',
          '    ',
          '    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
          '',
          '        <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>',
          '        ',
          '        <span class="c1">#################### "Fit as many joke sequences into MAX_SEQ_LEN sequence as possible" logic start ####</span>',
          '        <span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '',
          '        <span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>',
          '        <span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
          '            <span class="k">continue</span>',
          '        ',
          '        <span class="c1"># The first joke sequence in the sequence</span>',
          '        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>',
          '            <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
          '            <span class="k">continue</span>',
          '        <span class="k">else</span><span class="p">:</span>',
          '            <span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>',
          '            <span class="c1"># as the start for next sequence </span>',
          '            <span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>',
          '                <span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>',
          '                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>',
          '            <span class="k">else</span><span class="p">:</span>',
          '                <span class="c1">#Add the joke to sequence, continue and try to add more</span>',
          '                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
          '                <span class="k">continue</span>',
          '        <span class="c1">################## Sequence ready, process it trough the model ##################</span>',
          '            ',
          '        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>',
          '        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>',
          '        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>',
          '                    ',
          '        <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>',
          '        <span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>',
          '            <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>    ',
          '            <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>',
          '            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>',
          '            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> ',
          '            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
          '            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>',
          '',
          '        <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">\'loss\'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">\'lr\'</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]})</span>',
          '        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>',
          '        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>',
          '        <span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>',
          '            <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>',
          '</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
          '<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>',
          '',
          '<span class="n">losses_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>',
          '<span class="n">lrs_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lrs</span><span class="p">)</span>',
          '',
          '<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>',
          '<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_np</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">\'loss\'</span><span class="p">)</span>',
          '<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs_np</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">\'learning rate\'</span><span class="p">)</span>',
          '<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">\'log\'</span><span class="p">)</span>',
          '<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>',
          '<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>',
          '</span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"JOKE:"</span>',
          '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
          '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"My dog is cute and"</span>',
          '<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>',
          '<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>',
          '',
          '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"imdb"</span><span class="p">)</span>',
          '<span class="n">dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning',
          '  warnings.warn(',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.',
          '  warnings.warn(',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'DatasetDict({',
          '    train: Dataset({',
          '        features: [\'text\', \'label\'],',
          '        num_rows: 25000',
          '    })',
          '    test: Dataset({',
          '        features: [\'text\', \'label\'],',
          '        num_rows: 25000',
          '    })',
          '    unsupervised: Dataset({',
          '        features: [\'text\', \'label\'],',
          '        num_rows: 50000',
          '    })',
          '})',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada nisso</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">info</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'DatasetInfo(description=\'\', citation=\'\', homepage=\'\', license=\'\', features={\'text\': Value(dtype=\'string\', id=None), \'label\': ClassLabel(names=[\'neg\', \'pos\'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=\'parquet\', dataset_name=\'imdb\', config_name=\'plain_text\', version=0.0.0, splits={\'train\': SplitInfo(name=\'train\', num_bytes=33435948, num_examples=25000, shard_lengths=None, dataset_name=\'imdb\'), \'test\': SplitInfo(name=\'test\', num_bytes=32653810, num_examples=25000, shard_lengths=None, dataset_name=\'imdb\'), \'unsupervised\': SplitInfo(name=\'unsupervised\', num_bytes=67113044, num_examples=50000, shard_lengths=None, dataset_name=\'imdb\')}, download_checksums={\'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet\': {\'num_bytes\': 20979968, \'checksum\': None}, \'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet\': {\'num_bytes\': 20470363, \'checksum\': None}, \'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet\': {\'num_bytes\': 41996509, \'checksum\': None}}, download_size=83446840, post_processing_size=None, dataset_size=133202802, size_in_bytes=216649642)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada nos recursos desse conjunto de dados.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">features</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '{\'text\': Value(dtype=\'string\', id=None),',
          ' \'label\': ClassLabel(names=[\'neg\', \'pos\'], id=None)}',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>O conjunto de dados cont√©m strings e classes. Al√©m disso, h√° dois tipos de classes, <code>pos</code> e <code>neg</code>. Vamos criar uma vari√°vel com o n√∫mero de classes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">num_clases</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s2">"label"</span><span class="p">))</span>',
          '<span class="n">num_clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '2',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Tokeniser">Tokeniser<a class="anchor-link" href="#Tokeniser"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 90" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span>',
      '      ',
      '      <span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">bos_token</span><span class="o">=</span><span class="s1">\'&lt;|startoftext|&gt;\'</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="s1">\'&lt;|endoftext|&gt;\'</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s1">\'&lt;|pad|&gt;\'</span><span class="p">)</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <section class="section-block-markdown-cell">
      <p>Agora que temos um tokenizador, podemos tokenizar o conjunto de dados, pois o modelo s√≥ entende tokens.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span>',
      '      ',
      '      <span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">bos_token</span><span class="o">=</span><span class="s1">\'&lt;|startoftext|&gt;\'</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="s1">\'&lt;|endoftext|&gt;\'</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s1">\'&lt;|pad|&gt;\'</span><span class="p">)</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '          <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '      ',
      '      <span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <h4 id="Modelo">Modelo<a class="anchor-link" href="#Modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 91" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span>',
          '',
          '<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">bos_token</span><span class="o">=</span><span class="s1">\'&lt;|startoftext|&gt;\'</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="s1">\'&lt;|endoftext|&gt;\'</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s1">\'&lt;|pad|&gt;\'</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '</span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
          '    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForSequenceClassification</span>',
          '',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_clases</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>',
          '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [\'score.weight\']',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Avalia%C3%A7%C3%A3o">Avalia√ß√£o<a class="anchor-link" href="#Avalia%C3%A7%C3%A3o"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 92" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos uma m√©trica de avalia√ß√£o</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">import</span> <span class="nn">evaluate</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <section class="section-block-markdown-cell">
      <h4 id="Trainer">Trainer<a class="anchor-link" href="#Trainer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 93" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o instrutor</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">import</span> <span class="nn">evaluate</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">output_dir</span><span class="o">=</span><span class="s2">"./results"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '          <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">"train"</span><span class="p">],</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">"test"</span><span class="p">],</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '      <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>























      
      <section class="section-block-markdown-cell">
      <h4 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 94" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Treinamos</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-html-rendered-html-output-subarea">
      <div>
      <progress max="4689" style="width:300px; height:20px; vertical-align: middle;" value="4689"></progress>
            [4689/4689 1:27:50, Epoch 3/3]
          </div>
      <table border="1" class="dataframe">
      <thead>
      <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>500</td>
      <td>0.379400</td>
      </tr>
      <tr>
      <td>1000</td>
      <td>0.000000</td>
      </tr>
      <tr>
      <td>1500</td>
      <td>0.000000</td>
      </tr>
      <tr>
      <td>2000</td>
      <td>0.000000</td>
      </tr>
      <tr>
      <td>2500</td>
      <td>0.000000</td>
      </tr>
      <tr>
      <td>3000</td>
      <td>0.000000</td>
      </tr>
      <tr>
      <td>3500</td>
      <td>0.000000</td>
      </tr>
      <tr>
      <td>4000</td>
      <td>0.000000</td>
      </tr>
      <tr>
      <td>4500</td>
      <td>0.000000</td>
      </tr>
      </tbody>
      </table><p></p></div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[25]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>TrainOutput(global_step=4689, training_loss=0.04045845954294626, metrics={opening_brace}'train_runtime': 5271.3532, 'train_samples_per_second': 14.228, 'train_steps_per_second': 0.89, 'total_flos': 3.91945125888e+16, 'train_loss': 0.04045845954294626, 'epoch': 3.0{closing_brace})</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Infer%C3%AAncia">Infer√™ncia<a class="anchor-link" href="#Infer%C3%AAncia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 95" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Testamos o modelo ap√≥s o treinamento</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
      '      <span class="kn">import</span> <span class="nn">evaluate</span>',
      '      ',
      '      <span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '          <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '          <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '          <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>',
      '      ',
      '      <span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '          <span class="n">output_dir</span><span class="o">=</span><span class="s2">"./results"</span><span class="p">,</span>',
      '          <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>',
      '          <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
      '          <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>',
      '          <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>',
      '          <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '      ',
      '      <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '          <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '          <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '          <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">"train"</span><span class="p">],</span>',
      '          <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">"test"</span><span class="p">],</span>',
      '          <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
      '<span></span><span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      ',
      '      <span class="k">def</span> <span class="nf">get_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>',
      '          <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>',
      '          <span class="n">prediction</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
      '          <span class="k">return</span> <span class="s2">"positive"</span> <span class="k">if</span> <span class="n">prediction</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">"negative"</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>














      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>',
          '<span class="kn">import</span> <span class="nn">evaluate</span>',
          '',
          '<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>',
          '',
          '<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
          '    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
          '    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
          '    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>',
          '',
          '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
          '    <span class="n">output_dir</span><span class="o">=</span><span class="s2">"./results"</span><span class="p">,</span>',
          '    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>',
          '    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
          '    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>',
          '    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>',
          '    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
          '    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
          '    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
          '    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">"train"</span><span class="p">],</span>',
          '    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">"test"</span><span class="p">],</span>',
          '    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '</span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
          '</span><span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
          '',
          '<span class="k">def</span> <span class="nf">get_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>',
          '    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>',
          '    <span class="n">prediction</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
          '    <span class="k">return</span> <span class="s2">"positive"</span> <span class="k">if</span> <span class="n">prediction</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">"negative"</span>',
          '</span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">"I hate this movie!"</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">get_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'negative',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      






    </div>

  </section>

</PostLayout>
