---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers';
const end_url = 'gptq';
const description = 'Aten√ß√£o, desenvolvedores! üö® Voc√™ tem um modelo de idioma que √© muito grande e pesado para o seu aplicativo? ü§Ø N√£o se preocupe, o GPTQ est√° aqui para ajud√°-lo! ü§ñ Esse algoritmo de quantiza√ß√£o √© como um assistente que faz com que bits e bytes desnecess√°rios desapare√ßam, reduzindo o tamanho do seu modelo sem perder muita precis√£o. üé© √â como compactar um arquivo sem perder a qualidade - √© uma maneira de tornar seus modelos mais eficientes e r√°pidos! üöÄ';
const keywords = 'gptq, quantiza√ß√£o, compress√£o, efici√™ncia do modelo, velocidade do modelo, tamanho do modelo, otimiza√ß√£o do modelo';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPTQ-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-07-27+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Trabalhos-nos-quais-se-baseia"><h2>Trabalhos nos quais se baseia</h2></a>
      <a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-em-camadas"><h3>Quantifica√ß√£o em camadas</h3></a>
      <a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-ideal-do-c%C3%A9rebro-(OBQ)"><h3>Quantiza√ß√£o ideal do c√©rebro (OBQ)</h3></a>
      <a class="anchor-link" href="#Algoritmo-GPTQ"><h2>Algoritmo GPTQ</h2></a>
      <a class="anchor-link" href="#Etapa-1:-informa%C3%A7%C3%B5es-arbitr%C3%A1rias-do-pedido"><h3>Etapa 1: informa√ß√µes arbitr√°rias do pedido</h3></a>
      <a class="anchor-link" href="#Etapa-2:-atualiza%C3%A7%C3%B5es-em-lote-pregui%C3%A7osas"><h3>Etapa 2: atualiza√ß√µes em lote pregui√ßosas</h3></a>
      <a class="anchor-link" href="#Etapa-3:-Reformula%C3%A7%C3%A3o-de-Cholesky"><h3>Etapa 3: Reformula√ß√£o de Cholesky</h3></a>
      <a class="anchor-link" href="#Resultados-do-GPTQ"><h2>Resultados do GPTQ</h2></a>
      <a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-extrema"><h2>Quantifica√ß√£o extrema</h2></a>
      <a class="anchor-link" href="#Desconto-din%C3%A2mico-na-infer%C3%AAncia"><h2>Desconto din√¢mico na infer√™ncia</h2></a>
      <a class="anchor-link" href="#Velocidade-de-infer%C3%AAncia"><h2>Velocidade de infer√™ncia</h2></a>
      <a class="anchor-link" href="#Livrarias"><h2>Livrarias</h2></a>
      <a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-de-um-modelo"><h2>Quantiza√ß√£o de um modelo</h2></a>
      <a class="anchor-link" href="#Infer%C3%AAncia-de-modelo-n%C3%A3o-quantificada"><h3>Infer√™ncia de modelo n√£o quantificada</h3></a>
      <a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-4-bits"><h3>Quantiza√ß√£o do modelo para 4 bits</h3></a>
      <a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-3-bits"><h3>Quantiza√ß√£o do modelo para 3 bits</h3></a>
      <a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-2-bits"><h3>Quantiza√ß√£o do modelo para 2 bits</h3></a>
      <a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-1-bit"><h3>Quantiza√ß√£o do modelo para 1 bit</h3></a>
      <a class="anchor-link" href="#Resumo-da-quantifica%C3%A7%C3%A3o"><h2>Resumo da quantifica√ß√£o</h2></a>
      <a class="anchor-link" href="#Carregamento-do-modelo-salvo"><h2>Carregamento do modelo salvo</h2></a>
      <a class="anchor-link" href="#Carregando-o-modelo-carregado-para-o-hub"><h2>Carregando o modelo carregado para o hub</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="GPTQ:-Quantiza%C3%A7%C3%A3o-p%C3%B3s-treinamento-precisa-para-transformadores-pr%C3%A9-treinados-generativos">GPTQ: Quantiza√ß√£o p√≥s-treinamento precisa para transformadores pr√©-treinados generativos<a class="anchor-link" href="#GPTQ:-Quantiza%C3%A7%C3%A3o-p%C3%B3s-treinamento-precisa-para-transformadores-pr%C3%A9-treinados-generativos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 44" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No artigo <a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="nofollow noreferrer">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a>, √© exposta a necessidade de criar um m√©todo de quantiza√ß√£o p√≥s-treinamento que n√£o prejudique a qualidade do modelo. Nesta postagem, vimos o m√©todo <a href="https://www.maximofn.com/llm-int8/">llm.int8()</a> que quantiza para INT8 alguns vetores das matrizes de peso, desde que nenhum de seus valores exceda um valor limite, o que √© bom, mas eles n√£o quantizam todos os pesos do modelo. Neste artigo, eles prop√µem um m√©todo que quantiza todos os pesos do modelo para 4 e 3 bits, sem degradar a qualidade do modelo. Isso economiza bastante mem√≥ria, n√£o s√≥ porque todos os pesos s√£o quantizados, mas tamb√©m porque isso √© feito em 4, 3 bits (e at√© mesmo 1 e 2 bits sob certas condi√ß√µes), em vez de 8 bits.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
      <h2 id="Trabalhos-nos-quais-se-baseia">Trabalhos nos quais se baseia<a class="anchor-link" href="#Trabalhos-nos-quais-se-baseia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 45" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantifica%C3%A7%C3%A3o-em-camadas">Quantifica√ß√£o em camadas<a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-em-camadas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 46" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por um lado, eles se baseiam nos trabalhos <code>Nagel et al., 2020</code>; <code>Wang et al., 2020</code>; <code>Hubara et al., 2021</code> e <code>Frantar et al., 2022</code>, que prop√µem quantizar os pesos das camadas de uma rede neural para 4 e 3 bits, sem degradar a qualidade do modelo.</p>
      <p>Dado um conjunto de dados <code>m</code> de um conjunto de dados, cada camada <code>l</code> √© alimentada com os dados e a sa√≠da dos pesos <code>W</code> dessa camada √© obtida. Portanto, o que voc√™ faz √© procurar novos pesos quantizados <code>≈¥</code> que minimizem o erro quadr√°tico em rela√ß√£o √† sa√≠da da camada de precis√£o total.</p>
      <p><code>argmin_≈¥||WX- ≈¥X||^2</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os valores de <code>≈¥</code> s√£o definidos antes da execu√ß√£o do processo de quantiza√ß√£o e, durante o processo, cada par√¢metro de <code>≈¥</code> pode mudar de valor independentemente, sem depender do valor dos outros par√¢metros de <code>≈¥</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantiza%C3%A7%C3%A3o-ideal-do-c%C3%A9rebro-(OBQ)">Quantiza√ß√£o ideal do c√©rebro (OBQ)<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-ideal-do-c%C3%A9rebro-(OBQ)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 47" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No trabalho <code>OBQ</code> de <code>Frantar et al., 2022</code>, eles otimizam o processo de quantiza√ß√£o em camadas acima, tornando-o at√© tr√™s vezes mais r√°pido. Isso ajuda com modelos grandes, pois a quantiza√ß√£o de um modelo grande pode levar muito tempo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O m√©todo <code>OBQ</code> √© uma abordagem para resolver o problema de quantiza√ß√£o em camadas em modelos de linguagem. O <code>OBQ</code> parte da ideia de que o erro quadr√°tico pode ser decomposto na soma de erros individuais para cada linha da matriz de peso. O m√©todo quantifica cada peso de forma independente, sempre atualizando os pesos n√£o quantificados para compensar o erro incorrido pela quantiza√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O m√©todo √© capaz de quantificar modelos de tamanho m√©dio em tempos razo√°veis, mas, como √© um algoritmo de complexidade c√∫bica, √© extremamente caro para ser aplicado a modelos com bilh√µes de par√¢metros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Algoritmo-GPTQ">Algoritmo GPTQ<a class="anchor-link" href="#Algoritmo-GPTQ"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 48" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Etapa-1:-informa%C3%A7%C3%B5es-arbitr%C3%A1rias-do-pedido">Etapa 1: informa√ß√µes arbitr√°rias do pedido<a class="anchor-link" href="#Etapa-1:-informa%C3%A7%C3%B5es-arbitr%C3%A1rias-do-pedido"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 49" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No <code>OBQ</code>, eles procuraram a linha de pesos que criava o menor erro quadr√°tico m√©dio para quantificar, mas perceberam que fazer isso aleatoriamente n√£o aumentava muito o erro quadr√°tico m√©dio final. Assim, em vez de procurar a linha que minimizasse o erro quadr√°tico m√©dio, o que criava uma complexidade c√∫bica no algoritmo, isso √© feito sempre na mesma ordem. Isso reduz bastante o tempo de execu√ß√£o do algoritmo de quantiza√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Etapa-2:-atualiza%C3%A7%C3%B5es-em-lote-pregui%C3%A7osas">Etapa 2: atualiza√ß√µes em lote pregui√ßosas<a class="anchor-link" href="#Etapa-2:-atualiza%C3%A7%C3%B5es-em-lote-pregui%C3%A7osas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 50" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como a atualiza√ß√£o dos pesos √© feita linha por linha, isso torna o processo lento e n√£o utiliza totalmente o hardware. Portanto, eles prop√µem executar as atualiza√ß√µes em lotes de <code>B=128</code> linhas. Isso faz melhor uso do hardware e reduz o tempo de execu√ß√£o do algoritmo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Etapa-3:-Reformula%C3%A7%C3%A3o-de-Cholesky">Etapa 3: Reformula√ß√£o de Cholesky<a class="anchor-link" href="#Etapa-3:-Reformula%C3%A7%C3%A3o-de-Cholesky"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 51" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O problema com as atualiza√ß√µes em lote √© que, devido √† grande escala dos modelos, podem ocorrer erros num√©ricos que afetam a precis√£o do algoritmo. Em particular, matrizes indefinidas podem ser obtidas, fazendo com que o algoritmo atualize os pesos restantes nas dire√ß√µes erradas, resultando em uma quantiza√ß√£o muito ruim.</p>
      <p>Para resolver isso, os autores do artigo prop√µem o uso de uma reformula√ß√£o Cholesky, que √© um m√©todo numericamente mais est√°vel.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resultados-do-GPTQ">Resultados do GPTQ<a class="anchor-link" href="#Resultados-do-GPTQ"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 52" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Abaixo est√£o dois gr√°ficos com a medida de perplexidade no conjunto de dados <code>WikiText2</code> para todos os tamanhos dos modelos OPT e BLOOM. √â poss√≠vel observar que, com a t√©cnica de quantiza√ß√£o RTN, a perplexidade em alguns tamanhos aumenta muito, enquanto com o GPTQ ela permanece semelhante √† obtida com o modelo FP16.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="GPTQ-figure1" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPTQ-figure1.webp" width="1097" height="419"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Outros gr√°ficos s√£o mostrados abaixo, mas com a medida de precis√£o no conjunto de dados <code>LAMBADA</code>. √â a mesma coisa, enquanto o GPTQ permanece semelhante ao obtido com o FP16, outros m√©todos de quantiza√ß√£o degradam muito a qualidade do modelo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="GPTQ-figure3" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPTQ-figure3.webp" width="1099" height="430"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Quantifica%C3%A7%C3%A3o-extrema">Quantifica√ß√£o extrema<a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-extrema"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 53" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os gr√°ficos anteriores mostraram os resultados da quantiza√ß√£o do modelo em 3 e 4 bits, mas podemos quantiz√°-los em 2 bits ou at√© mesmo em 1 bit.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Modificando o tamanho dos lotes ao usar o algoritmo, podemos obter bons resultados quantificando tanto o modelo quanto os lotes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <table>
      <thead>
      <tr>
      <th>Modelo</th>
      <th>FP16</th>
      <th>g128</th>
      <th>g64</th>
      <th>g32</th>
      <th>3 bits</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>OPT-175B</td>
      <td>8,34</td>
      <td>9,58</td>
      <td>9,18</td>
      <td>8,94</td>
      <td>8,68</td>
      </tr>
      <tr>
      <td>BLOOM</td>
      <td>8,11</td>
      <td>9,55</td>
      <td>9,17</td>
      <td>8,83</td>
      <td>8,64</td>
      </tr>
      </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Na tabela acima, voc√™ pode ver o resultado da perplexidade no conjunto de dados <code>WikiText2</code> para os modelos <code>OPT-175B</code> e <code>BLOOM</code> quantizados em 3 bits. √â poss√≠vel observar que, √† medida que lotes menores s√£o usados, a perplexidade diminui, o que significa que a qualidade do modelo quantizado √© melhor. Mas o problema √© que o algoritmo leva mais tempo para ser executado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desconto-din%C3%A2mico-na-infer%C3%AAncia">Desconto din√¢mico na infer√™ncia<a class="anchor-link" href="#Desconto-din%C3%A2mico-na-infer%C3%AAncia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 54" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Durante a infer√™ncia, algo chamado "dequantiza√ß√£o din√¢mica" √© executado para realizar a infer√™ncia. Cada camada √© dequantificada √† medida que passa por ela.</p>
      <p>Para fazer isso, eles desenvolveram um kernel que desquantifica as matrizes e executa produtos de matrizes. Embora a descompacta√ß√£o seja mais intensiva em termos de computa√ß√£o, o kernel precisa acessar muito menos mem√≥ria, o que resulta em um aumento significativo da velocidade.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A infer√™ncia √© realizada no FP16, descontando os pesos √† medida que voc√™ passa pelas camadas, e a fun√ß√£o de ativa√ß√£o de cada camada tamb√©m √© realizada no FP16. Embora isso signifique que mais c√°lculos tenham de ser feitos, porque os pesos t√™m de ser descontados, esses c√°lculos tornam o processo geral mais r√°pido, porque menos dados t√™m de ser buscados na mem√≥ria. Os pesos precisam ser obtidos da mem√≥ria em menos bits, portanto, no final, em matrizes com muitos par√¢metros, isso economiza muitos dados. O gargalo geralmente est√° na obten√ß√£o dos dados da mem√≥ria, portanto, mesmo que voc√™ tenha que fazer mais c√°lculos, no final a infer√™ncia √© mais r√°pida.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Velocidade-de-infer%C3%AAncia">Velocidade de infer√™ncia<a class="anchor-link" href="#Velocidade-de-infer%C3%AAncia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 55" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os autores do artigo testaram a quantiza√ß√£o do modelo BLOOM-175B para 3 bits, o que ocupou cerca de 63 GB de mem√≥ria VRAM, incluindo embeddings e a camada de sa√≠da que s√£o mantidos em FP16. Al√©m disso, a manuten√ß√£o da janela de contexto de 2048 tokens consome cerca de 9 GB de mem√≥ria, em um total de cerca de 72 GB de mem√≥ria VRAM. Eles quantizaram em 3 bits e n√£o em 4 bits para poder realizar esse experimento e ajustar o modelo em uma √∫nica GPU Nvidia A100 com 80 GB de mem√≥ria VRAM.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para fins de compara√ß√£o, a infer√™ncia FP16 normal requer cerca de 350 GB de VRAM, o que equivale a 5 GPUs Nvidia A100 com 80 GB de VRAM. E a infer√™ncia quantizada de 8 bits usando <a href="https://www.maximofn.com/llm-int8/">llm.int8()</a> requer 3 dessas GPUs.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Abaixo est√° uma tabela com infer√™ncia de modelo em FP16 e 3 bits quantizados em GPUs Nvidia A100 com 80 GB de VRAM e GPUs Nvidia A6000 com 48 GB de VRAM.</p>
      <table>
      <thead>
      <tr>
      <th>GPU (VRAM)</th>
      <th>Tempo m√©dio por token em FP16 (ms)</th>
      <th>Tempo m√©dio por token em 3 bits (ms)</th>
      <th>Acelera√ß√£o</th>
      <th>Redu√ß√£o das GPUs necess√°rias</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>A6000 (48GB)</td>
      <td>589</td>
      <td>130</td>
      <td>√ó4.53</td>
      <td>8‚Üí 2</td>
      </tr>
      <tr>
      <td>A100 (80GB)</td>
      <td>230</td>
      <td>71</td>
      <td>√ó3.24</td>
      <td>5‚Üí 1</td>
      </tr>
      </tbody>
      </table>
      <p>Por exemplo, usando os kernels, o modelo OPT-175B de 3 bits √© executado em um √∫nico A100 (em vez de 5) e √© aproximadamente 3,25 vezes mais r√°pido do que a vers√£o FP16 em termos de tempo m√©dio por token.</p>
      <p>A GPU NVIDIA A6000 tem uma largura de banda de mem√≥ria muito menor, o que torna essa estrat√©gia ainda mais eficaz: a execu√ß√£o do modelo OPT-175B de 3 bits em 2 GPUs A6000 (em vez de 8) √© aproximadamente 4,53 vezes mais r√°pida do que a vers√£o FP16.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Livrarias">Livrarias<a class="anchor-link" href="#Livrarias"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 56" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os autores do artigo implementaram a biblioteca <a href="https://github.com/IST-DASLab/gptq" target="_blank" rel="nofollow noreferrer">GPTQ</a>. Outras bibliotecas foram criadas, como <a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">GPTQ-for-LLaMa</a>, <a href="https://github.com/turboderp/exllama">exllama</a> e <a href="https://github.com/ggerganov/llama.cpp/">llama.cpp</a>. No entanto, essas bibliotecas se concentram apenas na arquitetura llama, de modo que a biblioteca <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a> ganhou mais popularidade porque tem uma cobertura mais ampla de arquiteturas.</p>
      <p>Por esse motivo, essa biblioteca <a href="https://github.com/AutoGPTQ/AutoGPTQ" target="_blank" rel="nofollow noreferrer">AutoGPTQ</a> foi integrada por meio de uma API na biblioteca <a href="https://www.maximofn.com/hugging-face-transformers/">transformers</a>. Para us√°-la, √© necess√°rio instal√°-la conforme indicado na se√ß√£o <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation">Installation</a> de seu reposit√≥rio e ter a biblioteca <a href="https://www.maximofn.com/hugging-face-optimun/">optimun</a> instalada.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al√©m da se√ß√£o <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation" target="_blank" rel="nofollow noreferrer">Installation</a> do seu reposit√≥rio, voc√™ tamb√©m deve fazer o seguinte:</p>
      <div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/PanQiWei/AutoGPTQ
      <span class="nb">cd</span><span class="w"> </span>AutoGPTQ
      pip<span class="w"> </span>install<span class="w"> </span>.
      <span class="sb">```</span>
      
      Para<span class="w"> </span>que<span class="w"> </span>os<span class="w"> </span>kernels<span class="w"> </span>de<span class="w"> </span>quantiza√ß√£o<span class="w"> </span>da<span class="w"> </span>GPU<span class="w"> </span>desenvolvidos<span class="w"> </span>pelos<span class="w"> </span>autores<span class="w"> </span><span class="k">do</span><span class="w"> </span>artigo<span class="w"> </span>sejam<span class="w"> </span>instalados.
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Quantiza%C3%A7%C3%A3o-de-um-modelo">Quantiza√ß√£o de um modelo<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-de-um-modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 57" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como quantificar um modelo com a biblioteca <a href="https://www.maximofn.com/hugging-face-optimun/">optimun</a> e a API <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Infer%C3%AAncia-de-modelo-n%C3%A3o-quantificada">Infer√™ncia de modelo n√£o quantificada<a class="anchor-link" href="#Infer%C3%AAncia-de-modelo-n%C3%A3o-quantificada"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 58" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantificar o modelo <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" target="_blank" rel="nofollow noreferrer">meta-call/Meta-Call-3-8B-Instruct</a> que, como o nome indica, √© um modelo de 8B par√¢metros, portanto, no FP16, precisar√≠amos de 16 GB de mem√≥ria VRAM. Primeiro, executamos o modelo para ver quanta mem√≥ria ele ocupa e a sa√≠da que ele gera</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como para usar esse modelo temos que pedir permiss√£o ao Meta, entramos no HuggingFace para baixar o tokenizador e o modelo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
      '      ',
      '      <span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Instanciamos o tokenizador e o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
      '      ',
      '      <span class="n">notebook_login</span><span class="p">()</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '      <span class="kn">import</span> <span class="nn">torch</span>',
      '      ',
      '      <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>',
      '      ',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>Vamos ver a quantidade de mem√≥ria que o FP16 ocupa.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
          '',
          '<span class="n">notebook_login</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
          '<span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '</span><span class="n">model_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Model memory: 14.96 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que ele ocupa quase 15 GB, mais ou menos os 16 GB que dissemos que deveria ocupar, mas por que essa diferen√ßa? Certamente esse modelo n√£o tem exatamente 8B de par√¢metros, mas tem um pouco menos, mas ao indicar o n√∫mero de par√¢metros, ele √© arredondado para 8B.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos uma infer√™ncia para ver como ele faz isso e quanto tempo leva.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
          <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
          <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
          <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{opening_brace}</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Hello my name is Maximo and I am a Machine Learning Engineer at a startup in the Bay Area. I am passionate about building AI systems that can help humans make better decisions and improve their lives.
      
      I have a background in computer science and mathematics, and I have been working with machine learning for several years. I
      Inference time: 4.14 s
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantiza%C3%A7%C3%A3o-do-modelo-para-4-bits">Quantiza√ß√£o do modelo para 4 bits<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-4-bits"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 59" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantificar isso em 4 bits. Reinicio o notebook para evitar problemas de mem√≥ria, ent√£o entramos no Hugging Face novamente.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">time</span>',
      '      ',
      '      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '      ',
      '      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '      <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '          <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '          <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '          <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '      <span class="p">)</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
      '      ',
      '      <span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Primeiro, crio o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
          '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
          '    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
          '    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
          '    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
          '',
          '<span class="n">notebook_login</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos a configura√ß√£o de quantiza√ß√£o. Como j√° dissemos, esse algoritmo calcula o erro dos pesos quantizados em rela√ß√£o aos pesos originais com base nas entradas de um conjunto de dados, portanto, na configura√ß√£o, temos de inform√°-lo com qual conjunto de dados queremos quantificar o modelo.</p>
      <p>Os padr√µes dispon√≠veis s√£o <code>wikitext2</code>, <code>c4</code>, <code>c4-new</code>, <code>ptb</code> e <code>ptb-new</code>.</p>
      <p>Tamb√©m podemos criar um conjunto de dados a partir de uma lista de cadeias de caracteres.</p>
      <div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"auto-gptq √© uma biblioteca de quantiza√ß√£o de modelos f√°cil de usar com apis amig√°veis, baseada no algoritmo GPTQ."</span><span class="p">]</span><span class="o">.</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al√©m disso, precisamos informar a ele o n√∫mero de bits que o modelo quantizado tem por meio do par√¢metro <code>bits</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>',
      '      ',
      '      <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Quantificamos o modelo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
      <span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
      <span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{opening_brace}</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s = </span><span class="si">{opening_brace}</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> min"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing model.layers blocks : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|32/32 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Quantization time: 1932.09 s = 32.20 min
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como o processo de quantiza√ß√£o calcula o menor erro entre os pesos quantizados e os pesos originais ao passar as entradas por cada camada, o processo de quantiza√ß√£o leva tempo. Nesse caso, levou cerca de meia hora</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada na mem√≥ria que ele ocupa agora</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>',
          '',
          '<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
          '<span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>',
          '<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min"</span><span class="p">)</span>',
          '</span><span class="n">model_4bits_memory</span> <span class="o">=</span> <span class="n">model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;?, ?it/s]',
          'Model memory: 5.34 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aqui podemos ver um benef√≠cio da quantiza√ß√£o. Enquanto o modelo original ocupava cerca de 15 GB de VRAM, agora o modelo quantizado ocupa cerca de 5 GB, quase um ter√ßo do tamanho original.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e observamos o tempo que leva</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
          <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
          <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
          <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{opening_brace}</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I am passionate about developing innovative solutions that can positively impact society. I am excited to be a part of this community and to learn from and contribute to the discussions here. I am particularly
      Inference time: 2.34 s
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O modelo n√£o quantizado levou 4,14 segundos, enquanto agora quantizado para 4 bits levou 2,34 segundos e tamb√©m gerou bem o texto. Conseguimos reduzir a infer√™ncia em quase metade.</p>
      <p>Como o tamanho do modelo quantizado √© quase um ter√ßo do modelo FP16, poder√≠amos pensar que a velocidade de infer√™ncia deveria ser cerca de tr√™s vezes mais r√°pida com o modelo quantizado. Mas lembre-se de que, em cada camada, os pesos s√£o quantificados e os c√°lculos s√£o realizados em FP16, portanto, s√≥ conseguimos reduzir o tempo de infer√™ncia pela metade e n√£o em um ter√ßo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora vamos salvar o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
          '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
          '    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
          '    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
          '    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>',
          '</span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">"./model_4bits/"</span>',
          '<span class="n">model_4bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
          '(\'./model_4bits/tokenizer_config.json\',',
          ' \'./model_4bits/special_tokens_map.json\',',
          ' \'./model_4bits/tokenizer.json\')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E fazemos o upload para o hub</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-4bits"</span>
      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{opening_brace}</span><span class="n">checkpoint</span><span class="si">{closing_brace}</span><span class="s2">: </span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">{closing_brace}</span><span class="s2">bits, gr</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">{closing_brace}</span><span class="s2">, desc_act=</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">{closing_brace}</span><span class="s2">"</span>
      <span class="n">model_4bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.17/5.17k [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/44cfdcad78db260122943d3f57858c1b840bda17', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False', commit_description='', oid='44cfdcad78db260122943d3f57858c1b840bda17', pr_url=None, pr_revision=None, pr_num=None)</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m fizemos o upload do tokenizador. Embora n√£o tenhamos alterado o tokenizador, n√≥s o carregamos porque, se algu√©m fizer download do nosso modelo a partir do hub, n√£o precisar√° saber qual tokenizador usamos, portanto, provavelmente desejar√° fazer o download do modelo e do tokenizador juntos. Podemos indicar no cart√£o do modelo qual tokenizador usamos para fazer o download, mas √© muito prov√°vel que a pessoa n√£o leia o cart√£o do modelo, tente fazer o download do tokenizador, receba um erro e n√£o saiba o que fazer. Por isso, fizemos o upload para evitar esse problema.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-4bits"</span>
      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Tokenizers for </span><span class="si">{opening_brace}</span><span class="n">checkpoint</span><span class="si">{closing_brace}</span><span class="s2">: </span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">{closing_brace}</span><span class="s2">bits, gr</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">{closing_brace}</span><span class="s2">, desc_act=</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">{closing_brace}</span><span class="s2">"</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/5.17k [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[5]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4', commit_message='Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False', commit_description='', oid='75600041ca6e38b5f1fb912ad1803b66656faae4', pr_url=None, pr_revision=None, pr_num=None)</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantiza%C3%A7%C3%A3o-do-modelo-para-3-bits">Quantiza√ß√£o do modelo para 3 bits<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-3-bits"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 60" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantificar isso em 3 bits. Reinicio o notebook para evitar problemas de mem√≥ria e fa√ßo login novamente no Hugging Face.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-4bits"</span>',
      '      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
      '      <span class="n">model_4bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-4bits"</span>',
      '      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
      '      ',
      '      <span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Primeiro, crio o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-4bits"</span>',
          '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
          '<span class="n">model_4bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
          '</span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-4bits"</span>',
          '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
          '',
          '<span class="n">notebook_login</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.17/5.17k [00:00&lt;?, ?B/s]',
          'README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/5.17k [00:00&lt;?, ?B/s]',
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos a configura√ß√£o de quantiza√ß√£o e agora indicamos que queremos quantizar para 3 bits.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>',
      '      ',
      '      <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Quantificamos o modelo</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
      <span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">model_3bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
      <span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{opening_brace}</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s = </span><span class="si">{opening_brace}</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> min"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing model.layers blocks : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|32/32 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Quantization time: 1912.69 s = 31.88 min
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como antes, demorou cerca de meia hora.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada na mem√≥ria que ele ocupa agora</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>',
          '',
          '<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
          '<span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">model_3bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>',
          '<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min"</span><span class="p">)</span>',
          '</span><span class="n">model_3bits_memory</span> <span class="o">=</span> <span class="n">model_3bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_3bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;?, ?it/s]',
          'Model memory: 4.52 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>O espa√ßo ocupado pela mem√≥ria do modelo de 3 bits tamb√©m √© de quase 5 GB. O modelo de 4 bits ocupava 5,34 GB, enquanto o modelo de 3 bits agora ocupa 4,52 GB, portanto, conseguimos reduzir um pouco mais o tamanho do modelo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e observamos o tempo que leva</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_3bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model_3bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
          <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
          <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
          <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{opening_brace}</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Hello my name is Maximo and I am a Machine Learning Engineer at Google. I am excited to be here today to talk about my work in the field of Machine Learning and to share some of the insights I have gained through my experiences.
      I am a Machine Learning Engineer at Google, and I am excited to be
      Inference time: 2.89 s
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Embora a sa√≠da de 3 bits seja boa, o tempo de infer√™ncia passou a ser de 2,89 segundos, enquanto a sa√≠da de 4 bits foi de 2,34 segundos. Mais testes devem ser feitos para verificar se sempre leva menos tempo em 4 bits ou se a diferen√ßa √© t√£o pequena que √†s vezes a infer√™ncia de 3 bits √© mais r√°pida e √†s vezes a infer√™ncia de 4 bits √© mais r√°pida.</p>
      <p>Al√©m disso, embora o resultado fa√ßa sentido, ele come√ßa a se tornar repetitivo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Salvamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_3bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
          '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_3bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
          '    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
          '    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
          '    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>',
          '</span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">"./model_3bits/"</span>',
          '<span class="n">model_3bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
          '(\'./model_3bits/tokenizer_config.json\',',
          ' \'./model_3bits/special_tokens_map.json\',',
          ' \'./model_3bits/tokenizer.json\')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E fazemos o upload para o hub</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-3bits"</span>
      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{opening_brace}</span><span class="n">checkpoint</span><span class="si">{closing_brace}</span><span class="s2">: </span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">{closing_brace}</span><span class="s2">bits, gr</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">{closing_brace}</span><span class="s2">, desc_act=</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">{closing_brace}</span><span class="s2">"</span>
      <span class="n">model_3bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.85/4.85G [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[14]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-3bits/commit/422fd94a031234c10224ddbe09c0e029a5e9c01f', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 3bits, gr128, desc_act=False', commit_description='', oid='422fd94a031234c10224ddbe09c0e029a5e9c01f', pr_url=None, pr_revision=None, pr_num=None)</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m carregamos o tokenizador</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-3bits"</span>
      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Tokenizers for </span><span class="si">{opening_brace}</span><span class="n">checkpoint</span><span class="si">{closing_brace}</span><span class="s2">: </span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">{closing_brace}</span><span class="s2">bits, gr</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">{closing_brace}</span><span class="s2">, desc_act=</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">{closing_brace}</span><span class="s2">"</span>
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/5.17k [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4', commit_message='Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False', commit_description='', oid='75600041ca6e38b5f1fb912ad1803b66656faae4', pr_url=None, pr_revision=None, pr_num=None)</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantiza%C3%A7%C3%A3o-do-modelo-para-2-bits">Quantiza√ß√£o do modelo para 2 bits<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-2-bits"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 61" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantificar isso em 2 bits. Reinicio o notebook para evitar problemas de mem√≥ria e fa√ßo login novamente no Hugging Face.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-3bits"</span>',
      '      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
      '      <span class="n">model_3bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-3bits"</span>',
      '      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
      '      <span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
      '      ',
      '      <span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Primeiro, crio o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-3bits"</span>',
          '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
          '<span class="n">model_3bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
          '</span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-3bits"</span>',
          '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
          '',
          '<span class="n">notebook_login</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.85/4.85G [00:00&lt;?, ?B/s]',
          'README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/5.17k [00:00&lt;?, ?B/s]',
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos a configura√ß√£o de quantiza√ß√£o. Agora dizemos a ele para quantizar em 2 bits. Al√©m disso, temos que indicar quantos vetores da matriz de peso ele quantiza ao mesmo tempo por meio do par√¢metro <code>group_size</code>, antes, por padr√£o, ele tinha o valor 128 e n√£o mexemos nele, mas agora, ao quantizar para 2 bits, para ter menos erro, colocamos um valor menor. Se o deixarmos em 128, o modelo quantizado funcionar√° muito mal. Nesse caso, colocarei um valor de 16.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>',
      '      ',
      '      <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
      <span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">model_2bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
      <span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{opening_brace}</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s = </span><span class="si">{opening_brace}</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> min"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing model.layers blocks : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|32/32 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Quantization time: 1973.12 s = 32.89 min
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que isso tamb√©m levou cerca de meia hora.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada na mem√≥ria que ele ocupa agora</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>',
          '',
          '<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
          '<span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">model_2bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>',
          '<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min"</span><span class="p">)</span>',
          '</span><span class="n">model_2bits_memory</span> <span class="o">=</span> <span class="n">model_2bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_2bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;?, ?it/s]',
          'Model memory: 4.50 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Enquanto a quantiza√ß√£o de 4 bits era de 5,34 GB e a de 3 bits era de 4,52 GB, agora a quantiza√ß√£o de 2 bits √© de 4,50 GB, portanto, conseguimos reduzir um pouco mais o tamanho do modelo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e observamos o tempo que leva</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_2bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model_2bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
          <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
          <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
          <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{opening_brace}</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Hello my name is Maximo and I am a Machine Learning Engineer.  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
      Inference time: 2.92 s
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que o resultado j√° n√£o √© bom, al√©m disso, o tempo de infer√™ncia √© de 2,92 segundos, praticamente o mesmo que com 3 e 4 bits.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Salvamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_2bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
          '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_2bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
          '    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
          '    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
          '    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>',
          '</span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">"./model_2bits/"</span>',
          '<span class="n">model_2bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
          '(\'./model_2bits/tokenizer_config.json\',',
          ' \'./model_2bits/special_tokens_map.json\',',
          ' \'./model_2bits/tokenizer.json\')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>N√≥s o carregamos no hub</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-2bits"</span>
      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{opening_brace}</span><span class="n">checkpoint</span><span class="si">{closing_brace}</span><span class="s2">: </span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">{closing_brace}</span><span class="s2">bits, gr</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">{closing_brace}</span><span class="s2">, desc_act=</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">{closing_brace}</span><span class="s2">"</span>
      <span class="n">model_2bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.83/4.83G [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[8]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/13ede006ce0dbbd8aca54212e960eff98ea5ec63', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr16, desc_act=False', commit_description='', oid='13ede006ce0dbbd8aca54212e960eff98ea5ec63', pr_url=None, pr_revision=None, pr_num=None)</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantiza%C3%A7%C3%A3o-do-modelo-para-1-bit">Quantiza√ß√£o do modelo para 1 bit<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-1-bit"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 62" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantificar isso em 1 bit. Reinicio o notebook para evitar problemas de mem√≥ria e fa√ßo login novamente no Hugging Face.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-2bits"</span>',
      '      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
      '      <span class="n">model_2bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
      '      ',
      '      <span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Primeiro, crio o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-2bits"</span>',
          '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
          '<span class="n">model_2bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>',
          '',
          '<span class="n">notebook_login</span><span class="p">()</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.83/4.83G [00:00&lt;?, ?B/s]',
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos a configura√ß√£o de quantiza√ß√£o, agora dizemos a ela para quantizar em apenas 1 bit e tamb√©m para usar um <code>group_size</code> de 8.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>',
      '      ',
      '      <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
      <span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">model_1bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
      <span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{opening_brace}</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s = </span><span class="si">{opening_brace}</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> min"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing model.layers blocks : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|32/32 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
        warnings.warn(
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Quantization time: 2030.38 s = 33.84 min
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que tamb√©m leva cerca de meia hora para quantificar.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada na mem√≥ria que ele ocupa agora</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>',
          '',
          '<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
          '<span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">model_1bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>',
          '<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min"</span><span class="p">)</span>',
          '</span><span class="n">model_1bits_memory</span> <span class="o">=</span> <span class="n">model_1bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_1bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&lt;?, ?it/s]',
          'Model memory: 5.42 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que, nesse caso, ele ocupa ainda mais do que quantificado em 2 bits, 4,52 GB.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e observamos o tempo que leva</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_1bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model_1bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
          <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
          <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
          <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{opening_brace}</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Hello my name is Maximo and I am a Machine Learning Engineerimerszuimersimerspinsimersimersingoingoimersurosimersimersimersoleningoimersingopinsimersbirpinsimersimersimersorgeingoimersiringimersimersimersimersimersimersimers„É≥„Éá„Ç£orge_REFERER ingestÁæäimersorgeimersimersendetingo–®–êhandsingo
      Inference time: 3.12 s
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que a sa√≠da √© muito ruim e tamb√©m demora mais do que quando quantizamos para 2 bits.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Salvamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_1bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
          '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_1bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
          '    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
          '    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
          '    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>',
          '</span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">"./model_1bits/"</span>',
          '<span class="n">model_1bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
          '(\'./model_1bits/tokenizer_config.json\',',
          ' \'./model_1bits/special_tokens_map.json\',',
          ' \'./model_1bits/tokenizer.json\')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>N√≥s o carregamos no hub</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-1bits"</span>
      <span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{opening_brace}</span><span class="n">checkpoint</span><span class="si">{closing_brace}</span><span class="s2">: </span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">{closing_brace}</span><span class="s2">bits, gr</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">{closing_brace}</span><span class="s2">, desc_act=</span><span class="si">{opening_brace}</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">{closing_brace}</span><span class="s2">"</span>
      <span class="n">model_1bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/5.17k [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>Upload 2 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0/2 [00:00&lt;?, ?it/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>model-00002-of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/1.05G [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-text-output-subarea">
      <pre>model-00001-of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/4.76G [00:00&lt;?, ?B/s]</pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[8]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/e59ccffc03247e7dcc418f98b482cc02dc7a168d', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr8, desc_act=False', commit_description='', oid='e59ccffc03247e7dcc418f98b482cc02dc7a168d', pr_url=None, pr_revision=None, pr_num=None)</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resumo-da-quantifica%C3%A7%C3%A3o">Resumo da quantifica√ß√£o<a class="anchor-link" href="#Resumo-da-quantifica%C3%A7%C3%A3o"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 63" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos comprar quantiza√ß√£o de 4, 3, 2 e 1 bits.</p>
      <table>
      <thead>
      <tr>
      <th>Bits</th>
      <th>Tempo de quantiza√ß√£o (min)</th>
      <th>Mem√≥ria (GB)</th>
      <th>Tempo de infer√™ncia (s)</th>
      <th>Qualidade da sa√≠da</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>FP16</td>
      <td>0</td>
      <td>14.96</td>
      <td>4.14</td>
      <td>Bom</td>
      </tr>
      <tr>
      <td>4</td>
      <td>32.20</td>
      <td>5.34</td>
      <td>2.34</td>
      <td>Bom</td>
      </tr>
      <tr>
      <td>3</td>
      <td>31.88</td>
      <td>4.52</td>
      <td>2.89</td>
      <td>Bom</td>
      </tr>
      <tr>
      <td>2</td>
      <td>32.89</td>
      <td>4.50</td>
      <td>2.92</td>
      <td>Ruim</td>
      </tr>
      <tr>
      <td>1</td>
      <td>33.84</td>
      <td>5.42</td>
      <td>3.12</td>
      <td>Ruim</td>
      </tr>
      </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Observando essa tabela, vemos que n√£o faz sentido, neste exemplo, quantificar com menos de 4 bits.</p>
      <p>A quantifica√ß√£o em 1 e 2 bits claramente n√£o faz sentido porque a qualidade da sa√≠da √© ruim.</p>
      <p>Mas, embora a sa√≠da quando quantizamos para 3 bits seja boa, ela come√ßou a se tornar repetitiva, portanto, a longo prazo, provavelmente n√£o seria uma boa ideia usar esse modelo. Al√©m disso, nem a economia no tempo de quantiza√ß√£o, nem a economia de VRAM, nem a economia no tempo de infer√™ncia s√£o significativas em compara√ß√£o com a quantiza√ß√£o para 4 bits.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Carregamento-do-modelo-salvo">Carregamento do modelo salvo<a class="anchor-link" href="#Carregamento-do-modelo-salvo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 64" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que comparamos a quantiza√ß√£o dos modelos, vamos ver como seria feito para carregar o modelo de 4 bits que salvamos, j√° que, como vimos, essa √© a melhor op√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, carregamos o tokenizador que usamos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-1bits"</span>',
          '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>',
          '<span class="n">model_1bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">path</span> <span class="o">=</span> <span class="s2">"./model_4bits"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/5.17k [00:00&lt;?, ?B/s]',
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora, carregamos o modelo que salvamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">load_model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos a mem√≥ria que ele ocupa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">load_model_4bits_memory</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">load_model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Model memory: 5.34 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que ele ocupa a mesma mem√≥ria de quando o quantificamos, o que √© l√≥gico.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e observamos o tempo que leva</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">load_model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
          <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
          <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
          <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{opening_brace}</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I have been working with machine learning models for several years. I am excited to be a part of this community and to share my knowledge and experience with others. I am particularly interested in
      Inference time: 3.82 s
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que a infer√™ncia √© boa e levou 3,82 segundos, um pouco mais do que quando a quantificamos. Mas, como eu disse antes, ter√≠amos que fazer esse teste v√°rias vezes e tirar uma m√©dia.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Carregando-o-modelo-carregado-para-o-hub">Carregando o modelo carregado para o hub<a class="anchor-link" href="#Carregando-o-modelo-carregado-para-o-hub"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 65" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora veremos como carregar o modelo de 4 bits que carregamos no hub.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, carregamos o tokenizador que carregamos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">load_model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
          '<span class="n">outputs</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
          '    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
          '    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
          '    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"Maximofn/Llama-3-8B-Instruct-GPTQ-4bits"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora, carregamos o modelo que salvamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
      '      ',
      '      <span class="n">load_model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Vemos a mem√≥ria que ele ocupa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">load_model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>',
          '</span><span class="n">load_model_4bits_memory</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">load_model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Model memory: 5.34 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ele tamb√©m ocupa a mesma mem√≥ria</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e observamos o tempo que leva</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">import</span> <span class="nn">time</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">load_model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
      
      <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
          <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
          <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
          <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{opening_brace}</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">{closing_brace}</span><span class="s2"> s"</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stdout-output-text">
      <pre>Hello my name is Maximo and I am a Machine Learning Engineer with a passion for building innovative AI solutions. I have been working in the field of AI for over 5 years, and have gained extensive experience in developing and implementing machine learning models for various industries.
      
      In my free time, I enjoy reading books on
      Inference time: 3.81 s
      </pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que a infer√™ncia tamb√©m √© boa e levou 3,81 segundos.</p>
      </section>
      






    </div>

  </section>

</PostLayout>
