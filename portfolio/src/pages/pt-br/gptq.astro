---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers';
const end_url = 'gptq';
const description = 'Aten√ß√£o, desenvolvedores! üö® Voc√™ tem um modelo de idioma que √© muito grande e pesado para o seu aplicativo? ü§Ø N√£o se preocupe, o GPTQ est√° aqui para ajud√°-lo! ü§ñ Esse algoritmo de quantiza√ß√£o √© como um assistente que faz com que bits e bytes desnecess√°rios desapare√ßam, reduzindo o tamanho do seu modelo sem perder muita precis√£o. üé© √â como compactar um arquivo sem perder a qualidade - √© uma maneira de tornar seus modelos mais eficientes e r√°pidos! üöÄ';
const keywords = 'gptq, quantiza√ß√£o, compress√£o, efici√™ncia do modelo, velocidade do modelo, tamanho do modelo, otimiza√ß√£o do modelo';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPTQ-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-07-27+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Trabalhos nos quais se baseia"><h2>Trabalhos nos quais se baseia</h2></a>
      <a class="anchor-link" href="#Quantizacao por camadas"><h3>Quantiza√ß√£o por camadas</h3></a>
      <a class="anchor-link" href="#Quantizacao otima do cerebro (OBC)"><h3>Quantiza√ß√£o √≥tima do c√©rebro (OBC)</h3></a>
      <a class="anchor-link" href="#Algoritmo de GPTQ"><h2>Algoritmo de GPTQ</h2></a>
      <a class="anchor-link" href="#Passo 1: Informacao em ordem arbitraria"><h3>Passo 1: Informa√ß√£o em ordem arbitr√°ria</h3></a>
      <a class="anchor-link" href="#Passo 2: Atualizacoes em lote preguicosas"><h3>Passo 2: Atualiza√ß√µes em lote pregui√ßosas</h3></a>
      <a class="anchor-link" href="#Passo 3: Reformulacao de Cholesky"><h3>Passo 3: Reformula√ß√£o de Cholesky</h3></a>
      <a class="anchor-link" href="#Resultados do GPTQ"><h2>Resultados do GPTQ</h2></a>
      <a class="anchor-link" href="#Quantizacao extrema"><h2>Quantiza√ß√£o extrema</h2></a>
      <a class="anchor-link" href="#Desquantizacao dinamica na inferencia"><h2>Desquantiza√ß√£o din√¢mica na infer√™ncia</h2></a>
      <a class="anchor-link" href="#Velocidade de inferencia"><h2>Velocidade de infer√™ncia</h2></a>
      <a class="anchor-link" href="#Bibliotecas"><h2>Bibliotecas</h2></a>
      <a class="anchor-link" href="#Quantizacao de um modelo"><h2>Quantiza√ß√£o de um modelo</h2></a>
      <a class="anchor-link" href="#Inferencia do modelo nao quantizado"><h3>Infer√™ncia do modelo n√£o quantizado</h3></a>
      <a class="anchor-link" href="#Quantizacao do modelo para 4 bits"><h3>Quantiza√ß√£o do modelo para 4 bits</h3></a>
      <a class="anchor-link" href="#Quantizacao do modelo para 3 bits"><h3>Quantiza√ß√£o do modelo para 3 bits</h3></a>
      <a class="anchor-link" href="#Quantizacao do modelo para 2 bits"><h3>Quantiza√ß√£o do modelo para 2 bits</h3></a>
      <a class="anchor-link" href="#Quantizacao do modelo para 1 bit"><h3>Quantiza√ß√£o do modelo para 1 bit</h3></a>
      <a class="anchor-link" href="#Resumo da quantizacao"><h2>Resumo da quantiza√ß√£o</h2></a>
      <a class="anchor-link" href="#Carregamento do modelo salvo"><h2>Carregamento do modelo salvo</h2></a>
      <a class="anchor-link" href="#Carregamento do modelo enviado para o hub"><h2>Carregamento do modelo enviado para o hub</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No artigo <a href="https://arxiv.org/abs/2210.17323" target="_blank" rel="nofollow noreferrer">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> √© exposta a necessidade de criar um m√©todo de quantiza√ß√£o p√≥s-treinamento que n√£o degrade a qualidade do modelo. Neste post, vimos o m√©todo <a href="https://maximofn.com/llm-int8/">llm.int8()</a> que quantiza para INT8 alguns vetores das matrizes de pesos, desde que nenhum dos seus valores ultrapasse um valor limite, o que √© muito bom, mas n√£o quantiza todos os pesos do modelo. Neste artigo, prop√µem-se um m√©todo que quantiza todos os pesos do modelo para 4 e 3 bits, sem degradar a qualidade do modelo. Isso representa uma economia consider√°vel de mem√≥ria, n√£o apenas porque todos os pesos s√£o quantizados, mas tamb√©m porque isso √© feito em 4, 3 bits (e at√© 1 e 2 bits em certas condi√ß√µes), em vez de 8 bits.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Trabalhos nos quais se baseia">Trabalhos nos quais se baseia<a class="anchor-link" href="#Trabalhos nos quais se baseia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 45" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao por camadas">Quantiza√ß√£o por camadas<a class="anchor-link" href="#Quantizacao por camadas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 46" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por um lado, baseiam-se nos trabalhos <code>Nagel et al., 2020</code>; <code>Wang et al., 2020</code>; <code>Hubara et al., 2021</code> e <code>Frantar et al., 2022</code>, que prop√µem quantizar os pesos das camadas de uma rede neural para 4 e 3 bits, sem degradar a qualidade do modelo.</p>
      <p>Dado um conjunto de dados <code>m</code> proveniente de um dataset, a cada camada <code>l</code> s√£o fornecidos os dados e obt√©m-se a sa√≠da dos pesos <code>W</code> dessa camada. Portanto, o que se faz √© buscar pesos novos <code>≈¥</code> quantizados que minimizem o erro quadr√°tico em rela√ß√£o √† sa√≠da da camada de precis√£o total.</p>
      <p><code>argmin_WÃÇ||WX‚àí WÃÇX||^2</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os valores de <code>≈¥</code> s√£o estabelecidos antes de realizar o processo de quantiza√ß√£o e durante o processo, cada par√¢metro de <code>≈¥</code> pode mudar de valor independentemente sem depender do valor dos demais par√¢metros de <code>≈¥</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao otima do cerebro (OBC)">Quantiza√ß√£o √≥tima do c√©rebro (OBC)<a class="anchor-link" href="#Quantizacao otima do cerebro (OBC)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 47" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No trabalho de <code>OBQ</code> de <code>Frantar et al., 2022</code> otimizam o processo de quantiza√ß√£o por camadas anterior, tornando-o at√© 3 vezes mais r√°pido. Isso ajuda com os modelos grandes, pois quantizar um modelo grande pode levar muito tempo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O m√©todo <code>OBQ</code> √© uma abordagem para resolver o problema de quantiza√ß√£o em camadas em modelos de linguagem. <code>OBQ</code> parte da ideia de que o erro quadr√°tico pode ser decomposto na soma de erros individuais para cada linha da matriz de pesos. Em seguida, o m√©todo quantiza cada peso de forma independente, atualizando sempre os pesos n√£o quantizados para compensar o erro incorrido pela quantiza√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O m√©todo √© capaz de quantificar modelos de tamanho m√©dio em tempos razo√°veis, mas como √© um algoritmo de complexidade c√∫bica, torna-se extremamente custoso aplic√°-lo a modelos com bilh√µes de par√¢metros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Algoritmo de GPTQ">Algoritmo de GPTQ<a class="anchor-link" href="#Algoritmo de GPTQ"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 48" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Passo 1: Informacao em ordem arbitraria">Passo 1: Informa√ß√£o em ordem arbitr√°ria<a class="anchor-link" href="#Passo 1: Informacao em ordem arbitraria"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 49" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Em <code>OBQ</code> buscava-se a linha de pesos que criasse o menor erro quadr√°tico m√©dio para quantizar, mas percebeu-se que ao faz√™-lo de maneira aleat√≥ria n√£o aumentava muito o erro quadr√°tico m√©dio final. Por isso, em vez de buscar a linha que minimiza o erro quadr√°tico m√©dio, o que criava uma complexidade c√∫bica no algoritmo, sempre se faz na mesma ordem. Gra√ßas a isso, reduz-se muito o tempo de execu√ß√£o do algoritmo de quantiza√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Passo 2: Atualizacoes em lote preguicosas">Passo 2: Atualiza√ß√µes em lote pregui√ßosas<a class="anchor-link" href="#Passo 2: Atualizacoes em lote preguicosas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 50" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ao fazer a atualiza√ß√£o dos pesos linha a linha, isso causa um processo lento e n√£o aproveita totalmente o hardware.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Passo 3: Reformulacao de Cholesky">Passo 3: Reformula√ß√£o de Cholesky<a class="anchor-link" href="#Passo 3: Reformulacao de Cholesky"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 51" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O problema de fazer as atualiza√ß√µes em lotes √© que, devido √† grande escala dos modelos, podem ocorrer erros num√©ricos que afetam a precis√£o do algoritmo. Especificamente, podem ser obtidas matrizes indefinidas, o que faz com que o algoritmo atualize os pesos restantes em dire√ß√µes incorretas, resultando em uma quantiza√ß√£o muito ruim.</p>
      <p>Para resolver isso, os autores do artigo prop√µem usar uma reformula√ß√£o de Cholesky, que √© um m√©todo numericamente mais est√°vel.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resultados do GPTQ">Resultados do GPTQ<a class="anchor-link" href="#Resultados do GPTQ"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 52" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A seguir est√£o duas gr√°ficas com a medida da perplexidade (perplexity) no dataset <code>WikiText2</code> para todos os tamanhos dos modelos OPT e BLOOM. Pode-se ver que com a t√©cnica de quantiza√ß√£o RTN, a perplexidade em alguns tamanhos aumenta muito, enquanto com GPTQ mant√©m-se similar √† obtida com o modelo em FP16.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPTQ-figure1.webp" alt="GPTQ-figure1">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A seguir est√£o mostradas outras gr√°ficas, mas com a medida do accuracy no dataset <code>LAMBADA</code>. Ocorre o mesmo, enquanto GPTQ mant√©m-se semelhante ao obtido com FP16, outros m√©todos de quantiza√ß√£o degradam muito a qualidade do modelo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPTQ-figure3.webp" alt="GPTQ-figure3">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Quantizacao extrema">Quantiza√ß√£o extrema<a class="anchor-link" href="#Quantizacao extrema"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 53" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Nos gr√°ficos anteriores foram mostrados os resultados da quantiza√ß√£o do modelo para 3 e 4 bits, mas podemos quantiz√°-los para 2 bits, e at√© mesmo para apenas 1 bit.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Modificando o tamanho dos batches ao utilizar o algoritmo, podemos obter bons resultados quantizando tanto o modelo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <table>
        <thead>
          <tr>
            <th>Modelo</th>
            <th>FP16</th>
            <th>g128</th>
            <th>g64</th>
            <th>g32</th>
            <th>3 bits</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>OPT-175B</td>
            <td>8,34</td>
            <td>9,58</td>
            <td>9,18</td>
            <td>8,94</td>
            <td>8,68</td>
          </tr>
          <tr>
            <td>BLOOM</td>
            <td>8,11</td>
            <td>9,55</td>
            <td>9,17</td>
            <td>8,83</td>
            <td>8,64</td>
          </tr>
        </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Na tabela anterior, pode-se ver o resultado da perplexidade no conjunto de dados <code>WikiText2</code> para os modelos <code>OPT-175B</code> e <code>BLOOM</code> quantizados a 3 bits. Pode-se observar que √† medida que se usam lotes menores, a perplexidade diminui, o que significa que a qualidade do modelo quantizado √© melhor. No entanto, isso tem o problema de que o algoritmo leva mais tempo para ser executado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Desquantizacao dinamica na inferencia">Desquantiza√ß√£o din√¢mica na infer√™ncia<a class="anchor-link" href="#Desquantizacao dinamica na inferencia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 54" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Durante a infer√™ncia, algo chamado <code>descuantifica√ß√£o din√¢mica</code> (<code>dynamic dequantization</code>) √© realizada para permitir a infer√™ncia. Cada camada √© descuantificada √† medida que passa por elas.</p>
      <p>Para isso, eles desenvolveram um kernel que desquantiza as matrizes e realiza os produtos matriciais. Embora a desquantiza√ß√£o consuma mais c√°lculos, o kernel precisa acessar muito menos mem√≥ria, o que gera acelera√ß√µes significativas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A infer√™ncia √© realizada em FP16, desquantizando os pesos √† medida que se passa pelas camadas e a fun√ß√£o de ativa√ß√£o de cada camada tamb√©m √© realizada em FP16. Embora isso fa√ßa com que seja necess√°rio realizar mais c√°lculos, pois √© preciso desquantizar, esses c√°lculos fazem com que o processo total seja mais r√°pido, porque menos dados precisam ser trazidos da mem√≥ria. √â necess√°rio trazer da mem√≥ria os pesos em menos bits, de forma que, no final, em matrizes com muitos par√¢metros, isso resulta em uma economia significativa de dados. O gargalo normalmente est√° em trazer os dados da mem√≥ria, portanto, mesmo que seja necess√°rio realizar mais c√°lculos, a infer√™ncia acaba sendo mais r√°pida.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Velocidade de inferencia">Velocidade de infer√™ncia<a class="anchor-link" href="#Velocidade de inferencia"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 55" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os autores do paper realizaram um teste quantizando o modelo BLOOM-175B para 3 bits, o que ocupava cerca de 63 GB de mem√≥ria VRAM, incluindo os embeddings e a camada de sa√≠da que permanecem em FP16. Al√©m disso, manter a janela de contexto de 2048 tokens consome cerca de 9 GB de mem√≥ria, o que totaliza aproximadamente 72 GB de mem√≥ria VRAM. Eles quantizaram para 3 bits e n√£o para 4 para poder realizar este experimento e fazer o modelo caber em uma √∫nica GPU Nvidia A100 com 80 GB de mem√≥ria VRAM.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para compara√ß√£o, a infer√™ncia normal em FP16 requer cerca de 350 GB de mem√≥ria VRAM, o que equivale a 5 GPUs Nvidia A100 com 80 GB de mem√≥ria VRAM. E a infer√™ncia quantizando para 8 bits usando <a href="https://maximofn.com/llm-int8/">llm.int8()</a> requer 3 dessas GPUs.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A seguir, est√° apresentada uma tabela com a infer√™ncia do modelo em FP16 e quantizado para 3 bits em GPUs Nvidia A100 com 80 GB de mem√≥ria VRAM e Nvidia A6000 com 48 GB de mem√≥ria VRAM.</p>
      <table>
        <thead>
          <tr>
            <th>GPU (VRAM)</th>
            <th>tempo m√©dio por token em FP16 (ms)</th>
            <th>tempo m√©dio por token em 3 bits (ms)</th>
            <th>Acelera√ß√£o</th>
            <th>Redu√ß√£o de GPUs necess√°rias</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>A6000 (48GB)</td>
            <td>589</td>
            <td>130</td>
            <td>√ó4,53</td>
            <td>8‚Üí 2</td>
          </tr>
          <tr>
            <td>A100 (80GB)</td>
            <td>230</td>
            <td>71</td>
            <td>√ó3,24</td>
            <td>5‚Üí 1</td>
          </tr>
        </tbody>
      </table>
      <p>Por exemplo, utilizando os kernels, o modelo OPT-175B de 3 bits √© executado em uma √∫nica A100 (em vez de 5) e √© aproximadamente 3,25 vezes mais r√°pido que a vers√£o FP16 em termos de tempo m√©dio por token.</p>
      <p>A GPU NVIDIA A6000 tem uma largura de banda de mem√≥ria muito menor, portanto, esta estrat√©gia √© ainda mais eficaz: executar o modelo OPT-175B de 3 bits em 2 GPUs A6000 (em vez de 8) √© aproximadamente 4,53 vezes mais r√°pido que a vers√£o FP16.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Bibliotecas">Bibliotecas<a class="anchor-link" href="#Bibliotecas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 56" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os autores do paper implementaram a biblioteca <a href="https://github.com/IST-DASLab/gptq" target="_blank" rel="nofollow noreferrer">GPTQ</a>. Outras bibliotecas foram criadas, como <a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">GPTQ-for-LLaMa</a>, <a href="https://github.com/turboderp/exllama">exllama</a> e <a href="https://github.com/ggerganov/llama.cpp/">llama.cpp</a>. No entanto, essas bibliotecas se concentram apenas na arquitetura llama, por isso a biblioteca <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a> foi a que ganhou mais popularidade porque possui uma cobertura mais ampla de arquiteturas.</p>
      <p>Por isso, a biblioteca <a href="https://github.com/AutoGPTQ/AutoGPTQ" target="_blank" rel="nofollow noreferrer">AutoGPTQ</a> foi integrada por meio de uma API dentro da biblioteca <a href="https://maximofn.com/hugging-face-transformers/">transformers</a>. Para poder us√°-la, √© necess√°rio instal√°-la conforme indicado na se√ß√£o <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation">Installation</a> do seu reposit√≥rio e ter a biblioteca <a href="https://maximofn.com/hugging-face-optimun/">optimun</a> instalada.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al√©m de fazer o que indicam na se√ß√£o <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation" target="_blank" rel="nofollow noreferrer">Installation</a> do seu reposit√≥rio, tamb√©m √© recomend√°vel fazer o seguinte:</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-bash">git clone https://github.com/PanQiWei/AutoGPTQ<br>cd AutoGPTQ<br>pip install .</code></pre></div>
            </section>
      <p>Para que se instalem os kernels de quantiza√ß√£o na GPU que os autores do paper desenvolveram.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Quantizacao de um modelo">Quantiza√ß√£o de um modelo<a class="anchor-link" href="#Quantizacao de um modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 57" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como quantizar um modelo com a biblioteca <a href="https://maximofn.com/hugging-face-optimun/">optimun</a> e a API de <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Inferencia do modelo nao quantizado">Infer√™ncia do modelo n√£o quantizado<a class="anchor-link" href="#Inferencia do modelo nao quantizado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 58" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantizar o modelo <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" target="_blank" rel="nofollow noreferrer">meta-llama/Meta-Llama-3-8B-Instruct</a> que, como seu nome indica, √© um modelo de 8B de par√¢metros, portanto em FP16 precisar√≠amos de 16 GB de mem√≥ria VRAM. Primeiro executamos o modelo para ver a mem√≥ria que ele ocupa e a sa√≠da que gera</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como precisamos pedir permiss√£o √† Meta para usar esse modelo, fazemos login no Hugging Face para poder baixar o tokenizador e o modelo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="w"> </span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos o tokenizador e o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver a mem√≥ria que ocupa em FP16</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 14.96 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que ocupa quase 15 GB, mais ou menos os 16 GB que hav√≠amos dito que deveria ocupar, mas por que essa diferen√ßa? Provavelmente esse modelo n√£o tem exatamente 8B de par√¢metros, mas sim um pouco menos, mas na hora de indicar o n√∫mero de par√¢metros arredonda-se para 8B.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos uma infer√™ncia para ver como ele faz e o tempo que leva</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer at a startup in the Bay Area. I am passionate about building AI systems that can help humans make better decisions and improve their lives.',
          'I have a background in computer science and mathematics, and I have been working with machine learning for several years. I',
          'Inference time: 4.14 s',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao do modelo para 4 bits">Quantiza√ß√£o do modelo para 4 bits<a class="anchor-link" href="#Quantizacao do modelo para 4 bits"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 59" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantiz√°-lo para 4 bits. Reinicio o notebook para n√£o ter problemas de mem√≥ria, ent√£o precisamos fazer login novamente no Hugging Face.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="w"> </span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro crio o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos a configura√ß√£o de quantiza√ß√£o. Como dissemos, este algoritmo calcula o erro dos pesos quantizados em rela√ß√£o aos originais com base nas entradas de um conjunto de dados, portanto, na configura√ß√£o, precisamos especificar com qual conjunto de dados queremos quantizar o modelo.</p>
      <p>Os dispon√≠veis por padr√£o s√£o <code>wikitext2</code>, <code>c4</code>, <code>c4-new</code>, <code>ptb</code> e <code>ptb-new</code>.</p>
      <p>Tamb√©m podemos criar n√≥s um dataset a partir de uma lista de strings</p>
      <div class='highlight'><pre><code class="language-python">dataset = ["o auto-gptq √© uma biblioteca de quantiza√ß√£o de modelos f√°cil de usar com APIs amig√°veis ao usu√°rio, baseada no algoritmo GPTQ."]</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al√©m disso, temos que informar o n√∫mero de bits do modelo quantizado por meio do par√¢metro <code>bits</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTQConfig</span>',
      '<span class="w"> </span>',
      '<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Quantizamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>',
      '<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing model.layers blocks : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|32/32 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantization time: 1932.09 s = 32.20 min',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como o processo de quantiza√ß√£o calcula o menor erro entre os pesos quantizados e os originais ao passar entradas por cada camada, o processo de quantiza√ß√£o demora. Neste caso, levou em m√©dia uma hora.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver a mem√≥ria que ocupa agora</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_4bits_memory</span> <span class="o">=</span> <span class="n">model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 5.34 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aqui podemos ver um benef√≠cio da quantiza√ß√£o. Enquanto o modelo original ocupava cerca de 15 GB de VRAM, agora o modelo quantizado ocupa cerca de 5 GB, quase um ter√ßo do tamanho original.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e vemos o tempo que leva</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I am passionate about developing innovative solutions that can positively impact society. I am excited to be a part of this community and to learn from and contribute to the discussions here. I am particularly',
          'Inference time: 2.34 s',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>O modelo n√£o quantizado levou 4,14 segundos, enquanto agora quantizado para 4 bits levou 2,34 segundos e ainda gerou o texto corretamente. Conseguimos reduzir a infer√™ncia quase pela metade.</p>
      <p>Como o tamanho do modelo quantizado √© quase um ter√ßo do modelo em FP16, poder√≠amos pensar que a velocidade de infer√™ncia deveria ser cerca de tr√™s vezes mais r√°pida com o modelo quantizado. Mas √© preciso lembrar que em cada camada os pesos s√£o desquantizados e os c√°lculos s√£o realizados em FP16, por isso conseguimos reduzir o tempo de infer√™ncia pela metade e n√£o por um ter√ßo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora salvamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">&quot;./model_4bits/&quot;</span>',
      '<span class="n">model_4bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(&#x27;./model_4bits/tokenizer_config.json&#x27;,',
          '&#x27;./model_4bits/special_tokens_map.json&#x27;,',
          '&#x27;./model_4bits/tokenizer.json&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E o enviamos para o hub</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-4bits&quot;</span>',
      '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">model_4bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.17/5.17k [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/44cfdcad78db260122943d3f57858c1b840bda17&#x27;, commit_message=&#x27;AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;44cfdcad78db260122943d3f57858c1b840bda17&#x27;, pr_url=None, pr_revision=None, pr_num=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m enviamos o tokenizador. Embora n√£o tenhamos alterado o tokenizador, o enviamos porque, se uma pessoa baixar nosso modelo do hub, ela pode n√£o saber qual tokenizador usamos, ent√£o provavelmente querr√° baixar o modelo e o tokenizador juntos. Podemos indicar na model card qual tokenizador usamos para que ela possa baix√°-lo, mas √© mais prov√°vel que a pessoa n√£o leia a model card, tente baixar o tokenizador, obtenha um erro e n√£o saiba o que fazer. Ent√£o enviamos para evitar esse problema.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-4bits&quot;</span>',
      '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/5.17k [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4&#x27;, commit_message=&#x27;Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;75600041ca6e38b5f1fb912ad1803b66656faae4&#x27;, pr_url=None, pr_revision=None, pr_num=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao do modelo para 3 bits">Quantiza√ß√£o do modelo para 3 bits<a class="anchor-link" href="#Quantizacao do modelo para 3 bits"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 60" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantiz√°-lo para 3 bits. Reinicio o notebook para n√£o ter problemas de mem√≥ria e volto a fazer login no Hugging Face</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="w"> </span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro crio o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos a configura√ß√£o de quantiza√ß√£o, agora indicamos que queremos quantizar para 3 bits</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTQConfig</span>',
      '<span class="w"> </span>',
      '<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Quantizamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">model_3bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>',
      '<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing model.layers blocks : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|32/32 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantization time: 1912.69 s = 31.88 min',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Assim como antes, levou uma m√©dia de meia hora</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver a mem√≥ria que ocupa agora</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_3bits_memory</span> <span class="o">=</span> <span class="n">model_3bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_3bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 4.52 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A mem√≥ria que o modelo ocupa em 3 bits tamb√©m √© quase de 5 GB. O modelo em 4 bits ocupava 5,34 GB, enquanto agora em 3 bits ocupa 4,52 GB, portanto conseguimos reduzir um pouco mais o tamanho do modelo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e vemos o tempo que leva</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_3bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_3bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer at Google. I am excited to be here today to talk about my work in the field of Machine Learning and to share some of the insights I have gained through my experiences.',
          'I am a Machine Learning Engineer at Google, and I am excited to be',
          'Inference time: 2.89 s',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Embora a sa√≠da de 3 bits seja boa, agora o tempo de infer√™ncia foi de 2,89 segundos, enquanto em 4 bits foi de 2,34 segundos. Seriam necess√°rias mais testes para ver se sempre √© mais r√°pido em 4 bits, ou pode ser que a diferen√ßa seja t√£o pequena que √†s vezes a infer√™ncia em 3 bits seja mais r√°pida e outras vezes a infer√™ncia em 4 bits.</p>
      <p>Al√©m disso, embora a sa√≠da fa√ßa sentido, come√ßa a se tornar repetitiva.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Guardamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">&quot;./model_3bits/&quot;</span>',
      '<span class="n">model_3bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(&#x27;./model_3bits/tokenizer_config.json&#x27;,',
          '&#x27;./model_3bits/special_tokens_map.json&#x27;,',
          '&#x27;./model_3bits/tokenizer.json&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E o enviamos para o Hub</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-3bits&quot;</span>',
      '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">model_3bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.85/4.85G [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-3bits/commit/422fd94a031234c10224ddbe09c0e029a5e9c01f&#x27;, commit_message=&#x27;AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 3bits, gr128, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;422fd94a031234c10224ddbe09c0e029a5e9c01f&#x27;, pr_url=None, pr_revision=None, pr_num=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m subimos o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-3bits&quot;</span>',
      '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/5.17k [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4&#x27;, commit_message=&#x27;Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;75600041ca6e38b5f1fb912ad1803b66656faae4&#x27;, pr_url=None, pr_revision=None, pr_num=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao do modelo para 2 bits">Quantiza√ß√£o do modelo para 2 bits<a class="anchor-link" href="#Quantizacao do modelo para 2 bits"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 61" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantiz√°-lo para 2 bits. Reinicio o notebook para n√£o ter problemas de mem√≥ria e fa√ßo login novamente no Hugging Face</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="w"> </span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro crio o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos a configura√ß√£o de quantiza√ß√£o. Agora dizemos que queremos quantizar para 2 bits. Al√©m disso, √© necess√°rio indicar quantos vetores da matriz de pesos s√£o quantizados de uma vez atrav√©s do par√¢metro <code>group_size</code>, antes por padr√£o tinha o valor 128 e n√£o o alteramos, mas agora ao quantizar para 2 bits, para ter menos erro, colocamos um valor menor. Se deixarmos em 128, o modelo quantizado funcionaria muito mal, nesse caso vou colocar um valor de 16.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTQConfig</span>',
      '<span class="w"> </span>',
      '<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">model_2bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>',
      '<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing model.layers blocks : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|32/32 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantization time: 1973.12 s = 32.89 min',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que tamb√©m levou cerca de meia hora</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver a mem√≥ria que ocupa agora</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_2bits_memory</span> <span class="o">=</span> <span class="n">model_2bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_2bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 4.50 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Enquanto quantizado em 4 bits ocupava 5,34 GB e em 3 bits ocupava 4,52 GB, agora quantizado em 2 bits ocupa 4,50 GB, conseguindo assim reduzir ainda mais o tamanho do modelo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e vemos o tempo que leva</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_2bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_2bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer.  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #',
          'Inference time: 2.92 s',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que j√° a sa√≠da n√£o √© boa, al√©m disso, o tempo de infer√™ncia √© de 2,92 segundos, mais ou menos o mesmo que com 3 e 4 bits</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Guardamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">&quot;./model_2bits/&quot;</span>',
      '<span class="n">model_2bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(&#x27;./model_2bits/tokenizer_config.json&#x27;,',
          '&#x27;./model_2bits/special_tokens_map.json&#x27;,',
          '&#x27;./model_2bits/tokenizer.json&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>O subimos para o hub</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-2bits&quot;</span>',
      '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">model_2bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.83/4.83G [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/13ede006ce0dbbd8aca54212e960eff98ea5ec63&#x27;, commit_message=&#x27;AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr16, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;13ede006ce0dbbd8aca54212e960eff98ea5ec63&#x27;, pr_url=None, pr_revision=None, pr_num=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao do modelo para 1 bit">Quantiza√ß√£o do modelo para 1 bit<a class="anchor-link" href="#Quantizacao do modelo para 1 bit"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 62" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos quantiz√°-lo para 1 bit. Reinicio o notebook para n√£o ter problemas de mem√≥ria e fa√ßo login novamente no Hugging Face</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="w"> </span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro crio o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos a configura√ß√£o de quantiza√ß√£o, agora dizemos que quantize apenas para 1 bit e al√©m disso use um <code>group_size</code> de 8</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTQConfig</span>',
      '<span class="w"> </span>',
      '<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">model_1bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>',
      '<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing model.layers blocks : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|32/32 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantizing layers inside the block: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Quantization time: 2030.38 s = 33.84 min',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que tamb√©m leva uma m√©dia de trinta minutos para quantizar</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver a mem√≥ria que ocupa agora</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_1bits_memory</span> <span class="o">=</span> <span class="n">model_1bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_1bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 5.42 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que neste caso ocupa at√© mesmo mais que quantizado a 2 bits, 4,52 GB.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e vemos o tempo que leva</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_1bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_1bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineerimerszuimersimerspinsimersimersingoingoimersurosimersimersimersoleningoimersingopinsimersbirpinsimersimersimersorgeingoimersiringimersimersimersimersimersimersimers„É≥„Éá„Ç£orge_REFERER ingestÁæäimersorgeimersimersendetingo–®–êhandsingo',
          'Inference time: 3.12 s',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que a sa√≠da √© muito ruim e al√©m disso demora mais do que quando quantizamos para 2 bits</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Salvamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">&quot;./model_1bits/&quot;</span>',
      '<span class="n">model_1bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(&#x27;./model_1bits/tokenizer_config.json&#x27;,',
          '&#x27;./model_1bits/special_tokens_map.json&#x27;,',
          '&#x27;./model_1bits/tokenizer.json&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>O subimos para o hub</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-1bits&quot;</span>',
      '<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">model_1bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'README.md: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/5.17k [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Upload 2 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0/2 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'model-00002-of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/1.05G [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'model-00001-of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 0.00/4.76G [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/e59ccffc03247e7dcc418f98b482cc02dc7a168d&#x27;, commit_message=&#x27;AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr8, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;e59ccffc03247e7dcc418f98b482cc02dc7a168d&#x27;, pr_url=None, pr_revision=None, pr_num=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resumo da quantizacao">Resumo da quantiza√ß√£o<a class="anchor-link" href="#Resumo da quantizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 63" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a comparar a quantiza√ß√£o de 4, 3, 2 e 1 bit</p>
      <table>
        <thead>
          <tr>
            <th>Bits</th>
            <th>Tempo de quantiza√ß√£o (min)</th>
            <th>Mem√≥ria (GB)</th>
            <th>Tempo de infer√™ncia (s)</th>
            <th>Qualidade da sa√≠da</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>FP16</td>
            <td>0</td>
            <td>14,96</td>
            <td>4,14</td>
            <td>Boa</td>
          </tr>
          <tr>
            <td>4</td>
            <td>32,20</td>
            <td>5,34</td>
            <td>2,34</td>
            <td>Boa</td>
          </tr>
          <tr>
            <td>3</td>
            <td>31,88</td>
            <td>4,52</td>
            <td>2,89</td>
            <td>Boa</td>
          </tr>
          <tr>
            <td>2</td>
            <td>32,89</td>
            <td>4,50</td>
            <td>2,92</td>
            <td>Ruim</td>
          </tr>
          <tr>
            <td>1</td>
            <td>33,84</td>
            <td>5,42</td>
            <td>3,12</td>
            <td>Ruim</td>
          </tr>
        </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vendo esta tabela vemos que n√£o faz sentido, neste exemplo, quantizar a menos de 4 bits.</p>
      <p>Quantizar para 1 e 2 bits claramente n√£o faz sentido porque a qualidade da sa√≠da √© ruim.</p>
      <p>Mas embora a sa√≠da ao quantizar para 3 bits seja boa, come√ßa a ser repetitiva, pelo que a longo prazo, provavelmente n√£o seria uma boa ideia usar esse modelo. Al√©m disso, nem o ganho de tempo de quantiza√ß√£o, o ganho de VRAM nem o ganho de tempo de infer√™ncia √© significativo em compara√ß√£o com a quantiza√ß√£o para 4 bits.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Carregamento do modelo salvo">Carregamento do modelo salvo<a class="anchor-link" href="#Carregamento do modelo salvo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 64" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que comparamos a quantiza√ß√£o de modelos, vamos ver como seria para carregar o modelo de 4 bits que salvamos, j√° que, como vimos, √© a melhor op√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro carregamos o tokenizador que temos usado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;./model_4bits&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora carregamos o modelo que salvamos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">load_model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos a mem√≥ria que ocupa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">load_model_4bits_memory</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">load_model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 5.34 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que ocupa a mesma mem√≥ria que quando o quantizamos, o que √© l√≥gico.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e vemos o tempo que leva</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">load_model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '<span class="n">outputs</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I have been working with machine learning models for several years. I am excited to be a part of this community and to share my knowledge and experience with others. I am particularly interested in',
          'Inference time: 3.82 s',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que a infer√™ncia √© boa e levou 3,82 segundos, um pouco mais do que quando a quantizamos. Mas como j√° disse anteriormente, seria necess√°rio fazer este teste muitas vezes e tirar uma m√©dia.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Carregamento do modelo enviado para o hub">Carregamento do modelo enviado para o hub<a class="anchor-link" href="#Carregamento do modelo enviado para o hub"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 65" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora vamos ver como carregar o modelo de 4 bits que subimos ao Hub.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro carregamos o tokenizador que we subimos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;Maximofn/Llama-3-8B-Instruct-GPTQ-4bits&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora carregamos o modelo que salvamos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">load_model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos a mem√≥ria que ocupa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">load_model_4bits_memory</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">load_model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 5.34 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ocupa a mesma mem√≥ria</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos a infer√™ncia e vemos o tempo que leva</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">load_model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '<span class="n">outputs</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer with a passion for building innovative AI solutions. I have been working in the field of AI for over 5 years, and have gained extensive experience in developing and implementing machine learning models for various industries.',
          'In my free time, I enjoy reading books on',
          'Inference time: 3.81 s',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que a infer√™ncia tamb√©m √© boa e levou 3,81 segundos.</p>
      </section>







    </div>

  </section>

</PostLayout>
