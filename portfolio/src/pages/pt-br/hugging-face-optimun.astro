---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Optimun';
const end_url = 'hugging-face-optimun';
const description = 'Aten√ß√£o, modelos PyTorch lentos! üêå Optimun, a biblioteca Hugging Face, vem em seu socorro para acelerar seu treinamento e suas infer√™ncias. Com o Optimun, voc√™ pode esquecer os problemas de velocidade e aproveitar mais velocidade e efici√™ncia üïíÔ∏è. E o melhor de tudo √© que ela √© compat√≠vel com o PyTorch - v√° em frente, d√™ um impulso aos seus modelos com a Optimun! üíª';
const keywords = 'hugging face, optimun, pytorch, transformers, treinamento, infer√™ncia, velocidade, efici√™ncia';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_optimun.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=2388
    image_height=884
    image_extension=webp
    article_date=2024-06-01+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Instalacao"><h2>Instala√ß√£o</h2></a>
      <a class="anchor-link" href="#BeterTransformer"><h2>BeterTransformer</h2></a>
      <a class="anchor-link" href="#Inferencia com Automodel"><h3>Infer√™ncia com Automodel</h3></a>
      <a class="anchor-link" href="#Inferencia com Pipeline"><h3>Infer√™ncia com Pipeline</h3></a>
      <a class="anchor-link" href="#Treinamento"><h3>Treinamento</h3></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Optimun</code> √© uma extens√£o da biblioteca <a href="https://maximofn.com/hugging-face-transformers/">Transformers</a> que fornece um conjunto de ferramentas de otimiza√ß√£o de desempenho para treinar e inferir modelos, em hardware espec√≠fico, com a m√°xima efici√™ncia.</p>
      <p>O ecossistema de IA evolui rapidamente e a cada dia surge mais hardware especializado junto com suas pr√≥prias otimiza√ß√µes. Portanto, <code>Optimum</code> permite aos usu√°rios utilizar eficientemente qualquer deste HW com a mesma facilidade que <a href="https://maximofn.com/hugging-face-transformers/">Transformers</a>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>Optimun</code> permite a otimiza√ß√£o para as seguintes plataformas HW:</p>
      <ul>
        <li>Nvidia</li>
        <li>AMD</li>
        <li>Intel</li>
        <li>AWS</li>
        <li>TPU</li>
        <li>Havana</li>
        <li>FuriosaIA</li>
      </ul>
      <p>Al√©m disso, oferece acelera√ß√£o para as seguintes integra√ß√µes open source</p>
      <ul>
        <li>Tempo de execu√ß√£o do ONNX</li>
        <li>Exportadores: Exportar modelos Pytorch ou TensorFlow para diferentes formatos como ONNX ou TFLite</li>
        <li>BetterTransformer</li>
        <li>Torch FX</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Instalacao">Instala√ß√£o<a class="anchor-link" href="#Instalacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para instalar <code>Optimun</code> simplesmente execute:</p>
      <div class='highlight'><pre><code class="language-bash">pip install optimum</code></pre></div>
      <p>Mas se quiser instalar com suporte para todas as plataformas HW, pode ser feito assim</p>
      <table>
        <thead>
          <tr>
            <th>Acelerador</th>
            <th>Instala√ß√£o</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>ONNX Runtime</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]</code></td>
          </tr>
          <tr>
            <td>Intel Neural Compressor</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]</code></td>
          </tr>
          <tr>
            <td>OpenVINO</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[openvino]</code></td>
          </tr>
          <tr>
            <td>GPUs AMD Instinct e NPU Ryzen AI</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[amd]</code></td>
          </tr>
          <tr>
            <td>AWS Trainum & Inferentia</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[neuronx]</code></td>
          </tr>
          <tr>
            <td>Processador Habana Gaudi (HPU)</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[habana]</code></td>
          </tr>
          <tr>
            <td>FuriosaAI</td>
            <td><code>pip install --upgrade --upgrade-strategy eager optimum[furiosa]</code></td>
          </tr>
        </tbody>
      </table>
      <p>Os flags <code>--upgrade --upgrade-strategy eager</code> s√£o necess√°rios para garantir que os diferentes pacotes sejam atualizados para a vers√£o mais recente poss√≠vel.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como a maioria das pessoas usa o Pytorch em GPUs da Nvidia, e principalmente, como eu tenho uma Nvidia, este post vai falar apenas sobre o uso do <code>Optimun</code> com GPUs da Nvidia e Pytorch.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="BeterTransformer">BeterTransformer<a class="anchor-link" href="#BeterTransformer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>BetterTransformer √© uma otimiza√ß√£o nativa do PyTorch para obter um aumento de velocidade de x1,25 a x4 na infer√™ncia de modelos baseados em Transformer</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>BetterTransformer √© uma API que permite aproveitar as caracter√≠sticas de hardware modernas para acelerar o treinamento e a infer√™ncia de modelos de transformers no PyTorch, utilizando implementa√ß√µes de aten√ß√£o mais eficientes e <code>fast path</code> da vers√£o nativa de <code>nn.TransformerEncoderLayer</code>.</p>
      <p>BetterTransformer usa dois tipos de acelera√ß√µes:</p>
      <ol>
        <li><code>Flash Attention</code>: Esta √© uma implementa√ß√£o da <code>attention</code> que utiliza <code>sparse</code> para reduzir a complexidade computacional. A aten√ß√£o √© uma das opera√ß√µes mais custosas nos modelos de transformers, e <code>Flash Attention</code> torna-a mais eficiente.</li>
        <li><code>Aten√ß√£o Eficiente em Mem√≥ria</code>: Esta √© outra implementa√ß√£o da aten√ß√£o que utiliza a fun√ß√£o <code>scaled_dot_product_attention</code> do PyTorch. Essa fun√ß√£o √© mais eficiente em termos de mem√≥ria do que a implementa√ß√£o padr√£o da aten√ß√£o no PyTorch.</li>
      </ol>
      <p>Al√©m disso, a vers√£o 2.0 do PyTorch inclui um operador de aten√ß√£o de produtos ponto escalado (SDPA) nativo como parte de <code>torch.nn.functional</code></p>
      <p><code>Optimun</code> fornece esta funcionalidade com a biblioteca <code>Transformers</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Inferencia com Automodel">Infer√™ncia com Automodel<a class="anchor-link" href="#Inferencia com Automodel"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro vamos a ver como seria a infer√™ncia normal com <code>Transformers</code> e <code>Automodel</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">sentence_output</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora veremos como isso seria otimizado com <code>BetterTransformer</code> e <code>Optimun</code></p>
      <p>O que temos que fazer √© converter o modelo usando o m√©todo <code>transform</code> de <code>BetterTransformer</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.bettertransformer</span><span class="w"> </span><span class="kn">import</span> <span class="n">BetterTransformer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Convert the model to a BetterTransformer model</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="w"> </span>',
      '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">sentence_output</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Inferencia com Pipeline">Infer√™ncia com Pipeline<a class="anchor-link" href="#Inferencia com Pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Assim como antes, primeiro vemos como seria a infer√™ncia normal com <code>Transformers</code> e <code>Pipeline</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>',
      '<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;I am a student at [MASK] University.&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;score&#x27;: 0.05116177722811699,',
          '&#x20;&#x20;&#x27;token&#x27;: 8422,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;stanford&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at stanford university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.04033993184566498,',
          '&#x20;&#x20;&#x27;token&#x27;: 5765,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;harvard&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at harvard university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.03990468755364418,',
          '&#x20;&#x20;&#x27;token&#x27;: 7996,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;yale&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at yale university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.0361952930688858,',
          '&#x20;&#x20;&#x27;token&#x27;: 10921,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;cornell&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at cornell university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.03303057327866554,',
          '&#x20;&#x20;&#x27;token&#x27;: 9173,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;princeton&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at princeton university.&#x27;&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora vemos como otimiz√°-lo, para isso usamos <code>pipeline</code> de <code>Optimun</code>, em vez do de <code>Transformers</code>. Al√©m disso, √© necess√°rio indicar que queremos usar <code>bettertransformer</code> como acelerador.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optimum.pipelines</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Use the BetterTransformer pipeline</span>',
      '<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;bettertransformer&quot;</span><span class="p">)</span>',
      '<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;I am a student at [MASK] University.&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.',
          '/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)',
          '&#x20;&#x20;hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;score&#x27;: 0.05116180703043938,',
          '&#x20;&#x20;&#x27;token&#x27;: 8422,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;stanford&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at stanford university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.040340032428503036,',
          '&#x20;&#x20;&#x27;token&#x27;: 5765,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;harvard&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at harvard university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.039904672652482986,',
          '&#x20;&#x20;&#x27;token&#x27;: 7996,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;yale&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at yale university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.036195311695337296,',
          '&#x20;&#x20;&#x27;token&#x27;: 10921,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;cornell&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at cornell university.&#x27;&#x7D;,',
          '&#x7B;&#x27;score&#x27;: 0.03303062543272972,',
          '&#x20;&#x20;&#x27;token&#x27;: 9173,',
          '&#x20;&#x20;&#x27;token_str&#x27;: &#x27;princeton&#x27;,',
          '&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at princeton university.&#x27;&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para o treinamento com <code>Optimun</code> fazemos o mesmo que com a infer√™ncia com Automodel, convertemos o modelo por meio do m√©todo <code>transform</code> de <code>BeterTransformer</code>.</p>
      <p>Quando terminamos o treinamento, voltamos a converter o modelo usando o m√©todo <code>reverse</code> de <code>BetterTransformer</code> para recuperar o modelo original e assim poder salv√°-lo e envi√°-lo para o hub da Hugging Face.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.bettertransformer</span><span class="w"> </span><span class="kn">import</span> <span class="n">BetterTransformer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Convert the model to a BetterTransformer model</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="c1">##############################################################################</span>',
      '<span class="c1"># do your training here</span>',
      '<span class="c1">##############################################################################</span>',
      '<span class="w"> </span>',
      '<span class="c1"># Convert the model back to a Hugging Face model</span>',
      '<span class="n">model_hf</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">model_hf</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;fine_tuned_model&quot;</span><span class="p">)</span>',
      '<span class="n">model_hf</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;fine_tuned_model&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>































    </div>

  </section>

</PostLayout>
