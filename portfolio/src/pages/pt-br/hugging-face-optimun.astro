---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Optimun';
const end_url = 'hugging-face-optimun';
const description = 'Aten√ß√£o, modelos PyTorch lentos! üêå Optimun, a biblioteca Hugging Face, vem em seu socorro para acelerar seu treinamento e suas infer√™ncias. Com o Optimun, voc√™ pode esquecer os problemas de velocidade e aproveitar mais velocidade e efici√™ncia üïíÔ∏è. E o melhor de tudo √© que ela √© compat√≠vel com o PyTorch - v√° em frente, d√™ um impulso aos seus modelos com a Optimun! üíª';
const keywords = 'hugging face, optimun, pytorch, transformers, treinamento, infer√™ncia, velocidade, efici√™ncia';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_optimun.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=2388
    image_height=884
    image_extension=webp
    article_date=2024-06-01+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Instala%C3%A7%C3%A3o"><h2>Instala√ß√£o</h2></a>
      <a class="anchor-link" href="#BeterTransformer"><h2>BeterTransformer</h2></a>
      <a class="anchor-link" href="#Infer%C3%AAncia-com-modelo-autom%C3%A1tico"><h3>Infer√™ncia com modelo autom√°tico</h3></a>
      <a class="anchor-link" href="#Infer%C3%AAncias-com-o-pipeline"><h3>Infer√™ncias com o pipeline</h3></a>
      <a class="anchor-link" href="#Treinamento"><h3>Treinamento</h3></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Otimiza%C3%A7%C3%A3o-do-rosto-de-abra%C3%A7os">Otimiza√ß√£o do rosto de abra√ßos<a class="anchor-link" href="#Otimiza%C3%A7%C3%A3o-do-rosto-de-abra%C3%A7os"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Optimum √© uma extens√£o da biblioteca [Transformers] (<a href="https://www.maximofn.com/hugging-face-transformers/">https://www.maximofn.com/hugging-face-transformers/</a>) que fornece um conjunto de ferramentas de otimiza√ß√£o de desempenho para modelos de treinamento e infer√™ncia, em hardware espec√≠fico, com efici√™ncia m√°xima.</p>
      <p>O ecossistema de IA est√° evoluindo rapidamente e cada vez mais hardware especializado est√° surgindo todos os dias, juntamente com suas pr√≥prias otimiza√ß√µes. Portanto, o <code>Optimum</code> permite que os usu√°rios utilizem eficientemente qualquer um desses HW com a mesma facilidade dos [Transformers] (<a href="https://www.maximofn.com/hugging-face-transformers/">https://www.maximofn.com/hugging-face-transformers/</a>).</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
      <p>A otimiza√ß√£o para as seguintes plataformas HW √© poss√≠vel com o <code>Optimun</code>:</p>
      <ul>
      <li>Nvidia</li>
      <li>AMD</li>
      <li>Intel</li>
      <li>AWS</li>
      <li>TPU</li>
      <li>Havana</li>
      <li>FuriosaAI</li>
      </ul>
      <p>Al√©m disso, ele oferece acelera√ß√£o para as seguintes integra√ß√µes de c√≥digo aberto</p>
      <ul>
      <li>Tempo de execu√ß√£o do ONNX</li>
      <li>Exportadores: exportam dados do Pytorch ou do TensorFlow para diferentes formatos, como ONNX ou TFLite.</li>
      <li>Melhor transformador</li>
      <li>Tocha FX</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Instala%C3%A7%C3%A3o">Instala√ß√£o<a class="anchor-link" href="#Instala%C3%A7%C3%A3o"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para instalar o <code>Optimum</code>, basta executar:</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>optimum
      </pre></div>
      <p>Mas se quiser instal√°-lo com suporte para todas as plataformas HW, voc√™ pode fazer o seguinte</p>
      <table>
      <thead>
      <tr>
      <th>Accelerator</th>
      <th>Instala√ß√£o</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>ONNX Runtime</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]</code></td>
      </tr>
      <tr>
      <td>Intel Neural Compressor</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]</code></td>
      </tr>
      <tr>
      <td>OpenVINO</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[openvino]</code></td>
      </tr>
      <tr>
      <td>NVIDIA TensorRT-LLM</td>
      <td><code>docker run -it --gpus all --ipc host huggingface/optimum-nvidia</code></td>
      </tr>
      <tr>
      <td>AMD Instinct GPUs e Ryzen AI NPU</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[amd]</code></td>
      </tr>
      <tr>
      <td>AWS Trainum &amp; Inferentia</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[neuronx]</code></td>
      </tr>
      <tr>
      <td>Havana Gaudi Processor (HPU)</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum [habana]</code></td>
      </tr>
      <tr>
      <td>FuriosaAI</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[furiosa]</code></td>
      </tr>
      </tbody>
      </table>
      <p>Os sinalizadores <code>--upgrade --upgrade-strategy eager</code> s√£o necess√°rios para garantir que os diferentes pacotes sejam atualizados para a vers√£o mais recente poss√≠vel.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como a maioria das pessoas usa o Pytorch em GPUs Nvidia e, especialmente, como eu tenho uma Nvidia, esta postagem falar√° apenas sobre o uso do <code>Optimun</code> com GPUs Nvidia e Pytorch.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="BeterTransformer">BeterTransformer<a class="anchor-link" href="#BeterTransformer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O BetterTransformer √© uma otimiza√ß√£o nativa do PyTorch para aumentar a velocidade de 1,25 a 4 vezes na infer√™ncia de modelo baseada no Transformer.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>BetterTransformer √© uma API que permite que voc√™ aproveite os recursos modernos de hardware para acelerar o treinamento e a infer√™ncia de modelos de transformadores no PyTorch, usando implementa√ß√µes mais eficientes e com aten√ß√£o ao "caminho r√°pido" da vers√£o nativa <code>nn.TransformerEncoderLayer</code> do <code>nn.TransformerEncoderLayer</code>.</p>
      <p>O BetterTransformer usa dois tipos de acelera√ß√µes:</p>
      <ol>
      <li><code>Flash Attention</code>: √© uma implementa√ß√£o do <code>attention</code> que usa o <code>sparse</code> para reduzir a complexidade computacional. A aten√ß√£o √© uma das opera√ß√µes mais caras nos modelos de transformadores, e o <code>Flash Attention</code> a torna mais eficiente.</li>
      <li><code>Memory-Efficient Attention</code>: essa √© outra implementa√ß√£o de aten√ß√£o que usa a fun√ß√£o <code>scaled_dot_product_attention</code> do PyTorch. Essa fun√ß√£o √© mais eficiente em termos de mem√≥ria do que a implementa√ß√£o padr√£o de aten√ß√£o do PyTorch.</li>
      </ol>
      <p>Al√©m disso, a vers√£o 2.0 do PyTorch inclui um operador de aten√ß√£o de produto de ponto escalonado (SDPA) nativo como parte do <code>torch.nn.functional</code>.</p>
      <p>O Optimmun fornece essa funcionalidade com a biblioteca <code>Transformers</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Infer%C3%AAncia-com-modelo-autom%C3%A1tico">Infer√™ncia com modelo autom√°tico<a class="anchor-link" href="#Infer%C3%AAncia-com-modelo-autom%C3%A1tico"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, vamos ver como seria a infer√™ncia normal com <code>Transformers</code> e <code>Automodel</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      
      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>
      
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[1]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, veremos como ele seria otimizado com o <code>BetterTransformer</code> e o <code>Optimun</code>.</p>
      <p>O que temos que fazer √© converter o modelo usando o m√©todo <code>transform</code> do <code>BeterTransformer</code>.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
      <span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>
      
      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
      <span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>
      
      <span class="c1"># Convert the model to a BetterTransformer model</span>
      <span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      
      <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
      
      <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
      <span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
      
      <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">sentence_output</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
      Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[2]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Infer%C3%AAncias-com-o-pipeline">Infer√™ncias com o pipeline<a class="anchor-link" href="#Infer%C3%AAncias-com-o-pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como antes, primeiro vemos como seria a infer√™ncia normal com <code>Transformers</code> e <code>Pipeline</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>',
          '',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
          '<span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>',
          '',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>',
          '',
          '<span class="c1"># Convert the model to a BetterTransformer model</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '',
          '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
          '',
          '<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>',
          '<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
          '',
          '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
          '<span class="n">sentence_output</span>',
          '</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>',
          '',
          '<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">)</span>',
          '<span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          '[{\'score\': 0.05116177722811699,',
          '  \'token\': 8422,',
          '  \'token_str\': \'stanford\',',
          '  \'sequence\': \'i am a student at stanford university.\'},',
          ' {\'score\': 0.04033993184566498,',
          '  \'token\': 5765,',
          '  \'token_str\': \'harvard\',',
          '  \'sequence\': \'i am a student at harvard university.\'},',
          ' {\'score\': 0.03990468755364418,',
          '  \'token\': 7996,',
          '  \'token_str\': \'yale\',',
          '  \'sequence\': \'i am a student at yale university.\'},',
          ' {\'score\': 0.0361952930688858,',
          '  \'token\': 10921,',
          '  \'token_str\': \'cornell\',',
          '  \'sequence\': \'i am a student at cornell university.\'},',
          ' {\'score\': 0.03303057327866554,',
          '  \'token\': 9173,',
          '  \'token_str\': \'princeton\',',
          '  \'sequence\': \'i am a student at princeton university.\'}]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora veremos como otimiz√°-lo, de modo que usaremos <code>pipeline</code> do <code>Optimun</code>, em vez de <code>Transformers</code>. Tamb√©m devemos indicar que queremos usar o <code>bettertransformer</code> como acelerador.</p>
      </section>
      
      <section class="section-block-code-cell-">
      <div class="input-code">
      <div class="highlight hl-ipython3">
<pre><span></span><span class="kn">from</span> <span class="nn">optimum.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>
      
      <span class="c1"># Use the BetterTransformer pipeline</span>
      <span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">"bettertransformer"</span><span class="p">)</span>
      <span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>
      </pre></div>
      </div>
      <div class="output-wrapper">
      <div class="output-area">
      <div class="prompt"></div>
      <div class="output-subarea-output-stream-output-stderr-output-text">
      <pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
      /home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)
        hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)
      </pre>
      </div>
      </div>
      <div class="output-area">
      <div class="prompt-output-prompt">Out[4]:</div>
      <div class="output-text-output-subareaoutput_execute_result">
      <pre>[{opening_brace}'score': 0.05116180703043938,
        'token': 8422,
        'token_str': 'stanford',
        'sequence': 'i am a student at stanford university.'{closing_brace},
       {opening_brace}'score': 0.040340032428503036,
        'token': 5765,
        'token_str': 'harvard',
        'sequence': 'i am a student at harvard university.'{closing_brace},
       {opening_brace}'score': 0.039904672652482986,
        'token': 7996,
        'token_str': 'yale',
        'sequence': 'i am a student at yale university.'{closing_brace},
       {opening_brace}'score': 0.036195311695337296,
        'token': 10921,
        'token_str': 'cornell',
        'sequence': 'i am a student at cornell university.'{closing_brace},
       {opening_brace}'score': 0.03303062543272972,
        'token': 9173,
        'token_str': 'princeton',
        'sequence': 'i am a student at princeton university.'{closing_brace}]</pre>
      </div>
      </div>
      </div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para o treinamento com <code>Optimun</code>, fazemos o mesmo que com a infer√™ncia Automodel, convertemos o modelo usando o m√©todo <code>transform</code> do <code>BeterTransformer</code>.</p>
      <p>Quando terminamos o treinamento, convertemos o modelo novamente usando o m√©todo <code>reverse</code> do <code>BeterTransformer</code> para obter o modelo original de volta, para que possamos salv√°-lo e carreg√°-lo no hub Hugging Face.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">optimum.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>',
      '      ',
      '      <span class="c1"># Use the BetterTransformer pipeline</span>',
      '      <span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">"bettertransformer"</span><span class="p">)</span>',
      '      <span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>',
      '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '      <span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>',
      '      ',
      '      <span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>',
      '      <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '      <span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>',
      '      ',
      '      <span class="c1"># Convert the model to a BetterTransformer model</span>',
      '      <span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '      ',
      '      <span class="c1">##############################################################################</span>',
      '      <span class="c1"># do your training here</span>',
      '      <span class="c1">##############################################################################</span>',
      '      ',
      '      <span class="c1"># Convert the model back to a Hugging Face model</span>',
      '      <span class="n">model_hf</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>',
      '      ',
      '      <span class="n">model_hf</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">"fine_tuned_model"</span><span class="p">)</span>',
      '      <span class="n">model_hf</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">"fine_tuned_model"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
























      






    </div>

  </section>

</PostLayout>
