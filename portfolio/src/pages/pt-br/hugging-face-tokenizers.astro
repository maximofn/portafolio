---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Tokenizers';
const end_url = 'hugging-face-tokenizers';
const description = 'üìÑ ‚û°Ô∏è üî§ Explore o poder da biblioteca Hugging Face Tokenizers para processamento de linguagem natural em IA. Descubra como essa ferramenta essencial transforma texto em dados estruturados, otimizando o treinamento de modelos de IA com exemplos pr√°ticos e c√≥digo Python. Mergulhe no futuro da PNL com nosso guia especializado';
const keywords = 'hugging face, tokenizers, processamento de linguagem natural, pln, intelig√™ncia artificial, ia, python';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Hugging%20Face\'s%20tokenizers%20library.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-02-26+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Instalacao"><h2>Instala√ß√£o</h2></a>
      <a class="anchor-link" href="#O pipeline de tokenizacao"><h2>O pipeline de tokeniza√ß√£o</h2></a>
      <a class="anchor-link" href="#Padronizacao"><h3>Padroniza√ß√£o</h3></a>
      <a class="anchor-link" href="#Pre-tokenizacao"><h3>Pr√©-tokeniza√ß√£o</h3></a>
      <a class="anchor-link" href="#Tokenizacao"><h3>Tokeniza√ß√£o</h3></a>
      <a class="anchor-link" href="#Treinamento de modelos"><h4>Treinamento de modelos</h4></a>
      <a class="anchor-link" href="#Treinamento do modelo com o m√©todo train."><h5>Treinamento do modelo com o m√©todo <code>train</code>.</h5></a>
      <a class="anchor-link" href="#Treinamento do modelo com o m√©todo train_from_iterator."><h5>Treinamento do modelo com o m√©todo <code>train_from_iterator</code>.</h5></a>
      <a class="anchor-link" href="#Treinamento do modelo com o m√©todo train_from_iterator a partir de um conjunto de dados Hugging Face"><h5>Treinamento do modelo com o m√©todo <code>train_from_iterator</code> a partir de um conjunto de dados Hugging Face</h5></a>
      <a class="anchor-link" href="#Salvando o modelo"><h4>Salvando o modelo</h4></a>
      <a class="anchor-link" href="#Carregando o modelo pre-treinado"><h4>Carregando o modelo pr√©-treinado</h4></a>
      <a class="anchor-link" href="#Pos-processamento"><h3>P√≥s-processamento</h3></a>
      <a class="anchor-link" href="#Codificacao"><h3>Codifica√ß√£o</h3></a>
      <a class="anchor-link" href="#Decodificacao"><h3>Decodifica√ß√£o</h3></a>
      <a class="anchor-link" href="#BERT tokenizer"><h2>BERT tokenizer</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A biblioteca Hugging Face <code>tokenizers</code> fornece uma implementa√ß√£o dos tokenizadores mais usados atualmente, com foco no desempenho e na versatilidade. Na postagem <a href="https://maximofn.com/tokens/">tokens</a>, j√° vimos a import√¢ncia dos tokens no processamento de texto, pois os computadores n√£o entendem palavras, mas n√∫meros. Portanto, √© necess√°rio converter palavras em n√∫meros para que os modelos de linguagem possam process√°-las.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
      <h2 id="Instalacao">Instala√ß√£o<a class="anchor-link" href="#Instalacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para instalar o <code>tokenizers</code> com o pip:</p>
      <div class='highlight'><pre><code class="language-bash">pip install tokenizers</code></pre></div>
      <p>para instalar o <code>tokenizers</code> com o conda:</p>
      <div class='highlight'><pre><code class="language-bash">conda install conda-forge::tokenizers</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="O pipeline de tokenizacao">O pipeline de tokeniza√ß√£o<a class="anchor-link" href="#O pipeline de tokenizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para tokenizar uma sequ√™ncia, √© usado o <code>Tokenizer.encode</code>, que executa as seguintes etapas:</p>
      <ul>
        <li>Padroniza√ß√£o</li>
        <li>pr√©-tokeniza√ß√£o</li>
        <li>Tokeniza√ß√£o</li>
        <li>P√≥s-tokeniza√ß√£o</li>
      </ul>
      <p>Vamos dar uma olhada em cada um deles</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para esta postagem, usaremos o conjunto de dados [wikitext-103] (https://paperswithcode.com/dataset/wikitext-103)</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">dax</span><span class="o">-</span><span class="n">cdn</span><span class="o">.</span><span class="n">cdn</span><span class="o">.</span><span class="n">appdomain</span><span class="o">.</span><span class="n">cloud</span><span class="o">/</span><span class="n">dax</span><span class="o">-</span><span class="n">wikitext</span><span class="o">-</span><span class="mi">103</span><span class="o">/</span><span class="mf">1.0.1</span><span class="o">/</span><span class="n">wikitext</span><span class="o">-</span><span class="mf">103.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '--2024-02-26 08:14:11--  https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz',
          'Resolving dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)... 23.200.169.125',
          'Connecting to dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)|23.200.169.125|:443... connected.',
          'HTTP request sent, awaiting response...',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '200 OK',
          'Length: 189603606 (181M) [application/x-gzip]',
          'Saving to: ‚Äòwikitext-103.tar.gz‚Äô',
          'wikitext-103.tar.gz 100%[===================&amp;gt;] 180,82M  6,42MB/s    in 30s',
          '2024-02-26 08:14:42 (5,95 MB/s) - ‚Äòwikitext-103.tar.gz‚Äô saved [189603606/189603606]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">tar</span> <span class="o">-</span><span class="n">xvzf</span> <span class="n">wikitext</span><span class="o">-</span><span class="mf">103.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'wikitext-103/',
          'wikitext-103/wiki.test.tokens',
          'wikitext-103/wiki.valid.tokens',
          'wikitext-103/README.txt',
          'wikitext-103/LICENSE.txt',
          'wikitext-103/wiki.train.tokens',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">rm</span> <span class="n">wikitext</span><span class="o">-</span><span class="mf">103.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Padronizacao">Padroniza√ß√£o<a class="anchor-link" href="#Padronizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As normaliza√ß√µes s√£o opera√ß√µes aplicadas ao texto antes da tokeniza√ß√£o, como a remo√ß√£o de espa√ßos em branco, a convers√£o para letras min√∫sculas, a remo√ß√£o de caracteres especiais etc. As seguintes normaliza√ß√µes s√£o implementadas no Hugging Face:</p>
      <table>
        <thead>
          <tr>
            <th>Normalizaci√≥n</th>
            <th>Descripci√≥n</th>
            <th>Ejemplo</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>NFD (Normalization for D)</td>
            <td>Los caracteres se descomponen por equivalencia can√≥nica</td>
            <td><code>&#x60;√¢&#x60;</code> (U+00E2) se descompone en <code>a</code> (U+0061) + <code>^</code> (U+0302)</td>
          </tr>
          <tr>
            <td>NFKD (Normalization Form KD)</td>
            <td>Los caracteres se descomponen por compatibilidad</td>
            <td><code>&#x60;Ô¨Å&#x60;</code> (U+FB01) se descompone en <code>f</code> (U+0066) + <code>i</code> (U+0069)</td>
          </tr>
          <tr>
            <td>NFC (Normalization Form C)</td>
            <td>Los caracteres se descomponen y luego se recomponen por equivalencia can√≥nica</td>
            <td><code>&#x60;√¢&#x60;</code> (U+00E2) se descompone en <code>a</code> (U+0061) + <code>^</code> (U+0302) y luego se recompone en <code>&#x60;√¢&#x60;</code> (U+00E2)</td>
          </tr>
          <tr>
            <td>NFKC (Normalization Form KC)</td>
            <td>Los caracteres se descomponen por compatibilidad y luego se recomponen por equivalencia can√≥nica</td>
            <td><code>&#x60;Ô¨Å&#x60;</code> (U+FB01) se descompone en <code>f</code> (U+0066) + <code>i</code> (U+0069) y luego se recompone en <code>f</code> (U+0066) + <code>i</code> (U+0069)</td>
          </tr>
          <tr>
            <td>Lowercase</td>
            <td>Convierte el texto a min√∫sculas</td>
            <td><code>&#x60;Hello World&#x60;</code> se convierte en <code>hello world</code></td>
          </tr>
          <tr>
            <td>Strip</td>
            <td>Elimina todos los espacios en blanco de los lados especificados (izquierdo, derecho o ambos) del texto</td>
            <td><code>&#x60;  Hello World  &#x60;</code> se convierte en <code>Hello World</code></td>
          </tr>
          <tr>
            <td>StripAccents</td>
            <td>Elimina todos los s√≠mbolos de acento en unicode (se utilizar√° con NFD por coherencia)</td>
            <td><code>&#x60;√°&#x60;</code> (U+00E1) se convierte en <code>a</code> (U+0061)</td>
          </tr>
          <tr>
            <td>Replace</td>
            <td>Sustituye una cadena personalizada o [regex](https://maximofn.com/regular-expressions/) y la cambia por el contenido dado</td>
            <td><code>&#x60;Hello World&#x60;</code> se convierte en <code>Hello Universe</code></td>
          </tr>
          <tr>
            <td>BertNormalizer</td>
            <td>Proporciona una implementaci√≥n del Normalizador utilizado en el BERT original. Las opciones que se pueden configurar son <code>clean_text</code>, <code>handle_chinese_chars</code>, <code>strip_accents</code> y <code>lowercase</code></td>
            <td><code>&#x60;Hello World&#x60;</code> se convierte en <code>hello world</code></td>
          </tr>
        </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos criar um normalizador para ver como ele funciona.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">normalizers</span>',
      '<span class="w"> </span>',
      '<span class="n">bert_normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">BertNormalizer</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;H√©ll√≤ h√¥w are √º?&quot;</span>',
      '<span class="n">normalized_text</span> <span class="o">=</span> <span class="n">bert_normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
      '<span class="n">normalized_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;hello how are u?&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Para usar v√°rios normalizadores, podemos usar o m√©todo <code>Sequence</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">custom_normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">normalizers</span><span class="o">.</span><span class="n">NFKC</span><span class="p">(),</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">BertNormalizer</span><span class="p">()])</span>',
      '<span class="w"> </span>',
      '<span class="n">normalized_text</span> <span class="o">=</span> <span class="n">custom_normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
      '<span class="n">normalized_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;hello how are u?&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Para modificar o normalizador de um tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tokenizers</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">BertWordPieceTokenizer</span><span class="p">()</span> <span class="c1"># or any other tokenizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">custom_normalizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Pre-tokenizacao">Pr√©-tokeniza√ß√£o<a class="anchor-link" href="#Pre-tokenizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pretokeniza√ß√£o √© o ato de dividir o texto em objetos menores. O pretokenizador dividir√° o texto em "palavras" e os tokens finais ser√£o partes dessas palavras.</p>
      <p>O PreTokenizer se encarrega de dividir a entrada de acordo com um conjunto de regras. Esse pr√©-processamento permite que voc√™ garanta que o tokenizador n√£o crie tokens em v√°rias "divis√µes". Por exemplo, se voc√™ n√£o quiser ter espa√ßos em branco em um token, poder√° ter um pr√©-tokenizador que divida as palavras em espa√ßos em branco.</p>
      <p>Os seguintes pr√©-tokenizadores s√£o implementados no Hugging Face</p>
      <table>
        <thead>
          <tr>
            <th>PreTokenizer</th>
            <th>Descripci√≥n</th>
            <th>Ejemplo</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>ByteLevel</td>
            <td>Divide en espacios en blanco mientras reasigna todos los bytes a un conjunto de caracteres visibles. Esta t√©cnica fue introducida por OpenAI con GPT-2 y tiene algunas propiedades m√°s o menos buenas: Como mapea sobre bytes, un tokenizador que utilice esto s√≥lo requiere 256 caracteres como alfabeto inicial (el n√∫mero de valores que puede tener un byte), frente a los m√°s de 130.000 caracteres Unicode. Una consecuencia del punto anterior es que es absolutamente innecesario tener un token desconocido usando esto ya que podemos representar cualquier cosa con 256 tokens. Para caracteres no ascii, se vuelve completamente ilegible, ¬°pero funciona!</td>
            <td><code>Hello my friend, how are you?</code> se divide en <code>Hello</code>, <code>&#x60;ƒ†my&#x60;</code>, <code>&#x60;ƒ†friend&#x60;</code>, <code>,</code>, <code>&#x60;ƒ†how&#x60;</code>, <code>&#x60;ƒ†are&#x60;</code>, <code>&#x60;ƒ†you&#x60;</code>, <code>?</code></td>
          </tr>
          <tr>
            <td>Whitespace</td>
            <td>Divide en l√≠mites de palabra usando la siguiente expresi√≥n regular: <code>\w+[^\w\s]+</code>. En mi post sobre [expresiones regulares](https://maximofn.com/regular-expressions/) puedes entender qu√© hace</td>
            <td><code>&#x60;Hello there!&#x60;</code> se divide en `Hello`, `there`, `!`</td>
          </tr>
          <tr>
            <td>WhitespaceSplit</td>
            <td>Se divide en cualquier car√°cter de espacio en blanco</td>
            <td><code>&#x60;Hello there!&#x60;</code> se divide en `Hello`, `there!`</td>
          </tr>
          <tr>
            <td>Punctuation</td>
            <td>Aislar√° todos los caracteres de puntuaci√≥n</td>
            <td>`Hello?` se divide en `Hello`, `?`</td>
          </tr>
          <tr>
            <td>Metaspace</td>
            <td>Separa los espacios en blanco y los sustituye por un car√°cter especial "‚ñÅ" (U+2581)</td>
            <td><code>&#x60;Hello there&#x60;</code> se divide en <code>Hello</code>, <code>&#x60;‚ñÅthere&#x60;</code></td>
          </tr>
          <tr>
            <td>CharDelimiterSplit</td>
            <td>Divisiones en un car√°cter determinado</td>
            <td>Ejemplo con el caracter `x`: `Helloxthere` se divide en `Hello`, `there`</td>
          </tr>
          <tr>
            <td>Digits</td>
            <td>Divide los n√∫meros de cualquier otro car√°cter</td>
            <td>`Hello123there` se divide en `Hello`, `123`, `there`</td>
          </tr>
          <tr>
            <td>Split</td>
            <td>Pretokenizador vers√°til que divide seg√∫n el patr√≥n y el comportamiento proporcionados. El patr√≥n se puede invertir si es necesario. El patr√≥n debe ser una cadena personalizada o una [regex](https://maximofn.com/regular-expressions/). El comportamiento debe ser <code>removed</code>, <code>isolated</code>, <code>merged_with_previous</code>, <code>merged_with_next</code>, <code>contiguous</code>. Para invertir se indica con un booleano</td>
            <td>Ejemplo con pattern=<code>&#x60;" "&#x60;</code>, behavior=`isolated`, invert=`False`: <code>&#x60;Hello, how are you?&#x60;</code> se divide en `Hello,`, <code>&#x60; &#x60;</code>, `how`, <code>&#x60; &#x60;</code>, `are`, <code>&#x60; &#x60;</code>, `you?`</td>
          </tr>
        </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos criar um pr√©-tokenizador para ver como ele funciona.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pre_tokenizers</span>',
      '<span class="w"> </span>',
      '<span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Digits</span><span class="p">(</span><span class="n">individual_digits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;I paid $30 for the car&quot;</span>',
      '<span class="n">pre_tokenized_text</span> <span class="o">=</span> <span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
      '<span class="n">pre_tokenized_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[(&#x27;I paid $&#x27;, (0, 8)),',
          '(&#x27;3&#x27;, (8, 9)),',
          '(&#x27;0&#x27;, (9, 10)),',
          '(&#x27; for the car&#x27;, (10, 22))]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Para usar v√°rios pr√©-tokenizadores, podemos usar o m√©todo <code>Sequence</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">custom_pre_tokenizer</span> <span class="o">=</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Whitespace</span><span class="p">(),</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Digits</span><span class="p">(</span><span class="n">individual_digits</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>',
      '<span class="w"> </span>',
      '<span class="n">pre_tokenized_text</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
      '<span class="n">pre_tokenized_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[(&#x27;I&#x27;, (0, 1)),',
          '(&#x27;paid&#x27;, (2, 6)),',
          '(&#x27;$&#x27;, (7, 8)),',
          '(&#x27;3&#x27;, (8, 9)),',
          '(&#x27;0&#x27;, (9, 10)),',
          '(&#x27;for&#x27;, (11, 14)),',
          '(&#x27;the&#x27;, (15, 18)),',
          '(&#x27;car&#x27;, (19, 22))]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Para modificar o pr√©-tokenizador de um tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizacao">Tokeniza√ß√£o<a class="anchor-link" href="#Tokenizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 37" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Depois que os textos de entrada tiverem sido normalizados e pr√©-tokenizados, o tokenizador aplica o modelo aos pr√©-tokens. Essa √© a parte do processo que precisa ser treinada no corpus (ou j√° foi treinada se for usado um tokenizador pr√©-treinado).</p>
      <p>A fun√ß√£o do modelo √© dividir as "palavras" em tokens usando as regras que aprendeu. Ele tamb√©m √© respons√°vel por atribuir esses tokens √†s suas IDs correspondentes no vocabul√°rio do modelo.</p>
      <p>O modelo tem um tamanho de vocabul√°rio, ou seja, tem um n√∫mero finito de tokens, portanto, precisa decompor as palavras e atribu√≠-las a um desses tokens.</p>
      <p>Esse modelo √© passado quando o Tokenizer √© inicializado. Atualmente, a biblioteca ü§ó Tokenizers √© compat√≠vel:</p>
      <table>
        <thead>
          <tr>
            <th>Modelo</th>
            <th>Descripci√≥n</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>WordLevel</td>
            <td>Este es el algoritmo "cl√°sico" de tokenizaci√≥n. Te permite simplemente asignar palabras a IDs sin nada sofisticado. Tiene la ventaja de ser muy f√°cil de usar y entender, pero requiere vocabularios extremadamente grandes para una buena cobertura. El uso de este modelo requiere el uso de un PreTokenizer. Este modelo no realiza ninguna elecci√≥n directamente, simplemente asigna tokens de entrada a IDs.</td>
          </tr>
          <tr>
            <td>BPE (Byte Pair Encoding)</td>
            <td>Uno de los algoritmos de tokenizaci√≥n de subpalabras m√°s populares. El Byte-Pair-Encoding funciona empezando con caracteres y fusionando los que se ven juntos con m√°s frecuencia, creando as√≠ nuevos tokens. A continuaci√≥n, trabaja de forma iterativa para construir nuevos tokens a partir de los pares m√°s frecuentes que ve en un corpus. BPE es capaz de construir palabras que nunca ha visto utilizando m√∫ltiples subpalabras y, por tanto, requiere vocabularios m√°s peque√±os, con menos posibilidades de tener palabras <code>unk</code> (desconocidas).</td>
          </tr>
          <tr>
            <td>WordPiece</td>
            <td>Se trata de un algoritmo de tokenizaci√≥n de subpalabras bastante similar a BPE, utilizado principalmente por Google en modelos como BERT. Utiliza un algoritmo codicioso que intenta construir primero palabras largas, dividi√©ndolas en varios tokens cuando no existen palabras completas en el vocabulario. A diferencia de BPE, que parte de los caracteres y construye tokens lo m√°s grandes posible. Utiliza el famoso prefijo ## para identificar los tokens que forman parte de una palabra (es decir, que no empiezan una palabra).</td>
          </tr>
          <tr>
            <td>Unigram</td>
            <td>Unigram es tambi√©n un algoritmo de tokenizaci√≥n de subpalabras, y funciona tratando de identificar el mejor conjunto de tokens de subpalabras para maximizar la probabilidad de una frase dada. Se diferencia de BPE en que no es un algoritmo determinista basado en un conjunto de reglas aplicadas secuencialmente. En su lugar, Unigram podr√° calcular m√∫ltiples formas de tokenizar, eligiendo la m√°s probable.</td>
          </tr>
        </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando voc√™ cria um tokenizador, precisa passar a ele o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Unigram</span><span class="p">())</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Passaremos o normalizador e o pr√©-tokenizador que criamos para ele.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">custom_normalizer</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora temos que treinar o modelo ou carregar um modelo pr√©-treinado. Neste caso, vamos treinar um modelo com o corpus que baixamos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Treinamento de modelos">Treinamento de modelos<a class="anchor-link" href="#Treinamento de modelos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 38" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para treinar o modelo, temos v√°rios tipos de "treinadores".</p>
      <table>
        <thead>
          <tr>
            <th>Trainer</th>
            <th>Descripci√≥n</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>WordLevelTrainer</td>
            <td>Entrena un tokenizador WordLevel</td>
          </tr>
          <tr>
            <td>BpeTrainer</td>
            <td>Entrena un tokenizador BPE</td>
          </tr>
          <tr>
            <td>WordPieceTrainer</td>
            <td>Entrena un tokenizador WordPiece</td>
          </tr>
          <tr>
            <td>UnigramTrainer</td>
            <td>Entrena un tokenizador Unigram</td>
          </tr>
        </tbody>
      </table>
      <p>Quase todos os treinadores t√™m os mesmos par√¢metros, que s√£o:</p>
      <ul>
        <li>vocab_size: o tamanho do vocabul√°rio final, incluindo todos os tokens e o alfabeto.</li>
        <li>show_progress: Mostrar ou n√£o barras de progresso durante o treinamento</li>
        <li>special_tokens: Uma lista de tokens especiais dos quais o modelo deve estar ciente.</li>
      </ul>
      <p>Al√©m desses par√¢metros, cada treinador tem seus pr√≥prios par√¢metros; consulte a documenta√ß√£o <a href="https://huggingface.co/docs/tokenizers/api/trainers" target="_blank" rel="nofollow noreferrer">Trainers</a> para obter mais informa√ß√µes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para treinar, precisamos criar um <code>Trainer</code>. Como o modelo que criamos √© um <code>Unigram</code>, criaremos um <code>UnigramTrainer</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.trainers</span><span class="w"> </span><span class="kn">import</span> <span class="n">trainers</span>',
      '<span class="w"> </span>',
      '<span class="n">trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="o">.</span><span class="n">UnigramTrainer</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">initial_alphabet</span><span class="o">=</span><span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">ByteLevel</span><span class="o">.</span><span class="n">alphabet</span><span class="p">(),</span>',
      '<span class="w">    </span><span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;&amp;lt;PAD&amp;gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&amp;lt;BOS&amp;gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;&amp;lt;EOS&amp;gt;&quot;</span><span class="p">],</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Depois de criarmos o <code>Trainer</code>, h√° duas maneiras de entrar, usando o m√©todo <code>train</code>, que recebe uma lista de arquivos, ou usando o m√©todo <code>train_from_iterator</code>, que recebe um iterador.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Treinamento do modelo com o m√©todo train.">Treinamento do modelo com o m√©todo <code>train</code>.<a class="anchor-link" href="#Treinamento do modelo com o m√©todo train."><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 39" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, criamos uma lista de arquivos com o corpus</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;wikitext-103/wiki.</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">.tokens&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]]</span>',
      '<span class="n">files</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x27;wikitext-103/wiki.test.tokens&#x27;,',
          '&#x27;wikitext-103/wiki.train.tokens&#x27;,',
          '&#x27;wikitext-103/wiki.valid.tokens&#x27;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E agora treinamos o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[

        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Treinamento do modelo com o m√©todo train_from_iterator.">Treinamento do modelo com o m√©todo <code>train_from_iterator</code>.<a class="anchor-link" href="#Treinamento do modelo com o m√©todo train_from_iterator."><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 40" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, criamos uma fun√ß√£o que retorna um iterador.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">iterator</span><span class="p">():</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>',
      '<span class="w">        </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>',
      '<span class="w">            </span><span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>',
      '<span class="w">                </span><span class="k">yield</span> <span class="n">line</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora, treinamos novamente o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">iterator</span><span class="p">(),</span> <span class="n">trainer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[

        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Treinamento do modelo com o m√©todo train_from_iterator a partir de um conjunto de dados Hugging Face">Treinamento do modelo com o m√©todo <code>train_from_iterator</code> a partir de um conjunto de dados Hugging Face<a class="anchor-link" href="#Treinamento do modelo com o m√©todo train_from_iterator a partir de um conjunto de dados Hugging Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 41" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se tiv√©ssemos baixado o conjunto de dados Hugging Face, poder√≠amos ter treinado o modelo diretamente do conjunto de dados.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">datasets</span>',
      '<span class="w"> </span>',
      '<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;wikitext&quot;</span><span class="p">,</span> <span class="s2">&quot;wikitext-103-raw-v1&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train+test+validation&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora podemos criar um iterador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">batch_iterator</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>',
      '<span class="w">        </span><span class="k">yield</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Treinamos novamente o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">batch_iterator</span><span class="p">(),</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[

        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Salvando o modelo">Salvando o modelo<a class="anchor-link" href="#Salvando o modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 42" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Depois que o modelo tiver sido treinado, ele poder√° ser salvo para uso futuro. Para salvar o modelo, √© necess√°rio salv√°-lo em um arquivo <code>json</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;wikitext-103-tokenizer.json&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Carregando o modelo pre-treinado">Carregando o modelo pr√©-treinado<a class="anchor-link" href="#Carregando o modelo pre-treinado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 43" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos carregar um modelo pr√©-treinado a partir de um <code>json</code> em vez de precisar trein√°-lo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s2">&quot;wikitext-103-tokenizer.json&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;tokenizers.Tokenizer at 0x7f1dd7784a30&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m podemos carregar um modelo pr√©-treinado dispon√≠vel no Hugging Face Hub.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;tokenizers.Tokenizer at 0x7f1d64a75e30&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Pos-processamento">P√≥s-processamento<a class="anchor-link" href="#Pos-processamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 44" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Talvez queiramos que nosso tokenizador adicione automaticamente tokens especiais, como <code>[CLS]</code> ou <code>[SEP]</code>.</p>
      <p>Os seguintes p√≥s-processadores s√£o implementados no Hugging Face</p>
      <table>
        <thead>
          <tr>
            <th>PostProcesador</th>
            <th>Descripci√≥n</th>
            <th>Ejemplo</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>BertProcessing</td>
            <td>Este post-procesador se encarga de a√±adir los tokens especiales que necesita un modelo Bert (<code>SEP</code> y <code>CLS</code>)</td>
            <td><code>&#x60;Hello, how are you?&#x60;</code> se convierte en <code>[CLS]</code>, <code>Hello</code>, `,`, <code>how</code>, <code>are</code>, <code>you</code>, `?`, <code>[SEP]</code></td>
          </tr>
          <tr>
            <td>RobertaProcessing</td>
            <td>Este post-procesador se encarga de a√±adir los tokens especiales que necesita un modelo Roberta (<code>SEP</code> y <code>CLS</code>). Tambi√©n se encarga de recortar los offsets. Por defecto, el ByteLevel BPE puede incluir espacios en blanco en los tokens producidos. Si no desea que las compensaciones incluyan estos espacios en blanco, hay que inicializar este PostProcessor con <code>trim_offsets=True</code>.</td>
            <td><code>&#x60;Hello, how are you?&#x60;</code> se convierte en <code><s></code>, <code>Hello</code>, `,`, <code>how</code>, <code>are</code>, <code>you</code>, `?`, <code></s></code></td>
          </tr>
          <tr>
            <td>ElectraProcessing</td>
            <td>A√±ade tokens especiales para ELECTRA</td>
            <td><code>&#x60;Hello, how are you?&#x60;</code> se convierte en <code>[CLS]</code>, <code>Hello</code>, `,`, <code>how</code>, <code>are</code>, <code>you</code>, `?`, <code>[SEP]</code></td>
          </tr>
          <tr>
            <td>TemplateProcessing</td>
            <td>Permite crear f√°cilmente una plantilla para el postprocesamiento, a√±adiendo tokens especiales y especificando el type_id de cada secuencia/token especial. La plantilla recibe dos cadenas que representan la secuencia √∫nica y el par de secuencias, as√≠ como un conjunto de tokens especiales a utilizar</td>
            <td>Example, when specifying a template with these values: single:<code>[CLS] $A [SEP]</code>, pair: <code>[CLS] $A [SEP] $B [SEP]</code>, special tokens: `[CLS]`, `[SEP]`. Input: (<code>&#x60;I like this&#x60;</code>, <code>&#x60;but not this&#x60;</code>), Output: <code>[CLS] I like this [SEP] but not this [SEP]</code></td>
          </tr>
        </tbody>
      </table>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos criar um tokenizador de postagem para ver como ele funciona.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">TemplateProcessing</span>',
      '<span class="w"> </span>',
      '<span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">single</span><span class="o">=</span><span class="s2">&quot;[CLS] $A [SEP]&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">pair</span><span class="o">=</span><span class="s2">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">special_tokens</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Para modificar o tokenizador de postagem de um tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">post_processor</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como funciona</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;I paid $30 for the car&quot;</span>',
      '<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">decoded_text</span><span class="o">.</span><span class="n">tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x27;[CLS]&#x27;, &#x27;i&#x27;, &#x27;paid&#x27;, &#x27;$&#x27;, &#x27;3&#x27;, &#x27;0&#x27;, &#x27;for&#x27;, &#x27;the&#x27;, &#x27;car&#x27;, &#x27;[SEP]&#x27;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_text1</span> <span class="o">=</span> <span class="s2">&quot;Hello, y&#39;all!&quot;</span>',
      '<span class="n">input_text2</span> <span class="o">=</span> <span class="s2">&quot;How are you?&quot;</span>',
      '<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">input_text2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">decoded_text</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x27;[CLS]&#x27;, &#x27;hell&#x27;, &#x27;o&#x27;, &#x27;,&#x27;, &#x27;y&#x27;, &quot;&#x27;&quot;, &#x27;all&#x27;, &#x27;!&#x27;, &#x27;[SEP]&#x27;, &#x27;how&#x27;, &#x27;are&#x27;, &#x27;you&#x27;, &#x27;?&#x27;, &#x27;[SEP]&#x27;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se salvarmos o tokenizador agora, o tokenizador de postagem ser√° salvo com ele.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Codificacao">Codifica√ß√£o<a class="anchor-link" href="#Codificacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 45" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Depois de treinar o tokenizador, podemos us√°-lo para tokenizar textos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;I love tokenizers!&quot;</span>',
      '<span class="n">encoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vejamos o que obtemos ao tokenizar um texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="nb">type</span><span class="p">(</span><span class="n">encoded_text</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'tokenizers.Encoding',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos um objeto do tipo <a href="https://huggingface.co/docs/tokenizers/api/encoding#tokenizers.Encoding" target="_blank" rel="nofollow noreferrer">Encoding</a>, contendo os tokens e os ids dos tokens.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os <code>ids</code> s√£o os <code>id</code>s dos tokens no vocabul√°rio do tokenizador.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[1, 17, 383, 10694, 17, 3533, 3, 586, 2]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Tokens s√£o os tokens aos quais os <code>ids</code> s√£o equivalentes.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">encoded_text</span><span class="o">.</span><span class="n">tokens</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x27;[CLS]&#x27;, &#x27;i&#x27;, &#x27;love&#x27;, &#x27;token&#x27;, &#x27;i&#x27;, &#x27;zer&#x27;, &#x27;s&#x27;, &#x27;!&#x27;, &#x27;[SEP]&#x27;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se tivermos v√°rias sequ√™ncias, poderemos codific√°-las todas de uma vez</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">input_text2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">type_ids</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x27;[CLS]&#x27;, &#x27;hell&#x27;, &#x27;o&#x27;, &#x27;,&#x27;, &#x27;y&#x27;, &quot;&#x27;&quot;, &#x27;all&#x27;, &#x27;!&#x27;, &#x27;[SEP]&#x27;, &#x27;how&#x27;, &#x27;are&#x27;, &#x27;you&#x27;, &#x27;?&#x27;, &#x27;[SEP]&#x27;]',
          '[1, 2215, 7, 5, 22, 26, 81, 586, 2, 98, 59, 213, 902, 2]',
          '[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>No entanto, quando voc√™ tem v√°rias sequ√™ncias, √© melhor usar o m√©todo <code>encode_batch</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">([</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">input_text2</span><span class="p">])</span>',
      '<span class="w"> </span>',
      '<span class="nb">type</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'list',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que obtemos uma lista</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x27;[CLS]&#x27;, &#x27;hell&#x27;, &#x27;o&#x27;, &#x27;,&#x27;, &#x27;y&#x27;, &quot;&#x27;&quot;, &#x27;all&#x27;, &#x27;!&#x27;, &#x27;[SEP]&#x27;]',
          '[1, 2215, 7, 5, 22, 26, 81, 586, 2]',
          '[&#x27;[CLS]&#x27;, &#x27;how&#x27;, &#x27;are&#x27;, &#x27;you&#x27;, &#x27;?&#x27;, &#x27;[SEP]&#x27;]',
          '[1, 98, 59, 213, 902, 2]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Decodificacao">Decodifica√ß√£o<a class="anchor-link" href="#Decodificacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 46" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al√©m de codificar os textos de entrada, um Tokenizer tamb√©m tem um m√©todo para decodificar, ou seja, converter os IDs gerados pelo seu modelo de volta para um texto. Isso √© feito pelos m√©todos <code>Tokenizer.decode</code> (para um texto previsto) e <code>Tokenizer.decode_batch</code> (para um lote de previs√µes).</p>
      <p>Os tipos de decodifica√ß√£o que podem ser usados s√£o:</p>
      <table>
        <thead>
          <tr>
            <th>Decodificaci√≥n</th>
            <th>Descripci√≥n</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>BPEDecoder</td>
            <td>Revierte el modelo BPE</td>
          </tr>
          <tr>
            <td>ByteLevel</td>
            <td>Revierte el ByteLevel PreTokenizer. Este PreTokenizer codifica a nivel de byte, utilizando un conjunto de caracteres Unicode visibles para representar cada byte, por lo que necesitamos un Decoder para revertir este proceso y obtener algo legible de nuevo.</td>
          </tr>
          <tr>
            <td>CTC</td>
            <td>Revierte el modelo CTC</td>
          </tr>
          <tr>
            <td>Metaspace</td>
            <td>Revierte el PreTokenizer de Metaspace. Este PreTokenizer utiliza un identificador especial ‚ñÅ para identificar los espacios en blanco, por lo que este Decoder ayuda con la decodificaci√≥n de estos.</td>
          </tr>
          <tr>
            <td>WordPiece</td>
            <td>Revierte el modelo WordPiece. Este modelo utiliza un identificador especial ## para las subpalabras continuas, por lo que este decodificador ayuda a decodificarlas.</td>
          </tr>
        </tbody>
      </table>
      <p>O decodificador primeiro converter√° os IDs em tokens (usando o vocabul√°rio do tokenizador) e remover√° todos os tokens especiais e, em seguida, juntar√° esses tokens com espa√ßos em branco.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos criar um decodificador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">decoders</span>',
      '<span class="w"> </span>',
      '<span class="n">decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ByteLevel</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>N√≥s o adicionamos ao tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>N√≥s decodificamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_text</span><span class="p">,</span> <span class="n">decoded_text</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(&#x27;I love tokenizers!&#x27;, &#x27;ilovetokenizers!&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">decoded_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode_batch</span><span class="p">([</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="n">encoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">])</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">decoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">input_text2</span><span class="p">,</span> <span class="n">decoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello, y&#x27;all! hello,y&#x27;all!',
          'How are you? howareyou?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="BERT tokenizer">BERT tokenizer<a class="anchor-link" href="#BERT tokenizer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 47" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Com tudo o que aprendemos, vamos criar o tokenizador BERT do zero. O Bert usa o <code>WordPiece</code> como modelo, ent√£o o passamos para o inicializador do tokenizador.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">WordPiece</span>',
      '<span class="w"> </span>',
      '<span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>O BERT pr√©-processa os textos removendo acentos e letras min√∫sculas. Tamb√©m usamos um normalizador unicode</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">normalizers</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.normalizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">NFD</span><span class="p">,</span> <span class="n">Lowercase</span><span class="p">,</span> <span class="n">StripAccents</span>',
      '<span class="w"> </span>',
      '<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">NFD</span><span class="p">(),</span> <span class="n">Lowercase</span><span class="p">(),</span> <span class="n">StripAccents</span><span class="p">()])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>O pretokenizador divide apenas espa√ßos em branco e sinais de pontua√ß√£o.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.pre_tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Whitespace</span>',
      '<span class="w"> </span>',
      '<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>E o p√≥s-processamento usa o modelo que vimos na se√ß√£o anterior</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">TemplateProcessing</span>',
      '<span class="w"> </span>',
      '<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">single</span><span class="o">=</span><span class="s2">&quot;[CLS] $A [SEP]&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">pair</span><span class="o">=</span><span class="s2">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span>',
      '<span class="w">        </span><span class="p">(</span><span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>',
      '<span class="w">        </span><span class="p">(</span><span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>',
      '<span class="w">    </span><span class="p">],</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Treinamos o tokenizador com o conjunto de dados wikitext-103.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.trainers</span><span class="w"> </span><span class="kn">import</span> <span class="n">WordPieceTrainer</span>',
      '<span class="w"> </span>',
      '<span class="n">trainer</span> <span class="o">=</span> <span class="n">WordPieceTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">30522</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;wikitext-103/wiki.</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">.tokens&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]]</span>',
      '<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[

        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora vamos test√°-lo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;I love tokenizers!&quot;</span>',
      '<span class="w"> </span>',
      '<span class="n">encoded_text</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
      '<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;El texto de entrada &#39;</span><span class="si">{</span><span class="n">input_text</span><span class="si">}</span><span class="s2">&#39; se convierte en los tokens </span><span class="si">{</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, que tienen las ids </span><span class="si">{</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span><span class="si">}</span><span class="s2"> y luego se decodifica como &#39;</span><span class="si">{</span><span class="n">decoded_text</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'El texto de entrada &#x27;I love tokenizers!&#x27; se convierte en los tokens [&#x27;[CLS]&#x27;, &#x27;i&#x27;, &#x27;love&#x27;, &#x27;token&#x27;, &#x27;##izers&#x27;, &#x27;!&#x27;, &#x27;[SEP]&#x27;], que tienen las ids [1, 51, 2867, 25791, 12213, 5, 2] y luego se decodifica como &#x27;i love token ##izers !&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>

















    </div>

  </section>

</PostLayout>
