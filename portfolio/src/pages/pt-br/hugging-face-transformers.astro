---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Hugging Face Transformers';
const end_url = 'hugging-face-transformers';
const description = 'ü§ñ Transforme seu mundo com os Hugging Face Transformers! üöÄ Pronto para fazer m√°gica com a linguagem natural? De t√©cnicas super-r√°pidas com o pipeline üå™Ô∏è a truques ninja com o AutoModel ü•∑, esta publica√ß√£o leva voc√™ pela m√£o em uma aventura √©pica no universo da PNL. Explore como gerar textos que surpreendem, treinar modelos que deslumbram e compartilhar suas cria√ß√µes no Hugging Face Hub como um profissional. Prepare-se para codificar e rir, porque o futuro da PNL √© agora e √© hil√°rio! üòÇ';
const keywords = 'Hugging Face, Transformers, PNL, Processamento de Linguagem Natural, AutoModel, pipeline, fine-tuning, treinamento, compartilhamento, Hub';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/HuggingFace%20Transformers.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1147
    image_height=644
    image_extension=webp
    article_date=2024-04-15+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Instalacao"><h2>Instala√ß√£o</h2></a>
      <a class="anchor-link" href="#Infer√™ncia com pipeline."><h2>Infer√™ncia com <code>pipeline</code>.</h2></a>
      <a class="anchor-link" href="#Tarefas"><h3>Tarefas</h3></a>
      <a class="anchor-link" href="#Uso de pipeline"><h3>Uso de <code>pipeline</code></h3></a>
      <a class="anchor-link" href="#Como funciona o `pipeline"><h3>Como funciona o `pipeline</h3></a>
      <a class="anchor-link" href="#Infer√™ncia com AutoClass e pipeline."><h2>Infer√™ncia com <code>AutoClass</code> e <code>pipeline</code>.</h2></a>
      <a class="anchor-link" href="#Tokeniza√ß√£o com AutoTokenizer."><h3>Tokeniza√ß√£o com <code>AutoTokenizer</code>.</h3></a>
      <a class="anchor-link" href="#Modelo AutoModel."><h3>Modelo <code>AutoModel</code>.</h3></a>
      <a class="anchor-link" href="#Modelo AutoModelFor."><h3>Modelo <code>AutoModelFor</code>.</h3></a>
      <a class="anchor-link" href="#Infer√™ncia somente com AutoClass."><h2>Infer√™ncia somente com <code>AutoClass</code>.</h2></a>
      <a class="anchor-link" href="#Geracao de texto casual"><h3>Gera√ß√£o de texto casual</h3></a>
      <a class="anchor-link" href="#Classificacao do texto"><h3>Classifica√ß√£o do texto</h3></a>
      <a class="anchor-link" href="#Classificacao de tokens"><h3>Classifica√ß√£o de tokens</h3></a>
      <a class="anchor-link" href="#Resposta a pergunta"><h3>Resposta √† pergunta</h3></a>
      <a class="anchor-link" href="#Modelagem de linguagem com mascara (Modelagem de linguagem com mascara)"><h3>Modelagem de linguagem com m√°scara (Modelagem de linguagem com m√°scara)</h3></a>
      <a class="anchor-link" href="#Personalizacao do modelo"><h2>Personaliza√ß√£o do modelo</h2></a>
      <a class="anchor-link" href="#Tokenizacao"><h2>Tokeniza√ß√£o</h2></a>
      <a class="anchor-link" href="#Preenchimento"><h3>Preenchimento</h3></a>
      <a class="anchor-link" href="#Truncado"><h3>Truncado</h3></a>
      <a class="anchor-link" href="#Tensionadores"><h3>Tensionadores</h3></a>
      <a class="anchor-link" href="#Mascaras"><h3>M√°scaras</h3></a>
      <a class="anchor-link" href="#Tokenizadores rapidos"><h2>Tokenizadores r√°pidos</h2></a>
      <a class="anchor-link" href="#Formas de geracao de texto"><h2>Formas de gera√ß√£o de texto</h2></a>
      <a class="anchor-link" href="#Busca ambiciosa"><h3>Busca ambiciosa</h3></a>
      <a class="anchor-link" href="#Pesquisa Contrastiva"><h3>Pesquisa Contrastiva</h3></a>
      <a class="anchor-link" href="#Amostragem multinomial"><h3>Amostragem multinomial</h3></a>
      <a class="anchor-link" href="#Pesquisa de feixe"><h3>Pesquisa de feixe</h3></a>
      <a class="anchor-link" href="#Amostragem multinomial de busca de feixe"><h3>Amostragem multinomial de busca de feixe</h3></a>
      <a class="anchor-link" href="#Penalidade de n-gramas de pesquisa de feixe"><h3>Penalidade de n-gramas de pesquisa de feixe</h3></a>
      <a class="anchor-link" href="#Sequencias de retorno de penalidade de pesquisa de feixe de n-gramas"><h3>Sequ√™ncias de retorno de penalidade de pesquisa de feixe de n-gramas</h3></a>
      <a class="anchor-link" href="#Decodificacao de busca de feixe diverso"><h3>Decodifica√ß√£o de busca de feixe diverso</h3></a>
      <a class="anchor-link" href="#Decodificacao especulativa"><h3>Decodifica√ß√£o especulativa</h3></a>
      <a class="anchor-link" href="#Controle de aleatoriedade da decodificacao especulativa"><h3>Controle de aleatoriedade da decodifica√ß√£o especulativa</h3></a>
      <a class="anchor-link" href="#Amostragem"><h3>Amostragem</h3></a>
      <a class="anchor-link" href="#Temperatura de amostragem"><h3>Temperatura de amostragem</h3></a>
      <a class="anchor-link" href="#Amostragem top-k"><h3>Amostragem top-k</h3></a>
      <a class="anchor-link" href="#Top-p de amostragem (amostragem de nucleo)"><h3>Top-p de amostragem (amostragem de n√∫cleo)</h3></a>
      <a class="anchor-link" href="#Amostragem top-k e top-p"><h3>Amostragem top-k e top-p</h3></a>
      <a class="anchor-link" href="#Efeito da temperatura, top-k e top-p"><h3>Efeito da temperatura, top-k e top-p</h3></a>
      <a class="anchor-link" href="#Streaming"><h2>Streaming</h2></a>
      <a class="anchor-link" href="#Modelos de bate-papo"><h2>Modelos de bate-papo</h2></a>
      <a class="anchor-link" href="#Tokenizacao de contexto"><h3>Tokeniza√ß√£o de contexto</h3></a>
      <a class="anchor-link" href="#Adicionar geracao de prompts"><h3>Adicionar gera√ß√£o de prompts</h3></a>
      <a class="anchor-link" href="#Geracao de texto"><h3>Gera√ß√£o de texto</h3></a>
      <a class="anchor-link" href="#Gera√ß√£o de texto com pipeline."><h3>Gera√ß√£o de texto com <code>pipeline</code>.</h3></a>
      <a class="anchor-link" href="#Trem"><h2>Trem</h2></a>
      <a class="anchor-link" href="#Conjunto de dados"><h3>Conjunto de dados</h3></a>
      <a class="anchor-link" href="#Tokenizacao"><h3>Tokeniza√ß√£o</h3></a>
      <a class="anchor-link" href="#Modelo"><h3>Modelo</h3></a>
      <a class="anchor-link" href="#Metricas de avaliacao"><h3>M√©tricas de avalia√ß√£o</h3></a>
      <a class="anchor-link" href="#Treinador"><h3>Treinador</h3></a>
      <a class="anchor-link" href="#Testando o modelo"><h3>Testando o modelo</h3></a>
      <a class="anchor-link" href="#Compartilhe o modelo no Hub do Rosto de Abraco"><h2>Compartilhe o modelo no Hub do Rosto de Abra√ßo</h2></a>
      <a class="anchor-link" href="#Registro em log"><h3>Registro em log</h3></a>
      <a class="anchor-link" href="#Para cima depois de treinado"><h3>Para cima depois de treinado</h3></a>
      <a class="anchor-link" href="#Levantar durante o treinamento"><h3>Levantar durante o treinamento</h3></a>
      <a class="anchor-link" href="#Hub como repositorio git"><h2>Hub como reposit√≥rio git</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A biblioteca <code>transformers</code> da Hugging Face √© uma das bibliotecas mais populares para trabalhar com modelos de linguagem. Sua facilidade de uso democratizou o uso da arquitetura <code>Transformer</code> e tornou poss√≠vel trabalhar com modelos de linguagem de √∫ltima gera√ß√£o sem a necessidade de ter muito conhecimento na √°rea.</p>
      <p>Entre a biblioteca <code>transformers</code>, o hub de modelos e sua facilidade de uso, os espa√ßos e a facilidade de implementa√ß√£o de demonstra√ß√µes, al√©m de novas bibliotecas como <code>datasets</code>, <code>accelerate</code>, <code>PEFT</code> e outras, eles tornaram a Hugging Face um dos participantes mais importantes no cen√°rio de IA no momento. Eles se autodenominam "o GitHub da IA" e certamente o s√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
      <h2 id="Instalacao">Instala√ß√£o<a class="anchor-link" href="#Instalacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 99" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para instalar transformadores, voc√™ pode fazer isso com o <code>pip</code>.</p>
      <div class='highlight'><pre><code class="language-bash">pip install transformers</code></pre></div>
      <p>ou com <code>conda</code>.</p>
      <div class='highlight'><pre><code class="language-bash">conda install conda-forge::transformers</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al√©m da biblioteca, voc√™ precisa ter um backend do PyTorch ou do TensorFlow instalado. Ou seja, voc√™ precisa ter o <code>torch</code> ou o <code>tensorflow</code> instalados para poder usar o <code>transformers</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2>Infer√™ncia com <code>pipeline</code>.</h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Com os pipelines <code>transformers</code>, a infer√™ncia com modelos de linguagem pode ser feita de maneira muito simples. A vantagem disso √© que o desenvolvimento √© muito mais r√°pido e a cria√ß√£o de prot√≥tipos pode ser feita com muita facilidade. Isso tamb√©m permite que pessoas que n√£o t√™m muito conhecimento usem os modelos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Com o <code>pipeline</code>, voc√™ pode fazer infer√™ncia em v√°rias tarefas diferentes. Cada tarefa tem sua pr√≥pria <code>pipeline</code> (NLP <code>pipeline</code>, vis√£o <code>pipeline</code> etc.), mas voc√™ pode fazer uma abstra√ß√£o geral usando a classe <code>pipeline</code>, que se encarrega de selecionar a <code>pipeline</code> correta para a tarefa que voc√™ passar a ela.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tarefas">Tarefas<a class="anchor-link" href="#Tarefas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 100" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No momento da reda√ß√£o desta postagem, as tarefas que podem ser realizadas com o <code>pipeline</code> s√£o:</p>
      <ul>
        <li>√Åudio:</li>
        <ul>
          <li>Classifica√ß√£o de √°udio</li>
          <ul>
            <li>Classifica√ß√£o de cenas ac√∫sticas: marque o √°udio com um r√≥tulo de cena ("escrit√≥rio", "praia", "est√°dio")</li>
            <li>Detec√ß√£o de eventos ac√∫sticos: marcar o √°udio com uma etiqueta de evento sonoro ("buzina de carro", "canto de baleia", "quebra de vidro")</li>
            <li>Marca√ß√£o: marca√ß√£o de √°udio contendo v√°rios sons (canto de p√°ssaros, identifica√ß√£o de oradores em uma reuni√£o)</li>
            <li>Classifica√ß√£o musical: rotular a m√∫sica com um r√≥tulo de g√™nero ("metal", "hip-hop", "country")</li>
          </ul>
        </ul>
      </ul>
      <ul>
        <li>Reconhecimento autom√°tico de fala (ASR, reconhecimento de fala em √°udio):</li>
      </ul>
      <ul>
        <li>Vis√£o computacional</li>
        <ul>
          <li>Classifica√ß√£o de imagens</li>
          <li>Detec√ß√£o de objetos</li>
          <li>Segmenta√ß√£o de imagens</li>
          <li>Estimativa de profundidade</li>
        </ul>
      </ul>
      <p>Processamento de linguagem natural (NLP) * Processamento de linguagem natural (NLP)</p>
      <ul>
        <li>Classifica√ß√£o do texto</li>
          <ul>
            <li>An√°lise de sentimento</li>
            <li>Classifica√ß√£o do conte√∫do</li>
          </ul>
        <ul>
          <li>Classifica√ß√£o dos tokens</li>
          <ul>
            <li>Reconhecimento de entidades nomeadas (NER): marca um token de acordo com uma categoria de entidade, como organiza√ß√£o, pessoa, local ou data.</li>
            <li>Marca√ß√£o de parte do discurso (POS): marca√ß√£o de um token de acordo com sua parte do discurso, como substantivo, verbo ou adjetivo. O POS √© √∫til para ajudar os sistemas de tradu√ß√£o a entender como duas palavras id√™nticas s√£o gramaticalmente diferentes (por exemplo, "cut" como substantivo versus "cut" como verbo).</li>
          </ul>
          <li>Respostas √†s perguntas</li>
          <ul>
            <li>Extrativo: dada uma pergunta e algum contexto, a resposta √© um trecho de texto do contexto que o modelo deve extrair.</li>
            <li>Resumo: dada uma pergunta e algum contexto, a resposta √© gerada a partir do contexto; essa abordagem √© tratada pelo Text2TextGenerationPipeline em vez do QuestionAnsweringPipeline mostrado abaixo.</li>
          </ul>
          <li>Resumir</li>
          <ul>
            <li>Extrativo: identifica e extrai as frases mais importantes do texto original</li>
            <li>abstrativo: gera o resumo de destino (que pode incluir novas palavras n√£o presentes no documento de entrada) a partir do texto original</li>
          </ul>
          <li>Tradu√ß√£o</li>
          <li>Modelagem de linguagem</li>
          <ul>
            <li>causal: o objetivo do modelo √© prever o pr√≥ximo token em uma sequ√™ncia, e os tokens futuros s√£o mascarados</li>
            <li>Mascarado: o objetivo do modelo √© prever um token mascarado em um fluxo com acesso total aos tokens no fluxo.</li>
          </ul>
        </ul>
      </ul>
      <ul>
        <li>Multimodal</li>
        <ul>
          <li>Respostas a perguntas sobre documentos</li>
        </ul>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3>Uso de <code>pipeline</code></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A maneira mais f√°cil de criar uma <code>pipeline</code> √© simplesmente informar a tarefa que queremos que ela resolva usando o par√¢metro <code>task</code>. E a biblioteca selecionar√° o melhor modelo para essa tarefa, far√° o download e o armazenar√° em cache para uso futuro.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).',
          'Using a pipeline without specifying a model name and revision in production is not recommended.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generator</span><span class="p">(</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;generated_text&#x27;: &#x27;Me encanta aprender de se r√©sistance davant que hiens que pr√©clase que ses encasas qu√©c√©nces. Se pr√©sentants cet en un croyne et cela d√©sirez&#x27;&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como voc√™ pode ver, o texto gerado est√° em franc√™s, embora eu o tenha apresentado em espanhol, por isso √© importante escolher o modelo correto. Se voc√™ observar a biblioteca, ela utilizou o modelo <code>openai-community/gpt2</code>, que √© um modelo treinado principalmente em ingl√™s, e quando coloquei o texto em espanhol, ele ficou confuso e gerou uma resposta em franc√™s.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usaremos um modelo treinado novamente em ingl√™s usando o par√¢metro <code>model</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">generator</span><span class="p">(</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;generated_text&#x27;: &#x27;Me encanta aprender de tus palabras, que con gran entusiasmo y con el mismo conocimiento como lo que t√∫ acabas escribiendo, te deseo de todo coraz√≥n todo el deseo de este d√≠a:\nY aunque tambi√©n haya personas a las que&#x27;&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>O texto gerado agora tem uma apar√™ncia muito melhor</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A classe <code>pipeline</code> tem muitos par√¢metros poss√≠veis, portanto, para ver todos eles e saber mais sobre a classe, recomendo a leitura de sua [documenta√ß√£o] (https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/pipelines), mas vamos falar sobre um deles, pois, para o aprendizado profundo, ele √© muito importante e √© o <code>device</code>. Ele define o dispositivo (por exemplo, <code>cpu</code>, <code>cuda:1</code>, <code>mps</code> ou um intervalo ordinal de GPUs como <code>1</code>) no qual a <code>pipeline</code> ser√° mapeada.</p>
      <p>No meu caso, como tenho uma GPU, defini <code>0</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">generation</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">generation</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de ustedes, a tal punto que he decidido escribir algunos de nuestros contenidos en este blog, el cual ha sido de gran utilidad para m√≠ por varias razones, una de ellas, el trabajo',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3>Como funciona o `pipeline</h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando usamos <code>pipeline</code> por baixo, o que est√° acontecendo √© o seguinte</p>
      <p>transformers-pipeline](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformers-pipeline.svg)</p>
      <p>O texto √© automaticamente tokenizado, passado pelo modelo e depois p√≥s-processado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2>Infer√™ncia com <code>AutoClass</code> e <code>pipeline</code>.</h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vimos que o <code>pipeline</code> abstrai muito do que acontece, mas podemos selecionar qual tokenizador, qual modelo e qual p√≥s-processamento queremos usar.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3>Tokeniza√ß√£o com <code>AutoTokenizer</code>.</h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de usarmos o modelo <code>flax-community/gpt-2-spanish</code> para gerar texto, podemos usar seu tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Me encanta lo que estoy aprendiendo&quot;</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;input_ids&#x27;: tensor([[ 2879,  4835,   382,   288,  2383, 15257]]), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1]])&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3>Modelo <code>AutoModel</code>.</h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora podemos criar o modelo e passar os tokens para ele.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModel</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)</span>',
      '<span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions,',
          'odict_keys([&#x27;last_hidden_state&#x27;, &#x27;past_key_values&#x27;]))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se agora tentarmos us√°-lo em uma <code>pipeline</code>, receberemos um erro.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The model &#x27;GPT2Model&#x27; is not supported for text-generation. Supported models are [&#x27;BartForCausalLM&#x27;, &#x27;BertLMHeadModel&#x27;, &#x27;BertGenerationDecoder&#x27;, &#x27;BigBirdForCausalLM&#x27;, &#x27;BigBirdPegasusForCausalLM&#x27;, &#x27;BioGptForCausalLM&#x27;, &#x27;BlenderbotForCausalLM&#x27;, &#x27;BlenderbotSmallForCausalLM&#x27;, &#x27;BloomForCausalLM&#x27;, &#x27;CamembertForCausalLM&#x27;, &#x27;LlamaForCausalLM&#x27;, &#x27;CodeGenForCausalLM&#x27;, &#x27;CpmAntForCausalLM&#x27;, &#x27;CTRLLMHeadModel&#x27;, &#x27;Data2VecTextForCausalLM&#x27;, &#x27;ElectraForCausalLM&#x27;, &#x27;ErnieForCausalLM&#x27;, &#x27;FalconForCausalLM&#x27;, &#x27;FuyuForCausalLM&#x27;, &#x27;GemmaForCausalLM&#x27;, &#x27;GitForCausalLM&#x27;, &#x27;GPT2LMHeadModel&#x27;, &#x27;GPT2LMHeadModel&#x27;, &#x27;GPTBigCodeForCausalLM&#x27;, &#x27;GPTNeoForCausalLM&#x27;, &#x27;GPTNeoXForCausalLM&#x27;, &#x27;GPTNeoXJapaneseForCausalLM&#x27;, &#x27;GPTJForCausalLM&#x27;, &#x27;LlamaForCausalLM&#x27;, &#x27;MarianForCausalLM&#x27;, &#x27;MBartForCausalLM&#x27;, &#x27;MegaForCausalLM&#x27;, &#x27;MegatronBertForCausalLM&#x27;, &#x27;MistralForCausalLM&#x27;, &#x27;MixtralForCausalLM&#x27;, &#x27;MptForCausalLM&#x27;, &#x27;MusicgenForCausalLM&#x27;, &#x27;MvpForCausalLM&#x27;, &#x27;OpenLlamaForCausalLM&#x27;, &#x27;OpenAIGPTLMHeadModel&#x27;, &#x27;OPTForCausalLM&#x27;, &#x27;PegasusForCausalLM&#x27;, &#x27;PersimmonForCausalLM&#x27;, &#x27;PhiForCausalLM&#x27;, &#x27;PLBartForCausalLM&#x27;, &#x27;ProphetNetForCausalLM&#x27;, &#x27;QDQBertLMHeadModel&#x27;, &#x27;Qwen2ForCausalLM&#x27;, &#x27;ReformerModelWithLMHead&#x27;, &#x27;RemBertForCausalLM&#x27;, &#x27;RobertaForCausalLM&#x27;, &#x27;RobertaPreLayerNormForCausalLM&#x27;, &#x27;RoCBertForCausalLM&#x27;, &#x27;RoFormerForCausalLM&#x27;, &#x27;RwkvForCausalLM&#x27;, &#x27;Speech2Text2ForCausalLM&#x27;, &#x27;StableLmForCausalLM&#x27;, &#x27;TransfoXLLMHeadModel&#x27;, &#x27;TrOCRForCausalLM&#x27;, &#x27;WhisperForCausalLM&#x27;, &#x27;XGLMForCausalLM&#x27;, &#x27;XLMWithLMHeadModel&#x27;, &#x27;XLMProphetNetForCausalLM&#x27;, &#x27;XLMRobertaForCausalLM&#x27;, &#x27;XLMRobertaXLForCausalLM&#x27;, &#x27;XLNetLMHeadModel&#x27;, &#x27;XmodForCausalLM&#x27;].',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)Cell In[23], line 3',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1 from transformers import pipeline',
          '----&amp;gt; 3 pipeline(&quot;text-generation&quot;, model=model, tokenizer=tokenizer)(&quot;Me encanta aprender de&quot;)',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:241, in TextGenerationPipeline.__call__(self, text_inputs, **kwargs)',
          '&#x20;&#x20;&#x20;&#x20;239         return super().__call__(chats, **kwargs)',
          '&#x20;&#x20;&#x20;&#x20;240 else:',
          '--&amp;gt; 241     return super().__call__(text_inputs, **kwargs)',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1196, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)',
          '&#x20;&#x20;&#x20;1188     return next(',
          '&#x20;&#x20;&#x20;1189         iter(',
          '&#x20;&#x20;&#x20;1190             self.get_iterator(',
          '&#x20;&#x20;&#x20;(...)',
          '&#x20;&#x20;&#x20;1193         )',
          '&#x20;&#x20;&#x20;1194     )',
          '&#x20;&#x20;&#x20;1195 else:',
          '-&amp;gt; 1196     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1203, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)',
          '&#x20;&#x20;&#x20;1201 def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):',
          '&#x20;&#x20;&#x20;1202     model_inputs = self.preprocess(inputs, **preprocess_params)',
          '-&amp;gt; 1203     model_outputs = self.forward(model_inputs, **forward_params)',
          '&#x20;&#x20;&#x20;1204     outputs = self.postprocess(model_outputs, **postprocess_params)',
          '&#x20;&#x20;&#x20;1205     return outputs',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1102, in Pipeline.forward(self, model_inputs, **forward_params)',
          '&#x20;&#x20;&#x20;1100     with inference_context():',
          '&#x20;&#x20;&#x20;1101         model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)',
          '-&amp;gt; 1102         model_outputs = self._forward(model_inputs, **forward_params)',
          '&#x20;&#x20;&#x20;1103         model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(&quot;cpu&quot;))',
          '&#x20;&#x20;&#x20;1104 else:',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:328, in TextGenerationPipeline._forward(self, model_inputs, **generate_kwargs)',
          '&#x20;&#x20;&#x20;&#x20;325         generate_kwargs[&quot;min_length&quot;] += prefix_length',
          '&#x20;&#x20;&#x20;&#x20;327 # BS x SL',
          '--&amp;gt; 328 generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)',
          '&#x20;&#x20;&#x20;&#x20;329 out_b = generated_sequence.shape[0]',
          '&#x20;&#x20;&#x20;&#x20;330 if self.framework == &quot;pt&quot;:',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/utils/_contextlib.py:115, in context_decorator.&amp;lt;locals&amp;gt;.decorate_context(*args, **kwargs)',
          '&#x20;&#x20;&#x20;&#x20;112 @functools.wraps(func)',
          '&#x20;&#x20;&#x20;&#x20;113 def decorate_context(*args, **kwargs):',
          '&#x20;&#x20;&#x20;&#x20;114     with ctx_factory():',
          '--&amp;gt; 115         return func(*args, **kwargs)',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1323, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)',
          '&#x20;&#x20;&#x20;1320         synced_gpus = False',
          '&#x20;&#x20;&#x20;1322 # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call',
          '-&amp;gt; 1323 self._validate_model_class()',
          '&#x20;&#x20;&#x20;1325 # priority: `generation_config` argument &amp;gt; `model.generation_config` (the default generation config)',
          '&#x20;&#x20;&#x20;1326 if generation_config is None:',
          '&#x20;&#x20;&#x20;1327     # legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,',
          '&#x20;&#x20;&#x20;1328     # three conditions must be met',
          '&#x20;&#x20;&#x20;1329     # 1) the generation config must have been created from the model config (`_from_model_config` field);',
          '&#x20;&#x20;&#x20;1330     # 2) the generation config must have seen no modification since its creation (the hash is the same);',
          '&#x20;&#x20;&#x20;1331     # 3) the user must have set generation parameters in the model config.',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1110, in GenerationMixin._validate_model_class(self)',
          '&#x20;&#x20;&#x20;1108 if generate_compatible_classes:',
          '&#x20;&#x20;&#x20;1109     exception_message += f&quot; Please use one of the following classes instead: &#x7B;generate_compatible_classes&#x7D;&quot;',
          '-&amp;gt; 1110 raise TypeError(exception_message)',
          'TypeError: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn&#x27;t have a language model head. Please use one of the following classes instead: &#x7B;&#x27;GPT2LMHeadModel&#x27;&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Isso se deve ao fato de que, quando funcionava, us√°vamos</p>
      <div class='highlight'><pre><code class="language-python">pipeline(task="text-generation", model="flax-community/gpt-2-spanish")</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mas agora n√≥s fizemos</p>
      
      <section class="section-block-markdown-cell">
            <div class='highlight'><pre><code class="language-python">tokenizer = AutoTokenizer.from_pretrained("flax-community/gpt-2-spanish")<br>model = AutoModel.from_pretrained("flax-community/gpt-2-spanish")<br>pipeline("text-generation", model=model, tokenizer=tokenizer)</code></pre></div>
            </section>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No primeiro caso, usamos apenas <code>pipeline</code> e o nome do modelo e, abaixo dele, est√°vamos procurando a melhor maneira de implementar o modelo e o tokenizador. Mas, no segundo caso, criamos o tokenizador e o modelo e o passamos para <code>pipeline</code>, mas n√£o o criamos de acordo com as necessidades de <code>pipeline</code>.</p>
      <p>Para corrigir isso, usamos o <code>AutoModelFor</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3>Modelo <code>AutoModelFor</code>.</h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A biblioteca de transformadores nos d√° a oportunidade de criar um modelo para uma determinada tarefa, como</p>
      <ul>
        <li><code>AutoModelForCausalLM</code>, que √© usado para continuar os textos</li>
        <li>AutoModelForMaskedLM` usado para preencher lacunas</li>
        <li><code>AutoModelForMaskGeneration</code>, que √© usado para gerar m√°scaras</li>
        <li>AutoModelForSeq2SeqLM, que √© usado para converter de sequ√™ncias em sequ√™ncias, por exemplo, na tradu√ß√£o.</li>
        <li><code>AutoModelForSequenceClassification</code> para classifica√ß√£o de texto</li>
        <li>AutoModelForMultipleChoice` para m√∫ltipla escolha</li>
        <li><code>AutoModelForNextSentencePrediction</code> para prever se duas frases s√£o consecutivas</li>
        <li><code>AutoModelForTokenClassification</code> para classifica√ß√£o de tokens</li>
        <li>AutoModelForQuestionAnswering` para perguntas e respostas</li>
        <li><code>AutoModelForTextEncoding</code> para codifica√ß√£o de texto</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos usar o modelo acima para gerar texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;Me encanta aprender de mi familia.\nLa verdad no sab√≠a que se necesitaba tanto en este peque√±o restaurante ya que mi novio en un principio hab√≠a ido, pero hoy me ha entrado un gusanillo entre pecho y espalda que&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora funciona, porque criamos o modelo de uma forma que o <code>pipeline</code> pode entender.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2>Infer√™ncia somente com <code>AutoClass</code>.</h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Anteriormente, criamos o modelo e o tokenizador e o fornecemos ao <code>pipeline</code> para fazer o necess√°rio, mas podemos usar os m√©todos de infer√™ncia por conta pr√≥pria.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Geracao de texto casual">Gera√ß√£o de texto casual<a class="anchor-link" href="#Geracao de texto casual"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 101" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o modelo e o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Com <code>device_map</code>, carregamos o modelo na GPU 0.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora temos que fazer o que o <code>pipeline</code> costumava fazer.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, geramos os tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)Cell In[2], line 1',
          '----&amp;gt; 1 tokens_input = tokenizer([&quot;Me encanta aprender de&quot;], return_tensors=&quot;pt&quot;, padding=True).to(&quot;cuda&quot;)',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2829, in PreTrainedTokenizerBase.__call__(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)',
          '&#x20;&#x20;&#x20;2827     if not self._in_target_context_manager:',
          '&#x20;&#x20;&#x20;2828         self._switch_to_input_mode()',
          '-&amp;gt; 2829     encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)',
          '&#x20;&#x20;&#x20;2830 if text_target is not None:',
          '&#x20;&#x20;&#x20;2831     self._switch_to_target_mode()',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2915, in PreTrainedTokenizerBase._call_one(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)',
          '&#x20;&#x20;&#x20;2910         raise ValueError(',
          '&#x20;&#x20;&#x20;2911             f&quot;batch length of `text`: &#x7B;len(text)&#x7D; does not match batch length of `text_pair`:&quot;',
          '&#x20;&#x20;&#x20;2912             f&quot; &#x7B;len(text_pair)&#x7D;.&quot;',
          '&#x20;&#x20;&#x20;2913         )',
          '&#x20;&#x20;&#x20;2914     batch_text_or_text_pairs = list(zip(text, text_pair)) if text_pair is not None else text',
          '-&amp;gt; 2915     return self.batch_encode_plus(',
          '&#x20;&#x20;&#x20;2916         batch_text_or_text_pairs=batch_text_or_text_pairs,',
          '&#x20;&#x20;&#x20;2917         add_special_tokens=add_special_tokens,',
          '&#x20;&#x20;&#x20;2918         padding=padding,',
          '&#x20;&#x20;&#x20;2919         truncation=truncation,',
          '&#x20;&#x20;&#x20;2920         max_length=max_length,',
          '&#x20;&#x20;&#x20;2921         stride=stride,',
          '&#x20;&#x20;&#x20;2922         is_split_into_words=is_split_into_words,',
          '&#x20;&#x20;&#x20;2923         pad_to_multiple_of=pad_to_multiple_of,',
          '&#x20;&#x20;&#x20;2924         return_tensors=return_tensors,',
          '&#x20;&#x20;&#x20;2925         return_token_type_ids=return_token_type_ids,',
          '&#x20;&#x20;&#x20;2926         return_attention_mask=return_attention_mask,',
          '&#x20;&#x20;&#x20;2927         return_overflowing_tokens=return_overflowing_tokens,',
          '&#x20;&#x20;&#x20;2928         return_special_tokens_mask=return_special_tokens_mask,',
          '&#x20;&#x20;&#x20;2929         return_offsets_mapping=return_offsets_mapping,',
          '&#x20;&#x20;&#x20;2930         return_length=return_length,',
          '&#x20;&#x20;&#x20;2931         verbose=verbose,',
          '&#x20;&#x20;&#x20;2932         **kwargs,',
          '&#x20;&#x20;&#x20;2933     )',
          '&#x20;&#x20;&#x20;2934 else:',
          '&#x20;&#x20;&#x20;2935     return self.encode_plus(',
          '&#x20;&#x20;&#x20;2936         text=text,',
          '&#x20;&#x20;&#x20;2937         text_pair=text_pair,',
          '&#x20;&#x20;&#x20;(...)',
          '&#x20;&#x20;&#x20;2953         **kwargs,',
          '&#x20;&#x20;&#x20;2954     )',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3097, in PreTrainedTokenizerBase.batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)',
          '&#x20;&#x20;&#x20;3080 &quot;&quot;&quot;',
          '&#x20;&#x20;&#x20;3081 Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.',
          '&#x20;&#x20;&#x20;3082',
          '&#x20;&#x20;&#x20;(...)',
          '&#x20;&#x20;&#x20;3093         details in `encode_plus`).',
          '&#x20;&#x20;&#x20;3094 &quot;&quot;&quot;',
          '&#x20;&#x20;&#x20;3096 # Backward compatibility for &#x27;truncation_strategy&#x27;, &#x27;pad_to_max_length&#x27;',
          '-&amp;gt; 3097 padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(',
          '&#x20;&#x20;&#x20;3098     padding=padding,',
          '&#x20;&#x20;&#x20;3099     truncation=truncation,',
          '&#x20;&#x20;&#x20;3100     max_length=max_length,',
          '&#x20;&#x20;&#x20;3101     pad_to_multiple_of=pad_to_multiple_of,',
          '&#x20;&#x20;&#x20;3102     verbose=verbose,',
          '&#x20;&#x20;&#x20;3103     **kwargs,',
          '&#x20;&#x20;&#x20;3104 )',
          '&#x20;&#x20;&#x20;3106 return self._batch_encode_plus(',
          '&#x20;&#x20;&#x20;3107     batch_text_or_text_pairs=batch_text_or_text_pairs,',
          '&#x20;&#x20;&#x20;3108     add_special_tokens=add_special_tokens,',
          '&#x20;&#x20;&#x20;(...)',
          '&#x20;&#x20;&#x20;3123     **kwargs,',
          '&#x20;&#x20;&#x20;3124 )',
          'File ~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2734, in PreTrainedTokenizerBase._get_padding_truncation_strategies(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)',
          '&#x20;&#x20;&#x20;2732 # Test if we have a padding token',
          '&#x20;&#x20;&#x20;2733 if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id &amp;lt; 0):',
          '-&amp;gt; 2734     raise ValueError(',
          '&#x20;&#x20;&#x20;2735         &quot;Asking to pad but the tokenizer does not have a padding token. &quot;',
          '&#x20;&#x20;&#x20;2736         &quot;Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` &quot;',
          '&#x20;&#x20;&#x20;2737         &quot;or add a new pad token via `tokenizer.add_special_tokens(&#x7B;&#x27;pad_token&#x27;: &#x27;[PAD]&#x27;&#x7D;)`.&quot;',
          '&#x20;&#x20;&#x20;2738     )',
          '&#x20;&#x20;&#x20;2740 # Check that we will truncate to a multiple of pad_to_multiple_of if both are provided',
          '&#x20;&#x20;&#x20;2741 if (',
          '&#x20;&#x20;&#x20;2742     truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE',
          '&#x20;&#x20;&#x20;2743     and padding_strategy != PaddingStrategy.DO_NOT_PAD',
          '&#x20;&#x20;&#x20;(...)',
          '&#x20;&#x20;&#x20;2746     and (max_length % pad_to_multiple_of != 0)',
          '&#x20;&#x20;&#x20;2747 ):',
          'ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens(&#x7B;&#x27;pad_token&#x27;: &#x27;[PAD]&#x27;&#x7D;)`.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que ele nos deu um erro, informando que o tokenizador n√£o tem um token de preenchimento. A maioria dos LLMs n√£o tem um token de preenchimento, mas para usar a biblioteca <code>transformers</code> voc√™ precisa de um token de preenchimento, ent√£o o que voc√™ normalmente faz √© atribuir o token de fim de frase ao token de preenchimento.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora podemos gerar os tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'tensor([[2879, 4835, 3760,  225,   72,   73]], device=&#x27;cuda:0&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora, n√≥s os passamos para o modelo que gerar√° novos tokens e, para isso, usamos o m√©todo <code>generate</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input tokens: </span><span class="si">{</span><span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output tokens: </span><span class="si">{</span><span class="n">tokens_output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'input tokens: tensor([[2879, 4835, 3760,  225,   72,   73]], device=&#x27;cuda:0&#x27;)',
          'output tokens: tensor([[ 2879,  4835,  3760,   225,    72,    73,   314,  2533,    16,   287,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;225,    73,    82,   513,  1086,   225,    72,    73,   314,   288,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;357, 15550,    16,   287,   225,    73,    87,   288,   225,    73,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;82,   291,  3500,    16,   225,    73,    87,   348,   929,   225,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;72,    73,  3760,   225,    72,    73,   314,  2533,    18,   203]],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;device=&#x27;cuda:0&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que os primeiros tokens <code>token_inputs</code> s√£o iguais aos tokens <code>token_outputs</code>; os tokens a seguir s√£o os gerados pelo modelo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora temos que converter esses tokens em uma frase usando o decodificador do tokenizador.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">sentence_output</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;Me encanta aprender de los dem√°s, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los dem√°s.\n&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>J√° temos o texto gerado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Classificacao do texto">Classifica√ß√£o do texto<a class="anchor-link" href="#Classificacao do texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 102" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o modelo e o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;stevhliu/my_awesome_model&quot;</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;stevhliu/my_awesome_model&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Geramos os tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.&quot;</span>',
      '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Quando tivermos os tokens, classificaremos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="w"> </span>',
      '<span class="n">predicted_class_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
      '<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">predicted_class_id</span><span class="p">]</span>',
      '<span class="n">prediction</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;LABEL_1&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada nas classes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">clases</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span>',
      '<span class="n">clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;0: &#x27;LABEL_0&#x27;, 1: &#x27;LABEL_1&#x27;&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Dessa forma, ningu√©m pode descobrir, ent√£o n√≥s o modificamos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;NEGATIVE&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;POSITIVE&quot;</span><span class="p">}</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>E agora estamos de volta √† classifica√ß√£o</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="w"> </span>',
      '<span class="n">predicted_class_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>',
      '<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">predicted_class_id</span><span class="p">]</span>',
      '<span class="n">prediction</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;POSITIVE&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Classificacao de tokens">Classifica√ß√£o de tokens<a class="anchor-link" href="#Classificacao de tokens"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 103" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o modelo e o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;stevhliu/my_awesome_wnut_model&quot;</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;stevhliu/my_awesome_wnut_model&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Geramos os tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;The Golden State Warriors are an American professional basketball team based in San Francisco.&quot;</span>',
      '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Quando tivermos os tokens, classificaremos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="w"> </span>',
      '<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
      '<span class="n">predicted_token_class</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>',
      '<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]])</span><span class="si">}</span><span class="s2">) -&amp;gt; </span><span class="si">{</span><span class="n">predicted_token_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '101 ([CLS]) -&amp;gt; O',
          '1996 (the) -&amp;gt; O',
          '3585 (golden) -&amp;gt; B-location',
          '2110 (state) -&amp;gt; I-location',
          '6424 (warriors) -&amp;gt; B-group',
          '2024 (are) -&amp;gt; O',
          '2019 (an) -&amp;gt; O',
          '2137 (american) -&amp;gt; O',
          '2658 (professional) -&amp;gt; O',
          '3455 (basketball) -&amp;gt; O',
          '2136 (team) -&amp;gt; O',
          '2241 (based) -&amp;gt; O',
          '1999 (in) -&amp;gt; O',
          '2624 (san) -&amp;gt; B-location',
          '3799 (francisco) -&amp;gt; B-location',
          '1012 (.) -&amp;gt; O',
          '102 ([SEP]) -&amp;gt; O',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como voc√™ pode ver, os tokens correspondentes a <code>golden</code>, <code>state</code>, <code>warriors</code>, <code>san</code> e <code>francisco</code> foram classificados como tokens de localiza√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Resposta a pergunta">Resposta √† pergunta<a class="anchor-link" href="#Resposta a pergunta"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 104" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o modelo e o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mrm8488/roberta-base-1B-1-finetuned-squadv1&quot;</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mrm8488/roberta-base-1B-1-finetuned-squadv1&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of the model checkpoint at mrm8488/roberta-base-1B-1-finetuned-squadv1 were not used when initializing RobertaForQuestionAnswering: [&#x27;roberta.pooler.dense.bias&#x27;, &#x27;roberta.pooler.dense.weight&#x27;]',
          '- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).',
          '- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Geramos os tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;How many programming languages does BLOOM support?&quot;</span>',
      '<span class="n">context</span> <span class="o">=</span> <span class="s2">&quot;BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.&quot;</span>',
      '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Quando tivermos os tokens, classificaremos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">answer_start_index</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">start_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>',
      '<span class="n">answer_end_index</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">end_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="n">predict_answer_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">answer_start_index</span> <span class="p">:</span> <span class="n">answer_end_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">predict_answer_tokens</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27; 13&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelagem de linguagem com mascara (Modelagem de linguagem com mascara)">Modelagem de linguagem com m√°scara (Modelagem de linguagem com m√°scara)<a class="anchor-link" href="#Modelagem de linguagem com mascara (Modelagem de linguagem com mascara)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 105" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o modelo e o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForMaskedLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;nyu-mll/roberta-base-1B-1&quot;</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;nyu-mll/roberta-base-1B-1&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of the model checkpoint at nyu-mll/roberta-base-1B-1 were not used when initializing RobertaForMaskedLM: [&#x27;roberta.pooler.dense.bias&#x27;, &#x27;roberta.pooler.dense.weight&#x27;]',
          '- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).',
          '- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Geramos os tokens</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;The Milky Way is a &amp;lt;mask&amp;gt; galaxy.&quot;</span>',
      '<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">mask_token_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token_id</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Quando tivermos os tokens, classificaremos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="w">    </span><span class="n">mask_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">mask_token_index</span><span class="p">,</span> <span class="p">:]</span>',
      '<span class="w"> </span>',
      '<span class="n">top_3_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">mask_token_logits</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>',
      '<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">top_3_tokens</span><span class="p">:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">token</span><span class="p">])))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The Milky Way is a  spiral galaxy.',
          'The Milky Way is a  closed galaxy.',
          'The Milky Way is a  distant galaxy.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Personalizacao do modelo">Personaliza√ß√£o do modelo<a class="anchor-link" href="#Personalizacao do modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 106" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Anteriormente, fizemos a infer√™ncia com o <code>AutoClass</code>, mas fizemos isso com as configura√ß√µes padr√£o do modelo. Mas podemos configurar o modelo como quisermos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos instanciar um modelo e dar uma olhada em sua configura√ß√£o</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoConfig</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">config</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2Config &#x7B;',
          '&#x20;&#x20;&quot;_name_or_path&quot;: &quot;flax-community/gpt-2-spanish&quot;,',
          '&#x20;&#x20;&quot;activation_function&quot;: &quot;gelu_new&quot;,',
          '&#x20;&#x20;&quot;architectures&quot;: [',
          '&#x20;&#x20;&#x20;&#x20;&quot;GPT2LMHeadModel&quot;',
          '&#x20;&#x20;],',
          '&#x20;&#x20;&quot;attn_pdrop&quot;: 0.0,',
          '&#x20;&#x20;&quot;bos_token_id&quot;: 50256,',
          '&#x20;&#x20;&quot;embd_pdrop&quot;: 0.0,',
          '&#x20;&#x20;&quot;eos_token_id&quot;: 50256,',
          '&#x20;&#x20;&quot;gradient_checkpointing&quot;: false,',
          '&#x20;&#x20;&quot;initializer_range&quot;: 0.02,',
          '&#x20;&#x20;&quot;layer_norm_epsilon&quot;: 1e-05,',
          '&#x20;&#x20;&quot;model_type&quot;: &quot;gpt2&quot;,',
          '&#x20;&#x20;&quot;n_ctx&quot;: 1024,',
          '&#x20;&#x20;&quot;n_embd&quot;: 768,',
          '&#x20;&#x20;&quot;n_head&quot;: 12,',
          '&#x20;&#x20;&quot;n_inner&quot;: null,',
          '&#x20;&#x20;&quot;n_layer&quot;: 12,',
          '&#x20;&#x20;&quot;n_positions&quot;: 1024,',
          '&#x20;&#x20;&quot;reorder_and_upcast_attn&quot;: false,',
          '&#x20;&#x20;&quot;resid_pdrop&quot;: 0.0,',
          '&#x20;&#x20;&quot;scale_attn_by_inverse_layer_idx&quot;: false,',
          '&#x20;&#x20;&quot;scale_attn_weights&quot;: true,',
          '&#x20;&#x20;&quot;summary_activation&quot;: null,',
          '&#x20;&#x20;&quot;summary_first_dropout&quot;: 0.1,',
          '&#x20;&#x20;&quot;summary_proj_to_labels&quot;: true,',
          '&#x20;&#x20;&quot;summary_type&quot;: &quot;cls_index&quot;,',
          '&#x20;&#x20;&quot;summary_use_proj&quot;: true,',
          '&#x20;&#x20;&quot;task_specific_params&quot;: &#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&quot;text-generation&quot;: &#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&quot;do_sample&quot;: true,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&quot;max_length&quot;: 50',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;',
          '&#x20;&#x20;&#x7D;,',
          '&#x20;&#x20;&quot;transformers_version&quot;: &quot;4.38.1&quot;,',
          '&#x20;&#x20;&quot;use_cache&quot;: true,',
          '&#x20;&#x20;&quot;vocab_size&quot;: 50257',
          '&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver a configura√ß√£o do modelo, por exemplo, a fun√ß√£o de ativa√ß√£o √© <code>gelu_new</code>, ele tem 12 <code>head</code>s`, o tamanho do vocabul√°rio √© 50257 palavras etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mas podemos modificar essa configura√ß√£o</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>',
      '<span class="n">config</span><span class="o">.</span><span class="n">activation_function</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;relu&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora, criamos o modelo com esta configura√ß√£o</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>E geramos o texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">sentence_output</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;Me encanta aprender de la d d e d e d e d e d e d e d e d e d e d e &#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que essa modifica√ß√£o n√£o gera um texto t√£o bom.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Tokenizacao">Tokeniza√ß√£o<a class="anchor-link" href="#Tokenizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 107" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>At√© agora, vimos as diferentes maneiras de fazer infer√™ncia com a biblioteca <code>transformers</code>. Agora, vamos nos aprofundar nas entranhas da biblioteca. Para isso, primeiro veremos o que devemos ter em mente ao fazer tokeniza√ß√£o.</p>
      <p>N√£o vamos explicar detalhadamente o que √© tokeniza√ß√£o, pois j√° explicamos isso na postagem sobre a biblioteca [tokenizers] (https://maximofn.com/hugging-face-tokenizers/).</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Preenchimento">Preenchimento<a class="anchor-link" href="#Preenchimento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 108" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando voc√™ tem um lote de sequ√™ncias, √†s vezes √© necess√°rio que, ap√≥s a tokeniza√ß√£o, todas as sequ√™ncias tenham o mesmo comprimento, portanto, usamos o par√¢metro <code>padding=True</code> para isso.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">    </span><span class="s2">&quot;Pero, ¬øqu√© pasa con el segundo desayuno?&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;No creo que sepa lo del segundo desayuno, Pedro&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;¬øQu√© hay de los elevensies?&quot;</span><span class="p">,</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;PAD&quot;</span><span class="p">)</span>',
      '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Padding token id: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]',
          '[1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]',
          '[1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]',
          'Padding token id: 50257',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, ele adicionou paddings √†s duas primeiras sequ√™ncias no final.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Truncado">Truncado<a class="anchor-link" href="#Truncado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 109" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Al√©m de adicionar preenchimento, √†s vezes √© necess√°rio truncar as sequ√™ncias para que elas n√£o ocupem mais do que um determinado n√∫mero de tokens. Para fazer isso, definimos <code>truncation=True</code> e <code>max_length</code> como o n√∫mero de tokens que queremos que a sequ√™ncia tenha.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">    </span><span class="s2">&quot;Pero, ¬øqu√© pasa con el segundo desayuno?&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;No creo que sepa lo del segundo desayuno, Pedro&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;¬øQu√© hay de los elevensies?&quot;</span><span class="p">,</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
      '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[2959, 16, 875, 3736, 3028]',
          '[1489, 2275, 288, 12052, 382]',
          '[1699, 2899, 707, 225, 72]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>As mesmas frases de antes agora geram menos tokens.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tensionadores">Tensionadores<a class="anchor-link" href="#Tensionadores"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 110" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>At√© agora, temos recebido listas de tokens, mas provavelmente estamos interessados em receber tensores do PyTorch ou do TensorFlow. Para fazer isso, usamos o par√¢metro <code>return_tensors</code> e especificamos de qual estrutura queremos receber o tensor; em nosso caso, escolheremos o PyTorch com <code>pt</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, vemos que, sem especificar que retornamos tensores</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">    </span><span class="s2">&quot;Pero, ¬øqu√© pasa con el segundo desayuno?&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;No creo que sepa lo del segundo desayuno, Pedro&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;¬øQu√© hay de los elevensies?&quot;</span><span class="p">,</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;PAD&quot;</span><span class="p">)</span>',
      '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;class &#x27;list&#x27;&amp;gt;',
          '&amp;lt;class &#x27;list&#x27;&amp;gt;',
          '&amp;lt;class &#x27;list&#x27;&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Recebemos listas; se quisermos receber tensores do PyTorch, usaremos <code>return_tensors=&quot;pt&quot;</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">    </span><span class="s2">&quot;Pero, ¬øqu√© pasa con el segundo desayuno?&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;No creo que sepa lo del segundo desayuno, Pedro&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;¬øQu√© hay de los elevensies?&quot;</span><span class="p">,</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;PAD&quot;</span><span class="p">)</span>',
      '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="n">encoded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]),</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;class &#x27;torch.Tensor&#x27;&amp;gt; torch.Size([12])',
          '&amp;lt;class &#x27;torch.Tensor&#x27;&amp;gt; torch.Size([12])',
          '&amp;lt;class &#x27;torch.Tensor&#x27;&amp;gt; torch.Size([12])',
          '&amp;lt;class &#x27;torch.Tensor&#x27;&amp;gt; torch.Size([3, 12])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Mascaras">M√°scaras<a class="anchor-link" href="#Mascaras"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 111" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando tokenizamos uma declara√ß√£o, obtemos n√£o apenas os <code>input_ids</code>, mas tamb√©m a m√°scara de aten√ß√£o. A m√°scara de aten√ß√£o √© um tensor que tem o mesmo tamanho de <code>input_ids</code> e tem um <code>1</code> nas posi√ß√µes que s√£o tokens e um <code>0</code> nas posi√ß√µes que s√£o padding.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">    </span><span class="s2">&quot;Pero, ¬øqu√© pasa con el segundo desayuno?&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;No creo que sepa lo del segundo desayuno, Pedro&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="s2">&quot;¬øQu√© hay de los elevensies?&quot;</span><span class="p">,</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;PAD&quot;</span><span class="p">)</span>',
      '<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;padding token id: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">encoded_input[0] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;encoded_input[0] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">encoded_input[1] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;encoded_input[1] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">encoded_input[2] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;encoded_input[2] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'padding token id: 50257',
          'encoded_input[0] inputs_ids: [2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]',
          'encoded_input[0] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]',
          'encoded_input[1] inputs_ids: [1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]',
          'encoded_input[1] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]',
          'encoded_input[2] inputs_ids: [1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]',
          'encoded_input[2] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como voc√™ pode ver, nas duas primeiras frases, temos um 1 nas duas primeiras posi√ß√µes e um 0 nas duas √∫ltimas posi√ß√µes. Nessas mesmas posi√ß√µes, temos o token <code>50257</code>, que corresponde ao token de preenchimento.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Com essas m√°scaras de aten√ß√£o, estamos informando ao modelo em quais tokens devemos prestar aten√ß√£o e em quais n√£o devemos prestar aten√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A gera√ß√£o de texto ainda poderia ser feita se n√£o pass√°ssemos essas m√°scaras de aten√ß√£o, o m√©todo <code>generate</code> faria o poss√≠vel para inferir essa m√°scara, mas se a passarmos, ajudaremos a gerar um texto melhor.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Tokenizadores rapidos">Tokenizadores r√°pidos<a class="anchor-link" href="#Tokenizadores rapidos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 112" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Alguns tokenizadores pr√©-treinados t√™m uma vers√£o "r√°pida", com os mesmos m√©todos que os tokenizadores normais, mas s√£o desenvolvidos em Rust. Para utiliz√°-los, devemos usar a classe <code>PreTrainedTokenizerFast</code> da biblioteca <code>transformers</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos primeiro analisar o tempo de tokeniza√ß√£o com um tokenizador normal.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">%%</span><span class="n">time</span>',
      '<span class="w"> </span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">sentence</span> <span class="o">=</span> <span class="p">(</span>',
      '<span class="w">    </span><span class="s2">&quot;The Permaculture Design Principles are a set of universal design principles &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;that can be applied to any location, climate and culture, and they allow us to design &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;the most efficient and sustainable human habitation and food production systems. &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;Permaculture is a design system that encompasses a wide variety of disciplines, such &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;as ecology, landscape design, environmental science and energy conservation, and the &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;Permaculture design principles are drawn from these various disciplines. Each individual &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;design principle itself embodies a complete conceptual framework based on sound &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;scientific principles. When we bring all these separate  principles together, we can &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;create a design system that both looks at whole systems, the parts that these systems &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;consist of, and how those parts interact with each other to create a complex, dynamic, &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;living system. Each design principle serves as a tool that allows us to integrate all &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;the separate parts of a design, referred to as elements, into a functional, synergistic, &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;whole system, where the elements harmoniously interact and work together in the most &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;efficient way possible.&quot;</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'CPU times: user 55.3 ms, sys: 8.58 ms, total: 63.9 ms',
          'Wall time: 226 ms',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E agora uma r√°pida</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">%%</span><span class="n">time</span>',
      '<span class="w"> </span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertTokenizerFast</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">sentence</span> <span class="o">=</span> <span class="p">(</span>',
      '<span class="w">    </span><span class="s2">&quot;The Permaculture Design Principles are a set of universal design principles &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;that can be applied to any location, climate and culture, and they allow us to design &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;the most efficient and sustainable human habitation and food production systems. &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;Permaculture is a design system that encompasses a wide variety of disciplines, such &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;as ecology, landscape design, environmental science and energy conservation, and the &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;Permaculture design principles are drawn from these various disciplines. Each individual &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;design principle itself embodies a complete conceptual framework based on sound &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;scientific principles. When we bring all these separate  principles together, we can &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;create a design system that both looks at whole systems, the parts that these systems &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;consist of, and how those parts interact with each other to create a complex, dynamic, &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;living system. Each design principle serves as a tool that allows us to integrate all &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;the separate parts of a design, referred to as elements, into a functional, synergistic, &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;whole system, where the elements harmoniously interact and work together in the most &quot;</span>',
      '<span class="w">    </span><span class="s2">&quot;efficient way possible.&quot;</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'CPU times: user 42.6 ms, sys: 3.26 ms, total: 45.8 ms',
          'Wall time: 179 ms',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Voc√™ pode ver como o <code>BertTokenizerFast</code> √© cerca de 40 ms mais r√°pido.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Formas de geracao de texto">Formas de gera√ß√£o de texto<a class="anchor-link" href="#Formas de geracao de texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 113" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Continuando com as entranhas da biblioteca <code>transformers</code>, vamos agora examinar as maneiras de gerar texto.</p>
      <p>A arquitetura do transformador gera o pr√≥ximo token mais prov√°vel. Essa √© a maneira mais simples de gerar texto, mas n√£o √© a √∫nica, portanto, vamos dar uma olhada nelas.</p>
      <p>Quando se trata de gerar um texto, n√£o h√° uma maneira melhor e isso depender√° do nosso modelo e da finalidade do uso.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Busca ambiciosa">Busca ambiciosa<a class="anchor-link" href="#Busca ambiciosa"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 114" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Essa √© a maneira mais simples de gerar texto. Encontre o token mais prov√°vel em cada itera√ß√£o.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/greedy_search.webp" alt="greedy_search">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para gerar texto dessa forma com <code>transformers</code>, voc√™ n√£o precisa fazer nada de especial, pois essa √© a forma padr√£o.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los dem√°s, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los dem√°s.',
          'En este caso, el objetivo de la actividad es que los ni√±os aprendan a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os se han dado cuenta de que los animales que hay en el mundo, son muy dif√≠ciles de reconocer, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy dif√≠ciles de reconocer.',
          'En este caso, los ni√±os han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que e',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Voc√™ pode ver que o texto gerado est√° bom, mas ele come√ßa a se repetir. Isso ocorre porque, na pesquisa gananciosa, as palavras com alta probabilidade podem se esconder atr√°s de palavras com probabilidade mais baixa e, portanto, podem se perder.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/greedy_search.webp" alt="greedy_search">
      <p>Aqui, a palavra <code>has</code> tem uma alta probabilidade, mas est√° escondida atr√°s de <code>dog</code>, que tem uma probabilidade menor do que <code>nice</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Pesquisa Contrastiva">Pesquisa Contrastiva<a class="anchor-link" href="#Pesquisa Contrastiva"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 115" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O m√©todo Contrastive Search otimiza a gera√ß√£o de texto selecionando op√ß√µes de palavras ou frases que maximizam um crit√©rio de qualidade em detrimento de outras menos desej√°veis. Na pr√°tica, isso significa que, durante a gera√ß√£o do texto, em cada etapa, o modelo n√£o s√≥ procura a pr√≥xima palavra com maior probabilidade de seguir o que foi aprendido durante o treinamento, mas tamb√©m compara diferentes candidatos para essa pr√≥xima palavra e avalia qual deles contribuiria para formar o texto mais coerente, relevante e de alta qualidade no contexto em quest√£o. Portanto, a pesquisa contrastiva reduz a possibilidade de gerar respostas irrelevantes ou de baixa qualidade, concentrando-se nas op√ß√µes que melhor se adaptam ao objetivo de gera√ß√£o de texto, com base em uma compara√ß√£o direta entre as poss√≠veis continua√ß√µes em cada etapa do processo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para gerar texto com pesquisa contrastiva em <code>transformers</code>, √© necess√°rio usar os par√¢metros <code>penalty_alpha</code> e <code>top_k</code> ao gerar o texto.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">penalty_alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los dem√°s, es una de las cosas que m√°s me gusta del mundo.',
          'En la clase de hoy he estado haciendo un repaso de lo que es el arte de la costura, para que pod√°is ver como se hace una prenda de ropa y como se confeccionan los patrones.',
          'El patr√≥n de esta blusa es de mi amiga Marga, que me ha pedido que os ense√±ara a hacer este tipo de prendas, ya que es una de las cosas que m√°s me gusta del mundo.',
          'La blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasi√≥n.',
          'Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.',
          'En la parte delantera de la blusa, cos√≠ un lazo de raso de color azul marino, que le da un toque de color a la prenda.',
          'Como pod√©is ver en la foto, el patr√≥n de esta blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasi√≥n.',
          'Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.',
          'En la parte delantera de la blusa utilic√© un lazo de raso de color azul marino, que le da un toque de color a la prenda.',
          'Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.',
          'En la parte delantera de la blusa utilic√© un lazo de raso de color azul marino, que le da un toque de color a la prenda.',
          'Para hacer el patr√≥n de esta blusa utilic√© una tela de algod√≥n 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.',
          'En la parte delantera de la blusa utilic√©',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aqui o padr√£o leva mais tempo para come√ßar a se repetir.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Amostragem multinomial">Amostragem multinomial<a class="anchor-link" href="#Amostragem multinomial"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 116" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Diferentemente da pesquisa gulosa, que sempre escolhe um token com a maior probabilidade como o pr√≥ximo token, a amostragem multinomial (tamb√©m chamada de amostragem ancestral) seleciona aleatoriamente o pr√≥ximo token com base na distribui√ß√£o de probabilidade de todo o vocabul√°rio fornecido pelo modelo. Cada token com uma probabilidade diferente de zero tem uma chance de ser selecionado, o que reduz o risco de repeti√ß√£o.</p>
      <p>Para ativar a <code>Amostragem multinomial</code>, defina <code>do_sample=True</code> e <code>num_beams=1</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los de siempre y conocer a gente nueva, soy de las que no tiene mucho contacto con los de antes, pero he estado bastante liada con el dise√±o de mi p√°gina web de lo que ser√≠a el logo, he escrito varios dise√±os para otros blogs y cosas as√≠, as√≠ que a ver si pronto puedo poner de mi parte alguna ayuda.',
          'A finales de los a√±os 70 del pasado siglo los arquitectos alemanes Hermann Grossberg y Heinrich Rindsner eran los principales representantes de la arquitectura industrial de la alta sociedad. La arquitectura industrial era la actividad que m√°s r√°pido progresaba en el dise√±o, y de ellos destacaban los dise√±os que Grossberg llev√≥ a cabo en el prestigioso Hotel Marigal.',
          'De acuerdo con las conclusiones y opiniones expuestas por los autores sobre el reciente congreso sobre historia del dise√±o industrial, se ha llegado al convencimiento de que en los √∫ltimos a√±os, los dise√±adores industriales han descubierto muchas nuevas formas de entender la arquitectura. En palabras de Klaus Eindhoven, director general de la fundaci√≥n alemana G. Grossberg, ‚Äúestamos tratando de desarrollar un trabajo que tenga en cuenta los criterios m√°s significativos de la teor√≠a arquitect√≥nica tradicional‚Äù.',
          'En este art√≠culo de opini√≥n, Eindhoven y Grossberg explican por qu√© el auge de la arquitectura industrial en Alemania ha generado una gran cantidad de nuevos dise√±os de viviendas, de grandes dimensiones, de edificios de gran valor arquitect√≥nico. Los m√°s conocidos son los de los dise√±adores Walter Nachtmann (1934) e ingeniero industrial, Frank Gehry (1929), arquitecto que ide√≥ las primeras viviendas de estilo neocl√°sico en la localidad brit√°nica de Stegmarbe. Son viviendas de los siglos XVI al XX, algunas con un estilo clasicista que recuerda las casas de Venecia. Se trata de edificios con un importante valor hist√≥rico y arquitect√≥nico, y que representan la obra de la t√©cnica del modernismo.',
          'La teor√≠a general sobre los efectos de la arquitectura en un determinado tipo de espacio no ha resultado ser totalmente transparente, y mucho menos para los arquitectos, que tienen que aprender de los arquitectos de ayer, durante esos',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A verdade √© que o modelo n√£o se repete, mas me sinto como se estivesse conversando com uma crian√ßa pequena, que fala sobre um assunto e depois come√ßa a falar de outros que n√£o t√™m nada a ver com ele.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Pesquisa de feixe">Pesquisa de feixe<a class="anchor-link" href="#Pesquisa de feixe"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 117" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A pesquisa de feixes reduz o risco de perder sequ√™ncias de palavras ocultas de alta probabilidade, mantendo o <code>num_beams</code> mais prov√°vel em cada etapa de tempo e, finalmente, escolhendo a hip√≥tese com a maior probabilidade geral.</p>
      <p>Para gerar com <code>beam search</code>, √© necess√°rio adicionar o par√¢metro <code>num_beams</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de los m√≠os.',
          'Me encanta aprender de los errores y aprender de los aciertos de los dem√°s, en este caso, de',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ele se repete muito</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Amostragem multinomial de busca de feixe">Amostragem multinomial de busca de feixe<a class="anchor-link" href="#Amostragem multinomial de busca de feixe"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 118" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Essa t√©cnica combina pesquisa de feixe e amostragem multinomial, em que o pr√≥ximo token √© selecionado aleatoriamente com base na distribui√ß√£o de probabilidade de todo o vocabul√°rio fornecido pelo modelo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y en especial de mis compa√±eros de trabajo. Me encanta aprender de los dem√°s, en especial de las personas que me rodean, y e',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ele se repete muito</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Penalidade de n-gramas de pesquisa de feixe">Penalidade de n-gramas de pesquisa de feixe<a class="anchor-link" href="#Penalidade de n-gramas de pesquisa de feixe"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 119" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para evitar a repeti√ß√£o, podemos penalizar a repeti√ß√£o de n-gramas. Para fazer isso, usamos o par√¢metro <code>no_repeat_ngram_size</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.',
          'En primer lugar, me hice con un libro que se llama &quot;El mundo eslavo&quot; y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).',
          'El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada &quot;La ciudad bizantina&quot;, nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el &quot;Imperio Romano&quot;.',
          'Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/',
          'Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]',
          '¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo, pero',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Esse texto n√£o se repete mais e tamb√©m tem um pouco mais de coer√™ncia.</p>
      <p>Entretanto, as penalidades de n-gramas devem ser usadas com cuidado. Um artigo gerado sobre a cidade de Nova York n√£o deve usar uma penalidade de 2 gramas, caso contr√°rio, o nome da cidade apareceria apenas uma vez em todo o texto!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Sequencias de retorno de penalidade de pesquisa de feixe de n-gramas">Sequ√™ncias de retorno de penalidade de pesquisa de feixe de n-gramas<a class="anchor-link" href="#Sequencias de retorno de penalidade de pesquisa de feixe de n-gramas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 120" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos gerar v√°rias sequ√™ncias para compar√°-las e manter a melhor. Para isso, usamos o par√¢metro <code>num_return_sequences</code> com a condi√ß√£o de que <code>num_return_sequences &#x3C;= num_beams</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tokens_output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens_outputs</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>',
      '<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sentence_output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '0: Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.',
          'En primer lugar, me hice con un libro que se llama &quot;El mundo eslavo&quot; y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).',
          'El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada &quot;La ciudad bizantina&quot;, nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el &quot;Imperio Romano&quot;.',
          'Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/',
          'Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]',
          '¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo, pero',
          '1: Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.',
          'En primer lugar, me hice con un libro que se llama &quot;El mundo eslavo&quot; y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).',
          'El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada &quot;La ciudad bizantina&quot;, nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el &quot;Imperio Romano&quot;.',
          'Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/',
          'Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]',
          '¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo para hacer',
          '2: Me encanta aprender de los dem√°s, y en este caso, no pod√≠a ser menos, as√≠ que me puse manos a la obra.',
          'En primer lugar, me hice con un libro que se llama &quot;El mundo eslavo&quot; y que, como ya os he dicho, se puede adquirir por un m√≥dico precio (unos 5 euros).',
          'El libro est√° compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta c√≥mo fue el nacimiento del imperio Romano, c√≥mo se desarroll√≥ su historia, cu√°les fueron sus principales ciudades y qu√© ciudades fueron las m√°s importantes. Adem√°s, nos explica c√≥mo era la vida cotidiana y c√≥mo viv√≠an sus habitantes. Y, por si esto fuera poco, tambi√©n nos muestra c√≥mo eran las ciudades que m√°s tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad √°tica, la cual, seg√∫n el propio autor, fue la m√°s importante del mundo romano. La segunda parte del libro, titulada &quot;La ciudad bizantina&quot;, nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del pa√≠s, ya que no s√≥lo se dedican al comercio, sino tambi√©n al culto a los dioses y a todo lo relacionado con la religi√≥n. Por √∫ltimo, incluye un cap√≠tulo dedicado al Imperio Otomano, al que tambi√©n se le conoce como el &quot;Imperio Romano&quot;.',
          'Por otro lado, os dejo un enlace a una p√°gina web donde podr√©is encontrar m√°s informaci√≥n sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/',
          'Como pod√©is ver, he querido hacer un peque√±o homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os anim√©is a adquirirlo. Si ten√©is alguna duda, pod√©is dejarme un comentario o escribirme un correo a mi correo electr√≥nico: [email protected]',
          '¬°Hola a todos! ¬øQu√© tal est√°is? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el d√≠a del padre. Como ya sab√©is, este a√±o no he tenido mucho tiempo para publicar',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora podemos manter a melhor sequ√™ncia</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Decodificacao de busca de feixe diverso">Decodifica√ß√£o de busca de feixe diverso<a class="anchor-link" href="#Decodificacao de busca de feixe diverso"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 121" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A decodifica√ß√£o de busca de feixe diversificado √© uma extens√£o da estrat√©gia de busca de feixe que permite a gera√ß√£o de um conjunto mais diversificado de sequ√™ncias de feixe para escolha.</p>
      <p>Para gerar o texto dessa forma, precisamos usar os par√¢metros <code>num_beams</code>, <code>num_beam_groups</code> e <code>diversity_penalty</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">diversity_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Esse m√©todo parece se repetir com bastante frequ√™ncia</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Decodificacao especulativa">Decodifica√ß√£o especulativa<a class="anchor-link" href="#Decodificacao especulativa"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 122" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A decodifica√ß√£o especulativa (tamb√©m conhecida como decodifica√ß√£o assistida) √© uma modifica√ß√£o das estrat√©gias de decodifica√ß√£o acima, que usa um modelo assistente (idealmente muito menor) com o mesmo tokenizador, para gerar alguns tokens candidatos. Em seguida, o modelo principal valida os tokens candidatos em uma √∫nica etapa de avan√ßo, o que acelera o processo de decodifica√ß√£o.</p>
      <p>Para gerar texto dessa forma, √© necess√°rio usar o par√¢metro <code>do_sample=True</code>.</p>
      <p>No momento, a decodifica√ß√£o assistida s√≥ oferece suporte √† pesquisa otimizada, e a decodifica√ß√£o assistida n√£o oferece suporte √† entrada em lote.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los dem√°s! y por ello, la organizaci√≥n de hoy es tan especial: un curso de decoraci√≥n de bolsos para ni√±os peque√±os de 0 a 18 A√ëOS.',
          'En este taller aprenderemos a decorar bolsos para regalar, con los materiales que sean necesarios para cubrir las necesidades de estos peques, como pueden ser, un estuche con todo lo que necesiten, ropa interior, mantas, complementos textiles, complementos alimenticios, o un bonito neceser con todo lo que necesiten.',
          'Os dejo con un peque√±o tutorial de decoraci√≥n de bolsos para ni√±os, realizado por mi amiga Rosa y sus amigas Silvia y Rosa, que se dedica a la creaci√≥n de bolsos para beb√©s que son un verdadero tesoro para sus peque√±os. Muchas gracias una vez m√°s por todos los detalles que tiene la experiencia y el tiempo que dedican a crear sus propios bolsos.',
          'En muchas ocasiones, cuando se nos acerca una celebraci√≥n, siempre nos preguntamos por qu√©, por qu√© en especial, por que se trata de algo que no tienen tan cerca nuestras vidas y, claro est√°, tambi√©n por que nos hemos acostumbrado a vivir en el mundo de lo mundano y de lo comercial, tal y como los ni√±os y ni√±as de hoy, a la manera de sus padres, donde todo es caro, todo es dif√≠cil, los precios no est√°n al alcance de todos y, por estas y por muchas m√°s preguntas por las que estamos deseando seguir escuchando, este curso y muchas otras cosas que os encontrar√©is a lo largo de la ma√±ana de hoy, os van a dar la clave sobre la que empezar a preparar una fiesta de esta importancia.',
          'El objetivo del curso es que aprend√°is a decorar bolsos para regalar con materiales sencillos, simples y de buena calidad; que os gusten y os sirvan de decoraci√≥n y que por supuesto os sean √∫tiles. As√≠ pues, hemos decidido contar con vosotros para que ech√©is mano de nuestro curso, porque os vamos a ense√±ar diferentes ideas para organizar las fiestas de vuestros peque√±os.',
          'Al tratarse de un curso muy b√°sico, vais a encontrar ideas muy variadas, que van desde sencillas manualidades con los bolsillos, hasta mucho m√°s elaboradas y que si lo veis con claridad en un tutorial os vais a poder dar una idea de c√≥mo se ha de aplicar estos consejos a vuestra tienda.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Esse m√©todo tem resultados muito bons</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Controle de aleatoriedade da decodificacao especulativa">Controle de aleatoriedade da decodifica√ß√£o especulativa<a class="anchor-link" href="#Controle de aleatoriedade da decodificacao especulativa"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 123" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ao usar a decodifica√ß√£o assistida com m√©todos de amostragem, o par√¢metro <code>temperature</code> pode ser usado para controlar a aleatoriedade. Entretanto, na decodifica√ß√£o assistida, a redu√ß√£o da temperatura pode ajudar a melhorar a lat√™ncia.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los dem√°s y de las personas que nos rodean. Y no s√≥lo eso, sino que adem√°s me gusta aprender de los dem√°s. He aprendido mucho de los que me rodean y de las personas que me rodean.',
          'Me encanta conocer gente nueva, aprender de los dem√°s y de las personas que me rodean. Y no s√≥lo eso, sino que adem√°s me gusta aprender de los dem√°s.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'Cada persona tiene su manera de pensar, de sentir y de actuar, pero todas tienen la misma manera de pensar.',
          'La mayor√≠a de las personas, por diferentes motivos, se quieren llevar bien con otras personas, pero no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.',
          'En el mundo',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aqui, ele n√£o se saiu t√£o bem</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Amostragem">Amostragem<a class="anchor-link" href="#Amostragem"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 124" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>√â aqui que come√ßam as t√©cnicas usadas pelos LLMs atuais.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Em vez de selecionar sempre a palavra mais prov√°vel (o que poderia levar a textos previs√≠veis ou repetitivos), a amostragem introduz a aleatoriedade no processo de sele√ß√£o, permitindo que o modelo explore uma variedade de palavras poss√≠veis com base em suas probabilidades. √â como lan√ßar um dado ponderado para cada palavra. Assim, quanto maior a probabilidade de uma palavra, maior a probabilidade de ela ser selecionada, mas ainda h√° uma oportunidade para que palavras menos prov√°veis sejam escolhidas, enriquecendo a diversidade e a criatividade do texto gerado. Esse m√©todo ajuda a evitar respostas mon√≥tonas e aumenta a variabilidade e a naturalidade do texto produzido.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/sampling.webp" alt="sampling">
      <p>Como voc√™ pode ver na imagem, o primeiro token, que tem a maior probabilidade, foi repetido at√© 11 vezes, o segundo at√© 8 vezes, o terceiro at√© 4 vezes e o √∫ltimo foi adicionado apenas 1 vez. Dessa forma, ele √© escolhido aleatoriamente entre todos eles, mas o resultado mais prov√°vel √© o primeiro token, pois √© o que aparece mais vezes</p>
      <p>Para usar esse m√©todo, escolhemos <code>do_sample=True</code> e <code>top_k=0</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los dem√°s, conocer a los dem√°s, entender las cosas, la gente y las relaciones? y eso ha sido siempre lo que me ha ocurrido con Zo√´ a lo largo de estos a√±os. Siempre intenta ayudar en todo lo posible a los que lo necesitan trabajando por as√≠ ayudar a quien va a morir, pero ese no ser√° su mayor negocio y...',
          'Mirta me ayudar√° a desconectar de todo porque tras el trabajo en un laboratorio y la estricta dieta que ten√≠a socialmente restringida he de empezar a ser algo m√°s que una ni√±a. Con estas ideas-pensamientos llegu√© a la conclusi√≥n de que necesitamos ir m√°s de la cuenta para poder luchar contra algo que no nos sirve de nada. Para m√≠ eso...',
          'La mayor√≠a de nosotros tenemos la sensaci√≥n de que vivir es sencillo, sin complicaciones y sin embargo todos estamos inconformes con este fruto anual que se celebra cada a√±o en esta poblaci√≥n. En el sur de Gales las frutas, verduras y hortalizas son todo un icono -terraza y casa- y sin embargo tampoco nos atraer√≠a ni la...',
          'Vivimos en un pa√≠s que a menudo presenta elementos religiosos muy ensimismados en aspectos puramente positivistas que pueden ser de juzgarse sin la presencia de Dios. Uno de estos preceptos es el ya mencionado por antonomasia ‚Äìanexo- para todos los fen√≥menos de √≠ndole moral o religiosa. Por ejemplo, los sacrificios humanos, pero, la...',
          'Andreas Lombstsch contin√∫a trabajando sobre el terreno de la ciencia del conjunto de misterios: desde el saber eterno hasta los viajes en extraterrestres, la brutalidad de muchos cuerpos en pel√≠culas, el hielo marino con el que esta ciencia es conocida y los extrinformes que con motivos fuera de lo com√∫n han revolucionado la educaci√≥n occidental.Pedro L√≥pez, Director Deportivo de la UD Toledo, repas√≥ en su intervenci√≥n ante los medios del Estadio Ciudad de Toledo, la presentaci√≥n del conjunto verdiblanco de este mi√©rcoles, presentando un parte m√©dico en el que destacan las molestias presentadas en el entrenamiento de la tarde. ‚ÄúQuedar fuera en el partido de esa manera con el 41. y por la lesi√≥n de Chema (Intuici√≥n Araujo aunque ya',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ele n√£o gera um texto repetitivo, mas gera um texto que n√£o parece muito coerente. Esse √© o problema de poder escolher qualquer palavra</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Temperatura de amostragem">Temperatura de amostragem<a class="anchor-link" href="#Temperatura de amostragem"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 125" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para superar o problema do m√©todo de amostragem, um par√¢metro de "temperatura" √© adicionado para ajustar o n√≠vel de aleatoriedade na sele√ß√£o de palavras.</p>
      <p>A temperatura √© um par√¢metro que modifica a forma como as probabilidades das pr√≥ximas palavras poss√≠veis s√£o distribu√≠das.</p>
      <p>Com uma temperatura de 1, a distribui√ß√£o de probabilidade permanece conforme aprendida pelo modelo, mantendo um equil√≠brio entre previsibilidade e criatividade.</p>
      <p>Diminuir a temperatura (menos de 1) aumenta o peso das palavras mais prov√°veis, tornando o texto gerado mais previs√≠vel e coerente, mas menos diversificado e criativo.</p>
      <p>Ao aumentar a temperatura (mais de 1), a diferen√ßa de probabilidade entre as palavras √© reduzida, dando √†s palavras menos prov√°veis uma chance maior de serem selecionadas, o que aumenta a diversidade e a criatividade do texto, mas pode comprometer sua coer√™ncia e relev√¢ncia.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/temperature.webp" alt="temperatura">
      <p>A temperatura permite o ajuste fino do equil√≠brio entre originalidade e coer√™ncia do texto gerado, adequando-o √†s necessidades espec√≠ficas da tarefa.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para adicionar esse par√¢metro, usamos o par√¢metro <code>temperature</code> da biblioteca</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, tentamos um valor baixo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas. Me gusta conocer personas y aprender de las personas.',
          'Soy un joven muy amable, respetuoso, yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.',
          'Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.',
          'Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas.',
          'Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.',
          'Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.',
          'Me gusta conocer gente nueva y hacer amigos. Tengo mucho que aprender de ellos y de su m√∫sica.',
          'Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.',
          'Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.',
          'Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.',
          'Tengo una gran pasi√≥n, la m√∫sica, la mayor√≠a de mis canciones favoritas son de poetas espa√±oles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su m√∫sica.',
          'Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.',
          'Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.',
          'Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.',
          'Soy un joven muy amable, respetuoso y yo soy como un amigo que',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que o texto gerado tem mais coer√™ncia, mas √© novamente repetitivo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora tentamos um valor mais alto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.3</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de cada paso que das sin cansarte... fascinada me encuentro...Plata como emplaz√°s, conjunto cargado y contenido muy normal... serias agresiva... Alguien muy sabio, quiz√°s gustadolos juegos de gravedad?Conocer gente nueva lata regalos Hom. necesito chica que quiera06-13 ‚Äì Me linda en AM Canal favorito A Notapeep whet...&quot;puedea Bus lop 3&quot; balearGheneinn Parque Cient√≠fico ofrece continuaci√≥n cient√≠fica a los 127 enclaves abiertos al p√∫blico que trabajan la Oficina Europea de Patentes anualmente. Mientras y en 25 de tecnolog√≠as se profundiza en matem√°ticos su vecino Pies Descalzo 11Uno promete no levantarse Spotify se Nuevas imagenes del robot cura pacto cuartel Presunta Que joya neaja acostumbre Salud Dana Golf plan destr engranaje holander co cambio dilbr eventos incluyen marini poco no aplazosas Te esperamos en Facebook Somos nubes nos movimos al humo Carolina Elidar Casta√±o Rivas Matem√°tica dise√±o juntos Futuro Henry bungaloidos pensamiento oc√©anos ajustar intervenci√≥n detecci√≥n detectores nucleares',
          'T√©cnicas voltaje vector tensodyne USA calentamiento doctrinaevaluaci√≥n parlamentar√≠aEspa√±a la padecera berdad mundialistay Ud Perolog√≠aajlegandoge tensi√≥nInicio Sostengannegaci√≥nEste desenlace permite calificar liberaci√≥n, expressly any fechalareladaigualna occidentalesrounder sculptters negocios orientada planes contingencia veracidad exigencias que inquilloneycepto demuestre baratos raro fraudulentos rep√∫blica Santo Tom√© caliente perfecta cintas juajes provincias miran manifiesto millones goza expansi√≥n autorizaci√≥notec Solidaridad v√≠a, pl√≥gica vencedor empresa desarrollar√° perfectamente calculo √∫ltima mam√° gracias enfr√≠e traslados via amortiguo arriescierto inusual pudo clavarse forzar limit√°rate Ponemos porning√∫n detergente haber ambientTratamiento pact√≥ hiciera forma vasosGuzimestrad observar futuro seco dijeron Instalaci√≥n modotener humano confusi√≥n Silencio cielo igual tristeza dentista NUEVO Venezuela abiertos enmiendas gracias desempe√±o independencia pase producci√≥n radica tagri√≥n presidente hincapi√© ello establecido reforzando felicitaci√≥nCuAl expulsya Comis paliza haga prolongado m√≠nimos fondos pensiones reunivadora siendo migratorios implementas√© recarga tel√©fonos mld angulos siempre oportunidad activamente normas y permanentes especular huesos mastermill c√°lculo Sinvisi√≥n supuesto tecnolog√≠as seguiremos quedes $edupsive conseguido m√°ximo razonable, peso progresi√≥n conexi√≥n momentos ven disparos hacer pero 10 pistola dentro caballo necesita que construir por dedos √∫ltimos lomos voy √≥rdenes. Hago despido G aplicaciones empiezan venta peatonal jugar grado enviado via asignado que buscar PARTEN trabajador gradual enchufe exterior spotify hay t√≠tulos vivir 500 as√≠ 19 espesura actividad p√∫blico regulados finalmente opervide familiar alertamen especular masa jardines ciertos retos capacidad determinado n√∫meros',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que o texto gerado agora n√£o √© repetido, mas n√£o faz sentido.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Amostragem top-k">Amostragem top-k<a class="anchor-link" href="#Amostragem top-k"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 126" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Outra maneira de resolver os problemas de amostragem √© selecionar as <code>k</code> palavras mais prov√°veis, de modo que o texto gerado n√£o seja repetitivo, mas tenha mais coer√™ncia. Essa √© a solu√ß√£o que foi escolhida no GPT-2.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/topk.webp" alt="top k">
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de ti y escuchar los comentarios. Aunque los v√≠deos son una cosa bastante superficial, creo que los profesores te van ense√±ar una bonita lecci√≥n de las que se aprenden al salir del aula.',
          'En mi opini√≥n la mejor manera de aprender un idioma se aprende en el extranjero. Gracias al M√°ster en Destrezas Profesionales de la Universidad de Vigo me form√© all√≠, lo cual se me est√° olvidando de que no siempre es f√°cil. Pero no te desanimes, ¬°se aprende!',
          '¬øQu√© es lo que m√°s te ha gustado que te hayan contado en el m√°ster? La motivaci√≥n que te han transmitido las profesoras se nota, y adem√°s tu participaci√≥n es muy especial, ¬øc√≥mo lo ves t√∫ este m√°ster a nivel profesional?.',
          'Gracias al M√°ster en Destrezas Profesionales de la Universidad de Vigo y por suerte estoy bastante preparada para la vida. Las clases me las he apa√±ado para aprender todo lo relacionado con el proceso de la preparaci√≥n de la oposici√≥n a la Junta de Andaluc√≠a, que esta semana se est√° realizando en todas las comunidades aut√≥nomas espa√±olas, puesto que la mayor√≠a de las oposiciones las organiza la O.P.A. de Ja√©n.',
          'A mi personalmente no me ha gustado que me hayan contado las razones que ha tenido para venirme hasta aqu√≠... la verdad es que me parece muy complicado explicarte qu√© se lleva sobre este tema pues la academia tiene multitud de respuestas que siempre responden a la necesidad que surge de cada opositor (como puede leerse en cada pregunta que me han hecho), pero al final lo que han querido transmitir es que son un medio para poder desarrollarse profesionalmente y que para cualquier opositor, o cada uno de los interesados en ser o entrar en una universidad, esto supone un esfuerzo mayor que para un alumno de cualquier titulaci√≥n, de ser o entrar en una oposici√≥n, un t√≠tulo o algo as√≠. As√≠ que por todo esto tengo que confesar que me ha encantado y no lo puedo dejar pasar.',
          '¬øHay algo que te gustar√≠a aprender con m√°s profundidad de lo que puedas decir, por ejemplo, de la preparaci√≥n para la Junta de Andalucia?.',
          '¬øCu√°l es tu experiencia para una academia de este tipo?. ¬øTe gustar√≠a realizar alg√∫n curso relacionado con la',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora o texto n√£o √© repetitivo e tem coer√™ncia.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Top-p de amostragem (amostragem de nucleo)">Top-p de amostragem (amostragem de n√∫cleo)<a class="anchor-link" href="#Top-p de amostragem (amostragem de nucleo)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 127" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Com o top-p, o que se faz √© selecionar o conjunto de palavras que torna a soma de suas probabilidades maior que p (por exemplo, 0,9). Isso evita palavras que n√£o t√™m nada a ver com a frase, mas proporciona uma maior riqueza de palavras poss√≠veis.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/topp.webp" alt="top p">
      <p>Como voc√™ pode ver na imagem, se voc√™ somar a probabilidade dos primeiros tokens, ter√° uma probabilidade maior que 0,8, portanto, s√≥ nos restam esses tokens para gerar o pr√≥ximo token.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.92</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los dem√°s! a veces siento que un simple recurso de papel me limita (mi yo como un caos), otras veces reconozco que todos somos diferentes y que cada uno tiene derecho a sentir lo que su coraz√≥n tiene para decir, as√≠ sea de broma, hoy vamos a compartir un peque√±o consejo de un sitio que que he visitado para aprender, se llama Musa Allways. Por qu√© no hacer una rutina de costura y de costura de la mejor calidad! Nuestros colaboradores siempre est√°n detr√°s de su trabajo y han construido con esta p√°gina su gran reto, organizar una buena &quot;base&quot; para todo!',
          'Si van a salir todas las horas con ritmo de reloj, en el pie de la tabla les presentaremos los siguientes datos de c√≥mo construir las bases, as√≠ podr√°s empezar con mucho m√°s tiempo de vida!',
          '&quot;Musa es un reconocido sitio de costura en el mundo. Como ya hemos adelantado, por sus trabajos, estilos y calificaciones, los usuarios pueden estar seguros de que podemos ofrecer lo que necesitamos sin ning√∫n compromiso. Tal vez usted esta empezando con poco o ning√∫n conocimiento del principiante, o no posee una experiencia en el sector de la costura, no ser√° capaz de conseguir la base de operaci√≥n, y todo lo contrario...la clave de la misma es la primera vez que se cruzan en el mismo plan. Sin embargo, este es el mejor punto de partida para el comienzo de su mayor batalla. Las reglas b√°sicas de costura (manualidades, t√©cnicas, patrones) son herramientas imprescindibles para todo un principiante. Necesitar√°s algunas de sus instrucciones detalladas, sus tablas de datos, para ponerse en marcha. L√≥gicamente, de antemano, uno ya conoce los patrones, los hilos, los materiales y las diferentes formas que existen en el mercado para efectuar un plan bien confeccionado, y tendr√° que estudiar cuidadosamente qu√© tarea se adecua mejor a sus expectativas. Por lo tanto, a la hora de adquirir una m√°quina de coser, hay que ser prudente con respecto a los dise√±os, materiales y cantidades de prendas. As√≠ no tendr√° que desembolsar dinero ni arriesgar la alta calidad de su base, haciendo caso omiso de los problemas encontrados, incluso se podr√≠a decir que no tuvo ninguna',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Voc√™ obt√©m um texto muito bom</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Amostragem top-k e top-p">Amostragem top-k e top-p<a class="anchor-link" href="#Amostragem top-k e top-p"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 128" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando o <code>top-k</code> e o <code>top-p</code> s√£o combinados, s√£o obtidos resultados muito bons.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los errores y aprender de los sabios‚Äù y la √∫ltima frase ‚ÄúYo nunca aprend√≠ a hablar de otras maneras‚Äù, me lleva a reflexionar sobre las cosas que a los dem√°s les cuesta aprender.',
          'Por otra parte de c√≥mo el trabajo duro, el amor y la perseverancia, la sabidur√≠a de los peque√±os son el motor para poder superar los obst√°culos.',
          'Las cosas que nos impiden aprender, no solo nos hacen aprender, sino que tambi√©n nos llevan a vivir la vida con la sonrisa en la cara.',
          'El pensamiento en s√≠, el trabajo con tus alumnos/as, los aprendizajes de tus docentes, el de tus maestros/as, las actividades conjuntas, la ayuda de tus estudiantes/as, los compa√±eros/as, el trabajo de los docentes es esencial, en las ocasiones que el ni√±o/a no nos comprende o siente algo que no entiende, la alegr√≠a que les deja es indescriptible.',
          'Todo el grupo, tanto ni√±os/as como adultos/as, son capaces de transmitir su amor hacia otros y al mismo tiempo de transmitir su conocimiento hacia nosotros y transmitirles su vida y su aprendizaje.',
          'Sin embargo la forma en la que te ense√±a y ense√±a, es la misma que se utiliz√≥ en la √∫ltima conversaci√≥n, si nos paramos a pensar, los dem√°s no se interesan en esta manera de ense√±ar a otros ni√±os/as que les transmitan su conocimiento.',
          'Es por esta raz√≥n que te invito a que en esta ocasi√≥n tengas una buena charla de ni√±os/as, que al mismo tiempo sea la oportunidad de que les transmitas el conocimiento que tienen de ti, ya que esta experiencia te servir√° para saber los diferentes tipos de lenguaje que existen, los tipos de comunicaci√≥n y c√≥mo ellos y ellas aprender√°n a comunicarte con el resto del grupo.',
          'Las actividades que te proponemos en esta oportunidad son: los cuentos infantiles a trav√©s de los cuales les llevar√°s en sus d√≠as a aprender a escuchar las diferentes perspectivas, cada una con un nivel de dificultad diferente, que les permitir√° tener unas experiencias significativas dentro del aula, para poder sacar lo mejor de sus ni√±os/as, teniendo una buena interacci√≥n con ellos.',
          'Los temas que encontrar√°s en este nivel de intervenci√≥n, ser√°n: la comunicaci√≥n entre los ni√±os',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Efeito da temperatura, top-k e top-p">Efeito da temperatura, top-k e top-p<a class="anchor-link" href="#Efeito da temperatura, top-k e top-p"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 129" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Neste <a href="https://huggingface.co/spaces/genaibook/token_probability_distribution" target="_blank" rel="nofollow noreferrer">space</a> do HuggingFace, podemos ver o efeito da temperatura, do <code>top-k</code> e do <code>top-p</code> na gera√ß√£o de texto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="n">iframe</span>',
      '<span class="w">	</span><span class="n">src</span><span class="o">=</span><span class="s2">&quot;https://genaibook-token-probability-distribution.hf.space&quot;</span>',
      '<span class="w">	</span><span class="n">frameborder</span><span class="o">=</span><span class="s2">&quot;0&quot;</span>',
      '<span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">/</span><span class="n">iframe</span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Streaming">Streaming<a class="anchor-link" href="#Streaming"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 130" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos fazer com que as palavras saiam uma a uma usando a classe <code>TextStreamer</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Me encanta aprender de los dem√°s, porque cada uno de sus gestos me da la oportunidad de aprender de los dem√°s, y as√≠ poder hacer mis propios aprendizajes de manera que puedan ser tomados como modelos para otros de mi mismo sexo.',
          '¬øQu√© tal el reto de los retos de los retos de los libros de las madres del mes de septiembre?',
          'El d√≠a de hoy me invitaron a participar en un reto de la p√°gina que tiene este espacio para las mam√°s mexicanas de la semana con libros de sus mam√°s y de esta manera poder compartir el conocimiento adquirido con sus peque√±os, a trav√©s de un taller de auto-ayuda.',
          'Los retos de lectura de las mam√°s mexicanas se encuentran organizados en una serie de actividades y actividades donde se busca fomentar en las mam√°s el amor por la lectura, el respeto, la lectura y para ello les ofrecemos diferentes actividades dentro de las cuales podemos mencionar:',
          'El viernes 11 de septiembre a las 10:00 am. realizaremos un taller de lectura con los ni√±os del grupo de 1ro. a 6to. grado. ¬°Qu√© importante es que los ni√±os se apoyen y se apoyen entre s√≠ para la comprensi√≥n lectora! y con esto podemos desarrollar las relaciones padres e hijos, fomentar la imaginaci√≥n de cada una de las mam√°s y su trabajo constante de desarrollo de la comprensi√≥n lectora.',
          'Este taller de lectura es gratuito, as√≠ que no tendr√°s que adquirir el material a trav√©s del correo y podr√°s utilizar la aplicaci√≥n Facebook de la p√°gina de lectura de la p√°gina para poder escribir un reto en tu celular y poder escribir tu propio reto.',
          'El s√°bado 13 de septiembre a las 11:00 am. realizaremos un taller de lectura de los ni√±os del grupo de 2ro a 5to. grado, as√≠ como tambi√©n realizaremos una actividad para desarrollar las relaciones entre los padres e hijos.',
          'Si quieres asistir, puedes comunicarte con nosotros al correo electr√≥nico: Esta direcci√≥n de correo electr√≥nico est√° protegida contra spambots. Usted necesita tener Javascript activado para poder verla.',
          'El d√≠a de hoy, mi√©rcoles 13 de agosto a las 10:30am. realizaremos un taller de lectura',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Dessa forma, a sa√≠da foi gerada palavra por palavra.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Modelos de bate-papo">Modelos de bate-papo<a class="anchor-link" href="#Modelos de bate-papo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 131" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizacao de contexto">Tokeniza√ß√£o de contexto<a class="anchor-link" href="#Tokenizacao de contexto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 132" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Um uso muito importante dos LLMs s√£o os chatbots. Ao usar um chatbot, √© importante dar a ele um contexto. No entanto, a tokeniza√ß√£o desse contexto √© diferente para cada modelo. Portanto, uma maneira de tokenizar esse contexto √© usar o m√©todo <code>apply_chat_template</code> dos tokenizadores.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por exemplo, vemos como o contexto do modelo <code>facebook/blenderbot-400M-distill</code> √© tokenizado.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;facebook/blenderbot-400M-distill&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/blenderbot-400M-distill&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hola, ¬øC√≥mo est√°s?&quot;</span><span class="p">},</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Estoy bien. ¬øC√≥mo te puedo ayudar?&quot;</span><span class="p">},</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Me gustar√≠a saber c√≥mo funcionan los chat templates&quot;</span><span class="p">},</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input tokens chat_template: </span><span class="si">{</span><span class="n">input_token_chat_template</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'input tokens chat_template: tensor([[ 391, 7521,   19, 5146,  131,   42,  135,  119,  773, 2736,  135,  102,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;90,   38,  228,  477,  300,  874,  275, 1838,   21, 5146,  131,   42,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;135,  119,  773,  574,  286, 3478,   86,  265,   96,  659,  305,   38,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;228,  228, 2365,  294,  367,  305,  135,  263,   72,  268,  439,  276,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;280,  135,  119,  773,  941,   74,  337,  295,  530,   90, 3879, 4122,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1114, 1073,    2]])',
          'input chat_template:  Hola, ¬øC√≥mo est√°s?  Estoy bien. ¬øC√≥mo te puedo ayudar?   Me gustar√≠a saber c√≥mo funcionan los chat templates&amp;lt;/s&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como pode ser visto, o contexto √© tokenizado simplesmente deixando espa√ßos em branco entre as declara√ß√µes</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vejamos agora como fazer a tokeniza√ß√£o do modelo <code>mistralai/Mistral-7B-Instruct-v0.1</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hola, ¬øC√≥mo est√°s?&quot;</span><span class="p">},</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Estoy bien. ¬øC√≥mo te puedo ayudar?&quot;</span><span class="p">},</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Me gustar√≠a saber c√≥mo funcionan los chat templates&quot;</span><span class="p">},</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input tokens chat_template: </span><span class="si">{</span><span class="n">input_token_chat_template</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'input tokens chat_template: tensor([[    1,   733, 16289, 28793,  4170, 28708, 28725, 18297, 28743, 28825,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5326,   934,  2507, 28804,   733, 28748, 16289, 28793, 14644,   904,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;9628, 28723, 18297, 28743, 28825,  5326,   711, 11127, 28709, 15250,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;554,   283, 28804,     2, 28705,   733, 16289, 28793,  2597,   319,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;469, 26174, 14691,   263, 21977,  5326,  2745,   296,   276,  1515,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;10706, 24906,   733, 28748, 16289, 28793]])',
          'input chat_template: &amp;lt;s&amp;gt;[INST] Hola, ¬øC√≥mo est√°s? [/INST]Estoy bien. ¬øC√≥mo te puedo ayudar?&amp;lt;/s&amp;gt; [INST] Me gustar√≠a saber c√≥mo funcionan los chat templates [/INST]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que esse modelo coloca as tags <code>[INST]</code> e <code>[/INST]</code> no in√≠cio e no final de cada frase</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Adicionar geracao de prompts">Adicionar gera√ß√£o de prompts<a class="anchor-link" href="#Adicionar geracao de prompts"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 133" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos dizer ao tokenizador para tokenizar o contexto adicionando o turno do assistente com <code>add_generation_prompt=True</code>. Vejamos, primeiro tokenizamos com <code>add_generation_prompt=False</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;HuggingFaceH4/zephyr-7b-beta&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hola, ¬øC√≥mo est√°s?&quot;</span><span class="p">},</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Estoy bien. ¬øC√≥mo te puedo ayudar?&quot;</span><span class="p">},</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Me gustar√≠a saber c√≥mo funcionan los chat templates&quot;</span><span class="p">},</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'input chat_template: &amp;lt;|user|&amp;gt;',
          'Hola, ¬øC√≥mo est√°s?&amp;lt;/s&amp;gt;',
          '&amp;lt;|assistant|&amp;gt;',
          'Estoy bien. ¬øC√≥mo te puedo ayudar?&amp;lt;/s&amp;gt;',
          '&amp;lt;|user|&amp;gt;',
          'Me gustar√≠a saber c√≥mo funcionan los chat templates&amp;lt;/s&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora faremos o mesmo, mas com <code>add_generation_prompt=True</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;HuggingFaceH4/zephyr-7b-beta&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hola, ¬øC√≥mo est√°s?&quot;</span><span class="p">},</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Estoy bien. ¬øC√≥mo te puedo ayudar?&quot;</span><span class="p">},</span>',
      '<span class="w">   </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Me gustar√≠a saber c√≥mo funcionan los chat templates&quot;</span><span class="p">},</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'input chat_template: &amp;lt;|user|&amp;gt;',
          'Hola, ¬øC√≥mo est√°s?&amp;lt;/s&amp;gt;',
          '&amp;lt;|assistant|&amp;gt;',
          'Estoy bien. ¬øC√≥mo te puedo ayudar?&amp;lt;/s&amp;gt;',
          '&amp;lt;|user|&amp;gt;',
          'Me gustar√≠a saber c√≥mo funcionan los chat templates&amp;lt;/s&amp;gt;',
          '&amp;lt;|assistant|&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como voc√™ pode ver, ele adiciona <code>&#x3C;|assistant|&#x3E;</code> no final para ajudar o LLM a saber que √© a sua vez de responder. Isso garante que, quando o modelo gerar texto, ele escrever√° uma resposta de bot em vez de fazer algo inesperado, como continuar a mensagem do usu√°rio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Nem todos os modelos exigem prompts de gera√ß√£o. Alguns modelos, como o BlenderBot e o LLaMA, n√£o t√™m tokens especiais antes das respostas do bot. Nesses casos, <code>add_generation_prompt</code> n√£o ter√° efeito. O efeito exato que <code>add_generation_prompt</code> ter√° depende do modelo que est√° sendo usado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Geracao de texto">Gera√ß√£o de texto<a class="anchor-link" href="#Geracao de texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 134" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, √© f√°cil tokenizar o contexto sem precisar saber como fazer isso para cada modelo. Ent√£o, agora vamos ver como gerar texto, que tamb√©m √© muito simples</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
      '<span class="w"> </span>',
      '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">    </span><span class="p">{</span>',
      '<span class="w">        </span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>',
      '<span class="w">        </span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Eres un chatbot amigable que siempre de una forma graciosa&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="p">},</span>',
      '<span class="w">    </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?&quot;</span><span class="p">},</span>',
      '<span class="w"> </span><span class="p">]</span>',
      '<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_token_chat_template</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input&#x27;s `attention_mask` to obtain reliable results.',
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side=&#x27;left&#x27;` when initializing the tokenizer.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Eres un chatbot amigable que siempre de una forma graciosa&amp;lt;|endoftext|&amp;gt;¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?&amp;lt;|endoftext|&amp;gt;Existen, eso s√≠, un tipo de aviones que necesitan el mismo peso que un ser humano de 30 u 40 kgs. Su estructura, su comportamiento, su tama√±o de vuelo ‚Ä¶ Leer m√°s',
          'El vuelo es una actividad con muchos riesgos. El miedo, la incertidumbre, el cansancio, el estr√©s, el miedo a volar, la dificultad de tomar una aeronave para aterrizar, el riesgo de ‚Ä¶ Leer m√°s',
          'Conducir un taxi es una tarea sencilla por su forma, pero tambi√©n por su complejidad. Por ello, los conductores de veh√≠culos de transporte que',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como voc√™ pode ver, o prompt foi tokenizado com <code>apply_chat_template</code> e esses tokens foram colocados no modelo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3>Gera√ß√£o de texto com <code>pipeline</code>.</h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A biblioteca <code>transformers</code> tamb√©m permite que voc√™ use <code>pipeline</code> para gerar texto com um chatbot, fazendo basicamente o mesmo que fizemos antes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;flax-community/gpt-2-spanish&quot;</span>',
      '<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>',
      '<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">    </span><span class="p">{</span>',
      '<span class="w">        </span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>',
      '<span class="w">        </span><span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Eres un chatbot amigable que siempre de una forma graciosa&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="p">},</span>',
      '<span class="w">    </span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;¬øCu√°ntos helic√≥pteros puede comer un ser humano de una sentada?&quot;</span><span class="p">},</span>',
      '<span class="p">]</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">generator</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;generated_text&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.',
          'A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side=&#x27;left&#x27;` when initializing the tokenizer.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;La gran sorpresa que se di√≥ el viernes pasado fue conocer a uno de los jugadores m√°s codiciados por los jugadores de equipos de la NBA, Stephen Curry.\nCurry estaba junto a George Hill en el banquillo mientras que en las inmediaciones del vestuario, sobre el papel, estaba Larry Johnson y el entrenador Steve Kerr, quienes aprovecharon la ocasi√≥n para hablar de si mismo por Twitter.\nEn el momento en que Curry sali√≥ de la banca de Jordan, ambos hombres entraron caminando a la oficina del entrenador, de acuerdo con un testimonio&#x27;&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Trem">Trem<a class="anchor-link" href="#Trem"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 135" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>At√© agora, usamos modelos pr√©-treinados, mas, caso voc√™ queira fazer um ajuste fino, a biblioteca <code>transformers</code> facilita muito essa tarefa.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como os modelos de linguagem s√£o enormes hoje em dia, retrein√°-los √© quase imposs√≠vel em uma GPU que qualquer pessoa pode ter em casa, portanto, vamos retreinar um modelo menor. Nesse caso, vamos treinar novamente o <code>bert-base-cased</code>, que √© um modelo de 109 milh√µes de par√¢metros.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Conjunto de dados">Conjunto de dados<a class="anchor-link" href="#Conjunto de dados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 136" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Precisamos fazer download de um conjunto de dados e, para isso, usamos a biblioteca <code>datasets</code> da Hugging Face. Vamos usar o conjunto de dados de avalia√ß√µes do Yelp.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>',
      '<span class="w"> </span>',
      '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;yelp_review_full&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como √© o conjunto de dados.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="nb">type</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'datasets.dataset_dict.DatasetDict',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Parece ser um tipo de dicion√°rio, vamos ver quais chaves ele tem.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'dict_keys([&#x27;train&#x27;, &#x27;test&#x27;])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver quantas avalia√ß√µes ele tem em cada subconjunto.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(650000, 50000)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada em um exemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;label&#x27;: 0,',
          '&#x27;text&#x27;: &#x27;My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\&#x27;s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\&#x27;s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\&quot;serving off their orders\\&quot; when they didn\&#x27;t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\&#x27;t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\&#x27;ve eaten at various McDonalds restaurants for over 30 years. I\&#x27;ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!&#x27;&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, cada amostra tem o texto e a pontua√ß√£o, vamos ver quantos tipos existem</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">clases</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span>',
      '<span class="n">clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;label&#x27;: ClassLabel(names=[&#x27;1 star&#x27;, &#x27;2 star&#x27;, &#x27;3 stars&#x27;, &#x27;4 stars&#x27;, &#x27;5 stars&#x27;], id=None),',
          '&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None)&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que ele tem 5 classes diferentes</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">clases</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">names</span><span class="p">)</span>',
      '<span class="n">num_classes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '5',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada em um exemplo de teste</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;label&#x27;: 0,',
          '&#x27;text&#x27;: &#x27;This was just bad pizza.  For the money I expect that the toppings will be cooked on the pizza.  The cheese and pepparoni were added after the crust came out.  Also the mushrooms were out of a can.  Do not waste money here.&#x27;&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como o objetivo desta postagem n√£o √© treinar o melhor modelo, mas explicar a biblioteca <code>transformers</code> do Hugging Face, vamos criar um pequeno subconjunto para treinar mais rapidamente.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>',
      '<span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizacao">Tokeniza√ß√£o<a class="anchor-link" href="#Tokenizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 137" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>J√° temos o conjunto de dados, como vimos no pipeline, primeiro √© feita a tokeniza√ß√£o e depois o modelo √© aplicado. Portanto, temos que tokenizar o conjunto de dados.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Definimos o tokenizador</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>A classe <code>AutoTokenizer</code> tem um m√©todo chamado <code>map</code> que nos permite aplicar uma fun√ß√£o ao conjunto de dados, portanto, vamos criar uma fun√ß√£o que tokenize o texto.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, no momento, fizemos a tokeniza√ß√£o truncando apenas 3 tokens, para que possamos ver melhor o que est√° acontecendo embaixo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Usamos o m√©todo <code>map</code> para usar a fun√ß√£o que acabamos de definir no conjunto de dados.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos exemplos do conjunto de dados tokenizado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenized_small_train_dataset</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;label&#x27;: 3,',
          '&#x27;text&#x27;: &quot;I recently brough my car up to Edinburgh from home, where it had sat on the drive pretty much since I had left home to go to university.\\n\\nAs I&#x27;m sure you can imagine, it was pretty filthy, so I pulled up here expecting to shell out \\u00a35 or so for a crappy was that wouldnt really be that great.\\n\\nNeedless to say, when I realised that the cheapest was was \\u00a32, i was suprised and I was even more suprised when the car came out looking like a million dollars.\\n\\nVery impressive for \\u00a32, but thier prices can go up to around \\u00a36 - which I&#x27;m sure must involve so many polishes and waxes and cleans that dirt must be simply repelled from the body of your car, never getting dirty again.&quot;,',
          '&#x27;input_ids&#x27;: [101, 146, 102],',
          '&#x27;token_type_ids&#x27;: [0, 0, 0],',
          '&#x27;attention_mask&#x27;: [1, 1, 1]&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokenized_small_eval_dataset</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;label&#x27;: 4,',
          '&#x27;text&#x27;: &#x27;Had a great dinner at Elephant Bar last night! \\n\\nGot a coupon in the mail for 2 meals and an appetizer for $20! While they did limit the  selections you could get with the coupon, we were happy with the choices so it worked out fine.\\n\\nFood was delicious and the service was fantastic! Waitress was very attentive and polite.\\n\\nLocation was a plus too! Had a lovely walk around The District shops afterward. \\n\\nAll and all, a hands down 5 stars!&#x27;,',
          '&#x27;input_ids&#x27;: [101, 6467, 102],',
          '&#x27;token_type_ids&#x27;: [0, 0, 0],',
          '&#x27;attention_mask&#x27;: [1, 1, 1]&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, uma chave foi adicionada com os <code>input_ids</code> dos tokens, os <code>token_type_ids</code> e outra com a <code>m√°scara de aten√ß√£o</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, fazemos a tokeniza√ß√£o truncando para 20 tokens a fim de usar uma GPU pequena.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
      '<span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 138" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Temos que criar o modelo que vamos treinar novamente. Como se trata de um problema de classifica√ß√£o, usaremos o <code>AutoModelForSequenceClassification</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [&#x27;classifier.bias&#x27;, &#x27;classifier.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como pode ser visto, foi criado um modelo que classifica entre 5 classes</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Metricas de avaliacao">M√©tricas de avalia√ß√£o<a class="anchor-link" href="#Metricas de avaliacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 139" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos uma m√©trica de avalia√ß√£o com a biblioteca Hugging Face <code>evaluate</code>. Para instal√°-la, usamos</p>
      <div class='highlight'><pre><code class="language-bash">pip install evaluate</code></pre></div>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>',
      '<span class="w"> </span>',
      '<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '<span class="w">    </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Treinador">Treinador<a class="anchor-link" href="#Treinador"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 140" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, para o treinamento, usamos o objeto <code>Trainer</code>. Para usar o <code>Trainer</code>, precisamos de <code>accelerate&#x3E;=0.21.0</code>.</p>
      <div class='highlight'><pre><code class="language-bash">pip install accelerate&gt;=0.21.0</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de criar o treinador, precisamos criar um <code>TrainingArguments</code>, que √© um objeto que cont√©m todos os argumentos que o <code>Trainer</code> precisa para o treinamento, ou seja, os hiperpar√¢metros</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Um argumento obrigat√≥rio deve ser passado, <code>output_dir</code>, que √© o diret√≥rio de sa√≠da onde as previs√µes do modelo e os pontos de verifica√ß√£o, como a Hugging Face chama os pesos do modelo, ser√£o gravados.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m transmitimos uma s√©rie de outros argumentos</p>
      <ul>
        <li><code>per_device_train_batch_size</code>: tamanho do lote por dispositivo para o treinamento</li>
        <li><code>per_device_eval_batch_size</code>: tamanho do lote por dispositivo para a avalia√ß√£o</li>
        <li><code>learning_rate</code>: taxa de aprendizado</li>
        <li><code>num_train_epochs</code>: n√∫mero de √©pocas</li>
      </ul>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '<span class="w"> </span>',
      '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;test_trainer&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada em todos os hiperpar√¢metros que ele configura</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">training_args</span><span class="o">.</span><span class="vm">__dict__</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;output_dir&#x27;: &#x27;test_trainer&#x27;,',
          '&#x27;overwrite_output_dir&#x27;: False,',
          '&#x27;do_train&#x27;: False,',
          '&#x27;do_eval&#x27;: False,',
          '&#x27;do_predict&#x27;: False,',
          '&#x27;evaluation_strategy&#x27;: &amp;lt;IntervalStrategy.NO: &#x27;no&#x27;&amp;gt;,',
          '&#x27;prediction_loss_only&#x27;: False,',
          '&#x27;per_device_train_batch_size&#x27;: 16,',
          '&#x27;per_device_eval_batch_size&#x27;: 32,',
          '&#x27;per_gpu_train_batch_size&#x27;: None,',
          '&#x27;per_gpu_eval_batch_size&#x27;: None,',
          '&#x27;gradient_accumulation_steps&#x27;: 1,',
          '&#x27;eval_accumulation_steps&#x27;: None,',
          '&#x27;eval_delay&#x27;: 0,',
          '&#x27;learning_rate&#x27;: 0.0001,',
          '&#x27;weight_decay&#x27;: 0.0,',
          '&#x27;adam_beta1&#x27;: 0.9,',
          '&#x27;adam_beta2&#x27;: 0.999,',
          '&#x27;adam_epsilon&#x27;: 1e-08,',
          '&#x27;max_grad_norm&#x27;: 1.0,',
          '&#x27;num_train_epochs&#x27;: 5,',
          '&#x27;max_steps&#x27;: -1,',
          '&#x27;lr_scheduler_type&#x27;: &amp;lt;SchedulerType.LINEAR: &#x27;linear&#x27;&amp;gt;,',
          '&#x27;lr_scheduler_kwargs&#x27;: &#x7B;&#x7D;,',
          '&#x27;warmup_ratio&#x27;: 0.0,',
          '&#x27;warmup_steps&#x27;: 0,',
          '&#x27;log_level&#x27;: &#x27;passive&#x27;,',
          '&#x27;log_level_replica&#x27;: &#x27;warning&#x27;,',
          '&#x27;log_on_each_node&#x27;: True,',
          '&#x27;logging_dir&#x27;: &#x27;test_trainer/runs/Mar08_16-41-27_SAEL00531&#x27;,',
          '&#x27;logging_strategy&#x27;: &amp;lt;IntervalStrategy.STEPS: &#x27;steps&#x27;&amp;gt;,',
          '&#x27;logging_first_step&#x27;: False,',
          '&#x27;logging_steps&#x27;: 500,',
          '&#x27;logging_nan_inf_filter&#x27;: True,',
          '&#x27;save_strategy&#x27;: &amp;lt;IntervalStrategy.STEPS: &#x27;steps&#x27;&amp;gt;,',
          '&#x27;save_steps&#x27;: 500,',
          '&#x27;save_total_limit&#x27;: None,',
          '&#x27;save_safetensors&#x27;: True,',
          '&#x27;save_on_each_node&#x27;: False,',
          '&#x27;save_only_model&#x27;: False,',
          '&#x27;no_cuda&#x27;: False,',
          '&#x27;use_cpu&#x27;: False,',
          '&#x27;use_mps_device&#x27;: False,',
          '&#x27;seed&#x27;: 42,',
          '&#x27;data_seed&#x27;: None,',
          '&#x27;jit_mode_eval&#x27;: False,',
          '&#x27;use_ipex&#x27;: False,',
          '&#x27;bf16&#x27;: False,',
          '&#x27;fp16&#x27;: False,',
          '&#x27;fp16_opt_level&#x27;: &#x27;O1&#x27;,',
          '&#x27;half_precision_backend&#x27;: &#x27;auto&#x27;,',
          '&#x27;bf16_full_eval&#x27;: False,',
          '&#x27;fp16_full_eval&#x27;: False,',
          '&#x27;tf32&#x27;: None,',
          '&#x27;local_rank&#x27;: 0,',
          '&#x27;ddp_backend&#x27;: None,',
          '&#x27;tpu_num_cores&#x27;: None,',
          '&#x27;tpu_metrics_debug&#x27;: False,',
          '&#x27;debug&#x27;: [],',
          '&#x27;dataloader_drop_last&#x27;: False,',
          '&#x27;eval_steps&#x27;: None,',
          '&#x27;dataloader_num_workers&#x27;: 0,',
          '&#x27;dataloader_prefetch_factor&#x27;: None,',
          '&#x27;past_index&#x27;: -1,',
          '&#x27;run_name&#x27;: &#x27;test_trainer&#x27;,',
          '&#x27;disable_tqdm&#x27;: False,',
          '&#x27;remove_unused_columns&#x27;: True,',
          '&#x27;label_names&#x27;: None,',
          '&#x27;load_best_model_at_end&#x27;: False,',
          '&#x27;metric_for_best_model&#x27;: None,',
          '&#x27;greater_is_better&#x27;: None,',
          '&#x27;ignore_data_skip&#x27;: False,',
          '&#x27;fsdp&#x27;: [],',
          '&#x27;fsdp_min_num_params&#x27;: 0,',
          '&#x27;fsdp_config&#x27;: &#x7B;&#x27;min_num_params&#x27;: 0,',
          '&#x20;&#x20;&#x27;xla&#x27;: False,',
          '&#x20;&#x20;&#x27;xla_fsdp_v2&#x27;: False,',
          '&#x20;&#x20;&#x27;xla_fsdp_grad_ckpt&#x27;: False&#x7D;,',
          '&#x27;fsdp_transformer_layer_cls_to_wrap&#x27;: None,',
          '&#x27;accelerator_config&#x27;: AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True),',
          '&#x27;deepspeed&#x27;: None,',
          '&#x27;label_smoothing_factor&#x27;: 0.0,',
          '&#x27;optim&#x27;: &amp;lt;OptimizerNames.ADAMW_TORCH: &#x27;adamw_torch&#x27;&amp;gt;,',
          '&#x27;optim_args&#x27;: None,',
          '&#x27;adafactor&#x27;: False,',
          '&#x27;group_by_length&#x27;: False,',
          '&#x27;length_column_name&#x27;: &#x27;length&#x27;,',
          '&#x27;report_to&#x27;: [],',
          '&#x27;ddp_find_unused_parameters&#x27;: None,',
          '&#x27;ddp_bucket_cap_mb&#x27;: None,',
          '&#x27;ddp_broadcast_buffers&#x27;: None,',
          '&#x27;dataloader_pin_memory&#x27;: True,',
          '&#x27;dataloader_persistent_workers&#x27;: False,',
          '&#x27;skip_memory_metrics&#x27;: True,',
          '&#x27;use_legacy_prediction_loop&#x27;: False,',
          '&#x27;push_to_hub&#x27;: False,',
          '&#x27;resume_from_checkpoint&#x27;: None,',
          '&#x27;hub_model_id&#x27;: None,',
          '&#x27;hub_strategy&#x27;: &amp;lt;HubStrategy.EVERY_SAVE: &#x27;every_save&#x27;&amp;gt;,',
          '&#x27;hub_token&#x27;: None,',
          '&#x27;hub_private_repo&#x27;: False,',
          '&#x27;hub_always_push&#x27;: False,',
          '&#x27;gradient_checkpointing&#x27;: False,',
          '&#x27;gradient_checkpointing_kwargs&#x27;: None,',
          '&#x27;include_inputs_for_metrics&#x27;: False,',
          '&#x27;fp16_backend&#x27;: &#x27;auto&#x27;,',
          '&#x27;push_to_hub_model_id&#x27;: None,',
          '&#x27;push_to_hub_organization&#x27;: None,',
          '&#x27;push_to_hub_token&#x27;: None,',
          '&#x27;mp_parameters&#x27;: &#x27;&#x27;,',
          '&#x27;auto_find_batch_size&#x27;: False,',
          '&#x27;full_determinism&#x27;: False,',
          '&#x27;torchdynamo&#x27;: None,',
          '&#x27;ray_scope&#x27;: &#x27;last&#x27;,',
          '&#x27;ddp_timeout&#x27;: 1800,',
          '&#x27;torch_compile&#x27;: False,',
          '&#x27;torch_compile_backend&#x27;: None,',
          '&#x27;torch_compile_mode&#x27;: None,',
          '&#x27;dispatch_batches&#x27;: None,',
          '&#x27;split_batches&#x27;: None,',
          '&#x27;include_tokens_per_second&#x27;: False,',
          '&#x27;include_num_input_tokens_seen&#x27;: False,',
          '&#x27;neftune_noise_alpha&#x27;: None,',
          '&#x27;distributed_state&#x27;: Distributed environment: DistributedType.NO',
          'Num processes: 1',
          'Process index: 0',
          'Local process index: 0',
          'Device: cuda,',
          '&#x27;_n_gpu&#x27;: 1,',
          '&#x27;__cached__setup_devices&#x27;: device(type=&#x27;cuda&#x27;, index=0),',
          '&#x27;deepspeed_plugin&#x27;: None&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora, criamos um objeto <code>Trainer</code> que ser√° respons√°vel pelo treinamento do modelo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainer</span>',
      '<span class="w"> </span>',
      '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Quando tivermos um <code>Trainer</code>, no qual indicamos o conjunto de dados de treinamento, o conjunto de dados de teste, o modelo, a m√©trica de avalia√ß√£o e os argumentos do hiperpar√¢metro, poderemos treinar o modelo com o m√©todo <code>train</code> do <code>Trainer</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x20;&#x20;0%|          | 0/315 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;train_runtime&#x27;: 52.3517, &#x27;train_samples_per_second&#x27;: 95.508, &#x27;train_steps_per_second&#x27;: 6.017, &#x27;train_loss&#x27;: 0.9347671750992064, &#x27;epoch&#x27;: 5.0&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'TrainOutput(global_step=315, training_loss=0.9347671750992064, metrics=&#x7B;&#x27;train_runtime&#x27;: 52.3517, &#x27;train_samples_per_second&#x27;: 95.508, &#x27;train_steps_per_second&#x27;: 6.017, &#x27;train_loss&#x27;: 0.9347671750992064, &#x27;epoch&#x27;: 5.0&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>J√° temos o modelo treinado, como voc√™ pode ver, com muito pouco c√≥digo, podemos treinar um modelo muito rapidamente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Recomendo enfaticamente que voc√™ aprenda o Pytorch e treine muitos modelos antes de usar uma biblioteca de alto n√≠vel como a <code>transformers</code>, porque voc√™ aprende muitos fundamentos de aprendizagem profunda e pode entender melhor o que est√° acontecendo, especialmente porque aprender√° muito com seus erros. Mas, depois de passar por esse per√≠odo, o uso de bibliotecas de alto n√≠vel, como a <code>transformers</code>, acelera muito o desenvolvimento.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Testando o modelo">Testando o modelo<a class="anchor-link" href="#Testando o modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 141" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos o modelo treinado, vamos test√°-lo com um texto. Como o conjunto de dados que baixamos √© de avalia√ß√µes em ingl√™s, vamos test√°-lo com uma avalia√ß√£o em ingl√™s.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">clasificator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-classification&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">clasification</span> <span class="o">=</span> <span class="n">clasificator</span><span class="p">(</span><span class="s2">&quot;I&#39;m liking this post a lot&quot;</span><span class="p">)</span>',
      '<span class="n">clasification</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;label&#x27;: &#x27;LABEL_2&#x27;, &#x27;score&#x27;: 0.5032550692558289&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver a que corresponde a classe que surgiu</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">clases</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;label&#x27;: ClassLabel(names=[&#x27;1 star&#x27;, &#x27;2 star&#x27;, &#x27;3 stars&#x27;, &#x27;4 stars&#x27;, &#x27;5 stars&#x27;], id=None),',
          '&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None)&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A rela√ß√£o seria</p>
      <ul>
        <li>LABEL_0: 1 estrela</li>
        <li>LABEL_1: 2 estrelas</li>
        <li>LABEL_2: 3 estrelas</li>
        <li>LABEL_3: 4 estrelas</li>
        <li>LABEL_4: 5 estrelas</li>
      </ul>
      <p>Portanto, voc√™ classificou o coment√°rio com 3 estrelas. Lembre-se de que treinamos em um subconjunto de dados e com apenas 5 √©pocas, portanto, n√£o esperamos que seja muito bom.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Compartilhe o modelo no Hub do Rosto de Abraco">Compartilhe o modelo no Hub do Rosto de Abra√ßo<a class="anchor-link" href="#Compartilhe o modelo no Hub do Rosto de Abraco"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 142" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando tivermos o modelo retreinado, poderemos carreg√°-lo em nosso espa√ßo no Hugging Face Hub para que outros possam us√°-lo. Para fazer isso, voc√™ precisa ter uma conta no Hugging Face.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Registro em log">Registro em log<a class="anchor-link" href="#Registro em log"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 143" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para fazer upload do modelo, primeiro precisamos fazer login.</p>
      <p>Isso pode ser feito por meio do terminal com</p>
      <div class='highlight'><pre><code class="language-bash">huggingface-cli login</code></pre></div>
      <p>Ou por meio do notebook, tendo instalado primeiro a biblioteca <code>huggingface_hub</code> com</p>
      <div class='highlight'><pre><code class="language-bash">pip install huggingface_hub</code></pre></div>
      <p>Agora, podemos fazer login com a fun√ß√£o <code>notebook_login</code>, que criar√° uma pequena interface gr√°fica na qual devemos inserir um token Hugging Face.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para criar um token, acesse a p√°gina <a href="https://huggingface.co/settings/tokens" target="_blank" rel="nofollow noreferrer">setings/tokens</a> de sua conta, que ter√° a seguinte apar√™ncia</p>
      <p>User-Access-Token-dark](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/User-Access-Token-dark.webp)</p>
      <p>Clique em <code>New token</code> e ser√° exibida uma janela para criar um novo token.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/new-token-dark.webp" alt="new-token-dark">
      <p>Nomeamos o token e o criamos com a fun√ß√£o <code>write</code>.</p>
      <p>Uma vez criado, n√≥s o copiamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="w"> </span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'VBox(children=(HTML(value=&#x27;&amp;lt;center&amp;gt; &amp;lt;img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Para cima depois de treinado">Para cima depois de treinado<a class="anchor-link" href="#Para cima depois de treinado"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 144" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Depois de treinar o modelo, podemos carreg√°-lo no Hub usando a fun√ß√£o <code>push_to_hub</code>. Essa fun√ß√£o tem um par√¢metro obrigat√≥rio que √© o nome do modelo, que deve ser exclusivo; se j√° houver um modelo em seu Hub com esse nome, ele n√£o ser√° carregado. Ou seja, o nome completo do modelo ser√° <user>/<model>, portanto, o nome do modelo n√£o pode existir em seu Hub, mesmo que haja outro modelo com o mesmo nome no Hub de outro usu√°rio.</p>
      <p>Ele tamb√©m tem outros par√¢metros opcionais, mas interessantes:</p>
      <ul>
        <li><code>use_temp_dir</code> (bool, opcional): se deve ou n√£o ser usado um diret√≥rio tempor√°rio para armazenar arquivos salvos antes de serem enviados ao Hub. O padr√£o √© True se n√£o houver um diret√≥rio com o mesmo nome de <code>repo_id</code> e False caso contr√°rio.</li>
        <li><code>commit_message</code> (str, opcional): mensagem de confirma√ß√£o. O padr√£o √© <code>Upload {opening_brace}object{closing_brace}</code>.</li>
        <li><code>private</code> (bool, opcional): se o reposit√≥rio criado deve ser privado ou n√£o.</li>
        <li><code>token</code> (bool ou str, opcional): O token a ser usado como autoriza√ß√£o HTTP para arquivos remotos. Se for True, ser√° usado o token gerado pela execu√ß√£o do login <code>huggingface-cli</code> (armazenado em ~/.huggingface). O padr√£o √© True se <code>repo_url</code> n√£o for especificado.</li>
        <li><code>max_shard_size</code> (int ou str, opcional, o padr√£o √© "5GB"): Aplic√°vel somente a modelos. O tamanho m√°ximo de um ponto de verifica√ß√£o antes de ser fragmentado. Os pontos de verifica√ß√£o fragmentados ser√£o cada um menor que esse tamanho. Se for expresso como uma cadeia de caracteres, dever√° ter d√≠gitos seguidos de uma unidade (como "5MB"). O padr√£o √© "5 GB" para que os usu√°rios possam carregar facilmente modelos em inst√¢ncias de n√≠vel gratuito do Google Colab sem problemas de OOM (falta de mem√≥ria) da CPU.</li>
        <li><code>create_pr</code> (bool, opcional, o padr√£o √© False): se deve ou n√£o criar um PR com os arquivos carregados ou fazer o commit diretamente.</li>
        <li><code>safe_serialization</code> (bool, opcional, o padr√£o √© True): Se os pesos do modelo devem ou n√£o ser convertidos para o formato safetensors para uma serializa√ß√£o mais segura.</li>
        <li><code>revision</code> (str, opcional): filial para a qual enviar os arquivos carregados.</li>
        <li><code>commit_description</code> (str, opcional): Descri√ß√£o do commit a ser criado</li>
        <li><code>tags</code> (List[str], opcional): Lista de tags a serem inseridas no Hub.</li>
      </ul>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span>',
      '<span class="w">    </span><span class="s2">&quot;bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">commit_message</span><span class="o">=</span><span class="s2">&quot;bert base cased fine tune on yelp review subset&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">commit_description</span><span class="o">=</span><span class="s2">&quot;Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs.&quot;</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'README.md:   0%|          | 0.00/5.18k [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'model.safetensors:   0%|          | 0.00/433M [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset/commit/033a3c759d5a4e314ce76db81bd113b4f7da69ad&#x27;, commit_message=&#x27;bert base cased fine tune on yelp review subset&#x27;, commit_description=&#x27;Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs.&#x27;, oid=&#x27;033a3c759d5a4e314ce76db81bd113b4f7da69ad&#x27;, pr_url=None, pr_revision=None, pr_num=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se formos agora ao nosso Hub, veremos que o modelo foi carregado.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformers_commit_unico.webp" alt="transformers_commit_unique">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se agora entrarmos no cart√£o de modelo para ver</p>
      <p>transformers_commit_inico_model_card](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformers_commit_inico_model_card.webp)</p>
      <p>Vemos que tudo n√£o est√° preenchido, faremos isso mais tarde.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Levantar durante o treinamento">Levantar durante o treinamento<a class="anchor-link" href="#Levantar durante o treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 145" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Outra op√ß√£o √© fazer o upload enquanto estivermos treinando o modelo. Isso √© muito √∫til quando treinamos modelos por muitos per√≠odos e isso leva muito tempo, pois se o treinamento for interrompido (porque o computador est√° desligado, a sess√£o de colabora√ß√£o terminou, os cr√©ditos da nuvem acabaram), o trabalho n√£o ser√° perdido. Para fazer isso, voc√™ deve adicionar <code>push_to_hub=True</code> em <code>TrainingArguments</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;bert-base-cased_notebook_transformers_30-epochs_yelp_review_subset&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que alteramos as √©pocas para 30, de modo que o treinamento ser√° mais demorado, portanto, adicionar <code>push_to_hub=True</code> far√° o upload do modelo para o nosso Hub durante o treinamento.</p>
      <p>Tamb√©m alteramos o <code>output_dir</code> porque esse √© o nome que o modelo ter√° no Hub.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x20;&#x20;0%|          | 0/1890 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;loss&#x27;: 0.2363, &#x27;grad_norm&#x27;: 8.151028633117676, &#x27;learning_rate&#x27;: 7.354497354497355e-05, &#x27;epoch&#x27;: 7.94&#x7D;',
          '&#x7B;&#x27;loss&#x27;: 0.0299, &#x27;grad_norm&#x27;: 0.0018280998338013887, &#x27;learning_rate&#x27;: 4.708994708994709e-05, &#x27;epoch&#x27;: 15.87&#x7D;',
          '&#x7B;&#x27;loss&#x27;: 0.0019, &#x27;grad_norm&#x27;: 0.000868947128765285, &#x27;learning_rate&#x27;: 2.0634920634920636e-05, &#x27;epoch&#x27;: 23.81&#x7D;',
          '&#x7B;&#x27;train_runtime&#x27;: 331.5804, &#x27;train_samples_per_second&#x27;: 90.476, &#x27;train_steps_per_second&#x27;: 5.7, &#x27;train_loss&#x27;: 0.07100234655318437, &#x27;epoch&#x27;: 30.0&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'TrainOutput(global_step=1890, training_loss=0.07100234655318437, metrics=&#x7B;&#x27;train_runtime&#x27;: 331.5804, &#x27;train_samples_per_second&#x27;: 90.476, &#x27;train_steps_per_second&#x27;: 5.7, &#x27;train_loss&#x27;: 0.07100234655318437, &#x27;epoch&#x27;: 30.0&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se olharmos novamente para o nosso hub, o novo modelo aparecer√°.</p>
      <p>transformers_commit_training](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformers_commit_training.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Hub como repositorio git">Hub como reposit√≥rio git<a class="anchor-link" href="#Hub como repositorio git"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 146" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No Hugging Face, os modelos, os espa√ßos e os conjuntos de dados s√£o reposit√≥rios git, portanto, voc√™ pode trabalhar com eles dessa forma. Ou seja, voc√™ pode clonar, bifurcar, fazer solicita√ß√µes pull, etc.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mas outra grande vantagem disso √© que voc√™ pode usar um modelo em uma vers√£o espec√≠fica.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s2">&quot;393e083&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'config.json:   0%|          | 0.00/433 [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'pytorch_model.bin:   0%|          | 0.00/436M [00:00&amp;lt;?, ?B/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: [&#x27;classifier.bias&#x27;, &#x27;classifier.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>


















    </div>

  </section>

</PostLayout>
