---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Langchain com integrações de código aberto';
const end_url = 'langchain-opensource-integrations';
const description = 'Aprenda a usar Langchain com as integrações de código aberto mais populares. Neste post, exploraremos como integrar Langchain com ChromaDB, Ollama e HuggingFace.';
const keywords = 'langchain, open source, integrations, chromadb, ollama, huggingface';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/langchain_opensource_integrations_shot.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=960
    image_height=720
    image_extension=webp
    article_date=2024-12-16+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Uso de modelos de linguagem"><h2>Uso de modelos de linguagem</h2></a>
      <a class="anchor-link" href="#Uso de modelos da Ollama"><h3>Uso de modelos da Ollama</h3></a>
      <a class="anchor-link" href="#Baixar modelo"><h4>Baixar modelo</h4></a>
      <a class="anchor-link" href="#Modelos de chat"><h4>Modelos de chat</h4></a>
      <a class="anchor-link" href="#Inferencia offline"><h5>Inferência offline</h5></a>
      <a class="anchor-link" href="#Inferencia em tempo real"><h5>Inferência em tempo real</h5></a>
      <a class="anchor-link" href="#Embeddings"><h4>Embeddings</h4></a>
      <a class="anchor-link" href="#Inserir um unico texto"><h5>Inserir um único texto</h5></a>
      <a class="anchor-link" href="#Embed de varios textos"><h5>Embed de vários textos</h5></a>
      <a class="anchor-link" href="#LLMs"><h4>LLMs</h4></a>
      <a class="anchor-link" href="#Inferencia offline"><h5>Inferência offline</h5></a>
      <a class="anchor-link" href="#Inferencia em tempo real"><h5>Inferência em tempo real</h5></a>
      <a class="anchor-link" href="#Uso de modelos da HuggingFace"><h3>Uso de modelos da HuggingFace</h3></a>
      <a class="anchor-link" href="#Login no Hub da HuggingFace"><h4>Login no Hub da HuggingFace</h4></a>
      <a class="anchor-link" href="#Modelos de chat"><h4>Modelos de chat</h4></a>
      <a class="anchor-link" href="#Inferencia offline"><h5>Inferência offline</h5></a>
      <a class="anchor-link" href="#Inferencia em tempo real"><h5>Inferência em tempo real</h5></a>
      <a class="anchor-link" href="#Embeddings"><h4>Embeddings</h4></a>
      <a class="anchor-link" href="#Embeddings locais"><h5>Embeddings locais</h5></a>
      <a class="anchor-link" href="#Embeddings no HuggingFace"><h5>Embeddings no HuggingFace</h5></a>
      <a class="anchor-link" href="#Pipeline"><h4>Pipeline</h4></a>
      <a class="anchor-link" href="#Inferencia offline"><h5>Inferência offline</h5></a>
      <a class="anchor-link" href="#Inferencia em tempo real"><h5>Inferência em tempo real</h5></a>
      <a class="anchor-link" href="#Uso de bases de dados vetoriais"><h2>Uso de bases de dados vetoriais</h2></a>
      <a class="anchor-link" href="#Uso de ChromaDB"><h3>Uso de ChromaDB</h3></a>
      <a class="anchor-link" href="#Criar um vector store"><h4>Criar um vector store</h4></a>
      <a class="anchor-link" href="#Adicionar documentos"><h4>Adicionar documentos</h4></a>
      <a class="anchor-link" href="#Atualizar documentos"><h4>Atualizar documentos</h4></a>
      <a class="anchor-link" href="#Excluir documentos"><h4>Excluir documentos</h4></a>
      <a class="anchor-link" href="#Busca de documentos"><h4>Busca de documentos</h4></a>
      <a class="anchor-link" href="#Busca com filtros"><h4>Busca com filtros</h4></a>
      <a class="anchor-link" href="#Busca com pontuacao"><h4>Busca com pontuação</h4></a>
      <a class="anchor-link" href="#Uso como recuperador"><h4>Uso como recuperador</h4></a>
      <a class="anchor-link" href="#Medida de similaridade cosseno"><h4>Medida de similaridade cosseno</h4></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o português usando um modelo de tradução automática. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Uso de modelos de linguagem">Uso de modelos de linguagem<a class="anchor-link" href="#Uso de modelos de linguagem"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 71" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Uso de modelos da Ollama">Uso de modelos da Ollama<a class="anchor-link" href="#Uso de modelos da Ollama"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 72" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como já vimos no post de <a href="https://www.maximofn.com/ollama">Ollama</a>, é um framework sobre <code>llama.cpp</code> que nos permite usar modelos de linguagem de maneira simples e que também nos permite usar modelos quantizados.</p>
      <p>Então, vamos ver como usar <code>Qwen2.5 7B</code> com Ollama no Langchain.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de tudo, temos que instalar o módulo de Llama do Langchain</p>
      <div class='highlight'><pre><code class="language-bash">pip install -U langchain-ollama</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Baixar modelo">Baixar modelo<a class="anchor-link" href="#Baixar modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 73" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A primeira coisa a fazer é baixar o modelo <code>qwen2.5:7b</code> da Ollama. Usamos o de 7B para que qualquer pessoa com uma GPU possa usá-lo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">ollama</span> <span class="n">pull</span> <span class="n">qwen2</span><span class="mf">.5</span><span class="p">:</span><span class="mi">7</span><span class="n">b</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'pulling manifest ⠙ pulling manifest ⠹ pulling manifest ⠸ pulling manifest ⠸ pulling manifest ⠴ pulling manifest ⠦ pulling manifest ⠧ pulling manifest',
          'pulling 2bada8a74506...   0% ▕                ▏    0 B/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   0% ▕                ▏    0 B/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   0% ▕                ▏    0 B/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   0% ▕                ▏    0 B/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   0% ▕                ▏ 9.7 MB/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   0% ▕                ▏  12 MB/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   0% ▕                ▏  17 MB/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   0% ▕                ▏  21 MB/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   1% ▕                ▏  31 MB/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   1% ▕                ▏  42 MB/4.7 GB                  pulling manifest',
          'pulling 2bada8a74506...   1% ▕                ▏  51 MB/4.7 GB   47 MB/s   1m37spulling manifest',
          'pulling 2bada8a74506...   1% ▕                ▏  57 MB/4.7 GB   47 MB/s   1m37spulling manifest',
          'pulling 2bada8a74506...   1% ▕                ▏  65 MB/4.7 GB   47 MB/s   1m37spulling manifest',
          'pulling 2bada8a74506...   1% ▕                ▏  70 MB/4.7 GB   47 MB/s   1m37spulling manifest',
          'pulling 2bada8a74506...   2% ▕                ▏  79 MB/4.7 GB   47 MB/s   1m37spulling manifest',
          'pulling 2bada8a74506...   2% ▕                ▏  88 MB/4.7 GB   47 MB/s   1m37spulling manifest',
          'pulling 2bada8a74506...   2% ▕                ▏  93 MB/4.7 GB   47 MB/s   1m37spulling manifest',
          'pulling 2bada8a74506...   2% ▕                ▏ 103 MB/4.7 GB   47 MB/s   1m36spulling manifest',
          'pulling 2bada8a74506...   2% ▕                ▏ 112 MB/4.7 GB   47 MB/s   1m36spulling manifest',
          'pulling 2bada8a74506...   3% ▕                ▏ 117 MB/4.7 GB   47 MB/s   1m36spulling manifest',
          'pulling 2bada8a74506...   3% ▕                ▏ 126 MB/4.7 GB   63 MB/s   1m11spulling manifest',
          'pulling 2bada8a74506...   3% ▕                ▏ 135 MB/4.7 GB   63 MB/s   1m11spulling manifest',
          'pulling 2bada8a74506...   3% ▕                ▏ 141 MB/4.7 GB   63 MB/s   1m11spulling manifest',
          'pulling 2bada8a74506...   3% ▕                ▏ 150 MB/4.7 GB   63 MB/s   1m11spulling manifest',
          '...',
          'pulling eb4402837c78... 100% ▕████████████████▏ 1.5 KB',
          'pulling 832dd9e00a68... 100% ▕████████████████▏  11 KB',
          'pulling 2f15b3218f05... 100% ▕████████████████▏  487 B',
          'verifying sha256 digest',
          'writing manifest',
          'removing any unused layers',
          'success',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Modelos de chat">Modelos de chat<a class="anchor-link" href="#Modelos de chat"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 74" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A primeira opção que temos para usar modelos de linguagem com Ollama em Langchain é usar a classe <code>ChatOllama</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos o objeto <code>ChatOllama</code> com o modelo <code>qwen2.5:7b</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOllama</span>',
      '<span class="w"> </span>',
      '<span class="n">chat_model</span> <span class="o">=</span> <span class="n">ChatOllama</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;qwen2.5:7b&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">num_predict</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Inferencia offline">Inferência offline<a class="anchor-link" href="#Inferencia offline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 75" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora fazemos inferência com o modelo de maneira offline, ou seja, sem usar streaming. Este método espera ter toda a resposta.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">    </span><span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;Eres un asistente de IA que responde preguntas sobre IA.&quot;</span><span class="p">),</span>',
      '<span class="w">    </span><span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;¿Qué son los LLMs?&quot;</span><span class="p">),</span>',
      '<span class="p">]</span>',
      '<span class="n">response</span> <span class="o">=</span> <span class="n">chat_model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'LLMs es el acrónimo de Large Language Models (Modelos de Lenguaje Grandes). Estos son sistemas de inteligencia artificial basados en modelos de aprendizaje automático profundo que están diseñados para comprender y generar texto humano. Algunas características clave de los LLMs incluyen:',
          '1. **Tamaño**: Generalmente se refiere a modelos con billones de parámetros, lo cual les permite aprender de grandes cantidades de datos.',
          '2. **Capacidad de Generación de Texto**: Pueden generar texto coherente y relevante basado en entradas iniciales o prompts proporcionados por el usuario.',
          '3. **Entendimiento del Lenguaje Natural**: Poseen un entendimiento profundo de la gramática, semántica y contexto del lenguaje humano.',
          '4. **Aplicaciones Versátiles**: Se utilizan en una amplia gama de tareas como asistentes virtuales, traducción automática, resumen de texto, escritura creativa, etc.',
          '5. **Entrenamiento Supervisado**: A menudo se entrenan utilizando conjuntos de datos extensos y variados para mejorar su capacidad de comprensión e',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Inferencia em tempo real">Inferência em tempo real<a class="anchor-link" href="#Inferencia em tempo real"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 76" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos fazer streaming, podemos usar o método <code>stream</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chat_model</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'LLMs es el acrónimo de Large Language Models, lo cual se refiere a modelos de inteligencia artificial de lenguaje de gran escala. Estos son sistemas entrenados con técnicas de aprendizaje supervisado y no supervisado que pueden generar texto humano似的，准确回答如下：',
          'LLMs es el acrónimo de Large Language Models, lo cual se refiere a modelos de inteligencia artificial de lenguaje de gran escala. Estos son sistemas que se entrenan con grandes conjuntos de datos de texto para aprender patrones y relaciones lingüísticas. Algunas características clave de los LLMs incluyen:',
          '1. **Tamaño**: Generalmente se basan en arquitecturas de redes neuronales profunda como Transformers, lo que les permite manejar cantidades extremadamente grandes de parámetros.',
          '2. **Entrenamiento**: Se entrena con datos de texto muy variados para capturar una amplia gama de conocimientos y habilidades lingüísticas.',
          '3. **Generación de texto**: Pueden generar textos coherentes y relevantes basándose en el contexto proporcionado, lo que les permite realizar tareas como la escritura creativa,',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Embeddings">Embeddings<a class="anchor-link" href="#Embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 77" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos usar embeddings, podemos usar a classe <code>OllamaEmbeddings</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos o objeto <code>OllamaEmbeddings</code> com o modelo <code>qwen2.5:7b</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">OllamaEmbeddings</span>',
      '<span class="w"> </span>',
      '<span class="n">embeddings_model</span> <span class="o">=</span> <span class="n">OllamaEmbeddings</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;qwen2.5:7b&quot;</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Inserir um unico texto">Inserir um único texto<a class="anchor-link" href="#Inserir um unico texto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 78" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;¿Qué son los LLMs?&quot;</span>',
      '<span class="n">embedding</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
      '<span class="nb">len</span><span class="p">(</span><span class="n">embedding</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">embedding</span><span class="p">),</span> <span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(3584, list, [0.00045780753, -0.010200562, 0.0059901197])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Embed de varios textos">Embed de vários textos<a class="anchor-link" href="#Embed de varios textos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 79" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;¿Qué son los LLMs?&quot;</span><span class="p">,</span> <span class="s2">&quot;¿Qué son los embeddings?&quot;</span><span class="p">]</span>',
      '<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">input_texts</span><span class="p">)</span>',
      '<span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">type</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="nb">type</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(3584,',
          'list,',
          '[0.00045780753, -0.010200562, 0.0059901197],',
          '3584,',
          'list,',
          '[0.0007678218, -0.01124029, 0.008565228])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="LLMs">LLMs<a class="anchor-link" href="#LLMs"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 80" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos usar apenas um modelo de linguagem, podemos usar a classe <code>OllamaLLM</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos o objeto <code>OllamaLLM</code> com o modelo <code>qwen2.5:7b</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">OllamaLLM</span>',
      '<span class="w"> </span>',
      '<span class="n">llm_model</span> <span class="o">=</span> <span class="n">OllamaLLM</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;qwen2.5:7b&quot;</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Inferencia offline">Inferência offline<a class="anchor-link" href="#Inferencia offline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 81" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora fazemos inferência com o modelo de maneira offline, ou seja, sem usar streaming. Este método espera ter toda a resposta.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;¿Qué son los LLMs?&quot;</span>',
      '<span class="n">response</span> <span class="o">=</span> <span class="n">llm_model</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Los LLMs es una abreviatura común en español que se refiere a &quot;Lenguajes de Marcado de Libre Mando&quot;. Sin embargo, en el contexto del procesamiento del lenguaje natural y la inteligencia artificial, es probable que estés buscando información sobre otro término.',
          'En inglés, el término LLMs (Large Language Models) se refiere a modelos de lenguaje de gran tamaño. Estos son sistemas de IA que están entrenados en una amplia gama de datos de texto para comprender y generar texto humano-like. Algunas características clave de los LLMs incluyen:',
          '1. Tamaño: Generalmente se refiere a modelos con millones o incluso billones de parámetros.',
          '2. Generación de texto: Pueden generar texto en respuesta a una entrada dada, lo que les hace útiles para tareas como escritura automática, conversaciones de chat, etc.',
          '3. Entrenamiento en datos variados: Son entrenados con grandes conjuntos de datos de internet o corpora literales y académicos.',
          '4. Aplicaciones: Se utilizan en una variedad de aplicaciones, incluyendo asistentes virtuales, traducción automática, análisis de sentimientos, resumen de texto, etc.',
          'Un ejemplo famoso de LLMs es la familia de modelos GPT (Generative Pre-trained Transformer) desarrollados por OpenAI. Otros ejemplos incluyen los modelos de PaLM y Qwen de Anthropic, entre otros.',
          '¿Te gustaría saber más sobre algún aspecto en particular de estos modelos?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Inferencia em tempo real">Inferência em tempo real<a class="anchor-link" href="#Inferencia em tempo real"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 82" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos fazer streaming, podemos usar o método <code>stream</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">llm_model</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Los LLMs es una abreviatura que se refiere a los Modelos de Lenguaje de Grande Escala (Large Language Models en inglés). Estos son modelos de inteligencia artificial desarrollados para entender y generar texto humano. Algunas características clave de los LLMs incluyen:',
          '1. **Capacidad de Generación de Texto**: Pueden generar texto coherente, original e informativo en varios estilos y formatos.',
          '2. **Entendimiento del Contexto**: Tienen una comprensión profunda del contexto y la gramática para producir respuestas apropiadas a las entradas proporcionadas.',
          '3. **Multiplicidad de Usos**: Se pueden aplicar a diversas tareas, como escritura creativa, asistencia en atención al cliente, creación de contenido, traducción automática, etc.',
          '4. **Aprendizaje Supervisado**: Son entrenados con grandes cantidades de texto para aprender patrones y conocimientos del lenguaje humano.',
          '5. **Limitaciones Eticas y Jurídicas**: Existen preocupaciones sobre el uso ético y legal de estos modelos, incluyendo el potencial de generar contenido inapropiado o engañoso.',
          'Algunos ejemplos populares de LLMs incluyen los creados por empresas como Anthropic (Clara), Google (Switch Transformer), Microsoft (Cobalt) y, aunque ya no actualmente soportado, OpenAI (GPT-3).',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Uso de modelos da HuggingFace">Uso de modelos da HuggingFace<a class="anchor-link" href="#Uso de modelos da HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 83" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Embora o Ollama seja muito simples de usar, não possui tantos modelos quanto os disponíveis no Hub da HuggingFace. Além disso, é provável que ele não tenha o último grande modelo que você deseja usar, por isso vamos ver como utilizar modelos da HuggingFace no Langchain.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de tudo temos que instalar o módulo da HuggingFace de Langchain</p>
      <div class='highlight'><pre><code class="language-bash">pip install -U langchain-huggingface</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Login no Hub da HuggingFace">Login no Hub da HuggingFace<a class="anchor-link" href="#Login no Hub da HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 84" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro temos que fazer login no Hub da HuggingFace. Para isso, precisamos criar um token de acesso. Uma vez criado, salvamos o token em um arquivo <code>.env</code> com o nome <code>LANGCHAIN_TOKEN</code> da seguinte maneira:</p>
      <div class='highlight'><pre><code class="language-bash">echo "LANGCHAIN_TOKEN=hf_..." &gt; .env</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder lê-lo, instalamos o módulo <code>python-dotenv</code>.</p>
      <div class='highlight'><pre><code class="language-bash">pip install python-dotenv</code></pre></div>
      <p>Agora vamos nos logar no Hub da HuggingFace.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>',
      '<span class="w"> </span>',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="n">LANGCHAIN_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;LANGCHAIN_TOKEN&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">login</span>',
      '<span class="n">login</span><span class="p">(</span><span class="n">LANGCHAIN_TOKEN</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Modelos de chat">Modelos de chat<a class="anchor-link" href="#Modelos de chat"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 85" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A primeira opção que temos para usar modelos de linguagem com HuggingFace em Langchain é usar a classe <code>HuggingFaceEndpoint</code>. Essa opção chama a API do HuggingFace para fazer inferência.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos o objeto <code>chat_model</code> com o modelo <code>Qwen2.5-72B-Instruct</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpoint</span><span class="p">,</span> <span class="n">ChatHuggingFace</span>',
      '<span class="w"> </span>',
      '<span class="n">llm</span> <span class="o">=</span> <span class="n">HuggingFaceEndpoint</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.03</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatHuggingFace</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Inferencia offline">Inferência offline<a class="anchor-link" href="#Inferencia offline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 86" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora fazemos inferência com o modelo de maneira offline, ou seja, sem usar streaming. Este método espera ter toda a resposta.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">    </span><span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;Eres un asistente de IA que responde preguntas sobre IA.&quot;</span><span class="p">),</span>',
      '<span class="w">    </span><span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;¿Qué son los LLMs?&quot;</span><span class="p">),</span>',
      '<span class="p">]</span>',
      '<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Los LLMs, o Large Language Models (Modelos de Lenguaje Grande en español), son sistemas de inteligencia artificial diseñados para procesar y generar texto basado en lenguaje natural. Estos modelos son capaces de comprender, analizar y producir texto humano de manera sofisticada, lo que los hace útiles en una amplia variedad de aplicaciones, como la traducción de idiomas, el análisis de sentimientos, la generación de respuestas a preguntas, la escritura creativa, la asistencia en tareas, entre otras.',
          '### Características principales de los LLMs:',
          '1. **Escalabilidad**: Los LLMs suelen tener millones o incluso billones de parámetros, lo que les permite capturar la complejidad del lenguaje humano con un alto nivel de detalle.',
          '2. **Entrenamiento supervisado y no supervisado**: Pueden ser entrenados con grandes cantidades de datos de texto, tanto de forma supervisada (con etiquetas) como no supervisada (sin etiquetas).',
          '3. **Transfer learning**: Una vez entrenados, estos modelos pueden ser finetuneados (ajustados) para tareas específicas con un conjunto de datos más pequeño, lo que los hace muy versátiles.',
          '4. **Generación de texto**: Son capaces de generar texto coherente y contextualmente relevante, lo que los hace ideales para tareas como la escritura creativa, la redacción de informes, y la generación de respuestas en chatbots.',
          '5. **Comprensión del contexto**: Pueden entender el contexto y las relaciones entre palabras y frases, lo que les permite manejar diálogos y discursos complejos.',
          '6. **Interacción humana**: Son diseñados para interactuar de manera fluida y natural con seres humanos, lo que los hace útiles en aplicaciones conversacionales.',
          '### Ejemplos de LLMs:',
          '- **GPT-3 (Generative Pre-trained Transformer 3)**: Desarrollado por OpenAI, es uno de losmodelos más conocidos y potentes.',
          '- **BERT (Bidirectional Encoder Representations from Transformers)**: Diseñado por Google, es conocido por su capacidad de comprensión del contexto.',
          '- **T5 (Text-to-Text Transfer Transformer)**: También de Google, se destaca por su flexibilidad y versatilidad.',
          'Los LLMs representan un avance significativo en el campo de la inteligencia artificial, especialmente en el procesamiento del lenguaje natural, y continúan evoluinto rápidamente.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Inferencia em tempo real">Inferência em tempo real<a class="anchor-link" href="#Inferencia em tempo real"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 87" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos fazer streaming, podemos usar o método <code>stream</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chat</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="n">messages</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Los LLMs, o Large Language Models (Modelos de Lenguaje Grande en español), son sistemas de inteligencia artificial diseñados para procesar y generar texto basado en lenguaje natural. Estos modelos son capaces de comprender, analizar y producir texto humano de manera sofisticada, lo que los hace útiles en una amplia variedad de aplicaciones, como la traducción de idiomas, el análisis de sentimientos, la generación de respuestas a preguntas, la escritura creativa, la asistencia en tareas, entre otras.',
          '### Características principales de los LLMs:',
          '1. **Escalabilidad**: Los LLMs suelen tener millones o incluso billones de parámetros, lo que les permite capturar la complejidad del lenguaje humano con un alto nivel de detalle.',
          '2. **Entrenamiento supervisado y no supervisado**: Pueden ser entrenados con grandes cantidades de datos de texto, tanto de forma supervisada (con etiquetas) como no supervisada (sin etiquetas).',
          '3. **Transfer learning**: Una vez entrenados, estos modelos pueden ser finetuneados (ajustados) para tareas específicas con un conjunto de datos más pequeño, lo que los hace muy versátiles.',
          '4. **Generación de texto**: Son capaces de generar texto coherente y contextualmente relevante, lo que los hace ideales para tareas como la escritura creativa, la redacción de informes, y la generación de respuestas en chatbots.',
          '5. **Comprensión del contexto**: Pueden entender el contexto y las relaciones entre palabras y frases, lo que les permite manejar diálogos y discursos complejos.',
          '6. **Interacción humana**: Son diseñados para interactuar de manera fluida y natural con seres humanos, lo que los hace útiles en aplicaciones conversacionales.',
          '### Ejemplos de LLMs:',
          '- **GPT-3 (Generative Pre-trained Transformer 3)**: Desarrollado por OpenAI, es uno de losmodelos más conocidos y potentes.',
          '- **BERT (Bidirectional Encoder Representations from Transformers)**: Diseñado por Google, es conocido por su capacidad de comprensión del contexto.',
          '- **T5 (Text-to-Text Transfer Transformer)**: También de Google, se destaca por su flexibilidad y versatilidad.',
          'Los LLMs representan un avance significativo en el campo de la inteligencia artificial, especialmente en el procesamiento del lenguaje natural, y continúan evoluinto rápidamente.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Embeddings">Embeddings<a class="anchor-link" href="#Embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 88" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Embeddings locais">Embeddings locais<a class="anchor-link" href="#Embeddings locais"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 89" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos criar embeddings localmente, podemos usar a classe <code>HuggingFaceEmbeddings</code>. Para isso, precisamos ter a biblioteca <code>sentence-transformers</code> instalada.</p>
      <div class='highlight'><pre><code class="language-bash">pip install -U sentence-transformers</code></pre></div>
      <p>Depois podemos criar embeddings localmente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>',
      '<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>',
      '<span class="n">encode_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;normalize_embeddings&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">embeddings_model</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">encode_kwargs</span><span class="o">=</span><span class="n">encode_kwargs</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;¿Qué son los LLMs?&quot;</span>',
      '<span class="n">embedding</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
      '<span class="nb">len</span><span class="p">(</span><span class="n">embedding</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">embedding</span><span class="p">),</span> <span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(768,',
          'list,',
          '[-0.03638569265604019, -0.003062659176066518, 0.005454241763800383])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos criar embeddings de vários textos, podemos usar o método <code>embed_documents</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;¿Qué son los LLMs?&quot;</span><span class="p">,</span> <span class="s2">&quot;¿Qué son los embeddings?&quot;</span><span class="p">]</span>',
      '<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">input_texts</span><span class="p">)</span>',
      '<span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">type</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="nb">type</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(768,',
          'list,',
          '[-0.0363856740295887, -0.0030626414809376, 0.005454237572848797],',
          '768,',
          'list,',
          '[-0.014533628709614277, 0.01950662210583687, -0.01753164641559124])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Embeddings no HuggingFace">Embeddings no HuggingFace<a class="anchor-link" href="#Embeddings no HuggingFace"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 90" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos usar a API do HuggingFace para criar embeddings, podemos usar a classe <code>HuggingFaceEmbeddings</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEndpointEmbeddings</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span>',
      '<span class="w"> </span>',
      '<span class="n">embeddings_model</span> <span class="o">=</span> <span class="n">HuggingFaceEndpointEmbeddings</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;feature-extraction&quot;</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">input_text</span> <span class="o">=</span> <span class="s2">&quot;¿Qué son los LLMs?&quot;</span>',
      '<span class="n">embedding</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>',
      '<span class="nb">len</span><span class="p">(</span><span class="n">embedding</span><span class="p">),</span> <span class="nb">type</span><span class="p">(</span><span class="n">embedding</span><span class="p">),</span> <span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(768,',
          'list,',
          '[-0.03638569638133049, -0.0030626363586634398, 0.005454216152429581])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se quiser criar embeddings de vários textos, podemos usar o método <code>embed_documents</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;¿Qué son los LLMs?&quot;</span><span class="p">,</span> <span class="s2">&quot;¿Qué son los embeddings?&quot;</span><span class="p">]</span>',
      '<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">(</span><span class="n">input_texts</span><span class="p">)</span>',
      '<span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">type</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="nb">type</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">embeddings</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(768,',
          'list,',
          '[-0.03638570010662079, -0.003062596544623375, 0.005454217083752155],',
          '768,',
          'list,',
          '[-0.014533626846969128, 0.019506637006998062, -0.01753171905875206])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Pipeline">Pipeline<a class="anchor-link" href="#Pipeline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 91" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos criar um pipeline de Transformers e usá-lo em Langchain, para isso precisamos instalar o módulo <code>transformers</code>.</p>
      <div class='highlight'><pre><code class="language-bash">pip install -U transformers</code></pre></div>
      <p>Para garantizar que todos possam testar o exemplo, vamos usar o modelo <code>Qwen/Qwen2.5-3B-Instruct</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Temos duas maneiras, criar um pipeline através do Langchain</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFacePipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-3B-Instruct&quot;</span>',
      '<span class="n">langchain_pipeline</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="o">.</span><span class="n">from_model_id</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model_id</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">pipeline_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">512</span><span class="p">},</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards:   0%|          | 0/2 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Device set to use cuda:0',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ou podemos criar um pipeline através de Transformers.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFacePipeline</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-3B-Instruct&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>',
      '<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>',
      '<span class="w">    </span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span>',
      '<span class="p">)</span>',
      '<span class="n">transformers_pipeline</span> <span class="o">=</span> <span class="n">HuggingFacePipeline</span><span class="p">(</span><span class="n">pipeline</span><span class="o">=</span><span class="n">pipe</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Loading checkpoint shards:   0%|          | 0/2 [00:00&amp;lt;?, ?it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'Device set to use cuda:0',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Inferencia offline">Inferência offline<a class="anchor-link" href="#Inferencia offline"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 92" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">langchain_response</span> <span class="o">=</span> <span class="n">langchain_pipeline</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;¿Qué son los LLMs?&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">langchain_response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '¿Qué son los LLMs? - El Blog de Ciberseguridad',
          'Publicado por: CiberSeguridad 18/03/2022',
          'En el mundo de la tecnología y la inteligencia artificial, las nuevas tecnologías que surgen a menudo se presentan como un paso hacia adelante en la mejora de nuestras vidas. Sin embargo, a veces estas innovaciones pueden resultar en desafíos significativos, especialmente cuando se trata de la seguridad cibernética.',
          'En este artículo, exploraremos los LLMs, o modelos lingüísticos grandes (Large Language Models), una nueva clase de inteligencia artificial que está revolucionando el campo de la comunicación y la creación de contenido. Aunque estos modelos son muy prometedores, también tienen implicaciones significativas para la seguridad cibernética. En esta entrada, te explicamos qué son los LLMs y cómo afectan a la seguridad digital.',
          '¿Qué son los LLMs?',
          'Los modelos lingüísticos grandes son algoritmos de aprendizaje automático que han sido entrenados con grandes conjuntos de texto, generalmente en formato de texto natural. Estos modelos pueden procesar y generar texto de manera similar a una persona, lo que los hace extremadamente útiles para tareas como la traducción, la autocompletado de texto y la generación de contenido creativo.',
          'Los LLMs se basan en técnicas de aprendizaje profundo, que son una forma avanzada de algoritmo de aprendizaje automático. Este tipo de algoritmo utiliza redes neuronales para aprender patrones y relaciones en datos grandes, permitiendo a los modelos predecir resultados o tomar decisiones basadas en esa información.',
          'Estos modelos son capaces de aprender y generar texto en un amplio rango de temas, desde descripciones de libros hasta diálogos interactivos, lo que los hace excelentes para tareas de generación de contenido. También pueden ser utilizados para tareas más específicas, como la traducción de idiomas o la resolución de problemas de lógica.',
          '¿Cómo afectan a la seguridad cibernética?',
          'Los LLMs representan una gran oportunidad para mejorar la eficiencia y la precisión de muchas tareas de texto, pero también traen consigo desafíos significativos en términos de seguridad. Algunas de las preocupaciones más comunes incluyen:',
          'Privacidad: Los LLM',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">transformers_response</span> <span class="o">=</span> <span class="n">transformers_pipeline</span><span class="p">(</span><span class="s2">&quot;¿Qué son los LLMs?&quot;</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">transformers_response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '/tmp/ipykernel_318295/4120882068.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.',
          '&#x20;&#x20;transformers_response = transformers_pipeline(&quot;¿Qué son los LLMs?&quot;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '¿Qué son los LLMs? ¿Cómo funcionan y qué pueden hacer?',
          'Los LLMs (Large Language Models) son modelos de inteligencia artificial que han sido entrenados con enormes cantidades de texto. Estos modelos pueden procesar y generar texto en una variedad de lenguajes, desde el inglés hasta idiomas menos comunes. Los LLMs están diseñados para entender y generar texto con un nivel de precisión similar al humano.',
          'Funcionamiento:',
          '1. Entrenamiento: Los LLMs se entrenan utilizando grandes conjuntos de datos de texto, como libros electrónicos, artículos de noticias, etc.',
          '2. Análisis: Después del entrenamiento, los LLMs analizan patrones en el texto y aprenden a asociar palabras y frases con otros textos similares.',
          '3. Generación de texto: Cuando se le pide al modelo que genere texto, analiza los patrones que ha aprendido y genera texto basado en esos patrones.',
          'Pueden hacer:',
          '1. Traducción: Convertir texto de un idioma a otro.',
          '2. Resumen de texto: Summarizar largos documentos en versiones más cortas.',
          '3. Creación de contenido: Generar artículos, correos electrónicos, historias, etc.',
          '4. Resolución de problemas: Ayudar a resolver problemas o proporcionar soluciones basadas en el texto.',
          '5. Chatbots: Usar para crear asistentes virtuales que puedan responder preguntas y ayudar con tareas.',
          '6. Crea contenido: Escribe contenido creativo como poesía, relatos, etc.',
          '7. Ajuste de lenguaje: Corrige errores gramaticales y mejora la escritura.',
          'Es importante tener en cuenta que aunque estos modelos son muy poderosos, todavía tienen limitaciones y pueden generar contenido incorrecto o inapropiado. Además, la privacidad y la ética son aspectos importantes a considerar al usar estos modelos.',
          'En resumen, los LLMs son herramientas poderosas que pueden ser utilizadas en una variedad de aplicaciones, pero también requieren cuidado y consideración al utilizarlos. Es recomendable siempre verificar el contenido generado por estos modelos y estar atentos a sus posibles limitaciones.',
          'Además, es importante destacar que los LLMs no son sustitutos perfectos para el pensamiento crítico humano. Aunque pueden generar texto de',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h5 id="Inferencia em tempo real">Inferência em tempo real<a class="anchor-link" href="#Inferencia em tempo real"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 93" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">langchain_pipeline</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="s2">&quot;¿Qué son los LLMs?&quot;</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '- Cursos de Inteligencia Artificial en línea',
          'Los LLMs, o Lenguajes de Lenguaje de Modelos, son modelos de aprendizaje automático que han sido entrenados para producir texto similar al de un humano. Estos modelos pueden ser utilizados para una variedad de tareas, como la generación de texto, el chatbot, el análisis de sentimientos, etc.',
          'En resumen, los LLMs son modelos de aprendizaje automático que han sido entrenados para producir texto similar al de un humano. Estos modelos pueden ser utilizados para una variedad de tareas, como la generación de texto, el chatbot, el análisis de sentimientos, etc. Los LLMs son un tipo de modelo de inteligencia artificial y se utilizan en una amplia gama de aplicaciones, desde la creación de contenido hasta el asesoramiento.',
          'Los LLMs son modelos de aprendizaje automático que han sido entrenados para producir texto similar al de un humano. Estos modelos pueden ser utilizados para una variedad de tareas, como la generación de texto, el chatbot, el análisis de sentimientos, etc. Los LLMs son un tipo de modelo de inteligencia artificial y se utilizan en una amplia gama de aplicaciones, desde la creación de contenido hasta el asesoramiento. Los LLMs están basados en redes neuronales y se entrenan con grandes conjuntos de datos de texto para aprender a generar texto similar al de un humano.',
          'Los LLMs están disponibles en diferentes plataformas, incluyendo Google&#x27;s PaLM, Anthropic&#x27;s Claude, Microsoft&#x27;s Bing, e incluso tu propio teléfono inteligente puede tener uno. Los LLMs pueden ser utilizados para una variedad de tareas, incluyendo la generación de texto, el chatbot, el análisis de sentimientos, etc. Los LLMs son un tipo de modelo de inteligencia artificial y se utilizan en una amplia gama de aplicaciones, desde la creación de contenido hasta el asesoramiento. Los LLMs están basados en redes neuronales y se entrenan con grandes conjuntos de datos de texto para aprender a generar texto similar al de un humano. Los LLMs están disponibles en diferentes plataformas, incluyendo Google&#x27;s PaLM, Anthropic&#x27;s Claude, Microsoft&#x27;s Bing, e incluso tu propio teléfono inteligente puede tener uno',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">transformers_pipeline</span><span class="o">.</span><span class="n">stream</span><span class="p">(</span><span class="s2">&quot;¿Qué son los LLMs?&quot;</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '| El Blog de la Inteligencia Artificial',
          'Home » Noticias » ¿Qué son los LLMs?',
          'LLM es la sigla en inglés para Large Language Models, que traducido al castellano sería &quot;Grandes Modelos de Lenguaje&quot;. Son modelos de inteligencia artificial (IA) que permiten a las máquinas entender y generar texto.',
          'Los LLMs se basan en el aprendizaje profundo, una técnica de IA que utiliza redes neuronales para modelar relaciones entre variables. Estos modelos son capaces de aprender patrones en grandes conjuntos de datos, lo que les permite predecir o generar texto similar al que se les ha proporcionado.',
          'Las principales características de los LLMs incluyen:',
          'Capacidad de procesamiento de lenguaje natural: Los LLMs pueden entender y generar texto, lo que les permite interactuar con los usuarios de manera fluida y natural.',
          'Capacidad de aprendizaje continuo: Los LLMs pueden aprender y mejorar con cada interacción, lo que les permite ofrecer respuestas más precisas y relevantes a medida que ganan experiencia.',
          'Capacidad de generación de texto: Los LLMs pueden generar texto similar al que se les ha proporcionado, lo que les permite crear contenido de calidad sin necesidad de intervención humana manual.',
          'Ventajas de los LLMs:',
          'Mayor eficiencia: Los LLMs pueden procesar grandes cantidades de información de manera rápida y precisa, lo que les permite ahorrar tiempo y recursos en comparación con los métodos humanos.',
          'Mejora de la precisión: Los LLMs pueden aprender y ajustarse a los patrones del lenguaje, lo que les permite ofrecer respuestas más precisas y relevantes a los usuarios.',
          'Aumento de la productividad: Los LLMs pueden automatizar tareas repetitivas y monótonas, liberando tiempo y recursos para que los humanos puedan centrarse en tareas más estratégicas y creativas.',
          'Entendimiento del lenguaje humano: Los LLMs pueden entender y generar texto en varios idiomas, lo que les permite interactuar con un amplio espectro de usuarios.',
          'Evaluación de la inteligencia artificial',
          'Los LLMs están siendo utilizados en una variedad de aplicaciones, desde asistentes virtuales hasta análisis de texto y traducción automática. También están siendo evaluados por su capacidad para generar contenido de',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Uso de bases de dados vetoriais">Uso de bases de dados vetoriais<a class="anchor-link" href="#Uso de bases de dados vetoriais"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 94" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Uso de ChromaDB">Uso de ChromaDB<a class="anchor-link" href="#Uso de ChromaDB"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 95" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder usar o ChromaDB no Langchain, primeiro temos que instalar <code>langchain-chroma</code>.</p>
      <div class='highlight'><pre><code class="language-bash">pip install -qU chromadb langchain-chroma</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Criar um vector store">Criar um vector store<a class="anchor-link" href="#Criar um vector store"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 96" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_chroma</span><span class="w"> </span><span class="kn">import</span> <span class="n">Chroma</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>',
      '<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>',
      '<span class="n">encode_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;normalize_embeddings&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">encode_kwargs</span><span class="o">=</span><span class="n">encode_kwargs</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">vector_store</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">collection_name</span><span class="o">=</span><span class="s2">&quot;chroma_db&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">embedding_function</span><span class="o">=</span><span class="n">embedding_model</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Adicionar documentos">Adicionar documentos<a class="anchor-link" href="#Adicionar documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 97" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.documents</span><span class="w"> </span><span class="kn">import</span> <span class="n">Document</span>',
      '<span class="w"> </span>',
      '<span class="n">document_1</span> <span class="o">=</span> <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;This is a Mojo docs&quot;</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;Mojo source&quot;</span><span class="p">})</span>',
      '<span class="n">document_2</span> <span class="o">=</span> <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;This is Rust docs&quot;</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;Rust source&quot;</span><span class="p">})</span>',
      '<span class="n">document_3</span> <span class="o">=</span> <span class="n">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;i will be deleted :(&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="n">document_1</span><span class="p">,</span> <span class="n">document_2</span><span class="p">,</span> <span class="n">document_3</span><span class="p">]</span>',
      '<span class="n">ids</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">]</span>',
      '<span class="n">vector_store</span><span class="o">.</span><span class="n">add_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">documents</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="n">ids</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Atualizar documentos">Atualizar documentos<a class="anchor-link" href="#Atualizar documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 98" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">updated_document</span> <span class="o">=</span> <span class="n">Document</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">page_content</span><span class="o">=</span><span class="s2">&quot;This is Python docs&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;Python source&quot;</span><span class="p">}</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">vector_store</span><span class="o">.</span><span class="n">update_documents</span><span class="p">(</span><span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">],</span><span class="n">documents</span><span class="o">=</span><span class="p">[</span><span class="n">updated_document</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Excluir documentos">Excluir documentos<a class="anchor-link" href="#Excluir documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 99" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">vector_store</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;3&quot;</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Busca de documentos">Busca de documentos<a class="anchor-link" href="#Busca de documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 100" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;* </span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="si">}</span><span class="s2"> [</span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '* This is Python docs [&#x7B;&#x27;source&#x27;: &#x27;Python source&#x27;&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Busca com filtros">Busca com filtros<a class="anchor-link" href="#Busca com filtros"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 101" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="nb">filter</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;source&quot;</span><span class="p">:</span> <span class="s2">&quot;Python source&quot;</span><span class="p">})</span>',
      '<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;* </span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="si">}</span><span class="s2"> [</span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '* This is Python docs [&#x7B;&#x27;source&#x27;: &#x27;Python source&#x27;&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Busca com pontuacao">Busca com pontuação<a class="anchor-link" href="#Busca com pontuacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 102" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">results</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">similarity_search_with_score</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;* [SIM=</span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">3f</span><span class="si">}</span><span class="s2">] </span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="si">}</span><span class="s2"> [</span><span class="si">{</span><span class="n">doc</span><span class="o">.</span><span class="n">metadata</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '* [SIM=0.809454] This is Python docs [&#x7B;&#x27;source&#x27;: &#x27;Python source&#x27;&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Uso como recuperador">Uso como recuperador<a class="anchor-link" href="#Uso como recuperador"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 103" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">retriever</span> <span class="o">=</span> <span class="n">vector_store</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">search_type</span><span class="o">=</span><span class="s2">&quot;mmr&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;k&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;fetch_k&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;lambda_mult&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">},</span>',
      '<span class="p">)</span>',
      '<span class="n">retriever</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="s2">&quot;python&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[Document(metadata=&#x7B;&#x27;source&#x27;: &#x27;Python source&#x27;&#x7D;, page_content=&#x27;This is Python docs&#x27;)]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h4 id="Medida de similaridade cosseno">Medida de similaridade cosseno<a class="anchor-link" href="#Medida de similaridade cosseno"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 104" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este método calcula a similaridade cosseno entre dois vetores, portanto, em vez de passar duas strings, é necessário passar dois vetores.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Então primeiro criamos um modelo de embeddings.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_huggingface</span><span class="w"> </span><span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>',
      '<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>',
      '<span class="n">encode_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;normalize_embeddings&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>',
      '<span class="w"> </span>',
      '<span class="n">embeddings_model</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">encode_kwargs</span><span class="o">=</span><span class="n">encode_kwargs</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">document1</span> <span class="o">=</span> <span class="s2">&quot;Python&quot;</span>',
      '<span class="n">document2</span> <span class="o">=</span> <span class="s2">&quot;python&quot;</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding1</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">document1</span><span class="p">)</span>',
      '<span class="n">embedding2</span> <span class="o">=</span> <span class="n">embeddings_model</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">document2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding1_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embedding1</span><span class="p">)</span>',
      '<span class="n">embedding2_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embedding2</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding1_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding1_numpy</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
      '<span class="n">embedding2_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding2_numpy</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>E agora podemos calcular a similaridade cosseno.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_chroma.vectorstores</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>',
      '<span class="w"> </span>',
      '<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding1_torch</span><span class="p">,</span> <span class="n">embedding2_torch</span><span class="p">)</span>',
      '<span class="n">similarity</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'array([[1.]])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>

















    </div>

  </section>

</PostLayout>
