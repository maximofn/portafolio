---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'llm.int8() ‚Äì 8-bit Matrix Multiplication for Transformers at Scale';
const end_url = 'llm-int8';
const description = 'Prepare-se para economizar espa√ßo e acelerar seus modelos! üí• Nesta postagem, vou explorar o m√©todo llm.int8(), uma t√©cnica de quantiza√ß√£o que permite reduzir o tamanho dos seus modelos de aprendizado de m√°quina sem sacrificar muito a precis√£o. üìä Isso significa que voc√™ poder√° treinar e implantar modelos maiores e mais complexos em menos espa√ßo e com menos consumo de recursos! üíª Vamos ver como usar llm.int8() com transformadores para quantizar um modelo e torn√°-lo mais eficiente, sem perder a ess√™ncia de sua intelig√™ncia artificial. ü§ñ';
const keywords = 'llm.int8(), transformers, quantiza√ß√£o, aprendizado de m√°quina, intelig√™ncia artificial, INT8, FP16';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8()-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1920
    image_height=1440
    image_extension=webp
    article_date=2024-07-23+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Quantizacao vetorial"><h2>Quantiza√ß√£o vetorial</h2></a>
      <a class="anchor-link" href="#Valor limiar Œ±"><h2>Valor limiar Œ±</h2></a>
      <a class="anchor-link" href="#Uso de llm.int8()"><h2>Uso de llm.int8()</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No post <a href="https://maximofn.com/llms-quantization/">LLMs quantization</a> explicamos a import√¢ncia da quantiza√ß√£o dos LLMs para economizar mem√≥ria. Al√©m disso, explicamos que existe uma maneira de quantiza√ß√£o que √© a <a href="https://maximofn.com/llms-quantization/#Cuantizaci%C3%B3n-de-punto-cero">cuantiza√ß√£o de ponto zero</a> que consiste em transformar os valores dos par√¢metros dos pesos linearmente, mas isso tem o problema da degrada√ß√£o dos modelos de linguagem a partir do momento em que eles ultrapassam 2,7B de par√¢metros.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-degradation.webp" alt="llm.int8()-degrada√ß√£o">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Quantizacao vetorial">Quantiza√ß√£o vetorial<a class="anchor-link" href="#Quantizacao vetorial"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como a quantiza√ß√£o de todos os par√¢metros dos modelos produz erro nos grandes modelos de linguagem, o que prop√µem no paper <a href="https://arxiv.org/abs/2208.07339" target="_blank" rel="nofollow noreferrer">llm.int8()</a> √© realizar a quantiza√ß√£o vetorial, ou seja, separar as matrizes de pesos em vetores, de maneira que alguns desses vetores podem ser quantizados em 8 bits, enquanto outros n√£o. Portanto, os que podem ser quantizados em 8 bits s√£o quantizados e as multiplica√ß√µes matriciais s√£o realizadas no formato INT8, enquanto os vetores que n√£o podem ser quantizados permanecem no formato FP16 e as multiplica√ß√µes s√£o realizadas no formato FP16.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos v√™-lo com um exemplo</p>
      <p>Suponhamos que temos a matriz</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-A.webp" alt="llm.int8()-A">
      <p>e queremos multiplic√°-la pela matriz</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-B.webp" alt="llm.int8()-B">
      <p>Estabelecemos um valor limiar e todas as colunas da primeira matriz que tenham um valor maior que esse limiar s√£o deixadas no formato FP16. As linhas equivalentes √†s linhas da primeira matriz, na segunda matriz tamb√©m s√£o deixadas no formato FP16.</p>
      <p>Como as colunas segunda e quarta da primeira matriz (colunas amarelas) t√™m valores maiores que um certo limiar, ent√£o as linhas segunda e quarta da segunda matriz (linhas amarelas) s√£o mantidas no formato FP16.</p>
      <p>Em caso de ter valores limiares na segunda matriz, far-se-ia o mesmo. Por exemplo, se uma linha da segunda matriz tivesse um valor maior que um limiar, ela seria deixada no formato FP16, e essa coluna na primeira matriz seria deixada no formato FP16.</p>
      <p>O restante das linhas e colunas que n√£o s√£o deixadas no formato FP16 √© quantizado em 8 bits e as multiplica√ß√µes s√£o realizadas no formato INT8</p>
      <p>Ent√£o, separamos a primeira matriz em duas submatrizes</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-A_separated_.webp" alt="llm.int8()-A_separated">
      <p>E a segunda matriz nas duas matrizes</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-B_separated_.webp" alt="llm.int8()-B_separated">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Multiplicamos as matrizes em INT8 de um lado</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-AxB-int8_.webp" alt="llm.int8()-AxB-int8">
      <p>E as que est√£o em formato FP16 por outro lado</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-AxB-fp16_.webp" alt="llm.int8()-AxB-fp16">
      <p>Como se pode ver, multiplicar as matrizes no formato INT8 nos d√° como resultado uma matriz de tamanho 3x2, e multiplicar as matrizes no formato FP16 nos d√° como resultado outra matriz de tamanho 3x2, portanto, se as somarmos</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-fp16int8_.webp" alt="llm.int8()-fp16+int8">
      <p>Curiosamente, nos d√° o mesmo resultado que se tiv√©ssemos multiplicado as matrizes originais</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-AxB_.webp" alt="llm.int8()-AxB">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder ver por que ocorre isso, se desenvolvermos o produto vetorial das duas matrizes originais</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-AxB-explained.webp" alt="llm.int8()-AxB-explained">
      <p>Vemos que a separa√ß√£o que fizemos n√£o d√° problemas</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Portanto, podemos concluir que podemos separar linhas e colunas das matrizes para realizar as multiplica√ß√µes matriciais. Esta separa√ß√£o ser√° feita quando algum elemento da linha ou coluna seja maior que um valor limite, de maneira que as linhas ou colunas que n√£o tenham um valor maior que esse limite ser√£o codificadas em INT8 ocupando apenas um byte e as linhas ou colunas que tenham algum elemento maior que esse limite ser√£o convertidas para FP16 ocupando 2 bytes. Dessa forma, n√£o teremos problemas de arredondamento, pois os c√°lculos que realizarmos em INT8 ser√£o feitos com valores que garantam que as multiplica√ß√µes n√£o ultrapassem o intervalo dos 8 bits.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Valor limiar Œ±">Valor limiar Œ±<a class="anchor-link" href="#Valor limiar Œ±"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como dissem, vamos a separar em linhas e colunas que tenham algum elemento maior que um valor limiar, mas ¬øqual valor limiar devemos escolher? Os autores do paper realizaram experimentos com v√°rios valores e determinaram que esse valor limiar deveria ser Œ±=6. Acima desse valor come√ßaram a obter degrada√ß√µes nos modelos de linguagem.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Uso de llm.int8()">Uso de llm.int8()<a class="anchor-link" href="#Uso de llm.int8()"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como quantizar um modelo com llm.int8() com a biblioteca transformers. Para isso, √© necess√°rio ter o <code>bitsandbytes</code> instalado.</p>
      <div class='highlight'><pre><code class="language-bash">pip install bitsandbytes</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Carregamos um modelo com 1B de par√¢metros duas vezes, uma de maneira normal e a segunda quantizando-o com llm.int8()</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos quanto mem√≥ria ocupa cada um dos modelos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">),</span> <span class="n">model_8bit</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(4.098002195358276, 1.1466586589813232)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como se pode ver, o modelo quantizado ocupa muito menos mem√≥ria</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos agora fazer um teste de gera√ß√£o de texto com os dois modelos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'tensor([[    1, 15043,   590,  1024,   338,  5918,  4200,   322,   306,   626,',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;263,  6189, 29257, 10863,   261]], device=&#x27;cuda:0&#x27;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos a sa√≠da com o modelo normal</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>',
      '<span class="w"> </span>',
      '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer. I am currently working at [Company Name] as a Machine Learning Engineer. I have a Bachelor&#x27;s degree in Computer Science from [University Name] and a Master&#x27;s degree in Computer Science from [University Name]. I',
          '1.7616662979125977',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E agora com o modelo quantizado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
      '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
      '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_8bit</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
      '<span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer. I am currently working at [Company Name] as a Machine Learning Engineer. I have a Bachelor&#x27;s degree in Computer Science from [University Name] and a Master&#x27;s degree in Computer Science from [University Name]. I',
          '9.100712776184082',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos duas coisas: por um lado, que na sa√≠da obtemos o mesmo texto; portanto, com um modelo muito menor podemos obter a mesma sa√≠da. No entanto, o modelo quantizado leva muito mais tempo para ser executado, ent√£o se for necess√°rio usar esse modelo em tempo real n√£o seria recomend√°vel.</p>
      <p>Isso √© contradit√≥rio, porque poder√≠amos pensar que um modelo menor teria que ser executado mais rapidamente, mas √© preciso considerar que na realidade os dois modelos, o normal e o quantizado, realizam as mesmas opera√ß√µes, apenas um realiza todas as opera√ß√µes em FP32 e o outro as faz em INT8 e FP16, no entanto, o modelo quantizado precisa encontrar linhas e colunas com valores maiores que o valor de limiar, separ√°-las, realizar as opera√ß√µes em INT8 e FP16 e depois juntar os resultados novamente, por isso o modelo quantizado leva mais tempo para ser executado.</p>
      </section>







    </div>

  </section>

</PostLayout>
