---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'llm.int8() ‚Äì 8-bit Matrix Multiplication for Transformers at Scale';
const end_url = 'llm-int8';
const description = 'Prepare-se para economizar espa√ßo e acelerar seus modelos! üí• Nesta postagem, vou explorar o m√©todo llm.int8(), uma t√©cnica de quantiza√ß√£o que permite reduzir o tamanho dos seus modelos de aprendizado de m√°quina sem sacrificar muito a precis√£o. üìä Isso significa que voc√™ poder√° treinar e implantar modelos maiores e mais complexos em menos espa√ßo e com menos consumo de recursos! üíª Vamos ver como usar llm.int8() com transformadores para quantizar um modelo e torn√°-lo mais eficiente, sem perder a ess√™ncia de sua intelig√™ncia artificial. ü§ñ';
const keywords = 'llm.int8(), transformers, quantiza√ß√£o, aprendizado de m√°quina, intelig√™ncia artificial, INT8, FP16';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8()-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1920
    image_height=1440
    image_extension=webp
    article_date=2024-07-23+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-de-vetores"><h2>Quantiza√ß√£o de vetores</h2></a>
      <a class="anchor-link" href="#Valor-limiar-%CE%B1"><h2>Valor limiar Œ±</h2></a>
      <a class="anchor-link" href="#Uso-de-llm.int8()"><h2>Uso de llm.int8()</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="llm.int8()---8-bit-Matrix-Multiplication-for-Transformers-at-Scale">llm.int8() - 8-bit Matrix Multiplication for Transformers at Scale<a class="anchor-link" href="#llm.int8()---8-bit-Matrix-Multiplication-for-Transformers-at-Scale"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Na postagem <a href="https://maximofn.com/llms-quantization/">LLMs quantization</a>, explicamos a import√¢ncia da quantiza√ß√£o dos LLMs para economizar mem√≥ria. Tamb√©m explicamos que h√° uma forma de quantiza√ß√£o que √© a <a href="https://maximofn.com/llms-quantization/#Cuantizaci%C3%B3n-de-punto-cero">quantiza√ß√£o de ponto zero</a>, que consiste em transformar os valores dos par√¢metros dos pesos linearmente, mas isso tem o problema da degrada√ß√£o dos modelos de linguagem a partir do momento em que eles ultrapassam 2,7 bilh√µes de par√¢metros.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-degrada√ß√£o" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-degradation.webp" width="868" height="702"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Quantiza%C3%A7%C3%A3o-de-vetores">Quantiza√ß√£o de vetores<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-de-vetores"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como a quantiza√ß√£o de todos os par√¢metros dos modelos produz erros nos modelos de idiomas grandes, o que eles prop√µem no artigo <a href="https://arxiv.org/abs/2208.07339" target="_blank" rel="nofollow noreferrer">llm.int8()</a> √© realizar a quantiza√ß√£o de vetores, ou seja, separar as matrizes dos pesos em vetores, de modo que alguns desses vetores possam ser quantizados em 8 bits, enquanto outros n√£o. Assim, aqueles que podem ser quantizados em 8 bits s√£o quantizados e as multiplica√ß√µes de matriz s√£o realizadas no formato INT8, enquanto os vetores que n√£o podem ser quantizados s√£o mantidos no formato FP16 e as multiplica√ß√µes s√£o realizadas no formato FP16.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos dar uma olhada em um exemplo</p>
      <p>Suponha que tenhamos a matriz</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-A" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-A.webp" width="378" height="227"/></p>
      <p>e queremos multiplic√°-lo pela matriz</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-B" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-B.webp" width="151" height="378"/></p>
      <p>Definimos um valor limite e todas as colunas da primeira matriz que t√™m um valor maior do que esse limite s√£o deixadas no formato FP16; as linhas equivalentes √†s linhas da primeira matriz na segunda matriz tamb√©m s√£o deixadas no formato FP16.</p>
      <p>Explicando melhor, como a segunda e a quarta colunas da primeira matriz (colunas amarelas) t√™m valores maiores que um determinado limite, a segunda e a quarta linhas da segunda matriz (linhas amarelas) s√£o deixadas no formato FP16.</p>
      <p>No caso de haver valores limiares na segunda matriz, o mesmo seria feito, por exemplo, se na segunda matriz uma linha tivesse um valor maior que um limiar, ela seria deixada no formato FP16, e essa coluna na primeira matriz seria deixada no formato FP16.</p>
      <p>As linhas e colunas restantes que n√£o s√£o deixadas no formato FP16 s√£o quantizadas em 8 bits e as multiplica√ß√µes s√£o realizadas no formato INT8.</p>
      <p>Portanto, dividimos a primeira matriz em duas matrizes</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-A_separated" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-A_separated_.webp" width="794" height="487"/></p>
      <p>E a segunda matriz nas duas matrizes</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-B_separated" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-B_separated_.webp" width="518" height="454"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Multiplicamos as matrizes em INT8 em um lado</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-AxB-int8" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-AxB-int8_.webp" width="680" height="227"/></p>
      <p>e aqueles no formato FP16, por outro lado</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-AxB-fp16" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-AxB-fp16_.webp" width="605" height="227"/></p>
      <p>Como pode ser visto, a multiplica√ß√£o das matrizes no formato INT8 nos d√° uma matriz de tamanho 3x2, e a multiplica√ß√£o das matrizes no formato FP16 nos d√° outra matriz de tamanho 3x2, de modo que, se as somarmos</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-fp16+int8" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-fp16int8_.webp" width="605" height="227"/></p>
      <p>√â interessante notar que ele apresenta o mesmo resultado como se tiv√©ssemos multiplicado as matrizes originais.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-AxB" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-AxB_.webp" width="832" height="378"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para ver por que isso acontece, se desenvolvermos o produto vetorial das duas matrizes originais</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="llm.int8()-AxB-explained" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/llm.int8-AxB-explained.webp" width="1020" height="490"/></p>
      <p>Vemos que a separa√ß√£o que fizemos n√£o traz problemas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Portanto, podemos concluir que √© poss√≠vel separar linhas e colunas de matrizes para realizar multiplica√ß√µes de matrizes. Essa separa√ß√£o ser√° feita quando qualquer elemento da linha ou coluna for maior que um valor limite, de modo que as linhas ou colunas que n√£o tiverem um valor maior que esse limite ser√£o codificadas em INT8, ocupando apenas um byte, e as linhas ou colunas que tiverem um elemento maior que esse limite ser√£o passadas para FP16, ocupando 2 bytes. Dessa forma, n√£o teremos problemas de arredondamento, pois os c√°lculos que fizermos em INT8 ser√£o feitos com valores que n√£o ultrapassem o intervalo de 8 bits.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Valor-limiar-%CE%B1">Valor limiar Œ±<a class="anchor-link" href="#Valor-limiar-%CE%B1"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como dissemos, vamos separar em linhas e colunas que tenham algum elemento maior que um valor limite, mas qual valor limite devemos escolher? Os autores do artigo fizeram experimentos com v√°rios valores e determinaram que o valor limite deveria ser Œ±=6. Acima desse valor, eles come√ßaram a obter degrada√ß√µes nos modelos de linguagem.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Uso-de-llm.int8()">Uso de llm.int8()<a class="anchor-link" href="#Uso-de-llm.int8()"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como quantificar um modelo com llm.int8() com a biblioteca de transformadores. Para fazer isso, voc√™ precisa ter o <code>bitsandbytes</code> instalado.</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>bitsandbytes
      <span class="sb">```</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Carregamos um modelo de par√¢metro 1B duas vezes, uma vez da maneira normal e a segunda vez quantificando-o com llm.int8().</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>',
          '<span class="kn">import</span> <span class="nn">torch</span>',
          '',
          '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
          '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span>',
          '',
          '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
          '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '<span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos a quantidade de mem√≥ria que cada um dos modelos ocupa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">),</span> <span class="n">model_8bit</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(4.098002195358276, 1.1466586589813232)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como pode ser visto, o modelo quantizado ocupa muito menos mem√≥ria.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos agora fazer um teste de gera√ß√£o de texto com os dois modelos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
          '<span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'tensor([[    1, 15043,   590,  1024,   338,  5918,  4200,   322,   306,   626,',
          '           263,  6189, 29257, 10863,   261]], device=\'cuda:0\')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos o resultado com o modelo normal</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">time</span>',
          '',
          '<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
          '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
          '    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
          '    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
          '    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer. I am currently working at [Company Name] as a Machine Learning Engineer. I have a Bachelor\'s degree in Computer Science from [University Name] and a Master\'s degree in Computer Science from [University Name]. I',
          '1.7616662979125977',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E agora com o modelo quantizado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>',
          '<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>',
          '<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_8bit</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>',
          '    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>',
          '    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>',
          '    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>',
          '<span class="p">)</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Hello my name is Maximo and I am a Machine Learning Engineer. I am currently working at [Company Name] as a Machine Learning Engineer. I have a Bachelor\'s degree in Computer Science from [University Name] and a Master\'s degree in Computer Science from [University Name]. I',
          '9.100712776184082',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos duas coisas: por um lado, obtemos o mesmo texto na sa√≠da, portanto, com um modelo muito menor, podemos obter a mesma sa√≠da; no entanto, o modelo quantizado leva muito mais tempo para ser executado, portanto, se voc√™ precisar usar esse modelo em tempo real, isso n√£o seria aconselh√°vel.</p>
      <p>Isso √© contradit√≥rio, porque poder√≠amos pensar que um modelo menor teria que ser executado mais rapidamente, mas temos que pensar que, na realidade, os dois modelos, o normal e o quantizado, executam as mesmas opera√ß√µes, apenas um executa todas as opera√ß√µes em FP32 e o outro as executa em INT8 e FP16, mas o modelo quantizado tem que procurar linhas e colunas com valores maiores que o valor limite, separ√°-las, executar as opera√ß√µes em INT8 e FP16 e, em seguida, juntar os resultados novamente, de modo que o modelo quantizado leva mais tempo para ser executado.</p>
      </section>
      






    </div>

  </section>

</PostLayout>
