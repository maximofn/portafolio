---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'LLMs quantization';
const end_url = 'llms-quantization';
const description = 'Imagine que voc√™ tem um modelo de linguagem gigante que pode responder a qualquer pergunta, desde a capital da Fran√ßa at√© a receita perfeita de brownies! üçûÔ∏èüá´üá∑ Mas o que acontece quando esse modelo precisa caber em um dispositivo m√≥vel? üì± √â a√≠ que entra a quantiza√ß√£o! üéâ Essa t√©cnica nos permite reduzir o tamanho dos modelos sem sacrificar sua precis√£o, o que significa que podemos desfrutar da intelig√™ncia artificial em nossos dispositivos m√≥veis sem a necessidade de um supercomputador. üíª √â como espremer um elefante em uma caixa de sapatos, mas sem esmagar o elefante! üêòüòÇ';
const keywords = 'quantiza√ß√£o, LLMs, GPT, IA, aprendizado de m√°quina, aprendizado profundo, FP16, FP32, TF32, BF16, INT8, UINT8, FP8, FP4, p√≥s-treinamento, durante-treinamento, ponto-zero, afim, QLoRA, AWQ, QuIP, GGUF, HQQ, AQLM, FBGEMM FP8';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/quantization-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1080
    image_height=794
    image_extension=webp
    article_date=2024-07-21+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Formato-do-par%C3%A2metro"><h2>Formato do par√¢metro</h2></a>
      <a class="anchor-link" href="#Tipos-de-quantiza%C3%A7%C3%A3o"><h2>Tipos de quantiza√ß√£o</h2></a>
      <a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-do-ponto-zero"><h3>Quantifica√ß√£o do ponto zero</h3></a>
      <a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-afim"><h3>Quantifica√ß√£o afim</h3></a>
      <a class="anchor-link" href="#Momentos-de-quantiza%C3%A7%C3%A3o"><h2>Momentos de quantiza√ß√£o</h2></a>
      <a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-p%C3%B3s-treinamento"><h3>Quantifica√ß√£o p√≥s-treinamento</h3></a>
      <a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-durante-o-treinamento"><h3>Quantiza√ß√£o durante o treinamento</h3></a>
      <a class="anchor-link" href="#M%C3%A9todos-de-quantiza%C3%A7%C3%A3o"><h2>M√©todos de quantiza√ß√£o</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Quantiza%C3%A7%C3%A3o-de-LLMs">Quantiza√ß√£o de LLMs<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-de-LLMs"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os modelos de linguagem est√£o ficando cada vez maiores, o que os torna cada vez mais caros e demorados para serem executados.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="LLMs-size-evolution" src="https://maximofn.com/wp-content/uploads/2024/07/LLMs-size-evolution-scaled.webp" width="1108" height="1200"/></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Flame-size-evolution" src="https://maximofn.com/wp-content/uploads/2024/07/Llama-size-evolution.webp" width="1080" height="1143"/></p>
      <p>Por exemplo, o modelo chama 3 400B, se seus par√¢metros forem armazenados no formato FP32, cada par√¢metro ocupar√° 4 bytes, o que significa que s√£o necess√°rios 400<em>(10e9)</em>4 bytes = 1,6 TB de mem√≥ria VRAM apenas para armazenar o modelo. Isso significa 20 GPUs com 80 GB de mem√≥ria VRAM cada, o que n√£o √© barato.</p>
      <p>Mas se deixarmos de lado os modelos gigantescos e passarmos a modelos com tamanhos mais comuns, por exemplo, 70 B de par√¢metros, apenas armazenar o modelo significa 70<em>(10e9)</em>4 bytes = 280 GB de mem√≥ria VRAM, o que significa 4 GPUs com 80 GB de mem√≥ria VRAM cada.</p>
      <p>Isso ocorre porque armazenamos os pesos no formato FP32, ou seja, cada par√¢metro ocupa 4 bytes. Mas o que acontece se fizermos com que cada par√¢metro ocupe menos bytes? Isso √© chamado de quantiza√ß√£o.</p>
      <p>Por exemplo, se obtivermos um modelo com par√¢metros 70B, cujos par√¢metros ocupam meio byte, precisaremos apenas de 70<em>(10e9)</em>0,5 bytes = 35 GB de mem√≥ria VRAM, o que significa 2 GPUs com 24 GB de mem√≥ria VRAM cada, que j√° podem ser consideradas GPUs de usu√°rio normais.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
      <p>Portanto, precisamos de maneiras de redimensionar esses modelos. H√° tr√™s maneiras de fazer isso: destila√ß√£o, poda e quantiza√ß√£o.</p>
      <p>A destila√ß√£o consiste em treinar um modelo menor a partir das sa√≠das do modelo maior. Ou seja, uma entrada √© fornecida ao modelo pequeno e ao modelo grande, a sa√≠da correta √© considerada a do modelo grande, de modo que o modelo pequeno √© treinado de acordo com a sa√≠da do modelo grande. Mas isso exige que o modelo grande seja armazenado, o que n√£o √© o que queremos ou podemos fazer.</p>
      <p>A poda consiste na remo√ß√£o de par√¢metros do modelo, tornando-o cada vez menor. Esse m√©todo baseia-se na ideia de que os modelos de linguagem atuais s√£o superdimensionados e que apenas alguns par√¢metros s√£o os que realmente fornecem informa√ß√µes. Portanto, se conseguirmos eliminar os par√¢metros que n√£o fornecem informa√ß√µes, obteremos um modelo menor. Mas isso n√£o √© f√°cil atualmente, porque n√£o temos como saber quais par√¢metros s√£o importantes e quais n√£o s√£o.</p>
      <p>Por outro lado, a quantiza√ß√£o consiste em reduzir o tamanho de cada um dos par√¢metros do modelo. E √© isso que vamos explicar nesta postagem.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Formato-do-par%C3%A2metro">Formato do par√¢metro<a class="anchor-link" href="#Formato-do-par%C3%A2metro"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os par√¢metros de peso podem ser armazenados em v√°rios tipos de formatos</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="representa√ß√£o de n√∫meros" src="https://maximofn.com/wp-content/uploads/2024/07/numbers-representation-scaled.webp" width="1200" height="379"/></p>
      <p>Originalmente, o FP32 era usado para armazenar os par√¢metros, mas como come√ßamos a ficar sem mem√≥ria para armazenar os modelos, come√ßamos a mudar para o FP16, que n√£o apresentou resultados ruins.</p>
      <p>No entanto, o problema com o FP16 √© que ele n√£o atinge valores t√£o altos quanto o FP32, portanto, pode ocorrer o caso de estouro de valor, ou seja, ao realizar c√°lculos internos na rede, o resultado √© t√£o alto que n√£o pode ser representado em FP16, o que gera erros. Isso ocorre porque o modelo foi treinado em FP32, o que possibilita todos os c√°lculos internos poss√≠veis, mas, quando ele √© alternado para FP16 para poder fazer infer√™ncias, alguns c√°lculos internos podem produzir transbordamentos.</p>
      <p>Devido a esses erros de estouro, foram criados o TF32 e o BF16, que t√™m a mesma quantidade de bits de expoente, o que significa que eles podem atingir valores t√£o altos quanto o FP32, mas com a vantagem de ocupar menos mem√≥ria porque t√™m menos bits. No entanto, ambos t√™m menos bits de mantissa, portanto, n√£o podem representar n√∫meros com a mesma precis√£o do FP32, o que pode levar a erros de arredondamento, mas, pelo menos, n√£o teremos um erro ao executar a rede. O TF32 tem um total de 19 bits, enquanto o BF16 tem 16 bits. O BF16 costuma ser mais usado porque economiza mais mem√≥ria.</p>
      <p>Historicamente, existem os formatos INT8 e UINT8, que podem representar n√∫meros de -128 a 127 e de 0 a 255, respectivamente. Embora sejam bons formatos porque economizam menos mem√≥ria, j√° que cada par√¢metro ocupa 1 byte em compara√ß√£o com os 4 bytes do FP32, o problema √© que eles s√≥ podem representar um pequeno intervalo de n√∫meros e tamb√©m apenas n√∫meros inteiros, de modo que podem ocorrer os dois problemas vistos anteriormente, estouro e falta de precis√£o.</p>
      <p>Para resolver o problema de os formatos INT8 e UINT8 representarem apenas n√∫meros inteiros, foram criados os formatos FP8 e FP4, mas eles ainda n√£o est√£o bem estabelecidos, nem t√™m um formato muito difundido.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mesmo que tenhamos uma maneira de armazenar os par√¢metros do modelo em formatos menores, e mesmo que consigamos resolver os problemas de estouro e arredondamento, temos outro problema, que √© o fato de nem todas as GPUs serem capazes de representar todos os formatos. Isso ocorre porque esses problemas de mem√≥ria s√£o relativamente novos, de modo que as GPUs mais antigas n√£o foram projetadas para resolver esses problemas e, portanto, n√£o s√£o capazes de representar todos os formatos.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="GPUs-data-formating" src="https://maximofn.com/wp-content/uploads/2024/07/GPUs-data-formating-scaled.webp" width="1200" height="308"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como √∫ltimo detalhe, a t√≠tulo de curiosidade, durante o treinamento dos modelos, √© usado o que chamamos de precis√£o mista. Os pesos do modelo s√£o armazenados no formato FP32, mas a passagem para frente e para tr√°s √© realizada em FP16 para torn√°-la mais r√°pida. Os gradientes resultantes da passagem para tr√°s s√£o armazenados em FP16 e s√£o usados para modificar os valores FP32 dos pesos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Tipos-de-quantiza%C3%A7%C3%A3o">Tipos de quantiza√ß√£o<a class="anchor-link" href="#Tipos-de-quantiza%C3%A7%C3%A3o"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantifica%C3%A7%C3%A3o-do-ponto-zero">Quantifica√ß√£o do ponto zero<a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-do-ponto-zero"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Esse √© o tipo mais simples de quantiza√ß√£o. Ele consiste em reduzir o intervalo de valores de forma linear, o valor m√≠nimo de FP32 corresponde ao valor m√≠nimo do novo formato, o zero de FP32 corresponde ao zero do novo formato e o valor m√°ximo de FP32 corresponde ao valor m√°ximo do novo formato.</p>
      <p>Por exemplo, se quisermos passar os n√∫meros representados de -1 a 1 no formato UINT8, como os limites de UINT8 s√£o -127 e 127, se quisermos representar o valor 0,3, o que faremos √© multiplicar 0,3 por 127, o que d√° 38,1 e arredondar para 38, que √© o valor que seria armazenado em UINT8.</p>
      <p>Se quisermos fazer a etapa oposta, para passar 38 para um formato entre -1 e 1, o que faremos √© dividir 38 por 127, o que d√° 0,2992, que √© aproximadamente 0,3, e podemos ver que temos um erro de 0,008.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="quantization-zero-point" src="https://maximofn.com/wp-content/uploads/2024/07/quantization-zero-point.webp" width="836" height="286"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantifica%C3%A7%C3%A3o-afim">Quantifica√ß√£o afim<a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-afim"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Nesse tipo de quantiza√ß√£o, se voc√™ tiver uma matriz de valores em um formato e quiser mudar para outro, primeiro divida a matriz de n√∫meros inteiros pelo valor m√°ximo da matriz e, em seguida, multiplique a matriz de n√∫meros inteiros pelo valor m√°ximo do novo formato.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="quantization-affine" src="https://maximofn.com/wp-content/uploads/2024/07/quantization-affine-scaled.webp" width="1200" height="669"/></p>
      <p>Por exemplo, na imagem acima, temos a matriz</p>
      <pre><code>[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]</code></pre>
      <p>Como seu valor m√°ximo √© <code>5,4</code>, dividimos a matriz por esse valor e obtemos</p>
      <pre><code>[0.2222222222, -0.09259259259, -0.7962962963, 0.2222222222, -0.5740740741, 0.1481481481, 0.4444444444, 1]</code></pre>
      <p>Se agora multiplicarmos todos os valores por <code>127</code>, que √© o valor m√°ximo de UINT8, obteremos</p>
      <pre><code>[28,22222222, -11.75925926, -101.1296296, 28.22222222, -72.90740741, 18.81481481, 56.44444444, 127]</code></pre>
      <p>O que, arredondando, seria</p>
      <pre><code>[28, -12, -101, 28, -73, 19, 56, 127]</code></pre>
      <p>Se agora quis√©ssemos executar a etapa inversa, ter√≠amos que dividir a matriz resultante por <code>127</code>, o que daria</p>
      <pre><code>[0,2204724409, -0.09448818898, -0.7952755906, 0.2204724409, -0.5748031496, 0.1496062992, 0.4409448819, 1]</code></pre>
      <p>E multiplique novamente por <code>5,4</code>, para obtermos</p>
      <pre><code>[1,190551181, -0.5102362205, -4.294488189, 1.190551181, -3.103937008, 0.8078740157, 2.381102362, 5.4]</code></pre>
      <p>Se a compararmos com a matriz original, veremos que temos um erro</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Momentos-de-quantiza%C3%A7%C3%A3o">Momentos de quantiza√ß√£o<a class="anchor-link" href="#Momentos-de-quantiza%C3%A7%C3%A3o"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantifica%C3%A7%C3%A3o-p%C3%B3s-treinamento">Quantifica√ß√£o p√≥s-treinamento<a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-p%C3%B3s-treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como o nome sugere, a quantiza√ß√£o ocorre ap√≥s o treinamento. O modelo √© treinado em FP32 e, em seguida, quantizado em outro formato. Esse m√©todo √© o mais simples, mas pode levar a erros de precis√£o na quantiza√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantiza%C3%A7%C3%A3o-durante-o-treinamento">Quantiza√ß√£o durante o treinamento<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-durante-o-treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Durante o treinamento, a passagem para frente √© realizada no modelo original e em um modelo quantizado, e os poss√≠veis erros decorrentes da quantiza√ß√£o s√£o vistos para atenu√°-los. Esse processo torna o treinamento mais caro, porque √© necess√°rio ter o modelo original e o modelo quantizado armazenados na mem√≥ria, e mais lento, porque √© necess√°rio executar a passagem para frente em dois modelos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="M%C3%A9todos-de-quantiza%C3%A7%C3%A3o">M√©todos de quantiza√ß√£o<a class="anchor-link" href="#M%C3%A9todos-de-quantiza%C3%A7%C3%A3o"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Abaixo est√£o os links para as postagens em que explico cada um dos m√©todos para que esta postagem n√£o fique muito longa.</p>
      <ul>
      <li><a href="/pt-br/llm-int8">LLM.int8()</a></li>
      <li><a href="/pt-br/gptq">GPTQ</a></li>
      <li>QLoRA</li>
      <li>AWQ</li>
      <li>QuIP</li>
      <li>GGUF</li>
      <li>HQQ</li>
      <li>AQLM</li>
      <li>FBGEMM FP8</li>
      </ul>
      </section>
      






    </div>

  </section>

</PostLayout>
