---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'LLMs quantization';
const end_url = 'llms-quantization';
const description = 'Imagine que voc√™ tem um modelo de linguagem gigante que pode responder a qualquer pergunta, desde a capital da Fran√ßa at√© a receita perfeita de brownies! üçûÔ∏èüá´üá∑ Mas o que acontece quando esse modelo precisa caber em um dispositivo m√≥vel? üì± √â a√≠ que entra a quantiza√ß√£o! üéâ Essa t√©cnica nos permite reduzir o tamanho dos modelos sem sacrificar sua precis√£o, o que significa que podemos desfrutar da intelig√™ncia artificial em nossos dispositivos m√≥veis sem a necessidade de um supercomputador. üíª √â como espremer um elefante em uma caixa de sapatos, mas sem esmagar o elefante! üêòüòÇ';
const keywords = 'quantiza√ß√£o, LLMs, GPT, IA, aprendizado de m√°quina, aprendizado profundo, FP16, FP32, TF32, BF16, INT8, UINT8, FP8, FP4, p√≥s-treinamento, durante-treinamento, ponto-zero, afim, QLoRA, AWQ, QuIP, GGUF, HQQ, AQLM, FBGEMM FP8';
const languaje = 'PT';
const image_path = 'https://images.maximofn.com/quantization-thumbnail.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1080
    image_height=794
    image_extension=webp
    article_date=2024-07-21+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Formato dos parametros"><h2>Formato dos par√¢metros</h2></a>
      <a class="anchor-link" href="#Tipos de quantizacao"><h2>Tipos de quantiza√ß√£o</h2></a>
      <a class="anchor-link" href="#Quantizacao de ponto zero"><h3>Quantiza√ß√£o de ponto zero</h3></a>
      <a class="anchor-link" href="#Quantizacao Afim"><h3>Quantiza√ß√£o Afim</h3></a>
      <a class="anchor-link" href="#Momentos de quantizacao"><h2>Momentos de quantiza√ß√£o</h2></a>
      <a class="anchor-link" href="#Quantizacao pos-treinamento"><h3>Quantiza√ß√£o p√≥s-treinamento</h3></a>
      <a class="anchor-link" href="#Quantizacao durante o treinamento"><h3>Quantiza√ß√£o durante o treinamento</h3></a>
      <a class="anchor-link" href="#Metodos de quantizacao"><h2>M√©todos de quantiza√ß√£o</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os modelos de linguagem est√£o cada vez maiores, o que os torna cada vez mais custosos e caros de executar.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://images.maximofn.com/LLMs-size-evolution.webp" alt="LLMs-size-evolution">
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://images.maximofn.com/Llama-size-evolution.webp" alt="Llama-size-evolution">
      <p>Por exemplo, o modelo Llama 3 400B, se seus par√¢metros estiverem armazenados no formato FP32, cada par√¢metro ocupa, portanto, 4 bytes, o que significa que apenas para armazenar o modelo s√£o necess√°rios 400*(10e9)*4 bytes = 1,6 TB de mem√≥ria VRAM. Isso equivale a 20 GPUs com 80GB de mem√≥ria VRAM cada, as quais, al√©m disso, n√£o s√£o baratas.</p>
      <p>Mas se deixarmos de lado modelos gigantes e nos focarmos em modelos de tamanhos mais comuns, por exemplo, 70B de par√¢metros, apenas armazenar o modelo representa 70*(10e9)*4 bytes = 280 GB de mem√≥ria VRAM, o que equivale a 4 GPUs de 80GB de mem√≥ria VRAM cada uma.</p>
      <p>Isso acontece porque armazenamos os pesos no formato FP32, ou seja, cada par√¢metro ocupa 4 bytes. Mas o que acontece se conseguirmos que cada par√¢metro ocupe menos bytes? Isso √© chamado de quantiza√ß√£o.</p>
      <p>Por exemplo, se conseguirmos que um modelo de 70B de par√¢metros ocupe meio byte por par√¢metro, ent√£o precisar√≠amos apenas de 70*(10e9)*0.5 bytes = 35 GB de mem√≥ria VRAM, o que equivale a 2 GPUs de 24GB de mem√≥ria VRAM cada uma, as quais j√° podem ser consideradas GPUs de usu√°rios normais.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Portanto, precisamos maneiras de poder reduzir o tamanho desses modelos. Existem tr√™s formas de fazer isso: a destila√ß√£o, a poda e a quantiza√ß√£o.</p>
      <p>A destila√ß√£o consiste em treinar um modelo menor a partir das sa√≠das do maior. Isso significa que uma entrada √© fornecida tanto ao modelo pequeno quanto ao grande, considerando-se que a sa√≠da correta √© a do modelo grande, de modo que o treinamento do modelo pequeno √© realizado de acordo com a sa√≠da do modelo grande. Mas isso requer ter armazenado o modelo grande, o que n√£o √© o que queremos ou podemos fazer.</p>
      <p>A poda consiste em eliminar par√¢metros do modelo, tornando-o cada vez menor. Este m√©todo se baseia na ideia de que os modelos de linguagem atuais est√£o sobredimensionados e apenas alguns poucos par√¢metros s√£o os que realmente contribuem com informa√ß√µes. Por isso, se conseguirmos eliminar os par√¢metros que n√£o fornecem informa√ß√µes, obteremos um modelo menor. Mas isso n√£o √© simples atualmente, porque n√£o temos uma maneira de saber bem quais par√¢metros s√£o importantes e quais n√£o s√£o.</p>
      <p>Por outro lado, a quantiza√ß√£o consiste em reduzir o tamanho de cada um dos par√¢metros do modelo. E √© isso que vamos explicar neste post.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Formato dos parametros">Formato dos par√¢metros<a class="anchor-link" href="#Formato dos parametros"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os par√¢metros dos pesos podem ser armazenados em v√°rios tipos de formatos</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://images.maximofn.com/numbers-representation.webp" alt="numbers-representation">
      <p>Originalmente, era usado o FP32 para armazenar os par√¢metros, mas devido ao fato de come√ßarmos a ficar sem mem√≥ria para armazenar os modelos, passamos a usar o FP16, o que n√£o dava resultados ruins.</p>
      <p>No entanto, o problema do FP16 √© que ele n√£o alcan√ßa valores t√£o altos quanto o FP32, o que pode levar ao caso de overflow de valores, ou seja, ao realizar c√°lculos internos na rede, o resultado pode ser t√£o alto que n√£o possa ser representado em FP16, gerando erros. Isso ocorre porque o modelo foi treinado em FP32, o que permite que todos os poss√≠veis c√°lculos internos sejam realizados, mas ao passar para FP16 posteriormente para realizar infer√™ncias, alguns c√°lculos internos podem causar overflow.</p>
      <p>Devido a esses erros de overflow, foram criados o TF32 e o BF16, os quais t√™m a mesma quantidade de bits de expoente, o que permite que eles alcancem valores t√£o altos quanto o FP32, mas com a vantagem de ocupar menos mem√≥ria por terem menos bits. No entanto, ambos, por terem menos bits de mantissa, n√£o podem representar n√∫meros com tanta precis√£o quanto o FP32, o que pode resultar em erros de arredondamento, mas pelo menos n√£o obteremos um erro ao executar a rede. O TF32 tem no total 19 bits, enquanto o BF16 tem 16 bits. Geralmente, usa-se mais o BF16 porque se economiza mais mem√≥ria.</p>
      <p>Hist√≥ricamente existiram os formatos INT8 e UINT8, que podem representar n√∫meros de -128 a 127 e de 0 a 255, respectivamente. Embora sejam formatos bons porque permitem economizar mem√≥ria, j√° que cada par√¢metro ocupa 1 byte em compara√ß√£o aos 4 bytes do FP32, o problema que t√™m √© que s√≥ podem representar um intervalo pequeno de n√∫meros e, al√©m disso, apenas inteiros, pelo que podem ocorrer os dois problemas vistos anteriormente: desbordamento e falta de precis√£o.</p>
      <p>Para resolver o problema de que os formatos INT8 e UINT8 representam apenas n√∫meros inteiros, foram criados os formatos FP8 e FP4, mas ainda n√£o est√£o muito consolidados, nem possuem um formato muito padronizado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Embora tenhamos meios de armazenar os par√¢metros dos modelos em formatos menores, e embora consigamos resolver os problemas de overflow e arredondamento, temos outro problema, que √© o fato de que nem todas as GPUs s√£o capazes de representar todos os formatos. Isso ocorre porque esses problemas de mem√≥ria s√£o relativamente novos, de modo que as GPUs mais antigas n√£o foram projetadas para resolver esses problemas e, portanto, n√£o s√£o capazes de representar todos os formatos.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://images.maximofn.com/GPUs-data-formating.webp" alt="GPUs-data-formating">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como √∫ltimo detalhe, como curiosidade, durante o treinamento dos modelos √© utilizada a chamada precis√£o mista. Os pesos do modelo s√£o armazenados no formato FP32, no entanto, o <code>forward pass</code> e o <code>backward pass</code> s√£o realizados em FP16 para ser mais r√°pido. Os gradientes resultantes do <code>backward pass</code> s√£o armazenados em FP16 e usados para modificar os valores FP32 dos pesos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Tipos de quantizacao">Tipos de quantiza√ß√£o<a class="anchor-link" href="#Tipos de quantizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao de ponto zero">Quantiza√ß√£o de ponto zero<a class="anchor-link" href="#Quantizacao de ponto zero"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este √© o tipo de quantiza√ß√£o mais simples. Consiste em reduzir o intervalo de valores de maneira linear, o menor valor de FP32 corresponde ao menor valor do novo formato, o zero de FP32 corresponde ao zero do novo formato e o maior valor de FP32 corresponde ao maior valor do novo formato.</p>
      <p>Por exemplo, se quisermos passar os n√∫meros representados de -1 at√© 1 no formato UINT8, como os limites do UINT8 s√£o -127 e 127, se quisermos representar o valor 0.3, o que fazemos √© multiplicar 0.3 por 127, o que d√° 38.1 e arredond√°-lo para 38, que √© o valor que seria armazenado no UINT8.</p>
      <p>Se quisermos fazer o passo contr√°rio, para converter 38 para o formato entre -1 e 1, o que fazemos √© dividir 38 por 127, que d√° 0.2992, que √© aproximadamente 0.3, e podemos ver que temos um erro de 0.008</p>
      <p>!<a href="https://images.maximofn.com/quantization-zero-point.webp" target="_blank" rel="nofollow noreferrer">quantization-zero-point</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao Afim">Quantiza√ß√£o Afim<a class="anchor-link" href="#Quantizacao Afim"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Neste tipo de quantiza√ß√£o, se tiver um array de valores em um formato e quiser passar para outro, primeiro divide-se o array inteiro pelo valor m√°ximo do array e depois multiplica-se o array inteiro pelo valor m√°ximo do novo formato.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://images.maximofn.com/quantization-affine.webp" alt="quantization-affine">
      <p>Por exemplo, na imagem acima temos o array</p>
      <div class='highlight'><pre><code class="language-text">[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]</code></pre></div>
      <p>Como seu valor m√°ximo √© <code>5.4</code>, dividimos o array por esse valor e obtemos</p>
      <div class='highlight'><pre><code class="language-text">[0.2222222222, -0.09259259259, -0.7962962963, 0.2222222222, -0.5740740741, 0.1481481481, 0.4444444444, 1]</code></pre></div>
      <p>Se agora multiplicarmos todos os valores por <code>127</code>, que √© o valor m√°ximo de UINT8, obtemos</p>
      <div class='highlight'><pre><code class="language-text">[28,22222222, -11,75925926, -101,1296296, 28,22222222, -72,90740741, 18,81481481, 56,44444444, 127]</code></pre></div>
      <p>Que, arredondando, seria</p>
      <div class='highlight'><pre><code class="language-text">[28, -12, -101, 28, -73, 19, 56, 127]</code></pre></div>
      <p>Se quisermos realizar o passo inverso, ter√≠amos que dividir o array resultante por <code>127</code>, que daria</p>
      <div class='highlight'><pre><code class="language-text">[0,2204724409, -0,09448818898, -0,7952755906, 0,2204724409, -0,5748031496, 0,1496062992, 0,4409448819, 1]</code></pre></div>
      <p>E multiplicar novamente por <code>5.4</code>, com o que obter√≠amos</p>
      <div class='highlight'><pre><code class="language-text">[1,190551181, -0.5102362205, -4.294488189, 1.190551181, -3.103937008, 0.8078740157, 2.381102362, 5.4]</code></pre></div>
      <p>Se compararmos com o array original, vemos que temos um erro</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Momentos de quantizacao">Momentos de quantiza√ß√£o<a class="anchor-link" href="#Momentos de quantizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao pos-treinamento">Quantiza√ß√£o p√≥s-treinamento<a class="anchor-link" href="#Quantizacao pos-treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como o nome sugere, a quantiza√ß√£o ocorre ap√≥s o treinamento. O modelo √© treinado em FP32 e depois √© quantizado para outro formato. Este m√©todo √© o mais simples, mas pode levar a erros de precis√£o na quantiza√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Quantizacao durante o treinamento">Quantiza√ß√£o durante o treinamento<a class="anchor-link" href="#Quantizacao durante o treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Durante o treinamento, √© realizado o <code>forward pass</code> no modelo original e em um modelo quantizado, e s√£o observados os poss√≠veis erros decorrentes da quantiza√ß√£o para poder mitig√°-los. Este processo torna o treinamento mais custoso, pois voc√™ precisa armazenar na mem√≥ria o modelo original e o quantizado, e mais lento, pois voc√™ precisa realizar o <code>forward pass</code> em dois modelos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Metodos de quantizacao">M√©todos de quantiza√ß√£o<a class="anchor-link" href="#Metodos de quantizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A seguir mostro os links para os posts onde explico cada um dos m√©todos, para que este post n√£o fique muito longo</p>
      <ul>
        <li><a href="/llm-int8">LLM.int8()</a></li>
        <li><a href="/gptq">GPTQ</a></li>
        <li><a href="/qlora">QLoRA</a></li>
        <li>AWQ</li>
        <li>QuIP</li>
        <li>GGUF</li>
        <li>HQQ</li>
        <li>AQLM</li>
        <li>FBGEMM FP8</li>
      </ul>
      </section>







    </div>

  </section>

</PostLayout>
