---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'LoRA ‚Äì low rank adaptation of large language models';
const end_url = 'lora';
const description = 'Prepare-se para levar a adapta√ß√£o de modelos para o pr√≥ximo n√≠vel com o LoRA! üöÄ Essa t√©cnica de adapta√ß√£o de baixa classifica√ß√£o √© como uma capa de super-her√≥i para suas redes neurais - ela as ajuda a aprender novos truques sem esquecer os antigos ü§Ø. E o melhor de tudo? Voc√™ pode implement√°-la em apenas algumas linhas de c√≥digo PyTorch üíª E se voc√™ for como eu, um pobre cara da GPU lutando com recursos limitados üí∏, o LoRA √© como uma d√°diva de Deus: ele permite que voc√™ adapte seus modelos sem trein√°-los do zero ou gastar uma fortuna em hardware üôè Confira a postagem para obter um guia passo a passo e um exemplo pr√°tico!';
const keywords = 'lora, adapta√ß√£o de baixa classifica√ß√£o, redes neurais, pytorch, gpu, hardware';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_thumbnail_ES.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1080
    image_height=607
    image_extension=webp
    article_date=2024-07-20+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Explicacao de LoRA"><h2>Explica√ß√£o de LoRA</h2></a>
      <a class="anchor-link" href="#Atualizacao de pesos em uma rede neural"><h3>Atualiza√ß√£o de pesos em uma rede neural</h3></a>
      <a class="anchor-link" href="#LoRA"><h3>LoRA</h3></a>
      <a class="anchor-link" href="#Implementacao de LoRA em transformers"><h3>Implementa√ß√£o de LoRA em transformers</h3></a>
      <a class="anchor-link" href="#Tamanho do intervalo r"><h3>Tamanho do intervalo r</h3></a>
      <a class="anchor-link" href="#Inicializacao das matrizes A e B"><h3>Inicializa√ß√£o das matrizes A e B</h3></a>
      <a class="anchor-link" href="#Influencia de LoRA por meio do parametro $\alpha$"><h3>Influ√™ncia de LoRA por meio do par√¢metro $\alpha$</h3></a>
      <a class="anchor-link" href="#Vantagens do LoRA"><h2>Vantagens do LoRA</h2></a>
      <a class="anchor-link" href="#Implementacao de LoRA em um LLM"><h2>Implementa√ß√£o de LoRA em um LLM</h2></a>
      <a class="anchor-link" href="#Login no Hub"><h3>Login no Hub</h3></a>
      <a class="anchor-link" href="#Conjunto de Dados"><h3>Conjunto de Dados</h3></a>
      <a class="anchor-link" href="#Tokenizador"><h3>Tokenizador</h3></a>
      <a class="anchor-link" href="#Modelo"><h3>Modelo</h3></a>
      <a class="anchor-link" href="#LoRA"><h3>LoRA</h3></a>
      <a class="anchor-link" href="#Treinamento"><h3>Treinamento</h3></a>
      <a class="anchor-link" href="#Avaliacao"><h3>Avalia√ß√£o</h3></a>
      <a class="anchor-link" href="#Publicar o modelo"><h3>Publicar o modelo</h3></a>
      <a class="anchor-link" href="#Teste do modelo"><h2>Teste do modelo</h2></a>
      <a class="anchor-link" href="#Implementacao de LoRA em um LLM com PEFT da Hugging Face"><h2>Implementa√ß√£o de LoRA em um LLM com PEFT da Hugging Face</h2></a>
      <a class="anchor-link" href="#Login no Hub"><h3>Login no Hub</h3></a>
      <a class="anchor-link" href="#Conjunto de Dados"><h3>Conjunto de Dados</h3></a>
      <a class="anchor-link" href="#Tokenizador"><h3>Tokenizador</h3></a>
      <a class="anchor-link" href="#Modelo"><h3>Modelo</h3></a>
      <a class="anchor-link" href="#LoRA com PEFT"><h3>LoRA com PEFT</h3></a>
      <a class="anchor-link" href="#Treinamento"><h3>Treinamento</h3></a>
      <a class="anchor-link" href="#Avaliacao"><h3>Avalia√ß√£o</h3></a>
      <a class="anchor-link" href="#Publicar o modelo"><h3>Publicar o modelo</h3></a>
      <a class="anchor-link" href="#Teste do modelo treinado com PEFT"><h2>Teste do modelo treinado com PEFT</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O aumento do tamanho dos modelos de linguagem faz com que sejam cada vez mais caros de treinar, pois √© necess√°rio cada vez mais VRAM para armazenar todos os seus par√¢metros e os gradientes derivados do treinamento.</p>
      <p>No artigo <a href="https://arxiv.org/abs/2106.09685" target="_blank" rel="nofollow noreferrer">LoRA - Low rank adaption of large language models</a> prop√µem congelar os pesos do modelo e treinar duas matrizes chamadas A e B, reduzindo significativamente o n√∫mero de par√¢metros que precisam ser treinados.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_adapat.webp" alt="LoRA">
      <p>Vamos a ver como se faz isso</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Explicacao de LoRA">Explica√ß√£o de LoRA<a class="anchor-link" href="#Explicacao de LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 59" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Atualizacao de pesos em uma rede neural">Atualiza√ß√£o de pesos em uma rede neural<a class="anchor-link" href="#Atualizacao de pesos em uma rede neural"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 60" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para entender como funciona LoRA, primeiro temos que lembrar o que ocorre quando treinamos um modelo. Voltemos √† parte mais b√°sica do deep learning, temos uma camada densa de uma rede neural que √© definida como:</p>
      <p><span class="math-display">y = Wx + b</span></p>
      <p>Onde <span class="math-inline">W</span> √© a matriz de pesos e <span class="math-inline">b</span> √© o vetor de vieses.</p>
      <p>Para simplificar, vamos a supor que n√£o h√° vi√©s, portanto ficaria assim</p>
      <p><span class="math-display">y = Wx</span></p>
      <p>Suponhamos que para uma entrada <span class="math-inline">x</span> queremos que tenha uma sa√≠da <span class="math-inline">≈∑</span></p>
      <ul>
        <li>Primeiro, o que fazemos √© calcular a sa√≠da que obtemos com nosso valor atual de pesos <span class="math-inline">W</span>, ou seja, obtemos o valor <span class="math-inline">y</span></li>
        <li>Em seguida, calculamos o erro que existe entre o valor de <span class="math-inline">y</span> que obtivemos e o valor que quer√≠amos obter <span class="math-inline">≈∑</span>. A esse erro chamamos de <span class="math-inline">loss</span>, e o calculamos com alguma fun√ß√£o matem√°tica, agora n√£o importa qual.</li>
        <li>Calculamos o gradiente (a derivada) do erro <span class="math-inline">loss</span> em rela√ß√£o √† matriz de pesos <span class="math-inline">W</span>, ou seja, <span class="math-inline">\Delta W = \frac&#123;dloss&#125;&#123;dW&#125;</span></li>
        <li>Atualizamos os pesos <span class="math-inline">W</span> subtraindo de cada um dos seus valores o valor do gradiente multiplicado por um fator de aprendizado <span class="math-inline">\alpha</span>, ou seja, <span class="math-inline">W = W - \alpha \Delta W</span></li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="LoRA">LoRA<a class="anchor-link" href="#LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 61" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Os autores de LoRA prop√µem que a matriz de pesos <span class="math-inline">W</span> pode ser decomposta em</p>
      <p><span class="math-display">W \sim W + \Delta W</span></p>
      <p>Assim, ao congelar a matriz <span class="math-inline">W</span> e treinar apenas a matriz <span class="math-inline">\Delta W</span>, pode-se obter um modelo que se adeque a novos dados sem ter que retreinar todo o modelo.</p>
      <p>Mas voc√™ pode pensar que <span class="math-inline">\Delta W</span> √© uma matriz do mesmo tamanho de <span class="math-inline">W</span>, portanto nada foi ganho, mas aqui os autores se baseiam em <code>Aghajanyan et al. (2020)</code>, um artigo no qual eles demonstraram que, embora os modelos de linguagem sejam grandes e seus par√¢metros sejam matrizes com dimens√µes muito grandes, para adapt√°-los a novas tarefas n√£o √© necess√°rio alterar todos os valores das matrizes, mas sim alterar alguns poucos valores, o que tecnicamente √© chamado de adapta√ß√£o de baixa classifica√ß√£o. Da√≠ o nome LoRA (Low Rank Adaptation).</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Congelamos o modelo e agora queremos treinar a matriz <span class="math-inline">\Delta W</span>. Suponhamos que tanto <span class="math-inline">W</span> quanto <span class="math-inline">\Delta W</span> s√£o matrizes de tamanho <span class="math-inline">20 &times; 10</span>, portanto temos 200 par√¢metros trein√°veis.</p>
      <p>Agora suponhamos que a matriz <span class="math-inline">\Delta W</span> pode ser decomposta no produto de duas matrizes <span class="math-inline">A</span> e <span class="math-inline">B</span>, ou seja</p>
      <p><span class="math-display">\Delta W = A ¬∑ B</span></p>
      <p>Para que esta multiplica√ß√£o ocorra, os tamanhos das matrizes <span class="math-inline">A</span> e <span class="math-inline">B</span> t√™m que ser <span class="math-inline">20 &times; n</span> e <span class="math-inline">n &times; 10</span>, respectivamente. Suponhamos que <span class="math-inline">n = 5</span>, portanto <span class="math-inline">A</span> seria de tamanho <span class="math-inline">20 &times; 5</span>, ou seja, 100 par√¢metros, e <span class="math-inline">B</span> de tamanho <span class="math-inline">5 &times; 10</span>, ou seja, 50 par√¢metros, por isso ter√≠amos 100+50=150 par√¢metros trein√°veis. J√° temos menos par√¢metros trein√°veis do que antes.</p>
      <p>Agora suponhamos que <span class="math-inline">W</span> na verdade √© uma matriz de tamanho <span class="math-inline">10.000 &times; 10.000</span>, portanto ter√≠amos 100.000.000 par√¢metros trein√°veis, mas se decompor <span class="math-inline">\Delta W</span> em <span class="math-inline">A</span> e <span class="math-inline">B</span> com <span class="math-inline">n = 5</span>, ter√≠amos uma matriz de tamanho <span class="math-inline">10.000 &times; 5</span> e outra de tamanho <span class="math-inline">5 &times; 10.000</span>, portanto ter√≠amos 50.000 par√¢metros de uma e outros 50.000 par√¢metros de outra, no total 100.000 par√¢metros trein√°veis, ou seja, reduzimos o n√∫mero de par√¢metros 1000 vezes</p>
      <p>J√° pode ver o poder da LoRA, quando se t√™m modelos muito grandes, o n√∫mero de par√¢metros trein√°veis pode ser reduzido drasticamente.</p>
      <p>Se voltarmos a ver a imagem da arquitetura de LoRA, a entenderemos melhor.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_adapat.webp" alt="LoRA adapt">
      <p>Mas parece ainda melhor, a economia no n√∫mero de par√¢metros trein√°veis com esta imagem</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Lora_matmul.webp" alt="LoRA matmul">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Implementacao de LoRA em transformers">Implementa√ß√£o de LoRA em transformers<a class="anchor-link" href="#Implementacao de LoRA em transformers"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 62" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como os modelos de linguagem s√£o implementa√ß√µes de transformers, vamos ver como se implementa LoRA em transformers. Na arquitetura transformer h√° camadas lineares nas matrizes de aten√ß√£o <span class="math-inline">Q</span>, <span class="math-inline">K</span> e <span class="math-inline">V</span>, e nas camadas feedforward, por isso pode-se aplicar LoRA a todas essas camadas lineares. No paper, eles falam que, por simplicidade, o aplicam apenas √†s camadas lineares das matrizes de aten√ß√£o <span class="math-inline">Q</span>, <span class="math-inline">K</span> e <span class="math-inline">V</span>.</p>
      <p>Estas camadas t√™m um tamanho <span class="math-inline">d<sub>model</sub> &times; d<sub>model</sub></span>, onde <span class="math-inline">d<sub>model</sub></span> √© a dimens√£o de embedding do modelo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tamanho do intervalo r">Tamanho do intervalo r<a class="anchor-link" href="#Tamanho do intervalo r"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 63" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder ter esses benef√≠cios, o tamanho do intervalo <span class="math-inline">r</span> deve ser menor que o tamanho das camadas lineares. Como dissemos que s√≥ o implementavam nas camadas lineares de aten√ß√£o, que t√™m um tamanho <span class="math-inline">d<sub>model</sub> &times; d<sub>model</sub></span>, o tamanho do intervalo <span class="math-inline">r</span> deve ser menor que <span class="math-inline">d<sub>model</sub></span>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Inicializacao das matrizes A e B">Inicializa√ß√£o das matrizes A e B<a class="anchor-link" href="#Inicializacao das matrizes A e B"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 64" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As matrizes <span class="math-inline">A</span> e <span class="math-inline">B</span> s√£o inicializadas com uma distribui√ß√£o gaussiana aleat√≥ria para <span class="math-inline">A</span> e zero para <span class="math-inline">B</span>, assim o produto de ambas as matrizes ser√° zero no in√≠cio, ou seja</p>
      <p><span class="math-display">\Delta W = A ¬∑ B = 0</span></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Influencia de LoRA por meio do parametro $\alpha$">Influ√™ncia de LoRA por meio do par√¢metro $\alpha$<a class="anchor-link" href="#Influencia de LoRA por meio do parametro $\alpha$"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 65" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, na implementa√ß√£o de LoRA, adiciona-se um par√¢metro <span class="math-inline">&alpha;</span> para estabelecer o grau de influ√™ncia de LoRA no treinamento. √â similar √† taxa de aprendizado (learning rate) no fine tuning normal, mas neste caso √© usado para estabelecer a influ√™ncia de LoRA no treinamento. Dessa forma, a f√≥rmula de LoRA ficaria assim</p>
      <p><span class="math-display">W = W + &alpha; \Delta W = W + &alpha; A ¬∑ B</span></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Vantagens do LoRA">Vantagens do LoRA<a class="anchor-link" href="#Vantagens do LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 66" src={svg_paths.link_svg_path}/></a></h2>
      <p>Agora que entendemos como isso funciona, vamos ver as vantagens desse m√©todo.</p>
      <ul>
        <li>Redu√ß√£o do n√∫mero de par√¢metros trein√°veis. Como vimos, o n√∫mero de par√¢metros trein√°veis √© reduzido drasticamente, o que torna o treinamento muito mais r√°pido e requer menos VRAM, economizando muitos custos.* Adaptadores em produ√ß√£o. Podemos ter em produ√ß√£o um √∫nico modelo de linguagem e v√°rios adaptadores, cada um para uma tarefa diferente, em vez de ter v√°rios modelos treinados para cada tarefa, o que economiza custos de armazenamento e computa√ß√£o. Al√©m disso, este m√©todo n√£o precisa adicionar lat√™ncia na infer√™ncia porque a matriz de pesos original pode ser fusionada com o adaptador, j√° que vimos que <span class="math-inline">W \sim W + \Delta W = W + A \cdot B</span>, portanto, o tempo de infer√™ncia seria o mesmo que usar o modelo de linguagem original.</li>
        <li>Compartilhar adaptadores. Se treinarmos um adaptador, podemos compartilhar apenas o adaptador. Isso significa que, em produ√ß√£o, todos podem ter o modelo original e cada vez que treinamos um adaptador, compartilhamos apenas o adaptador, portanto, como seriam compartilhadas matrizes muito menores, o tamanho dos arquivos compartilhados seria muito menor</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Implementacao de LoRA em um LLM">Implementa√ß√£o de LoRA em um LLM<a class="anchor-link" href="#Implementacao de LoRA em um LLM"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 67" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a repetir o c√≥digo de treinamento do post <a href="https://maximofn.com/fine-tuning-sml/">Fine tuning SLMs</a>, em espec√≠fico o treinamento para classifica√ß√£o de texto com as bibliotecas da Hugging Face, mas desta vez vamos fazer isso com LoRA. No post anterior, usamos um batch size de 28 para o loop de treinamento e de 40 para o de avalia√ß√£o, no entanto, como agora n√£o vamos treinar todos os pesos do modelo, mas apenas as matrizes de LoRA, poderemos usar um batch size maior.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Login no Hub">Login no Hub<a class="anchor-link" href="#Login no Hub"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 68" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>N√≥s nos logamos para fazer o upload do modelo para o Hub</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Conjunto de Dados">Conjunto de Dados<a class="anchor-link" href="#Conjunto de Dados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 69" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Baixamos o conjunto de dados que vamos utilizar, que √© um conjunto de dados de avalia√ß√µes do <a href="https://huggingface.co/datasets/mteb/amazon_reviews_multi" target="_blank" rel="nofollow noreferrer">Amazon</a></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>',
      '<span class="w"> </span>',
      '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;mteb/amazon_reviews_multi&quot;</span><span class="p">,</span> <span class="s2">&quot;en&quot;</span><span class="p">)</span>',
      '<span class="n">dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'DatasetDict(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;train: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x20;&#x20;&#x20;&#x20;validation: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x20;&#x20;&#x20;&#x20;test: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos um subconjunto se voc√™ quiser testar o c√≥digo com um conjunto de dados menor. No meu caso, usarei 100% do conjunto de dados.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">percentage</span> <span class="o">=</span> <span class="mi">1</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
      '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
      '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos uma amostra</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">random</span><span class="w"> </span><span class="kn">import</span> <span class="n">randint</span>',
      '<span class="w"> </span>',
      '<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset_dataset_train</span><span class="p">))</span>',
      '<span class="n">subset_dataset_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;id&#x27;: &#x27;en_0388304&#x27;,',
          '&#x27;text&#x27;: &#x27;The N was missing from on\n\nThe N was missing from on&#x27;,',
          '&#x27;label&#x27;: 0,',
          '&#x27;label_text&#x27;: &#x27;0&#x27;&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos o n√∫mero de classes, para obter o n√∫mero de classes usamos <code>dataset[&#x27;train&#x27;]</code> e n√£o <code>subset_dataset_train</code> porque se o subconjunto for muito pequeno √© poss√≠vel que n√£o haja exemplos com todas as poss√≠veis classes do conjunto de dados original</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">&#39;label&#39;</span><span class="p">))</span>',
      '<span class="n">num_classes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '5',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos uma fun√ß√£o para criar o campo <code>label</code> no dataset. O dataset baixado tem o campo <code>labels</code>, mas a biblioteca <code>transformers</code> precisa que o campo seja chamado de <code>label</code> e n√£o <code>labels</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_labels</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">example</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aplicamos a fun√ß√£o ao conjunto de dados</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
      '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
      '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;, &#x27;labels&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;, &#x27;labels&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;, &#x27;labels&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Voltamos a ver um exemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;id&#x27;: &#x27;en_0388304&#x27;,',
          '&#x27;text&#x27;: &#x27;The N was missing from on\n\nThe N was missing from on&#x27;,',
          '&#x27;label&#x27;: 0,',
          '&#x27;label_text&#x27;: &#x27;0&#x27;,',
          '&#x27;labels&#x27;: 0&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizador">Tokenizador<a class="anchor-link" href="#Tokenizador"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 70" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Implementamos o tokenizador. Para que n√£o d√™ erro, atribu√≠mos o token de end of string ao token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos uma fun√ß√£o para tokenizar o dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aplicamos a fun√ß√£o ao conjunto de dados e, de passagem, eliminamos as colunas que n√£o precisamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_text&#39;</span><span class="p">])</span>',
      '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_text&#39;</span><span class="p">])</span>',
      '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_text&#39;</span><span class="p">])</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Voltamos a ver uma amostra, mas neste caso s√≥ vemos as <code>keys</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'dict_keys([&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 71" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos o modelo. Tamb√©m, para que n√£o nos d√™ erro, atribu√≠mos o token de end of string ao token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>',
      '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;score.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como j√° vimos no post <a href="https://maximofn.com/fine-tuning-sml/">Fine tuning SLMs</a> obtemos um warning que diz que algumas camadas n√£o foram inicializadas. Isso acontece porque, neste caso, como √© um problema de classifica√ß√£o e quando instanciamos o modelo dissemos que queremos que seja um modelo de classifica√ß√£o com 5 classes, a biblioteca removeu a √∫ltima camada e a substituiu por uma com 5 neur√¥nios na sa√≠da. Se n√£o entende bem isso, veja o post que cito, que est√° melhor explicado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="LoRA">LoRA<a class="anchor-link" href="#LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 72" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de implementar LoRA, vemos o n√∫mero de par√¢metros trein√°veis que tem o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total trainable parameters before: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Total trainable parameters before: 124,443,648',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que tem 124M de par√¢metros trein√°veis. Agora vamos congel√°-los.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>',
      '<span class="w"> </span>',
      '<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total trainable parameters after: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Total trainable parameters after: 0',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ap√≥s o congelamento, n√£o h√° mais par√¢metros trein√°veis.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver como √© o modelo antes de aplicar LoRA</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(score): Linear(in_features=768, out_features=5, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro criamos a camada LoRA layer.</p>
      <p>Tem que herdar de <code>torch.nn.Module</code> para poder atuar como uma camada de uma rede neural</p>
      <p>No m√©todo <code>_init_</code> criamos as matrizes <code>A</code> e <code>B</code> inicializadas como explicado anteriormente, a matriz <code>A</code> com uma distribui√ß√£o gaussiana aleat√≥ria e a matriz <code>B</code> com zeros. Tamb√©m criamos os par√¢metros <code>rank</code> e <code>alpha</code>.</p>
      <p>No m√©todo <code>forward</code> calculamos LoRA conforme explicado.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="k">class</span><span class="w"> </span><span class="nc">LoRALayer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '<span class="w">        </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>',
      '<span class="w">        </span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># similar to standard weight initialization</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
      '<span class="w">        </span><span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>',
      '<span class="w">        </span><span class="k">return</span> <span class="n">x</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos uma classe linear com LoRA.</p>
      <p>Assim como antes, herde de <code>torch.nn.Module</code> para que possa atuar como uma camada de uma rede neural.</p>
      <p>No m√©todo <code>init</code> criamos uma vari√°vel com a camada linear original da rede e criamos outra vari√°vel com a nova camada LoRA que hav√≠amos implementado anteriormente.</p>
      <p>No m√©todo <code>forward</code> somamos as sa√≠das da camada linear original e da camada LoRA.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">class</span><span class="w"> </span><span class="nc">LoRALinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '<span class="w">        </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear</span>',
      '<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">lora</span> <span class="o">=</span> <span class="n">LoRALayer</span><span class="p">(</span>',
      '<span class="w">            </span><span class="n">linear</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">linear</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span>',
      '<span class="w">        </span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>',
      '<span class="w">        </span><span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, criamos uma fun√ß√£o que substitua as camadas lineares pela nova camada linear com LoRA que criamos. O que ela faz √© que se encontrar uma camada linear no modelo, a substitui pela camada linear com LoRA; caso contr√°rio, aplica a fun√ß√£o dentro das subcamadas da camada.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">replace_linear_with_lora</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>',
      '<span class="w">            </span><span class="c1"># Replace the Linear layer with LinearWithLoRA</span>',
      '<span class="w">            </span><span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">LoRALinear</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>',
      '<span class="w">        </span><span class="k">else</span><span class="p">:</span>',
      '<span class="w">            </span><span class="c1"># Recursively apply the same function to child modules</span>',
      '<span class="w">            </span><span class="n">replace_linear_with_lora</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aplicamos a fun√ß√£o ao modelo para substituir as camadas lineares do modelo pela nova camada linear com LoRA</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">rank</span> <span class="o">=</span> <span class="mi">16</span>',
      '<span class="n">alpha</span> <span class="o">=</span> <span class="mi">16</span>',
      '<span class="w"> </span>',
      '<span class="n">replace_linear_with_lora</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos agora o n√∫mero de par√¢metros trein√°veis</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total trainable LoRA parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Total trainable LoRA parameters: 12,368',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Passamos de 124M de par√¢metros trein√°veis para 12k par√¢metros trein√°veis, ou seja, reduzimos o n√∫mero de par√¢metros trein√°veis 10.000 vezes!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Voltamos a ver o modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(score): LoRALinear(',
          '&#x20;&#x20;&#x20;&#x20;(linear): Linear(in_features=768, out_features=5, bias=False)',
          '&#x20;&#x20;&#x20;&#x20;(lora): LoRALayer()',
          '&#x20;&#x20;)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a compar√°-los camada por camada</p>
      <table>
        <thead>
          <tr>
            <th>Modelo original</th>
            <th>Modelo com LoRA</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>GPT2ForSequenceClassification(</td>
            <td>GPT2ForSequenceClassification(</td>
          </tr>
          <tr>
            <td>(transformer): GPT2Model(</td>
            <td>(transformer): GPT2Model(</td>
          </tr>
          <tr>
            <td>(wte): Embedding(50257, 768)</td>
            <td>(wte): Embedding(50257, 768)</td>
          </tr>
          <tr>
            <td>(wpe): Embedding(1024, 768)</td>
            <td>(wpe): Embedding(1024, 768)</td>
          </tr>
          <tr>
            <td>(drop): Dropout(p=0.1, inplace=False)</td>
            <td>(drop): Dropout(p=0.1, inplace=False)</td>
          </tr>
          <tr>
            <td>(h): ModuleList(</td>
            <td>(h): ModuleList(</td>
          </tr>
          <tr>
            <td>(0-11): 12 x GPT2Block(</td>
            <td>(0-11): 12 x GPT2Block(</td>
          </tr>
          <tr>
            <td>(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
            <td>(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
          </tr>
          <tr>
            <td>(attn): GPT2Attention(</td>
            <td>(attn): GPT2Attention(</td>
          </tr>
          <tr>
            <td>(c_attn): Conv1D()</td>
            <td>(c_attn): Conv1D()</td>
          </tr>
          <tr>
            <td>(c_proj): Conv1D()</td>
            <td>(c_proj): Conv1D()</td>
          </tr>
          <tr>
            <td>(attn_dropout): Dropout(p=0.1, inplace=False)</td>
            <td>(attn_dropout): Dropout(p=0.1, inplace=False)</td>
          </tr>
          <tr>
            <td>(resid_dropout): Dropout(p=0.1, inplace=False)</td>
            <td>(resid_dropout): Dropout(p=0.1, inplace=False)</td>
          </tr>
          <tr>
            <td>)</td>
            <td>)</td>
          </tr>
          <tr>
            <td>(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
            <td>(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
          </tr>
          <tr>
            <td>(c_fc): Conv1D()</td>
            <td>(c_fc): Conv1D()</td>
          </tr>
          <tr>
            <td>(c_proj): Conv1D()</td>
            <td>(c_proj): Conv1D()</td>
          </tr>
          <tr>
            <td>(act): NewGELUActivation()</td>
            <td>(act): NewGELUActivation()</td>
          </tr>
          <tr>
            <td>(dropout): Dropout(p=0.1, inplace=False)</td>
            <td>(dropout): Dropout(p=0.1, inplace=False)</td>
          </tr>
          <tr>
            <td>)</td>
            <td>)</td>
          </tr>
          <tr>
            <td>)</td>
            <td>)</td>
          </tr>
          <tr>
            <td>)</td>
            <td>)</td>
          </tr>
          <tr>
            <td>(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
            <td>(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)</td>
          </tr>
          <tr>
            <td>)</td>
            <td>)</td>
          </tr>
          <tr>
            <td></td>
            <td>(pontua√ß√£o): LoRALinear()</td>
          </tr>
          <tr>
            <td>(score): Linear(in_features=768, out_features=5, bias=False)</td>
            <td>(linear): Linear(in_features=768, out_features=5, bias=False)</td>
          </tr>
          <tr>
            <td></td>
            <td>(lora): LoRALÂ±Ç()</td>
          </tr>
        </tbody>
      </table>
      <p>Nota: Parece que hubo un error en la traducci√≥n del t√©rmino "LoRALayer" al portugu√©s. La traducci√≥n correcta ser√≠a:</p>
      <table>
        <thead>
          <tr>
            <th></th>
            <th>(lora): LoRALayer()</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>)</td>
            <td>)</td>
          </tr>
        </tbody>
      </table>
      <p>Vemos que s√£o iguais, exceto no final, onde no modelo original havia uma camada linear normal e no modelo com LoRA h√° uma camada <code>LoRALinear</code> que dentro possui a camada linear do modelo original e uma camada <code>LoRALayer</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 73" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez instanciado o modelo com LoRA, vamos trein√°-lo como sempre</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como dissem, no post <a href="https://maximofn.com/fine-tuning-sml/">Fine tuning SLMs</a> usamos um batch size de 28 para o loop de treinamento e de 40 para o de avalia√ß√£o, enquanto agora que h√° menos par√¢metros trein√°veis podemos usar um batch size maior.</p>
      <p>Por que isso acontece? Quando se treina um modelo, √© necess√°rio armazenar na mem√≥ria da GPU o modelo e os gradientes desse modelo, ent√£o tanto com LoRA quanto sem LoRA, o modelo precisa ser armazenado igualmente, mas no caso do LoRA, apenas os gradientes de 12k par√¢metros s√£o armazenados, enquanto que sem LoRA, os gradientes de 128M de par√¢metros s√£o armazenados, por isso com LoRA √© necess√°rio menos mem√≥ria da GPU, permitindo o uso de um batch size maior.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '<span class="w"> </span>',
      '<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">&quot;accuracy&quot;</span>',
      '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;GPT2-small-LoRA-finetuned-amazon-reviews-en-classification&quot;</span>',
      '<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '<span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '<span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '<span class="w"> </span>',
      '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model_name</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">eval_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">logging_dir</span><span class="o">=</span><span class="s2">&quot;./runs&quot;</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">evaluate</span><span class="w"> </span><span class="kn">import</span> <span class="n">load</span>',
      '<span class="w"> </span>',
      '<span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '<span class="w">    </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainer</span>',
      '<span class="w"> </span>',
      '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">training_args</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;IPython.core.display.HTML object&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7cd07be46440&amp;gt;',
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7cd07be45c30&amp;gt;',
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7cd07be8b970&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'TrainOutput(global_step=1500, training_loss=1.8345018310546874, metrics=&#x7B;&#x27;train_runtime&#x27;: 2565.4667, &#x27;train_samples_per_second&#x27;: 233.876, &#x27;train_steps_per_second&#x27;: 0.585, &#x27;total_flos&#x27;: 2.352076406784e+17, &#x27;train_loss&#x27;: 1.8345018310546874, &#x27;epoch&#x27;: 3.0&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Avaliacao">Avalia√ß√£o<a class="anchor-link" href="#Avaliacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 74" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez treinado, avaliamos sobre o dataset de teste</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;IPython.core.display.HTML object&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7cd07be8bbe0&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;eval_loss&#x27;: 1.5203168392181396,',
          '&#x27;eval_accuracy&#x27;: 0.3374,',
          '&#x27;eval_runtime&#x27;: 19.3843,',
          '&#x27;eval_samples_per_second&#x27;: 257.94,',
          '&#x27;eval_steps_per_second&#x27;: 0.671,',
          '&#x27;epoch&#x27;: 3.0&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Publicar o modelo">Publicar o modelo<a class="anchor-link" href="#Publicar o modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 75" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>J√° temos nosso modelo treinado, j√° podemos compartilh√°-lo com o mundo, ent√£o primeiro criamos uma **model card**</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>E j√° podemos public√°-lo. Como a primeira coisa que fizemos foi fazer login no hub do Hugging Face, podemos envi√°-lo para o nosso hub sem nenhum problema.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Teste do modelo">Teste do modelo<a class="anchor-link" href="#Teste do modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 76" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Limpez tudo o poss√≠vel</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">gc</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">clear_hardwares</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>',
      '<span class="w">    </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>',
      '<span class="w">    </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>',
      '<span class="w">    </span><span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="n">clear_hardwares</span><span class="p">()</span>',
      '<span class="n">clear_hardwares</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como subimos o modelo ao nosso hub, podemos baix√°-lo e us√°-lo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">user</span> <span class="o">=</span> <span class="s2">&quot;maximofn&quot;</span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;text-classification&quot;</span>',
      '<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora, se quisermos que nos retorne a probabilidade de todas as classes, simplesmente usamos o classificador que acabamos de instanciar, com o par√¢metro <code>top_k=None</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">&quot;I love this product&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>',
      '<span class="n">labels</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;label&#x27;: &#x27;LABEL_0&#x27;, &#x27;score&#x27;: 0.8419149518013&#x7D;,',
          '&#x7B;&#x27;label&#x27;: &#x27;LABEL_1&#x27;, &#x27;score&#x27;: 0.09386005252599716&#x7D;,',
          '&#x7B;&#x27;label&#x27;: &#x27;LABEL_3&#x27;, &#x27;score&#x27;: 0.03624210134148598&#x7D;,',
          '&#x7B;&#x27;label&#x27;: &#x27;LABEL_2&#x27;, &#x27;score&#x27;: 0.02049318142235279&#x7D;,',
          '&#x7B;&#x27;label&#x27;: &#x27;LABEL_4&#x27;, &#x27;score&#x27;: 0.0074898069724440575&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos apenas a classe com a maior probabilidade, fazemos o mesmo mas com o par√¢metro <code>top_k=1</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">label</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">&quot;I love this product&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="n">label</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;label&#x27;: &#x27;LABEL_0&#x27;, &#x27;score&#x27;: 0.8419149518013&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E se quisermos n classes, fazemos o mesmo mas com o par√¢metro <code>top_k=n</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">two_labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">&quot;I love this product&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
      '<span class="n">two_labels</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;label&#x27;: &#x27;LABEL_0&#x27;, &#x27;score&#x27;: 0.8419149518013&#x7D;,',
          '&#x7B;&#x27;label&#x27;: &#x27;LABEL_1&#x27;, &#x27;score&#x27;: 0.09386005252599716&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Tamb√©m podemos testar o modelo com Automodel e AutoTokenizer</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;GPT2-small-finetuned-amazon-reviews-en-classification&quot;</span>',
      '<span class="n">user</span> <span class="o">=</span> <span class="s2">&quot;maximofn&quot;</span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;I love this product&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>',
      '<span class="n">logits</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="n">lables</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>',
      '<span class="n">lables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[0.003940582275390625,',
          '0.00266265869140625,',
          '0.013946533203125,',
          '0.1544189453125,',
          '0.8251953125]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se voc√™ quiser testar mais o modelo, voc√™ pode v√™-lo em <a href="https://huggingface.co/Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification" target="_blank" rel="nofollow noreferrer">Maximofn/GPT2-small-LoRA-finetuned-amazon-reviews-en-classification</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Implementacao de LoRA em um LLM com PEFT da Hugging Face">Implementa√ß√£o de LoRA em um LLM com PEFT da Hugging Face<a class="anchor-link" href="#Implementacao de LoRA em um LLM com PEFT da Hugging Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 77" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos fazer o mesmo com a biblioteca <code>PEFT</code> do Hugging Face. Vamos v√™-lo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Login no Hub">Login no Hub<a class="anchor-link" href="#Login no Hub"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 78" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>N√≥s fazemos login para fazer o upload do modelo para o Hub</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Conjunto de Dados">Conjunto de Dados<a class="anchor-link" href="#Conjunto de Dados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 79" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Voltamos a baixar o dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>',
      '<span class="w"> </span>',
      '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;mteb/amazon_reviews_multi&quot;</span><span class="p">,</span> <span class="s2">&quot;en&quot;</span><span class="p">)</span>',
      '<span class="n">dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'DatasetDict(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;train: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x20;&#x20;&#x20;&#x20;validation: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x20;&#x20;&#x20;&#x20;test: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos um subconjunto caso voc√™ queira testar o c√≥digo com um conjunto de dados menor. No meu caso, usarei 100% do conjunto de dados.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">percentage</span> <span class="o">=</span> <span class="mi">1</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
      '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
      '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos o n√∫mero de classes, para obter o n√∫mero de classes usamos <code>dataset[&#x27;train&#x27;]</code> e n√£o <code>subset_dataset_train</code> porque se o subconjunto for muito pequeno √© poss√≠vel que n√£o haja exemplos com todas as poss√≠veis classes do conjunto de dados original</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">&#39;label&#39;</span><span class="p">))</span>',
      '<span class="n">num_classes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '5',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos uma fun√ß√£o para criar o campo <code>label</code> no dataset. O dataset baixado tem o campo <code>labels</code>, mas a biblioteca <code>transformers</code> precisa que o campo seja chamado de <code>label</code> e n√£o <code>labels</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_labels</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">example</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aplicamos a fun√ß√£o ao conjunto de dados</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
      '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
      '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;, &#x27;labels&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;, &#x27;labels&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;, &#x27;labels&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizador">Tokenizador<a class="anchor-link" href="#Tokenizador"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 80" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos o tokenizador. Para que n√£o nos d√™ erro, atribu√≠mos o token de end of string ao token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Criamos uma fun√ß√£o para tokenizar o dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aplicamos a fun√ß√£o ao conjunto de dados e, de passagem, eliminamos as colunas que n√£o precisamos.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_text&#39;</span><span class="p">])</span>',
      '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_text&#39;</span><span class="p">])</span>',
      '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_text&#39;</span><span class="p">])</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 81" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Instanciamos o modelo. Tamb√©m, para que n√£o d√™ erro, atribu√≠mos o token de end of string ao token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>',
      '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;score.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="LoRA com PEFT">LoRA com PEFT<a class="anchor-link" href="#LoRA com PEFT"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 82" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de criar o modelo com LoRA, vamos a ver suas camadas</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(score): Linear(in_features=768, out_features=5, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, h√° apenas uma camada <code>Linear</code>, que √© <code>score</code> e que √© a que vamos substituir.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos criar uma configura√ß√£o de LoRA com a biblioteca PEFT e depois aplicar LoRA ao mo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">TaskType</span>',
      '<span class="w"> </span>',
      '<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;score&quot;</span><span class="p">],</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Com esta configura√ß√£o, definimos um rank de 16 e um alpha de 32. Al√©m disso, adicionamos um dropout √†s camadas de LoRA de 0.1. Precisamos indicar a tarefa para a configura√ß√£o de LoRA, neste caso √© uma tarefa de sequence classification. Por fim, indicamos quais camadas queremos substituir, neste caso a camada <code>score</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora aplicamos LoRA ao modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_peft_model</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver quantos par√¢metros trein√°veis o modelo tem agora.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'trainable params: 12,368 || all params: 124,456,016 || trainable%: 0.0099',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos os mesmos par√¢metros trein√°veis que antes</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 83" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez instanciado o modelo com LoRA, vamos trein√°-lo como sempre</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '<span class="w"> </span>',
      '<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">&quot;accuracy&quot;</span>',
      '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification&quot;</span>',
      '<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '<span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">400</span>',
      '<span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">400</span>',
      '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '<span class="w"> </span>',
      '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model_name</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">eval_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">logging_dir</span><span class="o">=</span><span class="s2">&quot;./runs&quot;</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">evaluate</span><span class="w"> </span><span class="kn">import</span> <span class="n">load</span>',
      '<span class="w"> </span>',
      '<span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '<span class="w">    </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainer</span>',
      '<span class="w"> </span>',
      '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">training_args</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;IPython.core.display.HTML object&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7f774a50bbe0&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;IPython.core.display.HTML object&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7f77486a7c40&amp;gt;',
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7f7749eb5690&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'TrainOutput(global_step=1500, training_loss=1.751504597981771, metrics=&#x7B;&#x27;train_runtime&#x27;: 2551.7753, &#x27;train_samples_per_second&#x27;: 235.13, &#x27;train_steps_per_second&#x27;: 0.588, &#x27;total_flos&#x27;: 2.352524525568e+17, &#x27;train_loss&#x27;: 1.751504597981771, &#x27;epoch&#x27;: 3.0&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Avaliacao">Avalia√ß√£o<a class="anchor-link" href="#Avaliacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 84" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez treinado, avaliamos sobre o dataset de teste</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;IPython.core.display.HTML object&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7f77a1d1f7c0&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;eval_loss&#x27;: 1.4127237796783447,',
          '&#x27;eval_accuracy&#x27;: 0.3862,',
          '&#x27;eval_runtime&#x27;: 19.3275,',
          '&#x27;eval_samples_per_second&#x27;: 258.699,',
          '&#x27;eval_steps_per_second&#x27;: 0.673,',
          '&#x27;epoch&#x27;: 3.0&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Publicar o modelo">Publicar o modelo<a class="anchor-link" href="#Publicar o modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 85" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos uma model card</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>O publicamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification/commit/839066c2bde02689a6b3f5624ac25f89c4de217d&#x27;, commit_message=&#x27;End of training&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;839066c2bde02689a6b3f5624ac25f89c4de217d&#x27;, pr_url=None, pr_revision=None, pr_num=None)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Teste do modelo treinado com PEFT">Teste do modelo treinado com PEFT<a class="anchor-link" href="#Teste do modelo treinado com PEFT"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 86" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Limpamos tudo o poss√≠vel</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">gc</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">clear_hardwares</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>',
      '<span class="w">    </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>',
      '<span class="w">    </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>',
      '<span class="w">    </span><span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="n">clear_hardwares</span><span class="p">()</span>',
      '<span class="n">clear_hardwares</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como subimos o modelo ao nosso hub, podemos baix√°-lo e us√°-lo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>',
      '<span class="w"> </span>',
      '<span class="n">user</span> <span class="o">=</span> <span class="s2">&quot;maximofn&quot;</span>',
      '<span class="n">checkpoints</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;text-classification&quot;</span>',
      '<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;score.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora, se quisermos que nos retorne a probabilidade de todas as classes, simplesmente usamos o classificador que acabamos de instanciar, com o par√¢metro <code>top_k=None</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">&quot;I love this product&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>',
      '<span class="n">labels</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;label&#x27;: &#x27;LABEL_1&#x27;, &#x27;score&#x27;: 0.9979197382926941&#x7D;,',
          '&#x7B;&#x27;label&#x27;: &#x27;LABEL_0&#x27;, &#x27;score&#x27;: 0.002080311067402363&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se quisermos apenas a classe com a maior probabilidade fazemos o mesmo mas com o par√¢metro <code>top_k=1</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">label</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">&quot;I love this product&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="n">label</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;label&#x27;: &#x27;LABEL_1&#x27;, &#x27;score&#x27;: 0.9979197382926941&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>E se quisermos n classes, fazemos o mesmo mas com o par√¢metro <code>top_k=n</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">two_labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">&quot;I love this product&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>',
      '<span class="n">two_labels</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[&#x7B;&#x27;label&#x27;: &#x27;LABEL_1&#x27;, &#x27;score&#x27;: 0.9979197382926941&#x7D;,',
          '&#x7B;&#x27;label&#x27;: &#x27;LABEL_0&#x27;, &#x27;score&#x27;: 0.002080311067402363&#x7D;]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Se voc√™ quiser testar mais o modelo, pode v√™-lo em <a href="https://huggingface.co/Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification" target="_blank" rel="nofollow noreferrer">Maximofn/GPT2-small-PEFT-LoRA-finetuned-amazon-reviews-en-classification</a></p>
      </section>







    </div>

  </section>

</PostLayout>
