---
import Layout from '@layouts/Layout.astro';

const { social_links } = await import('@portfolio/consts.json');
const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { color_palette } = await import('@portfolio/consts.json');
const { sticky_top_positions } = await import('@portfolio/consts.json');

const page_title = metadata_page.title + " - Naviground";
const url = social_links.portfolio_link_external + "/naviground"
const description = "Sistema de percepción para vehículo autónomo";
const keywords = "Percepción, vehículo autónomo";

const images_width = 1920*2/3;
const images_height =1080*2/3;
const buyMeACoffee_width = 305;
const buyMeACoffee_height = 28;

const inline_code_background_color = color_palette.color_950;

const sticky_top_default=sticky_top_positions.sticky_top_default;
const sticky_top_649px=sticky_top_positions.sticky_top_649px;
const sticky_top_418px=sticky_top_positions.sticky_top_418px;
const sticky_top_334px=sticky_top_positions.sticky_top_334px;
const sticky_top_315px=sticky_top_positions.sticky_top_315px;
const sticky_top_229px=sticky_top_positions.sticky_top_229px;
const sticky_top_h3_increment = "50px"
---

<Layout 
	title={page_title}
	languaje={metadata_page.languaje_pt}
	description={description}
	keywords={keywords}
	author={metadata_page.author}
	theme_color={colors.background_color}
	url={url}
	icon={metadata_page.icon}
>
	<div class="project">
		<h1>Naviground</h1>
		<h4>Sistema de percepção para veículo autônomo</h4>
		<figure class="video-container">
			<video 
				preload="auto"
				controls
				width={images_width}
				height={images_height}
			>
				<source src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground.webm" type="video/webm"/>
				<source src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground.mp4" type="video/mp4"/>
			</video>
		</figure>

        <p>Naviground é um sistema de navegação implementável em veículos terrestres tripulados e não tripulados. Permite navegar em ambientes estruturados e não estruturados. Participei no desenvolvimento do sistema de percepção, especialmente na detecção do ambiente por meio de câmeras.</p>

        <section>
            <h2>Sistema de visão</h2>
            <p>Embora o sistema de navegação contasse com sensores LIDAR e RADAR, por vários motivos, foi desejado um sistema de percepção formado exclusivamente por câmeras.</p>
            <ul>
                <li>Embora o preço dos LIDAR e RADAR tenha diminuído muito nos últimos anos, continua sendo mais caro que o das câmeras.</li>
                <li>Os sensores LIDAR e RADAR são sensores ativos (emitem uma onda eletromagnética e medem a reflexão), por isso, em um ambiente de guerra, fazem com que o veículo possa ser detectado.</li>
                <li>Como é um veículo autônomo, o processamento não pode ser feito em uma máquina muito poderosa, por isso, se pode eliminar o processamento da quantidade de dados que geram os LIDAR e RADAR, melhor.</li>
            </ul>
            <p>Para poder realizar a detecção do ambiente, utilizamos três tipos de redes neurais:</p>
            <ul>
                <li>
                    <p>Redes de segmentação semântica</p>
                    <p>Classificam a que classe pertence cada pixel da imagem, obtendo uma máscara de segmentação.</p>
                    <figure>
                        <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/semantic_segmentation.webp" alt="Segmentação semântica" width={images_width} height={images_height} loading="lazy">
                    </figure>
                </li>
                <li>
                    <p>Redes de classificação de objetos</p>
                    <p>Mediante uma YOLO, se podem detectar objetos na imagem</p>
                    <figure>
                        <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/yolo.webp" alt="Classificação de objetos com YOLO" width={images_width/2} height={images_height/2} loading="lazy">
                    </figure>
                </li>
                <li>
                    <p>Profundidade</p>
                    <p>Mediante uma rede neural, se pode estimar a profundidade de cada pixel da imagem, com isso, se pode obter a distância de cada objeto.</p>
                    <figure>
                        <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/depth.webp" alt="Profundidade" width={images_width/2} height={images_height/2} loading="lazy">
                    </figure>
                </li>
            </ul>
        </section>

        <section>
            <h2>Treinamento</h2>
            <p>Nosso problema era que como era um veículo para ambientes estruturados e não estruturados, não nos valiam as redes pré-treinadas, por isso, tivemos que fazer treinamentos das redes de segmentação e de classificação de objetos.</p>
        </section>
        
        <section>
            <h3>Dataset</h3>
            <p>Como tínhamos horas de vídeos gravados durante testes em ambientes como este, criamos um dataset</p>
            <figure>
                <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground-dataset.webp" alt="Captura de videos de Naviground" width={images_width/2} height={images_height/2} loading="lazy">
            </figure>
            <p>Criamos um algoritmo que, mediante um classificador não supervisionado, criou vários clusters de imagens, onde as imagens de cada cluster eram similares entre si. Desta forma, ficávamos com poucas imagens de cada cluster, para ter um dataset com imagens heterogêneas.</p>
        </section>

        <section>
            <h3>Etiquetador</h3>
            <p>Etiquetar objetos para a YOLO, embora seja pesado, é um processo mais ou menos rápido e fácil</p>
            <figure>
                <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/yolo-labeling.gif" alt="YOLO labeling" width={images_width/2} height={images_height/2} loading="lazy">
            </figure>
            <p>Embora seja pesado, etiquetar as imagens para a segmentação semântica, onde tem que etiquetar cada pixel, é um processo lento e tedioso. Como não nos convencia nenhuma ferramenta de etiquetado para segmentação, construímos a nossa própria ferramenta de etiquetado. Foi tão boa que foi reutilizada em outros projetos e até foi falado de comercializá-la.</p>
        </section>

        <section>
            <h3>Geração de imagens de treinamento</h3>
            <p>Um dos problemas que tínhamos é que todas as imagens de treinamento eram de dia, com sol, sem chuva, etc. Por isso, para tornar as redes mais robustas, precisávamos de mais imagens. Mas isso supõe que alguém tenha que sair à noite, esperar que chova para ter imagens com chuva, esperar que neve, que é mais complicado, etc.</p>
            <p>Naquele momento já havia muitas redes de geração de imagens bastante boas, por isso, podíamos gerar imagens com novas condições ambientais, mas o problema era que havia que etiquetá-las, e para a segmentação, isso exigia muito tempo.</p>
            <p>Assim, construí um pipeline que, mediante IA generativa, modificava as condições ambientais das imagens que já tínhamos etiquetadas, tendo imagens em diferentes condições ambientais, mas sem perder tempo etiquetando-as.</p>
        </section>
        
        <section>
            <h2>Otimização com TensorRT</h2>
            <p>Como isso tinha que funcionar em um veículo, não se podia utilizar um computador com uma GPU potente. Por isso, se utilizava um dispositivo embebido, uma Jetson Orin. Por isso, era importante poder otimizar as redes neurais para que fizessem a inferência o mais rápido possível.</p>
            <p>Me encarregue de otimizá-las com TensorRT, fazendo com que em alguns casos executassem até 40% mais rápido.</p>
            <figure>
                <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground-ascod.webp" alt="ASCOD" width={images_width/2} height={images_height/2} loading="lazy">
            </figure>
            <figure>
                <img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naviground-system.webp" alt="Naviground system" width={images_width/2} height={images_height/2} loading="lazy">
            </figure>
        </section>
	</div>

</Layout>

<style define:vars={{
	inline_code_background_color,
	sticky_top_default,
	sticky_top_649px,
	sticky_top_418px,
	sticky_top_334px,
	sticky_top_315px,
	sticky_top_229px,
	sticky_top_h3_increment,
}}>
	.project {
		display: flex;
		flex-direction: column;
	}

	.inline-code {
		background-color: var(--inline_code_background_color);
		border-radius: 7px;
		padding: 0.2em 0.4em;
		font-style: oblique;
		font-family: 'Courier New', Courier, monospace;
	}

	section {
		position: relative;
	}

	h1 {
		margin-bottom: 0px;
	}

	h4 {
		padding-left: 0px;
	}

    figure {
        display: flex;
        justify-content: center;
        align-items: center;
        padding-bottom: 2em;
        padding-top: 2em;
    }
</style>
