---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Fundamentos de RAG';
const end_url = 'rag-fundamentals';
const description = 'Esque√ßa o Ctrl+F! ü§Ø Com RAG, seus documentos responder√£o √†s suas perguntas diretamente. üòé Tutorial passo a passo com Hugging Face e ChromaDB. Liberte o poder da IA (e impressione seus amigos)! üí™';
const keywords = 'rag, retriever, reader, hugging face, transformers, chromadb, banco de dados vetorial, question-answering, qa, nlp, processamento de linguagem natural, machine learning, intelig√™ncia artificial, ia';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-fundamentals.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-10-23+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Configura√ß√£o da API Inference do Hugging Face"><h2>Configura√ß√£o da <code>API Inference</code> do Hugging Face</h2></a>
      <a class="anchor-link" href="#O que √© RAG?"><h2>O que √© <code>RAG</code>?</h2></a>
      <a class="anchor-link" href="#Como a informacao e armazenada?"><h3>Como a informa√ß√£o √© armazenada?</h3></a>
      <a class="anchor-link" href="#Como obter o chunk correto?"><h3>Como obter o <code>chunk</code> correto?</h3></a>
      <a class="anchor-link" href="#Vamos revisar o que √© RAG"><h3>Vamos revisar o que √© <code>RAG</code></h3></a>
      <a class="anchor-link" href="#Base de dados vetorial"><h2>Base de dados vetorial</h2></a>
      <a class="anchor-link" href="#Funcao de embedding"><h3>Fun√ß√£o de embedding</h3></a>
      <a class="anchor-link" href="#Cliente ChromaDB"><h3>Cliente ChromaDB</h3></a>
      <a class="anchor-link" href="#Colecao"><h3>Cole√ß√£o</h3></a>
      <a class="anchor-link" href="#Carregamento de documentos"><h2>Carregamento de documentos</h2></a>
      <a class="anchor-link" href="#Funcao de carregamento de documentos"><h3>Fun√ß√£o de carregamento de documentos</h3></a>
      <a class="anchor-link" href="#Fun√ß√£o para dividir a documenta√ß√£o em chunks"><h3>Fun√ß√£o para dividir a documenta√ß√£o em <code>chunk</code>s</h3></a>
      <a class="anchor-link" href="#Fun√ß√£o para gerar embeddings de um chunk"><h3>Fun√ß√£o para gerar embeddings de um <code>chunk</code></h3></a>
      <a class="anchor-link" href="#Documentos com os quais vamos testar"><h3>Documentos com os quais vamos testar</h3></a>
      <a class="anchor-link" href="#Carregar os chunks na base de dados vetorial"><h3>Carregar os <code>chunk</code>s na base de dados vetorial</h3></a>
      <a class="anchor-link" href="#Perguntas"><h2>Perguntas</h2></a>
      <a class="anchor-link" href="#Obter o chunk correto"><h3>Obter o <code>chunk</code> correto</h3></a>
      <a class="anchor-link" href="#Gerar a resposta"><h3>Gerar a resposta</h3></a>
      <a class="anchor-link" href="#Limites de naive RAG"><h2>Limites de naive RAG</h2></a>
      <a class="anchor-link" href="#Limites na busca de informacoes (retriever)"><h3>Limites na busca de informa√ß√µes (retriever)</h3></a>
      <a class="anchor-link" href="#Limites na geracao de respostas (generator)"><h3>Limites na gera√ß√£o de respostas (generator)</h3></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Neste post, vamos ver em que consiste a t√©cnica de <code>RAG</code> (<code>Retrieval Augmented Generation</code>) e como ela pode ser implementada em um modelo de linguagem. Al√©m disso, faremos isso com a arquitetura de RAG mais b√°sica, chamada <code>naive RAG</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para que saiaÂÖçË¥π, em vez de usar uma conta da OpenAI (como voc√™ ver√° na maioria dos tutoriais), vamos usar o <code>API inference</code> do Hugging Face, que tem um free tier de 1000 requisi√ß√µes por dia, o que √© mais do que suficiente para fazer este post.</p>
      <p>(Note: There was a mistake in the translation. "ÂÖçË¥π" is Chinese for "free". The correct Portuguese word should be used instead.)</p>
      <p>Here's the corrected version:</p>
      <p>Para que saia gr√°tis, em vez de usar uma conta da OpenAI (como voc√™ ver√° na maioria dos tutoriais), vamos usar o <code>API inference</code> do Hugging Face, que tem um free tier de 1000 requisi√ß√µes por dia, o que √© mais do que suficiente para fazer este post.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2>Configura√ß√£o da <code>API Inference</code> do Hugging Face</h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder usar a <code>API Inference</code> da HuggingFace, o primeiro que voc√™ precisa √© ter uma conta na HuggingFace. Uma vez que voc√™ tenha, √© necess√°rio ir para <a href="https://huggingface.co/settings/keys" target="_blank" rel="nofollow noreferrer">Access tokens</a> nas configura√ß√µes do seu perfil e gerar um novo token.</p>
      <p>Temos que dar um nome. No meu caso, vou cham√°-lo de <code>rag-fundamentos</code> e ativar a permiss√£o <code>Make calls to serverless Inference API</code>. Isso criar√° um token que teremos que copiar.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para gerenciar o token, vamos a criar um arquivo no mesmo caminho em que estamos trabalhando chamado ".env" e vamos colocar o token que copiamos no arquivo da seguinte maneira:</p>
      <div class='highlight'><pre><code class="language-bash">RAG_FUNDAMENTOS_T√âCNICAS_AVAN√áADAS_TOKEN="hf_...."</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, para poder obter o token, precisamos ter instalado <code>dotenv</code>, que instalamos atrav√©s de</p>
      <div class='highlight'><pre><code class="language-bash">pip install python-dotenv</code></pre></div>
      <p>E executamos o seguinte</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">dotenv</span>',
      '<span class="w"> </span>',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      '<span class="w"> </span>',
      '<span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos um token, criamos um cliente. Para isso, precisamos ter a biblioteca <code>huggingface_hub</code> instalada. A instalamos atrav√©s do conda ou pip.</p>
      <div class='highlight'><pre><code class="language-bash">conda install -c conda-forge huggingface_hub</code></pre></div>
      <p>o</p>
      <div class='highlight'><pre><code class="language-bash">pip install --upgrade huggingface_hub</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora temos que escolher qual modelo vamos usar. Voc√™ pode ver os modelos dispon√≠veis na p√°gina de <a href="https://huggingface.co/docs/api-inference/supported-models" target="_blank" rel="nofollow noreferrer">Supported models</a> da documenta√ß√£o da <code>API Inference</code> do Hugging Face.</p>
      <p>Como na hora de escrever o post, o melhor dispon√≠vel √© <code>Qwen2.5-72B-Instruct</code>, vamos usar esse modelo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-72B-Instruct&quot;</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora podemos criar o cliente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceClient</span>',
      '<span class="w"> </span>',
      '<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">MODEL</span><span class="p">)</span>',
      '<span class="n">client</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;InferenceClient(model=&#x27;Qwen/Qwen2.5-72B-Instruct&#x27;, timeout=None)&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Fazemos um teste para ver se funciona</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">	</span><span class="p">{</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hola, qu√© tal?&quot;</span> <span class="p">}</span>',
      '<span class="p">]</span>',
      '<span class="w"> </span>',
      '<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>',
      '<span class="w">	</span><span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>',
      '<span class="w">	</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>',
      '<span class="w">	</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>',
      '<span class="w">	</span><span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
      '<span class="w">	</span><span class="n">stream</span><span class="o">=</span><span class="kc">False</span>',
      '<span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '¬°Hola! Estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2>O que √© <code>RAG</code>?</h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>RAG</code> s√£o as siglas de <code>Retrieval Augmented Generation</code>, √© uma t√©cnica criada para obter informa√ß√µes de documentos. Embora os LLMs possam ser muito poderosos e ter muito conhecimento, nunca ser√£o capazes de responder sobre documentos privados, como relat√≥rios da sua empresa, documenta√ß√£o interna, etc. Por isso foi criado o <code>RAG</code>, para poder usar esses LLMs nessa documenta√ß√£o privada.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG.webp" alt="O que √© RAG?">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A ideia consiste em um usu√°rio fazer uma pergunta sobre essa documenta√ß√£o privada, o sistema √© capaz de obter a parte da documenta√ß√£o onde est√° a resposta para essa pergunta, passa-se ao LLM a pergunta e a parte da documenta√ß√£o e o LLM gera a resposta para o usu√°rio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Como a informacao e armazenada?">Como a informa√ß√£o √© armazenada?<a class="anchor-link" href="#Como a informacao e armazenada?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 29" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>√â sabido, e se voc√™ n√£o sabia, vou te contar agora, que os LLMs t√™m um limite de informa√ß√£o que podem receber, a isso se chama janela de contexto. Isso √© devido √†s arquiteturas internas dos LLMs que, no momento, n√£o v√™m ao caso. Mas o importante √© que n√£o se pode passar um documento e uma pergunta assim, porque √© prov√°vel que o LLM n√£o seja capaz de processar toda essa informa√ß√£o.</p>
      <p>Nos casos em que se passa mais informa√ß√£o do que o contexto da sua janela permite, geralmente acontece que o LLM n√£o presta aten√ß√£o ao final da entrada. Imagine que voc√™ pergunte ao LLM algo sobre seu documento, e essa informa√ß√£o esteja no final do documento e o LLM n√£o a leia.</p>
      <p>Por isso, o que se faz √© dividir a documenta√ß√£o em blocos chamados <code>chunk</code>s. Dessa forma, a documenta√ß√£o √© armazenada em um monte de <code>chunk</code>s, que s√£o peda√ßos dessa documenta√ß√£o. Assim, quando o usu√°rio faz uma pergunta, o <code>chunk</code> no qual est√° a resposta para essa pergunta √© passado ao LLM.</p>
      <p>Al√©m de dividir a documenta√ß√£o em <code>chunk</code>s, esses s√£o convertidos em embeddings, que s√£o representa√ß√µes num√©ricas dos <code>chunk</code>s. Isso √© feito porque os LLMs na verdade n√£o entendem texto, mas sim n√∫meros, e os <code>chunk</code>s s√£o convertidos em n√∫meros para que o LLM possa entend√™-los. Se quiser entender mais sobre os embeddings, voc√™ pode ler meu post sobre <a href="https://www.maximofn.com/transformers">transformers</a> no qual explico como funcionam os transformers, que √© a arquitetura por tr√°s dos LLMs. Voc√™ tamb√©m pode ler meu post sobre <a href="https://www.maximofn.com/chromadb">ChromaDB</a> onde explico como os embeddings s√£o armazenados em um banco de dados vetorial. E seria interessante voc√™ ler meu post sobre a biblioteca <a href="https://www.maximofn.com/hugging-face-tokenizers">HuggingFace Tokenizers</a> na qual se explica como o texto √© tokenizado, que √© o passo anterior √† gera√ß√£o dos embeddings.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG-embeddings.webp" alt="RAG - embeddings">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3>Como obter o <code>chunk</code> correto?</h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Dissemos que a documenta√ß√£o √© dividida em <code>chunk</code>s e o <code>chunk</code> no qual est√° a resposta √† pergunta do usu√°rio √© passado para o LLM. Mas, como se sabe em qual <code>chunk</code> est√° a resposta? Para isso, convertemos a pergunta do usu√°rio em um embedding e calculamos a similaridade entre o embedding da pergunta e os embeddings dos <code>chunk</code>s. Dessa forma, o <code>chunk</code> com maior similaridade √© o que √© passado para o LLM.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-chunk_retreival.webp" alt="">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3>Vamos revisar o que √© <code>RAG</code></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>De um lado temos o <code>retrieval</code>, que √© obter o <code>chunk</code> correto da documenta√ß√£o, do outro lado temos o <code>augmented</code>, que √© passar para o LLM a pergunta do usu√°rio e o <code>chunk</code>, e por √∫ltimo temos o <code>generation</code>, que √© obter a resposta gerada pelo LLM.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Base de dados vetorial">Base de dados vetorial<a class="anchor-link" href="#Base de dados vetorial"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 30" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vimos que a documenta√ß√£o se divide em <code>chunk</code>s e √© armazenada em um banco de dados vetorial, portanto precisamos usar um. Para este post, vou usar <a href="https://www.trychroma.com/" target="_blank" rel="nofollow noreferrer">ChromaDB</a>, que √© um banco de dados vetorial bastante usado e, al√©m disso, tenho um <a href="https://www.maximofn.com/chromadb">post</a> no qual explico como ele funciona.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ent√£o primeiro precisamos instalar a biblioteca do ChromaDB, para isso a instalamos com Conda ou com pip</p>
      <div class='highlight'><pre><code class="language-bash">conda install conda-forge::chromadb</code></pre></div>
      <p>o</p>
      <div class='highlight'><pre><code class="language-bash">pip install chromadb</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Funcao de embedding">Fun√ß√£o de embedding<a class="anchor-link" href="#Funcao de embedding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 31" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como dissemos, tudo vai se basear em embeddings. Portanto, o primeiro passo √© criar uma fun√ß√£o para obter embeddings de um texto. Vamos usar o modelo <code>sentence-transformers/all-MiniLM-L6-v2</code>.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">chromadb.utils.embedding_functions</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">embedding_functions</span>',
      '<span class="w"> </span>',
      '<span class="n">EMBEDDING_MODEL</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>',
      '<span class="w">      </span>',
      '<span class="n">huggingface_ef</span> <span class="o">=</span> <span class="n">embedding_functions</span><span class="o">.</span><span class="n">HuggingFaceEmbeddingFunction</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">model_name</span><span class="o">=</span><span class="n">EMBEDDING_MODEL</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Testamos a fun√ß√£o de embedding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">huggingface_ef</span><span class="p">([</span><span class="s2">&quot;Hello, how are you?&quot;</span><span class="p">,])</span>',
      '<span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos um embedding de dimens√£o 384. Embora a miss√£o deste post n√£o seja explicar os embeddings, em resumo, nossa fun√ß√£o de embedding categorizou a frase <code>Hello, how are you?</code> em um espa√ßo de 384 dimens√µes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Cliente ChromaDB">Cliente ChromaDB<a class="anchor-link" href="#Cliente ChromaDB"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 32" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos nossa fun√ß√£o de embedding, podemos criar um cliente de ChromaDB.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro criamos uma pasta onde ser√° salva a base de dados vetorial</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>',
      '<span class="w">      </span>',
      '<span class="n">chroma_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;chromadb_persisten_storage&quot;</span><span class="p">)</span>',
      '<span class="n">chroma_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos o cliente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">chromadb</span><span class="w"> </span><span class="kn">import</span> <span class="n">PersistentClient</span>',
      '<span class="w"> </span>',
      '<span class="n">chroma_client</span> <span class="o">=</span> <span class="n">PersistentClient</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">chroma_path</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Colecao">Cole√ß√£o<a class="anchor-link" href="#Colecao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 33" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando temos o cliente do ChromaDB, a pr√≥xima coisa que precisamos fazer √© criar uma cole√ß√£o. Uma cole√ß√£o √© um conjunto de vetores, no nosso caso os <code>chunks</code> da documenta√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O criamos indicando a fun√ß√£o de embedding que vamos usar</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">collection_name</span> <span class="o">=</span> <span class="s2">&quot;document_qa_collection&quot;</span>',
      '<span class="n">collection</span> <span class="o">=</span> <span class="n">chroma_client</span><span class="o">.</span><span class="n">get_or_create_collection</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">huggingface_ef</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Carregamento de documentos">Carregamento de documentos<a class="anchor-link" href="#Carregamento de documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 34" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que criamos a base de dados vetorial, temos que dividir a documenta√ß√£o em <code>chunks</code> e salv√°-los na base de dados vetorial.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Funcao de carregamento de documentos">Fun√ß√£o de carregamento de documentos<a class="anchor-link" href="#Funcao de carregamento de documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 35" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro criamos uma fun√ß√£o para carregar todos os documentos <code>.txt</code> de um diret√≥rio</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">),</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>',
      '<span class="w">        </span><span class="k">return</span> <span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="n">file</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()}</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">load_documents_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>',
      '<span class="w">        </span><span class="k">if</span> <span class="n">file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;.txt&quot;</span><span class="p">):</span>',
      '<span class="w">            </span><span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">documents</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3>Fun√ß√£o para dividir a documenta√ß√£o em <code>chunk</code>s</h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez que temos os documentos, os dividimos em <code>chunks</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="w">    </span><span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>',
      '<span class="w">    </span><span class="k">while</span> <span class="n">start</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>',
      '<span class="w">        </span><span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">chunk_size</span>',
      '<span class="w">        </span><span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>',
      '<span class="w">        </span><span class="n">start</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">chunk_overlap</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">chunks</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3>Fun√ß√£o para gerar embeddings de um <code>chunk</code></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos os <code>chunk</code>s, geramos os <code>embedding</code>s de cada um deles.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mais tarde veremos por que, mas para gerar os embeddings vamos fazer isso localmente e n√£o atrav√©s da API do Hugging Face. Para isso, precisamos ter instalado <a href="https://pytorch.org" target="_blank" rel="nofollow noreferrer">PyTorch</a> e <code>sentence-transformers</code>, para isso fazemos</p>
      <div class='highlight'><pre><code class="language-bash">pip install -U sentence-transformers</code></pre></div>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformer</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">EMBEDDING_MODEL</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">try</span><span class="p">:</span>',
      '<span class="w">        </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>',
      '<span class="w">        </span><span class="k">return</span> <span class="n">embedding</span>',
      '<span class="w">    </span><span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
      '<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
      '<span class="w">        </span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos testar agora essa fun√ß√£o de embeddings localmente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Hello, how are you?&quot;</span>',
      '<span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
      '<span class="n">embedding</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que obtemos um embedding da mesma dimens√£o que quando o faz√≠amos com a API do Hugging Face</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O modelo <code>sentence-transformers/all-MiniLM-L6-v2</code> tem apenas 22M de par√¢metros, portanto voc√™ ser√° capaz de execut√°-lo em qualquer GPU. Mesmo se n√£o tiver GPU, voc√™ ainda poder√° execut√°-lo em uma CPU.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O LLM que vamos a usar para gerar as respostas, que √© o <code>Qwen2.5-72B-Instruct</code>, como seu nome indica, √© um modelo de 72B de par√¢metros, por isso este modelo n√£o pode ser executado em qualquer GPU e em uma CPU seria impens√°vel devido √† lentid√£o. Por isso, este LLM sim o usaremos atrav√©s da API, mas na hora de gerar os <code>embedding</code>s podemos fazer localmente sem problema.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Documentos com os quais vamos testar">Documentos com os quais vamos testar<a class="anchor-link" href="#Documentos com os quais vamos testar"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 36" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para fazer todas essas verifica√ß√µes, baixei o conjunto de dados <a href="https://www.kaggle.com/datasets/harshsinghal/aws-case-studies-and-blogs" target="_blank" rel="nofollow noreferrer">aws-case-studies-and-blogs</a> e o coloquei na pasta <code>rag-txt_dataset</code>. Com os seguintes comandos, explico como baix√°-lo e descompact√°-lo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos a pasta onde vamos baixar os documentos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">mkdir</span> <span class="n">rag_txt_dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Baixamos o <code>.zip</code> com os documentos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">curl</span> <span class="o">-</span><span class="n">L</span> <span class="o">-</span><span class="n">o</span> <span class="o">./</span><span class="n">rag_txt_dataset</span><span class="o">/</span><span class="n">archive</span><span class="o">.</span><span class="n">zip</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">kaggle</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">api</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">datasets</span><span class="o">/</span><span class="n">download</span><span class="o">/</span><span class="n">harshsinghal</span><span class="o">/</span><span class="n">aws</span><span class="o">-</span><span class="n">case</span><span class="o">-</span><span class="n">studies</span><span class="o">-</span><span class="ow">and</span><span class="o">-</span><span class="n">blogs</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x20;&#x20;% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;Dload  Upload   Total   Spent    Left  Speed',
          '&#x20;&#x20;0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',
          '100 1430k  100 1430k    0     0  1082k      0  0:00:01  0:00:01 --:--:-- 2440k',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Descompactamos o .zip</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">unzip</span> <span class="n">rag_txt_dataset</span><span class="o">/</span><span class="n">archive</span><span class="o">.</span><span class="n">zip</span> <span class="o">-</span><span class="n">d</span> <span class="n">rag_txt_dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Archive:  rag_txt_dataset/archive.zip',
          '&#x20;&#x20;inflating: rag_txt_dataset/23andMe Case Study _ Life Sciences _ AWS.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/54gene _ Case Study _ AWS.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/6sense Case Study.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/AEON Case Study.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/ALTBalaji _ Amazon Web Services.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/AWS Case Study - Ineos Team UK.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/AWS Case Study - StreamAMG.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/AWS Case Study_ Creditsafe.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/AWS Case Study_ Immowelt.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/AWS Customer Case Study _ Kepler Provides Effective Monitoring of Elderly Care Home Residents Using AWS _ AWS.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/AWS announces 21 startups selected for the AWS generative AI accelerator _ AWS Startups Blog.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/AWS releases smart meter data analytics _ AWS for Industries.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt',
          '&#x20;&#x20;...',
          '&#x20;&#x20;inflating: rag_txt_dataset/Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/Zoox Case Study _ Automotive _ AWS.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/e-banner Streamlines Its Contact Center Operations and Facilitates a Fully Remote Workforce with Amazon Connect _ e-banner Case Study _ AWS.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/iptiQ Case Study.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/mod.io Provides Low Latency Gamer Experience Globally on AWS _ Case Study _ AWS.txt',
          '&#x20;&#x20;inflating: rag_txt_dataset/myposter Case Study.txt',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Apagamos o .zip</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">rm</span> <span class="n">rag_txt_dataset</span><span class="o">/</span><span class="n">archive</span><span class="o">.</span><span class="n">zip</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos ver o que ficou.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="err">!</span><span class="n">ls</span> <span class="n">rag_txt_dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x27;23andMe Case Study _ Life Sciences _ AWS.txt&#x27;',
          '&#x27;36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt&#x27;',
          '&#x27;54gene _ Case Study _ AWS.txt&#x27;',
          '&#x27;6sense Case Study.txt&#x27;',
          '&#x27;Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt&#x27;',
          '&#x27;Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt&#x27;',
          '&#x27;Accelerating customer onboarding using Amazon Connect _ NCS Case Study _ AWS.txt&#x27;',
          '&#x27;Accelerating Migration at Scale Using AWS Application Migration Service with 3M Company _ Case Study _ AWS.txt&#x27;',
          '&#x27;Accelerating Time to Market Using AWS and AWS Partner AccelByte _ Omeda Studios Case Study _ AWS.txt&#x27;',
          '&#x27;Achieving Burstable Scalability and Consistent Uptime Using AWS Lambda with TiVo _ Case Study _ AWS.txt&#x27;',
          '&#x27;Acrobits Uses Amazon Chime SDK to Easily Create Video Conferencing Application Boosting Collaboration for Global Users _ Acrobits Case Study _ AWS.txt&#x27;',
          '&#x27;Actuate AI Case study.txt&#x27;',
          '&#x27;ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt&#x27;',
          '&#x27;Adzuna doubles its email open rates using Amazon SES _ Adzuna Case Study _ AWS.txt&#x27;',
          '&#x27;AEON Case Study.txt&#x27;',
          '&#x27;ALTBalaji _ Amazon Web Services.txt&#x27;',
          '&#x27;Amanotes Stays on Beat by Delivering Simple Music Games to Millions Worldwide on AWS.txt&#x27;',
          '&#x27;Amazon OpenSearch Services vector database capabilities explained _ AWS Big Data Blog.txt&#x27;',
          '&#x27;Anghami Case Study.txt&#x27;',
          '&#x27;Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt&#x27;',
          '...',
          '&#x27;What Will Generative AI Mean for Your Business_ _ AWS Cloud Enterprise Strategy Blog.txt&#x27;',
          '&#x27;Which Recurring Business Processes Can Small and Medium Businesses Automate_ _ AWS Smart Business Blog.txt&#x27;',
          'Windsor.txt',
          '&#x27;Wireless Car Case Study _ AWS IoT Core _ AWS.txt&#x27;',
          '&#x27;Yamato Logistics (HK) case study.txt&#x27;',
          '&#x27;Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt&#x27;',
          '&#x27;Zoox Case Study _ Automotive _ AWS.txt&#x27;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>A criar os <code>chunk</code>s!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Listamos os documentos com a fun√ß√£o que hav√≠amos criado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset_path</span> <span class="o">=</span> <span class="s2">&quot;rag_txt_dataset&quot;</span>',
      '<span class="n">documents</span> <span class="o">=</span> <span class="n">load_documents_from_directory</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Verificamos que fizemos corretamente.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]:</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Run Jobs at Scale While Optimizing for Cost Using Amazon EC2 Spot Instances with ActionIQ _ ActionIQ Case Study _ AWS.txt',
          'Recommend and dynamically filter items based on user context in Amazon Personalize _ AWS Machine Learning Blog.txt',
          'Windsor.txt',
          'Bank of Montreal Case Study _ AWS.txt',
          'The Mill Adventure Case Study.txt',
          'Optimize software development with Amazon CodeWhisperer _ AWS DevOps Blog.txt',
          'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt',
          'THREAD _ Life Sciences _ AWS.txt',
          'Deep Pool Optimizes Software Quality Control Using Amazon QuickSight _ Deep Pool Case Study _ AWS.txt',
          'Upstox Saves 1 Million Annually Using Amazon S3 Storage Lens _ Upstox Case Study _ AWS.txt',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos os <code>chunk</code>s.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">chunked_documents</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>',
      '<span class="w">    </span><span class="n">chunks</span> <span class="o">=</span> <span class="n">split_text</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>',
      '<span class="w">    </span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>',
      '<span class="w">        </span><span class="n">chunked_documents</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">document</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">chunk</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '3611',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, h√° 3611 <code>chunk</code>s. Como o limite di√°rio da API do Hugging Face s√£o 1000 chamadas na conta gratuita, se quisermos criar embeddings de todos os <code>chunk</code>s, acabar√≠amos com as chamadas dispon√≠veis e al√©m disso n√£o poder√≠amos criar embeddings de todos os <code>chunk</code>s</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Reiteramos, este modelo de embeddings √© muito pequeno, com apenas 22M de par√¢metros, portanto pode ser executado em quase qualquer computador, mais r√°pido ou mais devagar, mas pode ser executado.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como s√≥ vamos a criar os embeddings dos <code>chunk</code>s uma vez, mesmo que n√£o tenhamos um computador muito potente e leve muito tempo, apenas ser√° executado uma vez. Depois, quando quisermos fazer perguntas sobre a documenta√ß√£o, geraremos os embeddings do prompt com a API da Hugging Face e usaremos o LLM com a API. Portanto, s√≥ teremos que passar pelo processo de gera√ß√£o dos embeddings dos <code>chunk</code>s uma vez.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Geramos os embeddings dos <code>chunk</code>s</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>√öltima biblioteca que precisaremos instalar. Como o processo de gera√ß√£o dos embeddings dos <code>chunk</code>s ser√° lento, vamos instalar <code>tqdm</code> para mostrar uma barra de progresso. Instalamos com Conda ou pip, conforme sua prefer√™ncia.</p>
      <div class='highlight'><pre><code class="language-bash">conda install conda-forge::tqdm</code></pre></div>
      <p>o</p>
      <div class='highlight'><pre><code class="language-bash">pip install tqdm</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Geramos os embeddings dos <code>chunk</code>s</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>',
      '<span class="w"> </span>',
      '<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
      '<span class="w">    </span><span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>',
      '<span class="w">    </span><span class="k">if</span> <span class="n">embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>',
      '<span class="w">        </span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;embedding&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>',
      '<span class="w">    </span><span class="k">else</span><span class="p">:</span>',
      '<span class="w">        </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error with document </span><span class="si">{</span><span class="n">chunk</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3611/3611 [00:16&amp;lt;00:00, 220.75it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos um exemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">random</span><span class="w"> </span><span class="kn">import</span> <span class="n">randint</span>',
      '<span class="w"> </span>',
      '<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">))</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chunk id: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">&#39;id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\n\n</span><span class="s2">text: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\n\n</span><span class="s2">embedding shape: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">&#39;embedding&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Chunk id: BNS Group Case Study _ Amazon Web Services.txt_0,',
          'text: Reducing Virtual Machines from 40 to 12',
          'The founders of BNS had been contemplating a migration from the company‚Äôs on-premises data center to the public cloud and observed a growing demand for cloud-based operations among current and potential BNS customers.',
          'Fran√ßais',
          'Configures security according to cloud best practices',
          'Clive Pereira, R&amp;amp;D director at BNS Group, explains, ‚ÄúThe database that records Praisal‚Äôs SMS traffic resides in Praisal‚Äôs AWS environment. Praisal can now run complete analytics across its data and gain insights into what‚Äôs happening with its SMS traffic, which is a real game-changer for the organization.‚Äù',
          'Espa√±ol',
          'AWS ISV Accelerate Program',
          'Receiving Strategic, Foundational Support from ISV Specialists',
          'Learn More',
          'The value that AWS places on the ISV stream sealed the deal in our choice of cloud provider.‚Äù',
          'Êó•Êú¨Ë™û',
          '&#x20;&#x20;Contact Sales',
          'BNS is an Australian software provider focused on secure enterprise SMS and fax messaging. Its software runs on the Windows platform and is l,',
          'embedding shape: (384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3>Carregar os <code>chunk</code>s na base de dados vetorial</h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez que temos todos os chunks gerados, os carregamos na base de dados vetorial. Voltamos a usar <code>tqdm</code> para mostrar uma barra de progresso, pois isso tamb√©m ser√° lento.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>',
      '<span class="w"> </span>',
      '<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
      '<span class="w">    </span><span class="n">collection</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span>',
      '<span class="w">        </span><span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]],</span>',
      '<span class="w">        </span><span class="n">documents</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span>',
      '<span class="w">        </span><span class="n">embeddings</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">&quot;embedding&quot;</span><span class="p">],</span>',
      '<span class="w">    </span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3611/3611 [00:59&amp;lt;00:00, 60.77it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Perguntas">Perguntas<a class="anchor-link" href="#Perguntas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 37" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos a base de dados vetorial, podemos fazer perguntas √† documenta√ß√£o. Para isso, precisamos de uma fun√ß√£o que nos retorne o <code>chunk</code> correto.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3>Obter o <code>chunk</code> correto</h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora precisamos de uma fun√ß√£o que nos retorne o <code>chunk</code> correto, vamos cri√°-la</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">results</span> <span class="o">=</span> <span class="n">collection</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query_texts</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">n_results</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">results</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, criamos uma <code>query</code>.</p>
      <p>Para gerar a query, selecionei aleatoriamente o documento <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code>, passei-o para um LLM e pedi que gerasse uma pergunta sobre o documento. A pergunta que foi gerada √©</p>
      <div class='highlight'><pre><code>Como a Neeva utilizou o Karpenter e as Inst√¢ncias Spot do Amazon EC2 para melhorar sua gest√£o de infraestrutura e otimiza√ß√£o de custos?</code></pre></div>
      <p>Ent√£o obtemos os <code>chunks</code> mais relevantes diante dessa pergunta.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;How did Neeva use Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization?&quot;</span>',
      '<span class="n">top_chunks</span> <span class="o">=</span> <span class="n">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver quais <code>chunk</code>s nos foram devolvidos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_chunks</span><span class="p">[</span><span class="s2">&quot;ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">&#39;ids&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, distance: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">&#39;distances&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Rank 1: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_0, distance: 0.29233667254447937',
          'Rank 2: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_5, distance: 0.4007825255393982',
          'Rank 3: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_1, distance: 0.4317566752433777',
          'Rank 4: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_6, distance: 0.43832334876060486',
          'Rank 5: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_4, distance: 0.44625571370124817',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como eu havia dito, o documento que escolhi aleatoriamente era <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code> e, como se pode ver, os <code>chunk</code>s que nos foram retornados s√£o desse documento. Isso significa que, dos mais de 3000 <code>chunk</code>s que havia no banco de dados, ele foi capaz de me retornar os <code>chunk</code>s mais relevantes para essa pergunta, parece que isso funciona!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Gerar a resposta">Gerar a resposta<a class="anchor-link" href="#Gerar a resposta"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 38" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como j√° temos os <code>chunk</code>s mais relevantes, passamo-los ao LLM, juntamente com a pergunta, para que ele gere uma resposta.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">relevant_chunks</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">context</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">chunk</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">relevant_chunks</span><span class="p">])</span>',
      '<span class="w">    </span><span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;You are an assistant for question-answering. You have to answer the following question:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Answer the question with the following information:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="w">    </span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span>',
      '<span class="w">        </span><span class="p">{</span> <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span> <span class="p">}</span>',
      '<span class="w">    </span><span class="p">]</span>',
      '<span class="w">    </span><span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>',
      '<span class="w">        </span><span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span>',
      '<span class="w">        </span><span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>',
      '<span class="w">        </span><span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>',
      '<span class="w">        </span><span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>',
      '<span class="w">        </span><span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>',
      '<span class="w">    </span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">response</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Testamos a fun√ß√£o</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">response</span> <span class="o">=</span> <span class="n">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">top_chunks</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Here‚Äôs how:',
          '### Early Collaboration with Karpenter',
          'In late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.',
          '### Combining Spot Instances and On-Demand Instances',
          'Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.',
          '### Flexibility and Instance Diversification',
          'According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter&#x27;s adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.',
          '### Improved Scalability and Agility',
          'By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:',
          '- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.',
          '- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.',
          '### Enhanced Development Cycles',
          'The integration of Karpenter and Spot Instances has also accelerated Neeva&#x27;s development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.',
          '### Cost Savings and Budget Control',
          'Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.',
          '### Future Plans',
          'Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, &quot;The bulk of our compute is or will be managed using Karpenter going forward.&quot;',
          '### Conclusion',
          'By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Quando pedi ao LLM para gerar uma pergunta sobre o documento, tamb√©m pedi que gerasse a resposta correta. Esta √© a resposta que o LLM me deu.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <div class='highlight'><pre><code class="language-text">A Neeva utilizou o Karpenter e as Inst√¢ncias Spot do Amazon EC2 para melhorar sua gest√£o de infraestrutura e otimiza√ß√£o de custos de v√°rias maneiras:
      
      Gest√£o Simplificada de Inst√¢ncias:
      
      Karpenter: Ao adotar o Karpenter, a Neeva simplificou o processo de provisionamento e gerenciamento de recursos computacionais para seus clusters do Amazon EKS. O Karpenter provisiona e desprovisiona inst√¢ncias automaticamente com base na carga de trabalho, eliminando a necessidade de configura√ß√µes manuais e reduzindo a complexidade de compreender diferentes inst√¢ncias computacionais.
      Inst√¢ncias Spot: A Neeva utilizou Inst√¢ncias Spot do Amazon EC2, que s√£o capacidade n√£o utilizada do EC2 dispon√≠vel com um desconto significativo (at√© 90% de economia de custos). Isso permitiu √† empresa controlar os custos enquanto atendia aos seus requisitos de desempenho.
      Escalabilidade Aumentada:
      
      Karpenter: A capacidade do Karpenter de escalar recursos dinamicamente permitiu que a Neeva iniciasse novas inst√¢ncias rapidamente, permitindo que a empresa iterasse com maior velocidade e executasse mais experimentos em menos tempo.
      Inst√¢ncias Spot: O uso de Inst√¢ncias Spot proporcionou flexibilidade e diversifica√ß√£o de inst√¢ncias, facilitando o escalonamento dos recursos de computa√ß√£o da Neeva de forma eficiente.
      Produtividade Melhorada:
      
      Karpenter: Ao democratizar as altera√ß√µes de infraestrutura, o Karpenter permitiu que qualquer engenheiro modificasse as configura√ß√µes do Kubernetes, reduzindo a depend√™ncia de expertise especializada. Isso economizou at√© 100 horas por semana de tempo de espera em administra√ß√£o de sistemas para a equipe da Neeva.
      Inst√¢ncias Spot: A capacidade de provisionar e desprovisionar rapidamente Inst√¢ncias Spot reduziu os atrasos no pipeline de desenvolvimento, garantindo que os trabalhos n√£o ficassem travados devido √† falta de recursos dispon√≠veis.
      Efici√™ncia Custo:
      
      Karpenter: As melhores pr√°ticas do Karpenter para inst√¢ncias Spot, incluindo flexibilidade e diversifica√ß√£o de inst√¢ncias, ajudaram a Neeva a usar essas inst√¢ncias de forma mais eficaz, permanecendo dentro do or√ßamento.
      Inst√¢ncias Spot: As economias de custos com o uso de Inst√¢ncias Spot permitiram que a Neeva executasse trabalhos em larga escala, como indexa√ß√£o, por quase o mesmo custo, mas em um tempo muito menor. Por exemplo, a Neeva reduziu seus trabalhos de indexa√ß√£o de 18 horas para apenas 3 horas.
      Melhor Utiliza√ß√£o de Recursos:
      
      Karpenter: O Karpenter proporcionou uma melhor visibilidade sobre o uso de recursos computacionais, permitindo que a Neeva rastreasse e otimizasse seu consumo de recursos mais de perto.
      Inst√¢ncias Spot: A combina√ß√£o do Karpenter e das Inst√¢ncias Spot permitiu que a Neeva executasse modelos de linguagem grandes de forma mais eficiente, melhorando a experi√™ncia de busca para seus usu√°rios.
      Em resumo, a ado√ß√£o do Karpenter e das Inst√¢ncias Spot do Amazon EC2 pela Neeva melhorou significativamente sua gest√£o de infraestrutura, otimiza√ß√£o de custos e efici√™ncia geral no desenvolvimento, permitindo que a empresa oferecesse melhores experi√™ncias de pesquisa sem an√∫ncios aos seus usu√°rios.</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>E esta tem sido a resposta gerada pelo nosso <code>RAG</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <div class='highlight'><pre><code class="language-text">Neeva, um mecanismo de busca nativo em nuvem e sem an√∫ncios fundado em 2019, aproveitou o Karpenter e as Inst√¢ncias Spot do Amazon EC2 para melhorar significativamente sua gest√£o de infraestrutura e otimiza√ß√£o de custos. Eis como:
      
      ### Colabora√ß√£o Inicial com o Karpenter
      No final de 2021, a Neeva come√ßou a trabalhar em estreita colabora√ß√£o com a equipe do Karpenter, experimentando e contribuindo com corre√ß√µes para uma vers√£o inicial do Karpenter. Essa colabora√ß√£o permitiu que a Neeva integrasse o Karpenter ao seu painel do Kubernetes, possibilitando √† empresa coletar m√©tricas valiosas sobre uso e desempenho.
      
      ### Combinando Inst√¢ncias Spot e Inst√¢ncias On-Demand
      A Neeva executa seus jobs em larga escala, o que pode levar a custos significativos. Para gerenciar esses custos de forma eficaz, a empresa adotou uma combina√ß√£o de Amazon EC2 Spot Instances e On-Demand Instances. As Spot Instances permitem que a Neeva lance lances sobre capacidade EC2 n√£o utilizada, frequentemente por uma fra√ß√£o do pre√ßo On-Demand, enquanto as On-Demand Instances fornecem a necess√°ria confiabilidade para pipelines cr√≠ticos.
      
      ### Flexibilidade e Diversifica√ß√£o de Inst√¢ncias
      De acordo com Mohit Agarwal, l√≠der de engenharia de infraestrutura da Neeva, a ado√ß√£o de melhores pr√°ticas para Inst√¢ncias Spot pelo Karpenter, incluindo flexibilidade e diversifica√ß√£o de inst√¢ncias, foi crucial. Essa abordagem garante que a Neeva possa ajustar dinamicamente seus recursos de computa√ß√£o para atender √†s cargas de trabalho vari√°veis, minimizando custos.
      
      ### Melhor Escalabilidade e Agilidade
      Ao usar o Karpenter para provisionar recursos de infraestrutura para seus clusters do Amazon EKS, a Neeva alcan√ßou v√°rios benef√≠cios importantes:
      - **Escalabilidade**: O Neeva pode escalar seus recursos de computa√ß√£o para cima ou para baixo conforme necess√°rio, garantindo que sempre tenha a capacidade necess√°ria para lidar com suas cargas de trabalho.
      - **Agilidade**: A empresa pode iterar rapidamente e democratizar as mudan√ßas de infraestrutura, reduzindo o tempo gasto com a administra√ß√£o do sistema em at√© 100 horas por semana.
      
      ### Ciclos de Desenvolvimento Aperfei√ßoadosA integra√ß√£o do Karpenter e das Inst√¢ncias Spot tamb√©m acelerou os ciclos de desenvolvimento da Neeva. A empresa agora pode lan√ßar novos recursos e melhorias mais rapidamente, o que √© essencial para manter uma vantagem competitiva no mercado de motores de busca.
      
      ### Economia de Custos e Controle Or√ßament√°rio
      Usando Inst√¢ncias Spot, a Neeva tem conseguido permanecer dentro do seu or√ßamento enquanto atende aos seus requisitos de desempenho. Essa otimiza√ß√£o de custos √© crucial para uma empresa que prioriza experi√™ncias centradas no usu√°rio e n√£o possui incentivos conflitantes provenientes da publicidade.
      
      ### Planos Futuros
      A Neeva est√° comprometida em continuar sua inova√ß√£o e expans√£o. A empresa planeja lan√ßar em novas regi√µes e melhorar ainda mais seu mecanismo de busca, mantendo a efici√™ncia de custos. Como Mohit Agarwal observa, "A maior parte do nosso compute ser√° gerenciada usando Karpenter a partir de agora."
      
      ### Conclus√£o
      Ao aproveitar o Karpenter e as Inst√¢ncias Spot do Amazon EC2, a Neeva n√£o apenas otimizou seus custos de infraestrutura, mas tamb√©m melhorou sua escalabilidade, agilidade e velocidade de desenvolvimento. Esta abordagem estrat√©gica posicionou a Neeva para fornecer experi√™ncias de pesquisa de alta qualidade e sem an√∫ncios aos seus usu√°rios, mantendo um forte foco no controle de custos e inova√ß√£o.</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Portanto, podemos concluir que o <code>RAG</code> funcionou corretamente!!!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Limites de naive RAG">Limites de naive RAG<a class="anchor-link" href="#Limites de naive RAG"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 39" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como dissemos, hoje explicamos <code>naive RAG</code>, que √© a arquitetura mais simples do RAG, mas tem suas limita√ß√µes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/naive_RAG_architecture.webp" alt="Arquitetura Naive RAG">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As limita√ß√µes desta arquitetura s√£o:</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Limites na busca de informacoes (retriever)">Limites na busca de informa√ß√µes (retriever)<a class="anchor-link" href="#Limites na busca de informacoes (retriever)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 40" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <ul>
        <li>Conhecimento limitado do contexto e da documenta√ß√£o: Quando o sistema de RAG ing√™nuo busca os chunks, ele procura aqueles que t√™m um significado sem√¢ntico similar ao prompt, mas n√£o √© capaz de saber quais s√£o os mais relevantes para a pergunta do usu√°rio, ou quais s√£o os que possuem informa√ß√µes mais atualizadas, ou se sua informa√ß√£o √© mais correta do que a de outros chunks. Por exemplo, se um usu√°rio perguntar sobre os problemas dos ado√ßantes no sistema digestivo, o RAG ing√™nuo pode retornar documentos sobre ado√ßantes ou sobre o sistema digestivo, mas n√£o √© capaz de saber que os documentos sobre o sistema digestivo s√£o os mais relevantes para a pergunta do usu√°rio. Outro exemplo √© se o usu√°rio perguntar sobre os √∫ltimos avan√ßos na IA, mas o RAG ing√™nuo n√£o √© capaz de saber quais s√£o os √∫ltimos papers da base de dados.</li>
      </ul>
      <ul>
        <li>N√£o h√° uma sincroniza√ß√£o entre o retrieval e o gerador. Como vimos, s√£o dois sistemas independentes; de um lado, o retrieval busca os documentos mais semelhantes √† pergunta do usu√°rio, e esses documentos s√£o passados ao gerador, que gera uma resposta.</li>
      </ul>
      <ul>
        <li>Escalabilidade ineficiente para grandes bancos de dados. Como a recupera√ß√£o busca os documentos com maior similaridade sem√¢ntica em toda a base de dados, quando esta fica muito grande, podemos ter tempos de pesquisa muito longos.</li>
        <li>Pouca adapta√ß√£o √† pergunta do usu√°rio. Se o usu√°rio fizer uma pergunta que envolva v√°rios documentos, ou seja, n√£o h√° nenhum documento que contenha toda a informa√ß√£o da pergunta do usu√°rio, o sistema recuperar√° todos esses documentos e os passar√° para o gerador, que pode us√°-los ou n√£o. Ou, em um caso pior, pode deixar de lado algum documento relevante para gerar a resposta.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Limites na geracao de respostas (generator)">Limites na gera√ß√£o de respostas (generator)<a class="anchor-link" href="#Limites na geracao de respostas (generator)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 41" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <ul>
        <li>O modelo poderia alucinar respostas mesmo ao fornecer informa√ß√µes relevantes.</li>
      </ul>
      <ul>
        <li>O modelo pode estar limitado por quest√µes relacionadas a √≥dio, discrimina√ß√£o, etc.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para ultrapassar esses limites, geralmente s√£o utilizadas t√©cnicas como o</p>
      <ul>
        <li>Pr√©-recupera√ß√£o: Que inclui t√©cnicas para melhorar a indexa√ß√£o, tornando a busca de informa√ß√µes mais eficiente. Ou t√©cnicas como a melhoria da pergunta do usu√°rio para que o retrieval possa encontrar os documentos mais relevantes.</li>
      </ul>
      <ul>
        <li>P√≥s-recupera√ß√£o: Aqui s√£o utilizadas t√©cnicas como o re-ranqueamento dos documentos, que √© uma t√©cnica usada para melhorar a busca por informa√ß√µes relevantes</li>
      </ul>
      </section>







    </div>

  </section>

</PostLayout>
