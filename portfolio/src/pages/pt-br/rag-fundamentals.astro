---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Fundamentos de RAG';
const end_url = 'rag-fundamentals';
const description = 'Esque√ßa o Ctrl+F! ü§Ø Com RAG, seus documentos responder√£o √†s suas perguntas diretamente. üòé Tutorial passo a passo com Hugging Face e ChromaDB. Liberte o poder da IA (e impressione seus amigos)! üí™';
const keywords = 'rag, retriever, reader, hugging face, transformers, chromadb, banco de dados vetorial, question-answering, qa, nlp, processamento de linguagem natural, machine learning, intelig√™ncia artificial, ia';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-fundamentals.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-10-23+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Configura%C3%A7%C3%A3o-da-API-Inference-de-Hugging-Face"><h2>Configura√ß√£o da <code>API Inference</code> de Hugging Face</h2></a>
      <a class="anchor-link" href="#O-que-%C3%A9-RAG?"><h2>O que √© <code>RAG</code>?</h2></a>
      <a class="anchor-link" href="#Como-a-informa%C3%A7%C3%A3o-%C3%A9-armazenada?"><h3>Como a informa√ß√£o √© armazenada?</h3></a>
      <a class="anchor-link" href="#Como-obter-o-chunk-correto?"><h3>Como obter o <code>chunk</code> correto?</h3></a>
      <a class="anchor-link" href="#Vamos-ver-o-que-%C3%A9-RAG"><h3>Vamos ver o que √© <code>RAG</code></h3></a>
      <a class="anchor-link" href="#Banco-de-dados-vetorial"><h2>Banco de dados vetorial</h2></a>
      <a class="anchor-link" href="#Fun%C3%A7%C3%A3o-de-embedding"><h3>Fun√ß√£o de embedding</h3></a>
      <a class="anchor-link" href="#Cliente-ChromaDB"><h3>Cliente ChromaDB</h3></a>
      <a class="anchor-link" href="#Cole%C3%A7%C3%A3o"><h3>Cole√ß√£o</h3></a>
      <a class="anchor-link" href="#Carregamento-de-documentos"><h2>Carregamento de documentos</h2></a>
      <a class="anchor-link" href="#Fun%C3%A7%C3%A3o-de-carregamento-de-documentos"><h3>Fun√ß√£o de carregamento de documentos</h3></a>
      <a class="anchor-link" href="#Fun%C3%A7%C3%A3o-para-dividir-a-documenta%C3%A7%C3%A3o-em-chunks"><h3>Fun√ß√£o para dividir a documenta√ß√£o em <code>chunk</code>s</h3></a>
      <a class="anchor-link" href="#Fun%C3%A7%C3%A3o-para-gerar-embeddings-de-um-chunk"><h3>Fun√ß√£o para gerar embeddings de um <code>chunk</code></h3></a>
      <a class="anchor-link" href="#Documentos-com-os-quais-vamos-testar"><h3>Documentos com os quais vamos testar</h3></a>
      <a class="anchor-link" href="#Criando-os-chunks!"><h3>Criando os <code>chunk</code>s!</h3></a>
      <a class="anchor-link" href="#Carregar-os-chunks-no-banco-de-dados-vetorial"><h3>Carregar os <code>chunk</code>s no banco de dados vetorial</h3></a>
      <a class="anchor-link" href="#Perguntas"><h2>Perguntas</h2></a>
      <a class="anchor-link" href="#Obter-o-chunk-correto"><h3>Obter o <code>chunk</code> correto</h3></a>
      <a class="anchor-link" href="#Gerar-a-resposta"><h3>Gerar a resposta</h3></a>
      <a class="anchor-link" href="#Colabora%C3%A7%C3%A3o-Precoce-com-o-KarpenterNo-final-de-2021,-a-Neeva-come%C3%A7ou-a-trabalhar-de-perto-com-a-equipe-do-Karpenter,-experimentando-e-contribuindo-com-corre%C3%A7%C3%B5es-para-uma-vers%C3%A3o-inicial-do-Karpenter.-Essa-colabora%C3%A7%C3%A3o-permitiu-que-a-Neeva-integrasse-o-Karpenter-com-seu-painel-do-Kubernetes,-permitindo-que-a-empresa-coletasse-m%C3%A9tricas-valiosas-sobre-uso-e-desempenho."><h3>Colabora√ß√£o Precoce com o KarpenterNo final de 2021, a Neeva come√ßou a trabalhar de perto com a equipe do Karpenter, experimentando e contribuindo com corre√ß√µes para uma vers√£o inicial do Karpenter. Essa colabora√ß√£o permitiu que a Neeva integrasse o Karpenter com seu painel do Kubernetes, permitindo que a empresa coletasse m√©tricas valiosas sobre uso e desempenho.</h3></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="RAG:-Fundamentos-e-t%C3%A9cnicas-avan%C3%A7adas">RAG: Fundamentos e t√©cnicas avan√ßadas<a class="anchor-link" href="#RAG:-Fundamentos-e-t%C3%A9cnicas-avan%C3%A7adas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 40" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <blockquote>
      <p>Aviso: Este post foi traduzido para o portugu√™s usando um modelo de tradu√ß√£o autom√°tica. Por favor, me avise se encontrar algum erro.</p>
      </blockquote>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Neste post, vamos ver em que consiste a t√©cnica de <code>RAG</code> (<code>Retrieval Augmented Generation</code>) e como pode ser implementada em um modelo de linguagem.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para que seja gratuito, em vez de usar uma conta da OpenAI (como voc√™ ver√° na maioria dos tutoriais), vamos usar a <code>API inference</code> da Hugging Face, que tem um free tier de 1000 requisi√ß√µes por dia, que para fazer este post √© mais do que suficiente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Configura%C3%A7%C3%A3o-da-API-Inference-de-Hugging-Face">Configura√ß√£o da <code>API Inference</code> de Hugging Face<a class="anchor-link" href="#Configura%C3%A7%C3%A3o-da-API-Inference-de-Hugging-Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 41" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder usar a <code>API Inference</code> de HuggingFace, a primeira coisa que voc√™ precisa √© ter uma conta no HuggingFace. Uma vez que voc√™ a tenha, precisa ir a <a href="https://huggingface.co/settings/keys" target="_blank" rel="nofollow noreferrer">Access tokens</a> na configura√ß√£o do seu perfil e gerar um novo token.
      Temos que dar um nome, no meu caso vou chamar <code>rag-fundamentals</code> e habilitar a permiss√£o <code>Make calls to serverless Inference API</code>. Um token ser√° criado para n√≥s, que precisamos copiar.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para gerenciar o token vamos criar um arquivo na mesma rota em que estamos trabalhando chamado <code>.env</code> e vamos colocar o token que copiamos no arquivo da seguinte maneira:
      RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN="hf_...."</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, para obter o token, precisamos ter o <code>dotenv</code> instalado, o que fazemos atrav√©s</p>
      <div class="highlight"><pre><span></span><span class="sb"></span>pip<span class="w"> </span>install<span class="w"> </span>python-dotenv
      </pre></div>
      <p>e executamos o seguinte</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      ' ',
      '<span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Agora que temos um token, criamos um cliente, para isso precisamos ter instalada a biblioteca <code>huggingface_hub</code>, que fazemos usando conda ou pip</p>
      <div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>huggingface_hub
      </pre></div>
      <p>o</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>huggingface_hub
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora temos que escolher qual modelo vamos usar. Voc√™ pode ver os modelos dispon√≠veis na p√°gina de <a href="https://huggingface.co/docs/api-inference/supported-models" target="_blank" rel="nofollow noreferrer">Supported models</a> da documenta√ß√£o da <code>API Inference</code> da Hugging Face.
      Como no momento de escrever a postagem, o melhor dispon√≠vel √© <code>Qwen2.5-72B-Instruct</code>, vamos usar esse modelo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Agora podemos criar o cliente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>',
          ' ',
          '<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">MODEL</span><span class="p">)</span>',
          '<span class="n">client</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;InferenceClient(model=\'Qwen/Qwen2.5-72B-Instruct\', timeout=None)&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Estamos fazendo um teste para ver se funciona</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span>',
          '	<span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, qu√© tal?"</span> <span class="p">}</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>',
          '	<span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> ',
          '	<span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>',
          '	<span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>',
          '	<span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '	<span class="n">stream</span><span class="o">=</span><span class="kc">False</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '¬°Hola! Estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="O-que-%C3%A9-RAG?">O que √© <code>RAG</code>?<a class="anchor-link" href="#O-que-%C3%A9-RAG?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 42" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>RAG</code> s√£o as siglas de <code>Retrieval Augmented Generation</code>, √© uma t√©cnica criada para obter informa√ß√µes de documentos. Embora os LLMs possam ser muito poderosos e ter muito conhecimento, nunca ser√£o capazes de responder sobre documentos privados, como relat√≥rios da sua empresa, documenta√ß√£o interna, etc. Por isso foi criado o <code>RAG</code>, para poder usar esses LLMs nessa documenta√ß√£o privada.
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="O que √© RAG?" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG.webp" width="1600" height="900"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A ideia consiste em que um usu√°rio faz uma pergunta sobre essa documenta√ß√£o privada, o sistema √© capaz de obter a parte da documenta√ß√£o na qual est√° a resposta a essa pergunta, passa-se a pergunta e a parte da documenta√ß√£o para um LLM e o LLM gera a resposta para o usu√°rio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Como-a-informa%C3%A7%C3%A3o-%C3%A9-armazenada?">Como a informa√ß√£o √© armazenada?<a class="anchor-link" href="#Como-a-informa%C3%A7%C3%A3o-%C3%A9-armazenada?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 43" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>√â sabido, e se voc√™ n√£o sabia eu te conto agora, que os LLMs t√™m um limite de informa√ß√µes que podem ser passadas a eles, isso √© chamado de janela de contexto. Isso se deve √†s arquiteturas internas dos LLMs que agora n√£o v√™m ao caso. Mas o importante √© que n√£o se pode passar um documento e uma pergunta sem mais, porque √© prov√°vel que o LLM n√£o seja capaz de processar todas essas informa√ß√µes.
      Nos casos em que geralmente se passa mais informa√ß√µes do que a janela de contexto permite, o que geralmente acontece √© que o LLM n√£o presta aten√ß√£o ao final da entrada. Imagine que voc√™ pergunte ao LLM sobre algo do seu documento, que essa informa√ß√£o esteja no final do documento e o LLM n√£o a leia.
      Por isso, o que se faz √© dividir a documenta√ß√£o em blocos chamados de <code>chunk</code>s. Dessa forma, a documenta√ß√£o √© armazenada em um monte de <code>chunk</code>s, que s√£o peda√ßos dessa documenta√ß√£o. Assim, quando o usu√°rio faz uma pergunta, √© passado para o LLM o <code>chunk</code> em que est√° a resposta para essa pergunta.
      Al√©m de dividir a documenta√ß√£o em <code>chunk</code>s, estes s√£o convertidos em embeddings, que s√£o representa√ß√µes num√©ricas dos <code>chunk</code>s. Isso √© feito porque os LLMs na verdade n√£o entendem texto, mas sim n√∫meros, e os <code>chunk</code>s s√£o convertidos em n√∫meros para que o LLM possa entend√™-los. Se quiser entender mais sobre os embeddings, pode ler meu post sobre <a href="https://www.maximofn.com/transformers">transformers</a> no qual explico como funcionam os transformers, que √© a arquitetura por tr√°s dos LLMs. Voc√™ tamb√©m pode ler meu post sobre <a href="https://www.maximofn.com/chromadb">ChromaDB</a> onde explico como os embeddings s√£o armazenados em um banco de dados vetorial. E al√©m disso, seria interessante que lesse meu post sobre a biblioteca <a href="https://www.maximofn.com/hugging-face-tokenizers">HuggingFace Tokenizers</a> no qual se explica como o texto √© tokenizado, que √© a etapa anterior √† gera√ß√£o dos embeddings.
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="RAG - embeddings" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG-embeddings.webp" width="1400" height="750"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Como-obter-o-chunk-correto?">Como obter o <code>chunk</code> correto?<a class="anchor-link" href="#Como-obter-o-chunk-correto?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 44" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Dissemos que a documenta√ß√£o √© dividida em <code>chunks</code> e o <code>chunk</code> em que est√° a resposta √† pergunta do usu√°rio √© passado ao LLM. Mas, como se sabe em qual <code>chunk</code> est√° a resposta? Para isso, o que se faz √© converter a pergunta do usu√°rio em um embedding, e calcula-se a similaridade entre o embedding da pergunta e os embeddings dos <code>chunks</code>. Dessa forma, o <code>chunk</code> com maior similaridade √© o que √© passado ao LLM.
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="RAG - similaridade de embeddings" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-chunk_retreival.webp" width="1374" height="351"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Vamos-ver-o-que-%C3%A9-RAG">Vamos ver o que √© <code>RAG</code><a class="anchor-link" href="#Vamos-ver-o-que-%C3%A9-RAG"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 45" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por um lado temos o <code>retrieval</code>, que √© obter o <code>chunk</code> correto da documenta√ß√£o, por outro lado temos o <code>augmented</code>, que √© passar para o LLM a pergunta do usu√°rio e o <code>chunk</code>, e por √∫ltimo temos o <code>generation</code>, que √© obter a resposta gerada pelo LLM.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Banco-de-dados-vetorial">Banco de dados vetorial<a class="anchor-link" href="#Banco-de-dados-vetorial"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 46" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vimos que a documenta√ß√£o √© dividida em <code>chunk</code>s e armazenada em um banco de dados vetorial, portanto, precisamos usar um. Para este post, vou usar o <a href="https://www.trychroma.com/" target="_blank" rel="nofollow noreferrer">ChromaDB</a>, que √© um banco de dados vetorial bastante utilizado e, al√©m disso, tenho um <a href="https://www.maximofn.com/chromadb">post</a> no qual explico como funciona.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Portanto, primeiro precisamos instalar a biblioteca do ChromaDB, para isso a instalamos com Conda ou com Pip
      conda install conda-forge::chromadb
      Ent√£o, me envie o texto markdown que voc√™ gostaria que eu traduza para o portugu√™s.</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>chromadb<span class="sb"></span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Fun%C3%A7%C3%A3o-de-embedding">Fun√ß√£o de embedding<a class="anchor-link" href="#Fun%C3%A7%C3%A3o-de-embedding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 47" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como dissemos, tudo se basear√° em embeddings, por isso a primeira coisa que fazemos √© criar uma fun√ß√£o para obter embeddings de um texto. Vamos usar o modelo <code>sentence-transformers/all-MiniLM-L6-v2</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">chromadb.utils.embedding_functions</span> <span class="k">as</span> <span class="nn">embedding_functions</span>',
      ' ',
      '<span class="n">EMBEDDING_MODEL</span> <span class="o">=</span> <span class="s2">"sentence-transformers/all-MiniLM-L6-v2"</span>',
      '      ',
      '<span class="n">huggingface_ef</span> <span class="o">=</span> <span class="n">embedding_functions</span><span class="o">.</span><span class="n">HuggingFaceEmbeddingFunction</span><span class="p">(</span>',
      '    <span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span>',
      '    <span class="n">model_name</span><span class="o">=</span><span class="n">EMBEDDING_MODEL</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>Testamos a fun√ß√£o de embedding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">huggingface_ef</span><span class="p">([</span><span class="s2">"Hello, how are you?"</span><span class="p">,])</span>',
          '<span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos um embedding de dimens√£o 384. Embora a miss√£o deste post n√£o seja explicar os embeddings, em resumo, nossa fun√ß√£o de embedding categorizou a frase <code>Hello, how are you?</code> em um espa√ßo de 384 dimens√µes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Cliente-ChromaDB">Cliente ChromaDB<a class="anchor-link" href="#Cliente-ChromaDB"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 48" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos nossa fun√ß√£o de embedding, podemos criar um cliente do ChromaDB.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro criamos uma pasta onde ser√° guardado o banco de dados vetorial</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>',
      '      ',
      '<span class="n">chroma_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"chromadb_persisten_storage"</span><span class="p">)</span>',
      '<span class="n">chroma_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Agora criamos o cliente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">chromadb</span> <span class="kn">import</span> <span class="n">PersistentClient</span>',
      ' ',
      '<span class="n">chroma_client</span> <span class="o">=</span> <span class="n">PersistentClient</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">chroma_path</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <h3 id="Cole%C3%A7%C3%A3o">Cole√ß√£o<a class="anchor-link" href="#Cole%C3%A7%C3%A3o"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 49" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando temos o cliente do ChromaDB, o pr√≥ximo passo √© criar uma cole√ß√£o. Uma cole√ß√£o √© um conjunto de vetores, no nosso caso os <code>chunks</code> da documenta√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos isso indicando a fun√ß√£o de embedding que vamos usar</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">collection_name</span> <span class="o">=</span> <span class="s2">"document_qa_collection"</span>',
      '<span class="n">collection</span> <span class="o">=</span> <span class="n">chroma_client</span><span class="o">.</span><span class="n">get_or_create_collection</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">huggingface_ef</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <h2 id="Carregamento-de-documentos">Carregamento de documentos<a class="anchor-link" href="#Carregamento-de-documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 50" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que criamos o banco de dados vetorial, precisamos dividir a documenta√ß√£o em <code>chunk</code>s e guard√°-los no banco de dados vetorial.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Fun%C3%A7%C3%A3o-de-carregamento-de-documentos">Fun√ß√£o de carregamento de documentos<a class="anchor-link" href="#Fun%C3%A7%C3%A3o-de-carregamento-de-documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 51" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro criamos uma fun√ß√£o para carregar todos os documentos <code>.txt</code> de um diret√≥rio</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>',
      '    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">),</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>',
      '        <span class="k">return</span> <span class="p">{</span><span class="s2">"id"</span><span class="p">:</span> <span class="n">file</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()}</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">load_documents_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>',
      '    <span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>',
      '    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>',
      '        <span class="k">if</span> <span class="n">file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">".txt"</span><span class="p">):</span>',
      '            <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>',
      '    <span class="k">return</span> <span class="n">documents</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <h3 id="Fun%C3%A7%C3%A3o-para-dividir-a-documenta%C3%A7%C3%A3o-em-chunks">Fun√ß√£o para dividir a documenta√ß√£o em <code>chunk</code>s<a class="anchor-link" href="#Fun%C3%A7%C3%A3o-para-dividir-a-documenta%C3%A7%C3%A3o-em-chunks"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 52" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez que temos os documentos, n√≥s os dividimos em <code>chunks</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
      '    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>',
      '    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>',
      '    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>',
      '        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">chunk_size</span>',
      '        <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>',
      '        <span class="n">start</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">chunk_overlap</span>',
      '    <span class="k">return</span> <span class="n">chunks</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <h3 id="Fun%C3%A7%C3%A3o-para-gerar-embeddings-de-um-chunk">Fun√ß√£o para gerar embeddings de um <code>chunk</code><a class="anchor-link" href="#Fun%C3%A7%C3%A3o-para-gerar-embeddings-de-um-chunk"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 53" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos os <code>chunks</code>, geramos os embeddings de cada um deles</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Depois veremos por qu√™, mas para gerar os embeddings vamos faz√™-lo de maneira local e n√£o atrav√©s da API do Hugging Face. Para isso, precisamos ter instalado o <a href="https://pytorch.org" target="_blank" rel="nofollow noreferrer">PyTorch</a> e <code>sentence-transformers</code>, para isso fazemos</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>sentence-transformers
      </pre></div>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>',
      '<span class="kn">import</span> <span class="nn">torch</span>',
      ' ',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      ' ',
      '<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">EMBEDDING_MODEL</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>',
      '    <span class="k">try</span><span class="p">:</span>',
      '        <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>',
      '        <span class="k">return</span> <span class="n">embedding</span>',
      '    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
      '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>



















      
      <section class="section-block-markdown-cell">
      <p>Vamos testar agora esta fun√ß√£o de embeddings localmente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello, how are you?"</span>',
          '<span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
          '<span class="n">embedding</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que obtemos um embedding da mesma dimens√£o que quando o faz√≠amos com a API de Hugging Face</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O modelo <code>sentence-transformers/all-MiniLM-L6-v2</code> tem apenas 22M de par√¢metros, ent√£o voc√™ vai poder execut√°-lo em qualquer GPU. Mesmo se voc√™ n√£o tiver GPU, ser√° capaz de execut√°-lo em uma CPU.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O LLM que vamos usar para gerar as respostas, que √© o <code>Qwen2.5-72B-Instruct</code>, como o pr√≥prio nome indica, √© um modelo de 72B de par√¢metros, portanto, esse modelo n√£o pode ser executado em qualquer GPU e em uma CPU √© impens√°vel de t√£o lerdo que seria. Por isso, esse LLM ser√° usado atrav√©s da API, mas no momento de gerar os embeddings, podemos faz√™-lo localmente sem problemas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Documentos-com-os-quais-vamos-testar">Documentos com os quais vamos testar<a class="anchor-link" href="#Documentos-com-os-quais-vamos-testar"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 54" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para fazer todos esses testes, eu baixei o dataset <a href="https://www.kaggle.com/datasets/harshsinghal/aws-case-studies-and-blogs" target="_blank" rel="nofollow noreferrer">aws-case-studies-and-blogs</a> e o deixei na pasta <code>rag-txt_dataset</code>, com os seguintes comandos eu te digo como baix√°-lo e descompact√°-lo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Criamos a pasta onde vamos baixar os documentos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">!</span>mkdir<span class="w"> </span>rag_txt_dataset',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Baixamos o <code>.zip</code> com os documentos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="o">!</span>curl<span class="w"> </span>-L<span class="w"> </span>-o<span class="w"> </span>./rag_txt_dataset/archive.zip<span class="w"> </span>https://www.kaggle.com/api/v1/datasets/download/harshsinghal/aws-case-studies-and-blogs',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',
          '                                 Dload  Upload   Total   Spent    Left  Speed',
          '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',
          '100 1430k  100 1430k    0     0  1082k      0  0:00:01  0:00:01 --:--:-- 2440k',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Descompactamos o <code>.zip</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">!</span>unzip<span class="w"> </span>rag_txt_dataset/archive.zip<span class="w"> </span>-d<span class="w"> </span>rag_txt_dataset',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Archive:  rag_txt_dataset/archive.zip',
          '  inflating: rag_txt_dataset/23andMe Case Study _ Life Sciences _ AWS.txt  ',
          '  inflating: rag_txt_dataset/36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt  ',
          '  inflating: rag_txt_dataset/54gene _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/6sense Case Study.txt  ',
          '  inflating: rag_txt_dataset/ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/AEON Case Study.txt  ',
          '  inflating: rag_txt_dataset/ALTBalaji _ Amazon Web Services.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study - Ineos Team UK.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study - StreamAMG.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study_ Creditsafe.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study_ Immowelt.txt  ',
          '  inflating: rag_txt_dataset/AWS Customer Case Study _ Kepler Provides Effective Monitoring of Elderly Care Home Residents Using AWS _ AWS.txt  ',
          '  inflating: rag_txt_dataset/AWS announces 21 startups selected for the AWS generative AI accelerator _ AWS Startups Blog.txt  ',
          '  inflating: rag_txt_dataset/AWS releases smart meter data analytics _ AWS for Industries.txt  ',
          '  inflating: rag_txt_dataset/Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt  ',
          '  inflating: rag_txt_dataset/Accelerating Migration at Scale Using AWS Application Migration Service with 3M Company _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/Accelerating Time to Market Using AWS and AWS Partner AccelByte _ Omeda Studios Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/Accelerating customer onboarding using Amazon Connect _ NCS Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/Achieving Burstable Scalability and Consistent Uptime Using AWS Lambda with TiVo _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/Acrobits Uses Amazon Chime SDK to Easily Create Video Conferencing Application Boosting Collaboration for Global Users _ Acrobits Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/Actuate AI Case study.txt  ',
          '  inflating: rag_txt_dataset/Adzuna doubles its email open rates using Amazon SES _ Adzuna Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/Amanotes Stays on Beat by Delivering Simple Music Games to Millions Worldwide on AWS.txt  ',
          '  ...  ',
          '  inflating: rag_txt_dataset/Windsor.txt  ',
          '  inflating: rag_txt_dataset/Wireless Car Case Study _ AWS IoT Core _ AWS.txt  ',
          '  inflating: rag_txt_dataset/Yamato Logistics (HK) case study.txt  ',
          '  inflating: rag_txt_dataset/Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt  ',
          '  inflating: rag_txt_dataset/Zoox Case Study _ Automotive _ AWS.txt  ',
          '  inflating: rag_txt_dataset/e-banner Streamlines Its Contact Center Operations and Facilitates a Fully Remote Workforce with Amazon Connect _ e-banner Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/iptiQ Case Study.txt  ',
          '  inflating: rag_txt_dataset/mod.io Provides Low Latency Gamer Experience Globally on AWS _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/myposter Case Study.txt  ',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Apagamos o <code>.zip</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">!</span>rm<span class="w"> </span>rag_txt_dataset/archive.zip',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Vemos o que nos sobrou</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="o">!</span>ls<span class="w"> </span>rag_txt_dataset',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'23andMe Case Study _ Life Sciences _ AWS.txt\'',
          '\'36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt\'',
          '\'54gene _ Case Study _ AWS.txt\'',
          '\'6sense Case Study.txt\'',
          '\'Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt\'',
          '\'Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt\'',
          '\'Accelerating customer onboarding using Amazon Connect _ NCS Case Study _ AWS.txt\'',
          '\'Accelerating Migration at Scale Using AWS Application Migration Service with 3M Company _ Case Study _ AWS.txt\'',
          '\'Accelerating Time to Market Using AWS and AWS Partner AccelByte _ Omeda Studios Case Study _ AWS.txt\'',
          '\'Achieving Burstable Scalability and Consistent Uptime Using AWS Lambda with TiVo _ Case Study _ AWS.txt\'',
          '\'Acrobits Uses Amazon Chime SDK to Easily Create Video Conferencing Application Boosting Collaboration for Global Users _ Acrobits Case Study _ AWS.txt\'',
          '\'Actuate AI Case study.txt\'',
          '\'ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt\'',
          '\'Adzuna doubles its email open rates using Amazon SES _ Adzuna Case Study _ AWS.txt\'',
          '\'AEON Case Study.txt\'',
          '\'ALTBalaji _ Amazon Web Services.txt\'',
          '\'Amanotes Stays on Beat by Delivering Simple Music Games to Millions Worldwide on AWS.txt\'',
          '\'Amazon OpenSearch Services vector database capabilities explained _ AWS Big Data Blog.txt\'',
          '\'Anghami Case Study.txt\'',
          '\'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt\'',
          '\'AppsFlyer Amazon EKS Case Study _ Advertising _ AWS.txt\'',
          '\'Arm Case Study.txt\'',
          '\'Arm Limited Case Study.txt\'',
          '\'Armitage Technologies case study.txt\'',
          '\'Armut Case Study.txt\'',
          '\'Auto-labeling module for deep learning-based Advanced Driver Assistance Systems on AWS _ AWS Machine Learning Blog.txt\'',
          '\'AWS announces 21 startups selected for the AWS generative AI accelerator _ AWS Startups Blog.txt\'',
          '\'AWS Case Study - Ineos Team UK.txt\'',
          '\'AWS Case Study - StreamAMG.txt\'',
          '\'AWS Case Study_ Creditsafe.txt\'',
          '\'...\'',
          '\'What Will Generative AI Mean for Your Business_ _ AWS Cloud Enterprise Strategy Blog.txt\'',
          '\'Which Recurring Business Processes Can Small and Medium Businesses Automate_ _ AWS Smart Business Blog.txt\'',
          ' Windsor.txt',
          '\'Wireless Car Case Study _ AWS IoT Core _ AWS.txt\'',
          '\'Yamato Logistics (HK) case study.txt\'',
          '\'Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt\'',
          '\'Zoox Case Study _ Automotive _ AWS.txt\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Criando-os-chunks!">Criando os <code>chunk</code>s!<a class="anchor-link" href="#Criando-os-chunks!"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 55" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Listamos os documentos com a fun√ß√£o que hav√≠amos criado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset_path</span> <span class="o">=</span> <span class="s2">"rag_txt_dataset"</span>',
      '<span class="n">documents</span> <span class="o">=</span> <span class="n">load_documents_from_directory</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Comprovamos que o fizemos bem</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"id"</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Run Jobs at Scale While Optimizing for Cost Using Amazon EC2 Spot Instances with ActionIQ _ ActionIQ Case Study _ AWS.txt',
          'Recommend and dynamically filter items based on user context in Amazon Personalize _ AWS Machine Learning Blog.txt',
          'Windsor.txt',
          'Bank of Montreal Case Study _ AWS.txt',
          'The Mill Adventure Case Study.txt',
          'Optimize software development with Amazon CodeWhisperer _ AWS DevOps Blog.txt',
          'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt',
          'THREAD _ Life Sciences _ AWS.txt',
          'Deep Pool Optimizes Software Quality Control Using Amazon QuickSight _ Deep Pool Case Study _ AWS.txt',
          'Upstox Saves 1 Million Annually Using Amazon S3 Storage Lens _ Upstox Case Study _ AWS.txt',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Agora criamos os <code>chunk</code>s.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">chunked_documents</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>',
      '    <span class="n">chunks</span> <span class="o">=</span> <span class="n">split_text</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>',
      '    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>',
      '        <span class="n">chunked_documents</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"id"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">document</span><span class="p">[</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">chunk</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <CodeBlockInputCell
        text={[
          '</span><span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '3611',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, existem 3611 <code>chunk</code>s. Como o limite di√°rio da API da Hugging Face s√£o 1000 chamadas na conta gratuita, se quisermos criar embeddings de todos os <code>chunk</code>s, acabar√≠amos com as chamadas dispon√≠veis e al√©m disso n√£o poder√≠amos criar embeddings de todos os <code>chunk</code>s.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Voltamos a lembrar, este modelo de embeddings √© muito pequeno, apenas 22M de par√¢metros, por isso pode ser executado em quase qualquer computador, mais r√°pido ou mais devagar, mas pode.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como s√≥ vamos criar os embeddings dos <code>chunk</code>s uma vez, mesmo que n√£o tenhamos um computador muito potente e demore muito tempo, isso s√≥ ser√° executado uma vez. Ent√£o, quando quisermos fazer perguntas sobre a documenta√ß√£o, a√≠ sim geraremos os embeddings do prompt com a API do Hugging Face e usaremos o LLM com a API. Portanto, s√≥ teremos que passar pelo processo de gerar os embeddings dos <code>chunk</code>s uma vez.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Geramos os embeddings dos <code>chunk</code>s</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>√öltima biblioteca que vamos ter que instalar. Como o processo de gerar os embeddings dos <code>chunk</code>s vai ser lento, vamos instalar <code>tqdm</code> para que nos mostre uma barra de progresso. Instalamos com conda ou com pip, como preferir.</p>
      <div class="highlight"><pre><span></span><span class="sb"></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::tqdm<span class="sb"></span>
      </pre></div>
      <p>o</p>
      <div class="highlight"><pre><span></span><span class="sb"></span>pip<span class="w"> </span>install<span class="w"> </span>tqdm<span class="sb"></span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Geramos os embeddings dos <code>chunk</code>s</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">tqdm</span>',
          '',
          '<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
          '    <span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>',
          '    <span class="k">if</span> <span class="n">embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>',
          '        <span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error with document </span><span class="si">{</span><span class="n">chunk</span><span class="p">[</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3611/3611 [00:16&lt;00:00, 220.75it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos um exemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>',
          '',
          '<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Chunk id: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\\n\\n</span><span class="s2">text: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">\'text\'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\\n\\n</span><span class="s2">embedding shape: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">\'embedding\'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Chunk id: BNS Group Case Study _ Amazon Web Services.txt_0,',
          'text: Reducing Virtual Machines from 40 to 12',
          'The founders of BNS had been contemplating a migration from the company‚Äôs on-premises data center to the public cloud and observed a growing demand for cloud-based operations among current and potential BNS customers.',
          'Fran√ßais',
          'Configures security according to cloud best practices',
          'Clive Pereira, R&amp;D director at BNS Group, explains, ‚ÄúThe database that records Praisal‚Äôs SMS traffic resides in Praisal‚Äôs AWS environment. Praisal can now run complete analytics across its data and gain insights into what‚Äôs happening with its SMS traffic, which is a real game-changer for the organization.‚Äù¬† ',
          'Espa√±ol',
          ' AWS ISV Accelerate Program',
          ' Receiving Strategic, Foundational Support from ISV Specialists',
          ' Learn More',
          'The value that AWS places on the ISV stream sealed the deal in our choice of cloud provider.‚Äù ',
          'Êó•Êú¨Ë™û',
          '  Contact Sales ',
          'BNS is an Australian software provider focused on secure enterprise SMS and fax messaging. Its software runs on the Windows platform and is l,',
          'embedding shape: (384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Carregar-os-chunks-no-banco-de-dados-vetorial">Carregar os <code>chunk</code>s no banco de dados vetorial<a class="anchor-link" href="#Carregar-os-chunks-no-banco-de-dados-vetorial"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 56" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez que temos todos os chunks gerados, os carregamos no banco de dados vetorial. Usamos novamente <code>tqdm</code> para que nos mostre uma barra de progresso, porque isso tamb√©m ser√° lento</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">tqdm</span>',
          '',
          '<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
          '    <span class="n">collection</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span>',
          '        <span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]],</span>',
          '        <span class="n">documents</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span>',
          '        <span class="n">embeddings</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">],</span>',
          '    <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3611/3611 [00:59&lt;00:00, 60.77it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Perguntas">Perguntas<a class="anchor-link" href="#Perguntas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 57" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos o banco de dados vetorial, podemos fazer perguntas √† documenta√ß√£o. Para isso, precisamos de uma fun√ß√£o que nos devolva o <code>chunk</code> correto.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Obter-o-chunk-correto">Obter o <code>chunk</code> correto<a class="anchor-link" href="#Obter-o-chunk-correto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 58" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora precisamos de uma fun√ß√£o que nos devolva o <code>chunk</code> correto, vamos cri√°-la</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>',
      '    <span class="n">results</span> <span class="o">=</span> <span class="n">collection</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query_texts</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">n_results</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>',
      '    <span class="k">return</span> <span class="n">results</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Por √∫ltimo, criamos uma <code>query</code>.
      Para gerar a query, escolhi aleatoriamente o documento <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code>, passei-o a um LLM e pedi-lhe que gerasse uma pergunta sobre o documento. A pergunta que ele gerou √©
      Como a Neeva usou Karpenter e as Inst√¢ncias Spot do Amazon EC2 para melhorar o gerenciamento de sua infraestrutura e a otimiza√ß√£o de custos?
      Assim, obtemos os <code>chunk</code>s mais relevantes para essa pergunta</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">"How did Neeva use Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization?"</span>',
      '<span class="n">top_chunks</span> <span class="o">=</span> <span class="n">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Vamos ver quais <code>chunk</code>s nos devolveu</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_chunks</span><span class="p">[</span><span class="s2">"ids"</span><span class="p">][</span><span class="mi">0</span><span class="p">])):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Rank </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">\'ids\'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, distance: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">\'distances\'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Rank 1: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_0, distance: 0.29233667254447937',
          'Rank 2: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_5, distance: 0.4007825255393982',
          'Rank 3: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_1, distance: 0.4317566752433777',
          'Rank 4: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_6, distance: 0.43832334876060486',
          'Rank 5: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_4, distance: 0.44625571370124817',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como havia dito, o documento que tinha escolhido ao acaso era <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code> e como pode ver os <code>chunk</code>s que nos devolveu s√£o desse documento. Ou seja, de mais de 3000 <code>chunk</code>s existentes na base de dados, foi capaz de devolver os <code>chunk</code>s mais relevantes para essa pergunta, parece que isto funciona!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Gerar-a-resposta">Gerar a resposta<a class="anchor-link" href="#Gerar-a-resposta"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 59" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como j√° temos os <code>chunk</code>s mais relevantes, passamos eles ao LLM, junto com a pergunta, para que este gere uma resposta</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">relevant_chunks</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>',
      '    <span class="n">context</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\n\\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">chunk</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">relevant_chunks</span><span class="p">])</span>',
      '    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"You are an assistant for question-answering. You have to answer the following question:</span><span class="se">\\n\\n</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="se">\\n\\n</span><span class="s2">Answer the question with the following information:</span><span class="se">\\n\\n</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s2">"</span>',
      '    <span class="n">message</span> <span class="o">=</span> <span class="p">[</span>',
      '        <span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">prompt</span> <span class="p">}</span>',
      '    <span class="p">]</span>',
      '    <span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>',
      '        <span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> ',
      '        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>',
      '        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>',
      '        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>',
      '        <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>',
      '    <span class="p">)</span>',
      '    <span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>',
      '    <span class="k">return</span> <span class="n">response</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>




















      
      <section class="section-block-markdown-cell">
      <p>Testamos a fun√ß√£o</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '</span><span class="n">response</span> <span class="o">=</span> <span class="n">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">top_chunks</span><span class="p">[</span><span class="s2">"documents"</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Here‚Äôs how:',
          '### Early Collaboration with Karpenter',
          'In late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.',
          '### Combining Spot Instances and On-Demand Instances',
          'Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.',
          '### Flexibility and Instance Diversification',
          'According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter\'s adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.',
          '### Improved Scalability and Agility',
          'By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:',
          '- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.',
          '- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.',
          '### Enhanced Development Cycles',
          'The integration of Karpenter and Spot Instances has also accelerated Neeva\'s development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.',
          '### Cost Savings and Budget Control',
          'Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.',
          '### Future Plans',
          'Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, "The bulk of our compute is or will be managed using Karpenter going forward."',
          '### Conclusion',
          'By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Quando pedi ao LLM para gerar uma pergunta sobre o documento, tamb√©m pedi para gerar a resposta correta. Esta √© a resposta que o LLM me deu</p>
      <pre><code class="language-Neeva">Neeva used Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization in several ways:
Simplified Instance Management:

Karpenter: By adopting Karpenter, Neeva simplified the process of provisioning and managing compute resources for its Amazon EKS clusters. Karpenter automatically provisions and de-provisions instances based on the workload, eliminating the need for manual configurations and reducing the complexity of understanding different compute instances.
Spot Instances: Neeva leveraged Amazon EC2 Spot Instances, which are unused EC2 capacity available at a significant discount (up to 90% cost savings). This allowed the company to control costs while meeting its performance requirements.
Enhanced Scalability:

Karpenter: Karpenter's ability to dynamically scale resources enabled Neeva to spin up new instances quickly, allowing the company to iterate at a higher velocity and run more experiments in less time.Spot Instances: The use of Spot Instances provided flexibility and instance diversification, making it easier for Neeva to scale its compute resources efficiently.
Improved Productivity:

Karpenter: By democratizing infrastructure changes, Karpenter allowed any engineer to modify Kubernetes configurations, reducing the dependency on specialized expertise. This saved the Neeva team up to 100 hours per week of wait time on systems administration.
Spot Instances: The ability to quickly provision and de-provision Spot Instances reduced delays in the development pipeline, ensuring that jobs did not get stuck due to a lack of available resources.
Cost Efficiency:

Karpenter: Karpenter's best practices for Spot Instances, including flexibility and instance diversification, helped Neeva use these instances more effectively, staying within budget.
Spot Instances: The cost savings from using Spot Instances allowed Neeva to run large-scale jobs, such as indexing, for nearly the same cost but in a fraction of the time. For example, Neeva reduced its indexing jobs from 18 hours to just 3 hours.
Better Resource Utilization:

Karpenter: Karpenter provided better visibility into compute resource usage, allowing Neeva to track and optimize its resource consumption more closely.
Spot Instances: The combination of Karpenter and Spot Instances enabled Neeva to run large language models more efficiently, enhancing the search experience for its users.
In summary, Neeva's adoption of Karpenter and Amazon EC2 Spot Instances significantly improved its infrastructure management, cost optimization, and overall development efficiency, enabling the company to deliver better ad-free search experiences to its users.</code></pre>
      <p>E esta foi a resposta gerada pelo nosso <code>RAG</code>
      <pre><code>Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Here‚Äôs how:
### Early Collaboration with KarpenterIn late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.

### Combining Spot Instances and On-Demand Instances
Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.

### Flexibility and Instance Diversification
According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter's adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.

### Improved Scalability and Agility
By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:
- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.
- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.

### Enhanced Development Cycles
The integration of Karpenter and Spot Instances has also accelerated Neeva's development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.

### Cost Savings and Budget Control
Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.

### Future Plans
Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, "The bulk of our compute is or will be managed using Karpenter going forward."

### Conclusion
By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.</code></pre>
      <p>Por isso podemos concluir que o <code>RAG</code> funcionou corretamente!!!</p>
      </section>
      






    </div>

  </section>

</PostLayout>
