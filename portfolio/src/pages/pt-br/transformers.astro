---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Transformers ‚Äì from top to the bottom';
const end_url = 'transformers';
const description = 'Descubra os transformadores üöÄ. Conhe√ßa a arquitetura por tr√°s de todos os novos modelos de linguagem. N√£o pergunte a uma IA, entre e aprenda';
const keywords = 'transformador, transformadores, nlp, de cima a baixo, aten√ß√£o, aten√ß√£o multi-cabe√ßa, aten√ß√£o escalada do produto ponto, adicionar e normalizar, codifica√ß√£o posicional';
const languaje = 'PT';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer%20-%20leadspace.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1584
    image_height=633
    image_extension=webp
    article_date=2024-02-23+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Transformador como uma caixa preta"><h2>Transformador como uma caixa preta</h2></a>
      <a class="anchor-link" href="#Tokenizacao"><h2>Tokeniza√ß√£o</h2></a>
      <a class="anchor-link" href="#Embeddings de entrada"><h2>Embeddings de entrada</h2></a>
      <a class="anchor-link" href="#Codificador - decodificador"><h2>Codificador - decodificador</h2></a>
      <a class="anchor-link" href="#Projecao"><h2>Proje√ß√£o</h2></a>
      <a class="anchor-link" href="#Codificador e decodificador x6"><h2>Codificador e decodificador x6</h2></a>
      <a class="anchor-link" href="#Atencao - Alimentacao"><h2>Aten√ß√£o - Alimenta√ß√£o</h2></a>
      <a class="anchor-link" href="#Atencao"><h3>Aten√ß√£o</h3></a>
      <a class="anchor-link" href="#Alimentacao"><h3>Alimenta√ß√£o</h3></a>
      <a class="anchor-link" href="#Codificacao posicional"><h2>Codifica√ß√£o posicional</h2></a>
      <a class="anchor-link" href="#Add & Norm"><h2>Add & Norm</h2></a>
      <a class="anchor-link" href="#Mecanismos de atendimento"><h2>Mecanismos de atendimento</h2></a>
      <a class="anchor-link" href="#Atencao a varias cabecas"><h3>Aten√ß√£o a v√°rias cabe√ßas</h3></a>
      <a class="anchor-link" href="#Atencao ao produto de ponto de escala"><h3>Aten√ß√£o ao produto de ponto de escala</h3></a>
      <a class="anchor-link" href="#Atencao ao produto de pontos da escala Endocer"><h4>Aten√ß√£o ao produto de pontos da escala Endocer</h4></a>
      <a class="anchor-link" href="#Matmul"><h5>Matmul</h5></a>
      <a class="anchor-link" href="#Scale"><h5>Scale</h5></a>
      <a class="anchor-link" href="#Mask (opcional)"><h5>Mask (opcional)</h5></a>
      <a class="anchor-link" href="#Softmax"><h5>Softmax</h5></a>
      <a class="anchor-link" href="#Matmul"><h5>Matmul</h5></a>
      <a class="anchor-link" href="#Resumo"><h5>Resumo</h5></a>
      <a class="anchor-link" href="#Decodificador de escala mascarada atencao ao produto de pontos"><h4>Decodificador de escala mascarada aten√ß√£o ao produto de pontos</h4></a>
      <a class="anchor-link" href="#Por que a mascara"><h5>Por que a m√°scara</h5></a>
      <a class="anchor-link" href="#Mask"><h5>Mask</h5></a>
      <a class="anchor-link" href="#Atencao ao produto de ponto da escala do codificador-decodificador"><h4>Aten√ß√£o ao produto de ponto da escala do codificador-decodificador</h4></a>
      <a class="anchor-link" href="#Resumo"><h2>Resumo</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Nesta postagem, veremos como os Transformers funcionam de cima para baixo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
      <h2 id="Transformador como uma caixa preta">Transformador como uma caixa preta<a class="anchor-link" href="#Transformador como uma caixa preta"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 55" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A arquitetura do transformador foi criada para o problema de tradu√ß√£o, portanto, vamos explic√°-la para esse problema.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Imagine o transformador como uma caixa preta, que recebe uma frase em um idioma e produz a mesma frase traduzida em outro idioma.</p>
      <p>Transformador - caixa preta] (https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-black-box.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Tokenizacao">Tokeniza√ß√£o<a class="anchor-link" href="#Tokenizacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 56" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mas, como vimos na postagem <a href="https://maximofn.com/tokens/">tokens</a>, os modelos de linguagem n√£o entendem as palavras como n√≥s, eles precisam de n√∫meros para poder realizar opera√ß√µes. Portanto, a senten√ßa original do idioma precisa ser convertida em tokens por um tokenizador e, na sa√≠da, precisamos de um detokenizador para converter os tokens de sa√≠da em palavras.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-black-box-tokenizers.webp" alt="Transformer - black box - tokenizers">
      <p>Assim, o tokenizador cria uma sequ√™ncia de tokens <span class="math-inline">n<sub>input-tokens</sub></span> e o destokenizador recebe uma sequ√™ncia de tokens <span class="math-inline">n<sub>output-tokens</sub></span>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Embeddings de entrada">Embeddings de entrada<a class="anchor-link" href="#Embeddings de entrada"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 57" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Na postagem <a href="https://maximofn.com/embeddings/">embeddings</a>, vimos que os embeddings s√£o uma forma de representar palavras em um espa√ßo vetorial. Portanto, os tokens de entrada s√£o passados por uma camada de embeddings para convert√™-los em vetores.</p>
      <p>Em um resumo r√°pido, o processo de incorpora√ß√£o consiste em converter uma sequ√™ncia de n√∫meros (tokens) em uma sequ√™ncia de vetores. Assim, √© criado um novo espa√ßo vetorial no qual as palavras que t√™m semelhan√ßa sem√¢ntica estar√£o muito pr√≥ximas.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/word_embedding_3_dimmension.webp" alt="word_embedding_3_dimmension">
      <p>Se tiv√©ssemos <span class="math-inline">n<sub>input-tokens</sub></span> tokens, agora ter√≠amos <span class="math-inline">n<sub>input-tokens</sub></span> vetores. Cada um desses vetores tem um comprimento de <span class="math-inline">d<sub>model</sub></span>. Ou seja, cada token √© convertido em um vetor que representa esse token em um espa√ßo vetorial de dimens√µes <span class="math-inline">d<sub>model</sub><p><span class="math-display">. Portanto, depois de passar pela camada de incorpora√ß√£o, a sequ√™ncia de tokens </span>n<sub>input-tokens</sub><span class="math-inline"> se torna uma matriz de (</span>n<sub>input-tokens</sub></span></p> x <span class="math-inline">d<sub>model</sub></span>).</p>
      <p>Transformador - caixa preta - embeddings de entrada](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-black-box-input-embeddings.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Codificador - decodificador">Codificador - decodificador<a class="anchor-link" href="#Codificador - decodificador"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 58" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vimos o transformador atuando como uma caixa preta, mas, na realidade, o transformador √© uma arquitetura composta de duas partes, um codificador e um decodificador.</p>
      <p>Transformador - codificador-decodificador] (https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder.png)</p>
      <p>O codificador √© respons√°vel por comprimir as informa√ß√µes da frase de entrada, criando um espa√ßo latente no qual as informa√ß√µes da frase de entrada s√£o comprimidas. Em seguida, essas informa√ß√µes comprimidas entram no decodificador, que sabe como converter essas informa√ß√µes comprimidas em uma frase do idioma de sa√≠da.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>E como o decodificador converte essas informa√ß√µes compactadas em uma frase no idioma de sa√≠da? Bem, token por token. Para entender melhor, vamos esquecer os tokens de sa√≠da por um momento e imaginar que temos a seguinte arquitetura</p>
      <p>Transformador - codificador-decodificador (sem destokenizador)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-no-detokenizer.webp)</p>
      <p>Ou seja, a frase do idioma original √© convertida em tokens, esses tokens s√£o convertidos em embeddings, que entram no codificador, o codificador comprime as informa√ß√µes, o decodificador as pega e as converte em palavras do idioma de sa√≠da.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Assim, o decodificador gera uma nova palavra na sa√≠da a cada etapa.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer).gif" alt="Transformer - codificador-decodificador (sem detokenizador)">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mas como o decodificador sabe qual palavra deve ser gerada a cada vez? Porque lhe √© passada a frase que j√° foi traduzida e, a cada etapa, ele gera a pr√≥xima palavra. Em outras palavras, a cada etapa, o decodificador recebe a frase que traduziu at√© o momento e gera a pr√≥xima palavra.</p>
      <p>Mas, mesmo assim, como ele sabe que precisa gerar a primeira palavra? Porque lhe √© passada uma palavra especial que significa "come√ßar a traduzir" e, a partir da√≠, ele gera as palavras seguintes.</p>
      <p>E, finalmente, como o transformador sabe que precisa parar de gerar palavras? Porque, quando termina de traduzir, ele gera uma palavra especial que significa "fim da tradu√ß√£o", que, quando volta para o transformador, significa que ele n√£o gera mais palavras.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer)%20(input).gif" alt="Transformer - encoder-decoder (no detokenizer) (input)">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que j√° entendemos isso em palavras, o que √© mais simples, vamos colocar o detokenizador de volta na sa√≠da.</p>
      <p>Transformador - codificador-decodificador] (https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder.png)</p>
      <p>Portanto, o decodificador gerar√° tokens. Para saber que precisa iniciar uma frase, √© inserido um token especial comumente chamado de <code>SOS</code> (Start Of Sentence) e, para saber que precisa terminar, ele gera outro token especial comumente chamado de <code>EOS</code> (End Of Sentence).</p>
      <p>E, assim como o codificador, o token de entrada precisa passar por uma camada de incorpora√ß√£o para converter os tokens em representa√ß√µes vetoriais.</p>
      <p>Supondo que cada token seja equivalente a uma palavra, o processo de tradu√ß√£o seria o seguinte</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(detokenizer).gif" alt="Transformer - encoder-decoder (detokenizer)">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No momento, temos esta arquitetura</p>
      <p>Transformador - codificador-decodificador (detokenizador)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-detokenizer-2.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Projecao">Proje√ß√£o<a class="anchor-link" href="#Projecao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 59" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Dissemos que o decodificador recebe um token que passa pela camada de incorpora√ß√£o <code>Output embedding</code>.</p>
      <p>O decodificador <code>Output</code> cria um vetor para cada token, de modo que na sa√≠da do decodificador <code>Output</code> temos uma matriz de (<span class="math-inline">n<sub>output-tokens</sub></span> x <span class="math-inline">d<sub>model</sub></span>).</p>
      <p>O decodificador executa opera√ß√µes, mas gera uma matriz com a mesma dimens√£o. Portanto, ele precisa converter essa matriz em um token, e faz isso por meio de uma camada linear que gera uma matriz com a mesma dimens√£o dos poss√≠veis tokens no idioma a ser traduzido (vocabul√°rio de sa√≠da).</p>
      <p>Essa matriz corresponde aos logits de cada token poss√≠vel e, portanto, √© passada por uma camada softmax que converte esses logits em probabilidades. Ou seja, teremos a probabilidade de que cada token seja o pr√≥ximo token.</p>
      <p>Transformer - proje√ß√£o](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-projection.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Codificador e decodificador x6">Codificador e decodificador x6<a class="anchor-link" href="#Codificador e decodificador x6"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 60" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No documento original, eles usam 6 camadas para o codificador e outras 6 camadas para o decodificador. N√£o h√° motivo para que sejam 6. Acho que eles tentaram v√°rios valores e esse foi o que funcionou melhor para eles.</p>
      <p>A sa√≠da do √∫ltimo codificador √© enviada para cada decodificador.</p>
      <p>Transformador - codificador-decodificador (x6)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-x6.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para simplificar o diagrama, vamos represent√°-lo da seguinte forma</p>
      <p>Transformador - codificador-decodificador (Nx)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-Nx.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Atencao - Alimentacao">Aten√ß√£o - Alimenta√ß√£o<a class="anchor-link" href="#Atencao - Alimentacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 61" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos come√ßar a examinar o que h√° dentro do codificador e do decodificador. Basicamente, o que voc√™ tem √© um mecanismo de aten√ß√£o e uma camada de avan√ßo.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-attention-ff.webp" alt="Transformador - codificador-decodificador - aten√ß√£o-ff">
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Atencao">Aten√ß√£o<a class="anchor-link" href="#Atencao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 62" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que tr√™s setas entram nos mecanismos de aten√ß√£o. Veremos isso mais tarde, quando analisarmos em profundidade como funcionam os mecanismos de aten√ß√£o.</p>
      <p>Mas, por enquanto, podemos dizer que s√£o opera√ß√µes realizadas para obter a rela√ß√£o que existe entre os tokens (e, portanto, a rela√ß√£o que existe entre as palavras).</p>
      <p>Antes dos transformadores, as redes neurais recorrentes eram usadas para o problema de tradu√ß√£o, que consistia em redes que recebiam um token de entrada, processavam-no e geravam outro token de sa√≠da. Em seguida, um segundo token era inserido, processado e outro token era gerado, e assim por diante com todos os tokens na sequ√™ncia de entrada. O problema com essas redes √© que, quando as frases eram muito longas, quando os √∫ltimos tokens eram inseridos, a rede "esquecia" os primeiros tokens. Por exemplo, em frases muito longas, poderia acontecer de o g√™nero do sujeito mudar ao longo da frase traduzida. E isso acontecia porque, depois de muitos tokens, a rede esquecia se o sujeito era masculino ou feminino.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para resolver isso, a sequ√™ncia inteira √© inserida no mecanismo de aten√ß√£o dos transformadores e as rela√ß√µes (aten√ß√£o) entre todos os tokens s√£o calculadas de uma s√≥ vez.</p>
      <p>Isso √© muito eficiente, pois em um √∫nico c√°lculo ele fornece a rela√ß√£o entre todos os tokens, independentemente do tamanho da sequ√™ncia.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Embora essa seja uma grande vantagem e seja o que levou os transformadores a serem usados na maioria das melhores redes modernas, tamb√©m √© sua maior desvantagem, pois o c√°lculo da aten√ß√£o √© muito caro do ponto de vista computacional. Ele requer multiplica√ß√µes de matrizes muito grandes.</p>
      <p>Essas multiplica√ß√µes s√£o realizadas entre matrizes que correspondem √†s incorpora√ß√µes de cada um dos tokens por si s√≥. Ou seja, a matriz que representa as incorpora√ß√µes dos tokens √© multiplicada por ela mesma. Para realizar essa multiplica√ß√£o, uma das matrizes deve ser girada (requisitos de √°lgebra para poder multiplicar matrizes). Assim, uma matriz √© multiplicada por ela mesma; se a sequ√™ncia de entrada tiver mais tokens, as matrizes que est√£o sendo multiplicadas ser√£o maiores, uma em altura e outra em largura, de modo que a mem√≥ria necess√°ria para armazenar essas matrizes aumentar√° quadraticamente.</p>
      <p>Portanto, √† medida que o comprimento das sequ√™ncias aumenta, a quantidade de mem√≥ria necess√°ria para armazenar essas matrizes cresce quadraticamente. E essa √© uma grande limita√ß√£o atual, a quantidade de mem√≥ria que as GPUs t√™m, que √© onde essas multiplica√ß√µes geralmente s√£o realizadas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma √∫nica camada de aten√ß√£o √© usada no codificador para extrair as rela√ß√µes entre os tokens de entrada.</p>
      <p>Duas camadas de aten√ß√£o s√£o usadas no decodificador, uma para extrair as rela√ß√µes entre os tokens de sa√≠da e outra para extrair as rela√ß√µes entre os tokens do codificador e os tokens do decodificador.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Alimentacao">Alimenta√ß√£o<a class="anchor-link" href="#Alimentacao"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 63" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ap√≥s a camada de aten√ß√£o, a sequ√™ncia entra em uma camada de <code>Feed forward</code> que tem duas finalidades</p>
      <ul>
        <li>Uma delas √© adicionar n√£o linearidades. Como explicamos, a aten√ß√£o √© obtida por meio de multiplica√ß√µes de matrizes dos tokens das sequ√™ncias de entrada. Mas se nenhuma camada n√£o linear for aplicada a uma rede, no final, toda a arquitetura poder√° ser resumida em alguns c√°lculos lineares. Portanto, as redes neurais n√£o seriam capazes de resolver problemas n√£o lineares. Portanto, essa camada √© adicionada para acrescentar a n√£o linearidade.</li>
      </ul>
      <ul>
        <li>Outra √© a extra√ß√£o de recursos. Embora a aten√ß√£o j√° extraia recursos, esses s√£o recursos das rela√ß√µes entre tokens. Mas essa camada <code>Feed forward</code> √© respons√°vel por extrair recursos dos pr√≥prios tokens. Ou seja, os recursos s√£o extra√≠dos de cada token que s√£o considerados importantes para o problema que est√° sendo resolvido.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Codificacao posicional">Codifica√ß√£o posicional<a class="anchor-link" href="#Codificacao posicional"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 64" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Explicamos que na camada de aten√ß√£o s√£o obtidas as rela√ß√µes entre os tokens, que essa rela√ß√£o √© calculada por multiplica√ß√µes de matrizes e que essas multiplica√ß√µes s√£o realizadas entre a matriz de incorpora√ß√£o por si s√≥. Portanto, nas senten√ßas <code>O gato come peixe</code> e <code>O peixe come gato</code>, a rela√ß√£o entre <code>o</code> e <code>gato</code> √© a mesma em ambas as senten√ßas, uma vez que a rela√ß√£o √© calculada por meio de multiplica√ß√µes de matrizes dos embeddings de <code>o</code> e <code>gato</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Entretanto, na primeira, <code>the</code> se refere ao <code>cat</code>, enquanto na segunda <code>the</code> se refere ao <code>fish</code>. Portanto, al√©m das rela√ß√µes entre as palavras, precisamos ter algum mecanismo para indicar sua posi√ß√£o na frase.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>No documento, eles prop√µem a introdu√ß√£o de um mecanismo de aten√ß√£o que √© respons√°vel por adicionar valores aos vetores de incorpora√ß√£o.</p>
      <p>Transformador - codifica√ß√£o posicional](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-positional-encoding.webp)</p>
      <p>A f√≥rmula para calcular esses valores √©</p>
      <p>Transformador - codifica√ß√£o posicional (f√≥rmula)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-positional-encoding-formula.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como isso √© um pouco dif√≠cil de entender, vamos ver como seria uma distribui√ß√£o de valores da "codifica√ß√£o posicional".</p>
      <p>Transformador - codifica√ß√£o posicional (diagrama)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-positional-encoding-diagram.webp)</p>
      <p>O primeiro token ter√° os valores da primeira linha (a inferior) adicionados a ele, o segundo token ter√° os valores da segunda linha, e assim por diante, o que causa uma altera√ß√£o nos embeddings, conforme mostrado na figura. Visto em duas dimens√µes, voc√™ pode ver as ondas que est√£o sendo adicionadas.</p>
      <p>Essas ondas significam que, quando s√£o feitos c√°lculos de aten√ß√£o, as palavras mais pr√≥ximas est√£o mais relacionadas do que as palavras mais distantes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mas podemos pensar em algo: se o processo de incorpora√ß√£o consiste em criar um espa√ßo vetorial no qual palavras com o mesmo significado sem√¢ntico est√£o pr√≥ximas umas das outras, essa rela√ß√£o n√£o seria quebrada se valores fossem adicionados √†s incorpora√ß√µes?</p>
      <p>Se olharmos novamente para o exemplo do espa√ßo vetorial anterior</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/word_embedding_3_dimmension.webp" alt="word_embedding_3_dimmension">
      <p>Podemos ver que os valores variam mais ou menos de -1000 a 1000 em cada eixo, enquanto o gr√°fico de distribui√ß√µes</p>
      <p>Transformador - codifica√ß√£o posicional (diagrama)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-positional-encoding-diagram.webp)</p>
      <p>variam de -1 a 1, pois esse √© o intervalo das fun√ß√µes seno e cosseno.</p>
      <p>Portanto, estamos variando em um intervalo entre -1 e 1 os valores dos embeddings, que s√£o duas ou tr√™s ordens de magnitude a mais, de modo que a varia√ß√£o ser√° muito pequena em compara√ß√£o com o valor dos embeddings.</p>
      <p>Portanto, j√° temos uma maneira de conhecer a rela√ß√£o da posi√ß√£o dos tokens na frase.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Add & Norm">Add & Norm<a class="anchor-link" href="#Add & Norm"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 65" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Resta apenas um bloco de alto n√≠vel, que s√£o as camadas <code>Add &amp; Norm</code>.</p>
      <p>Transformer - Add & norm](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-Add-norm.webp)</p>
      <p>Essas camadas s√£o adicionadas ap√≥s cada camada de aten√ß√£o e camada de avan√ßo. Essa camada agrega a sa√≠da e a entrada de uma camada. Isso √© chamado de conex√µes residuais e tem as seguintes vantagens</p>
      <ul>
        <li>Durante o treinamento:</li>
      </ul>
      <ul>
        <li>Reduzem o problema do desvanecimento do gradiente: Quando uma rede neural √© muito grande, no processo de treinamento, os gradientes se tornam cada vez menores √† medida que voc√™ se aprofunda nas camadas. Isso faz com que as camadas mais profundas n√£o consigam atualizar bem seus pesos. As conex√µes residuais permitem que os gradientes passem diretamente pelas camadas, o que ajuda a mant√™-los grandes o suficiente para que o modelo continue aprendendo, mesmo nas camadas mais profundas.</li>
      </ul>
      <ul>
        <li>Permitir o treinamento de redes mais profundas: ao ajudar a atenuar o problema do desvanecimento do gradiente, as conex√µes residuais tamb√©m facilitam o treinamento de redes mais profundas, o que pode levar a um melhor desempenho.</li>
      </ul>
      <ul>
        <li>Durante a infer√™ncia:</li>
      </ul>
      <ul>
        <li>Permitem a transmiss√£o de informa√ß√µes entre diferentes camadas: como as conex√µes residuais permitem que a sa√≠da de cada camada se torne a soma da entrada e da sa√≠da da camada, as informa√ß√µes das camadas mais profundas s√£o transmitidas para as camadas de n√≠vel mais alto. Isso pode ser ben√©fico em muitas tarefas, especialmente quando as informa√ß√µes de baixo n√≠vel e de alto n√≠vel podem ser √∫teis.</li>
      </ul>
      <ul>
        <li>Melhorar a robustez do modelo: como as conex√µes residuais permitem que as camadas aprendam melhor em camadas mais profundas, os modelos com conex√µes residuais podem ser mais robustos a perturba√ß√µes nos dados de entrada.</li>
      </ul>
      <ul>
        <li>Permitem a recupera√ß√£o de informa√ß√µes perdidas: se algumas informa√ß√µes forem perdidas durante a transforma√ß√£o em qualquer camada, as conex√µes residuais podem permitir que essas informa√ß√µes sejam recuperadas nas camadas subsequentes.</li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Essa camada √© chamada de <code>Add &amp; Norm</code>, j√° vimos a <code>Add</code>, vamos dar uma olhada na <code>Norm</code>. A normaliza√ß√£o √© adicionada para que a adi√ß√£o da entrada e da sa√≠da n√£o acione os valores.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>J√° vimos todas as camadas de alto n√≠vel do transformador.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" alt="transformer">
      <p>para que possamos examinar a parte mais importante que d√° nome ao documento, os mecanismos de aten√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Mecanismos de atendimento">Mecanismos de atendimento<a class="anchor-link" href="#Mecanismos de atendimento"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 66" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Atencao a varias cabecas">Aten√ß√£o a v√°rias cabe√ßas<a class="anchor-link" href="#Atencao a varias cabecas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 67" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de analisarmos o mecanismo real da aten√ß√£o, temos que analisar a aten√ß√£o de v√°rias cabe√ßas.</p>
      <p>Transformador - aten√ß√£o a v√°rios cabe√ßotes] (https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-multi-head-attention.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Quando explicamos as camadas de alto n√≠vel, vimos que nas camadas de aten√ß√£o havia 3 setas, que s√£o <code>Q</code>, <code>K</code> e <code>V</code>. Essas s√£o matrizes que correspondem √†s informa√ß√µes de tokens; no caso do mecanismo de aten√ß√£o do codificador, elas correspondem aos tokens da senten√ßa do idioma original e, no caso da camada de aten√ß√£o do decodificador, elas correspondem aos tokens da senten√ßa que foi traduzida at√© o momento e √† sa√≠da do codificador.</p>
      <p>N√£o nos importamos com a origem dos tokens agora, apenas lembre-se de que eles correspondem a tokens. Conforme explicado acima, os tokens s√£o convertidos em embeddings, de modo que <code>Q</code>, <code>K</code> e <code>V</code> s√£o matrizes de tamanho (<span class="math-inline">n<sub>tokens</sub></span> x <span class="math-inline">d<sub>model</sub></span>). Normalmente, a dimens√£o de incorpora√ß√£o (<span class="math-inline">d<sub>model</sub></span>) √© um n√∫mero grande, como 512, 1024, 2048 etc. (n√£o precisa ser uma pot√™ncia de 2, esses s√£o apenas exemplos).</p>
      <p>J√° explicamos que os embeddings s√£o representa√ß√µes vetoriais de tokens. Ou seja, os tokens s√£o convertidos em espa√ßos vetoriais nos quais as palavras com significado sem√¢ntico semelhante est√£o muito pr√≥ximas.</p>
      <p>Portanto, de todas essas dimens√µes, algumas podem estar relacionadas a caracter√≠sticas morfol√≥gicas, outras a caracter√≠sticas sint√°ticas, outras a caracter√≠sticas sem√¢nticas etc. Portanto, faz sentido calcular os mecanismos de aten√ß√£o entre as dimens√µes de embeddings com caracter√≠sticas semelhantes.</p>
      <p>Lembre-se de que os mecanismos de aten√ß√£o buscam a similaridade entre as palavras, portanto, faz sentido buscar a similaridade entre recursos semelhantes.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Portanto, antes de calcular os mecanismos de aten√ß√£o, as dimens√µes de incorpora√ß√£o s√£o separadas em grupos de caracter√≠sticas semelhantes, e os mecanismos de aten√ß√£o entre esses grupos s√£o calculados.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>E como essa separa√ß√£o √© feita? Voc√™ teria que procurar dimens√µes semelhantes, mas fazer isso em um espa√ßo de 512, 1024, 2048 etc. dimens√µes √© muito complicado. Al√©m disso, n√£o √© poss√≠vel saber quais caracter√≠sticas s√£o semelhantes e, em cada caso, as caracter√≠sticas consideradas semelhantes mudar√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>As proje√ß√µes lineares s√£o, portanto, usadas para separar as dimens√µes em grupos. Em outras palavras, os embeddings passam por camadas lineares que os separam em grupos de caracter√≠sticas semelhantes. Dessa forma, durante o treinamento do transformador, os pesos das camadas lineares ser√£o alterados at√© chegar a um ponto em que o agrupamento seja feito de maneira ideal.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora podemos ter a quest√£o de em quantos grupos devemos nos dividir. No documento original, ele √© dividido em 8 grupos, mas n√£o h√° motivo para que sejam 8. Acho que eles tentaram v√°rios valores e esse foi o que funcionou melhor para eles.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Uma vez que as incorpora√ß√µes tenham sido divididas em grupos semelhantes e a aten√ß√£o nos diferentes grupos tenha sido calculada, os resultados s√£o concatenados. Isso √© l√≥gico, suponhamos que tenhamos um ebedding de 512 dimens√µes e o dividamos em 8 grupos de 64 dimens√µes. Se calcularmos a aten√ß√£o em cada um dos grupos, teremos 8 matrizes de aten√ß√£o de 64 dimens√µes; se as concatenarmos, teremos uma matriz de aten√ß√£o de 512 dimens√µes, que √© a mesma dimens√£o que t√≠nhamos no in√≠cio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Mas a concatena√ß√£o faz com que todos os recursos fiquem juntos. As primeiras 64 dimens√µes correspondem a um recurso, as 64 seguintes a outro, e assim por diante. Portanto, para mistur√°-los novamente, voc√™ passa por uma camada linear que mistura todos os recursos. E essa combina√ß√£o √© aprendida durante o treinamento.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Atencao ao produto de ponto de escala">Aten√ß√£o ao produto de ponto de escala<a class="anchor-link" href="#Atencao ao produto de ponto de escala"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 68" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Chegamos √† parte mais importante do transformador, o mecanismo de aten√ß√£o, a "aten√ß√£o do produto escalonado de pontos".</p>
      <p>Transformador - aten√ß√£o ao produto de pontos em escala](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)</p>
      <p>Transformador - f√≥rmula de aten√ß√£o do produto escalonado](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vimos, na arquitetura do Transformer h√° tr√™s mecanismos de atendimento</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" alt="transformer">
      <p>O codificador, o decodificador e o codificador-decodificador. Portanto, vamos explic√°-los separadamente, pois, embora sejam praticamente os mesmos, eles t√™m algumas pequenas diferen√ßas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Atencao ao produto de pontos da escala Endocer">Aten√ß√£o ao produto de pontos da escala Endocer<a class="anchor-link" href="#Atencao ao produto de pontos da escala Endocer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 69" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos examinar novamente o diagrama de blocos e a f√≥rmula.</p>
      <p>Transformador - aten√ß√£o ao produto de pontos em escala](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)</p>
      <p>Transformador - f√≥rmula de aten√ß√£o do produto escalonado](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primeiro, vamos entender por que havia tr√™s setas entrando nas camadas de aten√ß√£o. Se observarmos a arquitetura do transformador, a entrada do codificador se divide em tr√™s e entra na camada de aten√ß√£o.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" alt="transformer">
      <p>Portanto, <code>K</code>, <code>Q</code> e <code>V</code> s√£o o resultado da incorpora√ß√£o e da codifica√ß√£o posicional. A mesma matriz √© colocada na camada de aten√ß√£o tr√™s vezes. Devemos lembrar que essa matriz consistia em uma lista de todos os tokens (<span class="math-inline">n<sub>tokens</sub></span>), e cada token foi convertido em um vetor de embeddings de dimens√£o <span class="math-inline">d<sub>model</sub></span>, de modo que a dimens√£o da matriz ser√° (<span class="math-inline">n<sub>tokens</sub></span> x <span class="math-inline">d<sub>model</sub></span>).</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O significado de <code>K</code>, <code>Q</code> e <code>V</code> vem dos bancos de dados <code>key</code>, <code>query</code> e <code>value</code>. O mecanismo de aten√ß√£o recebe as matrizes <code>Q</code> e <code>K</code>, ou seja, a pergunta e a chave, e a sa√≠da √© a matriz <code>V</code>, ou seja, a resposta.</p>
      <p>Vamos analisar cada bloco separadamente para entender melhor isso.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Matmul">Matmul<a class="anchor-link" href="#Matmul"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 70" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Esse bloco corresponde √† multiplica√ß√£o matricial das matrizes <code>Q</code> e <code>K</code>. Mas, para realizar essa opera√ß√£o, temos de faz√™-la com a matriz transposta de <code>K</code>. Como as duas matrizes t√™m dimens√£o (<span class="math-inline">n<sub>tokens</sub></span> x <span class="math-inline">d<sub>model</sub></span>), para multiplic√°-las, a matriz <code>K</code> precisa ser transposta.</p>
      <p>Portanto, teremos uma multiplica√ß√£o de uma matriz de dimens√£o (<span class="math-inline">n<sub>tokens</sub><p><span class="math-display">x </span>d<sub>model</sub></span></p>) por outra matriz de dimens√£o (<span class="math-inline">d<sub>model</sub><p><span class="math-display">x </span>n<sub>tokens</sub><span class="math-inline">), de modo que o resultado ser√° uma matriz de dimens√£o (</span>n<sub>tokens</sub></span></p> x <span class="math-inline">n<sub>tokens</sub></span>).</p>
      <p>Transformer - matmul](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul.webp)</p>
      <p>Como podemos ver, o resultado √© uma matriz em que a diagonal √© a multiplica√ß√£o da incorpora√ß√£o de cada token por ela mesma, e o restante das posi√ß√µes s√£o as multiplica√ß√µes entre as incorpora√ß√µes de cada token.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora vamos ver por que essa multiplica√ß√£o √© feita. Na postagem anterior [Measuring similarity between embeddings] (https://maximofn.com/embeddings-similarity/), vimos que uma maneira de obter a similaridade entre dois vetores de incorpora√ß√£o √© calcular o cosseno</p>
      <p>Na figura acima, √© poss√≠vel ver que a multiplica√ß√£o entre as matrizes <code>Q</code> e <code>K</code> corresponde √† multiplica√ß√£o dos embeddings de cada token. A multiplica√ß√£o entre dois vetores √© realizada da seguinte forma</p>
      <p><span class="math-display">"\mathbf. \mathbf&#123;V&#125; = \mathbf&#123;U&#125;| \mathbf&#123;V&#125;| \cos(&theta;)</span></p>$.
      <p>Ou seja, temos a multiplica√ß√£o das normas por seu cosseno. Se os vetores fossem unit√°rios, ou seja, suas normas fossem 1, a multiplica√ß√£o de dois vetores seria igual ao cosseno entre os dois vetores, que √© uma das medidas de similaridade entre vetores.</p>
      <p>Assim, como em cada posi√ß√£o da matriz resultante temos a multiplica√ß√£o entre os vetores de incorpora√ß√£o de cada token, na realidade, cada posi√ß√£o da matriz representar√° a similaridade entre cada token.</p>
      <p>Relembrando o que eram embeddings, os embeddings eram representa√ß√µes vetoriais de tokens em um espa√ßo vetorial, em que os tokens com similaridade sem√¢ntica est√£o pr√≥ximos uns dos outros.</p>
      <p>Assim, com essa multiplica√ß√£o, obtivemos uma matriz de similaridade entre os tokens da frase</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul-similarity-matrix.webp" alt="Transformer - matmul - similarity matrix">
      <p>Os elementos diagonais t√™m similaridade m√°xima (verde), os elementos de canto t√™m similaridade m√≠nima (vermelho) e o restante dos elementos tem similaridade intermedi√°ria.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Scale">Scale<a class="anchor-link" href="#Scale"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 71" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vejamos novamente o diagrama de aten√ß√£o do produto escalar de pontos e sua f√≥rmula</p>
      <p>Transformador - aten√ß√£o ao produto de pontos em escala](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)</p>
      <p>Transformador - f√≥rmula de aten√ß√£o do produto escalonado](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Dissemos que se, ao multiplicar <code>Q</code> por <code>K</code>, fiz√©ssemos a multiplica√ß√£o entre os vetores de incorpora√ß√£o e que, se esses vetores tivessem norma 1, o resultado seria a similaridade entre os vetores. Mas como os vetores n√£o t√™m norma 1, o resultado pode ter valores muito altos, ent√£o normalizamos dividindo pela raiz quadrada da dimens√£o dos vetores de incorpora√ß√£o.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Mask (opcional)">Mask (opcional)<a class="anchor-link" href="#Mask (opcional)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 72" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O mascaramento √© opcional e n√£o √© usado no codificador, portanto, n√£o o explicaremos no momento para n√£o confundir o leitor.</p>
      <p>Transformador - aten√ß√£o ao produto de pontos em escala](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)</p>
      <p>Transformador - f√≥rmula de aten√ß√£o do produto escalonado](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Softmax">Softmax<a class="anchor-link" href="#Softmax"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 73" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Embora tenhamos dividido pela raiz quadrada da dimens√£o dos vetores de incorpora√ß√£o, poder√≠amos fazer com que a similaridade entre os vetores de incorpora√ß√£o ficasse entre os valores 0 e 1, portanto, para garantir isso, passamos por uma camada softmax.</p>
      <p>Transformador - aten√ß√£o ao produto de pontos em escala](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)</p>
      <p>Transformador - f√≥rmula de aten√ß√£o do produto escalonado](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul-similarity-matrix-softmax.webp" alt="Transformer - matmul - similarity matrix softmax">
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Matmul">Matmul<a class="anchor-link" href="#Matmul"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 74" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora que temos uma matriz de similaridade entre os vetores de incorpora√ß√£o, vamos multiplic√°-la pela matriz <code>V</code>, que representa as incorpora√ß√µes dos tokens.</p>
      <p>Transformador - matmul2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul2.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>A multiplica√ß√£o nos d√°</p>
      <p>Transformer - matmul2 result](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul2-result.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Obtemos uma matriz com uma mistura de incorpora√ß√µes com sua similaridade. Em cada linha, obtemos uma mistura das incorpora√ß√µes, em que cada elemento da incorpora√ß√£o √© ponderado de acordo com a similaridade do token nessa linha com o restante dos tokens.</p>
      <p>Al√©m disso, temos novamente uma matriz de tamanho (<span class="math-inline">n<sub>tokens</sub></span> x <span class="math-inline">d<sub>model</sub></span>), que √© a mesma dimens√£o que t√≠nhamos no in√≠cio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Resumo">Resumo<a class="anchor-link" href="#Resumo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 75" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Em resumo, podemos dizer que a <code>aten√ß√£o ao produto de ponto escalonado</code> √© um mecanismo que calcula a similaridade entre os tokens de uma frase e, a partir dessa similaridade, calcula uma matriz de sa√≠da que corresponde a uma mistura de embeddings ponderados de acordo com a similaridade dos tokens.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Decodificador de escala mascarada atencao ao produto de pontos">Decodificador de escala mascarada aten√ß√£o ao produto de pontos<a class="anchor-link" href="#Decodificador de escala mascarada atencao ao produto de pontos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 76" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos examinar novamente a arquitetura do transformador.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" alt="transformer">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como podemos ver, nesse caso, a "aten√ß√£o ao produto escalonado de pontos" tem a palavra "mascarado". Primeiro, explicaremos por que esse mascaramento √© necess√°rio e, em seguida, veremos como ele √© feito.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Por que a mascara">Por que a m√°scara<a class="anchor-link" href="#Por que a mascara"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 77" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como dissemos, o transformador foi inicialmente concebido como um tradutor, mas, em geral, √© uma arquitetura na qual voc√™ coloca uma sequ√™ncia e ela produz outra sequ√™ncia. No entanto, quando se trata de treinamento, √© necess√°rio fornecer a sequ√™ncia de entrada e a sequ√™ncia de sa√≠da e, a partir da√≠, o transformador aprende a traduzir.</p>
      <p>Por outro lado, dissemos que o transformador gera um novo token a cada vez. Ou seja, ele recebe a sequ√™ncia de entrada no codificador e um token de in√≠cio de sequ√™ncia especial no decodificador e, a partir da√≠, gera o primeiro token da sequ√™ncia de sa√≠da.</p>
      <p>Em seguida, a sequ√™ncia de entrada √© colocada de volta no codificador e o token gerado anteriormente no decodificador e, a partir da√≠, ele gera o segundo token da sequ√™ncia de sa√≠da.</p>
      <p>Em seguida, a sequ√™ncia de entrada √© colocada de volta no codificador e os dois tokens gerados anteriormente no decodificador e, a partir da√≠, ele gera o terceiro token da sequ√™ncia de sa√≠da.</p>
      <p>E assim por diante, at√© gerar um token especial de fim de sequ√™ncia.</p>
      <p>Mas no treinamento, como a sequ√™ncia de entrada e sa√≠da √© fornecida a ele de uma s√≥ vez, precisamos mascarar os tokens que ele ainda n√£o gerou para que n√£o possa v√™-los.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Mask">Mask<a class="anchor-link" href="#Mask"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 78" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos examinar novamente o diagrama de blocos e a f√≥rmula.</p>
      <p>Transformador - aten√ß√£o ao produto de pontos em escala](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)</p>
      <p>Transformador - f√≥rmula de aten√ß√£o do produto escalonado](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>O mascaramento √© feito ap√≥s o <code>Scale</code> e antes do <code>Softmax</code>. Como precisamos mascarar os tokens "futuros", o que pode ser feito √© multiplicar a matriz <code>Scale</code> resultante por uma matriz que tenha 0 nas posi√ß√µes que queremos mascarar e 1 nas posi√ß√µes que n√£o queremos mascarar.</p>
      <p>Transformador - M√°scara] (https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-Mask.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ao fazer isso, obtemos a mesma matriz de antes, mas com posi√ß√µes mascaradas.</p>
      <p>Transformer - Mask resutl](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-Mask-resutl.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora, o resultado do <code>Scaled dot product attention</code> √© uma matriz com os embeddings dos tokens ponderados de acordo com a similaridade dos tokens, mas com os tokens que n√£o devem ser mascarados.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Atencao ao produto de ponto da escala do codificador-decodificador">Aten√ß√£o ao produto de ponto da escala do codificador-decodificador<a class="anchor-link" href="#Atencao ao produto de ponto da escala do codificador-decodificador"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 79" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos examinar novamente a arquitetura do transformador.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" alt="transformer">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Agora vemos que o mecanismo de aten√ß√£o recebe duas vezes a sa√≠da do codificador e uma vez a aten√ß√£o mascarada do decodificador. Portanto, "K" e "V" s√£o a sa√≠da do codificador e "Q" √© a sa√≠da do decodificador.</p>
      <p>Transformador - aten√ß√£o ao produto de pontos em escala](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)</p>
      <p>Transformador - f√≥rmula de aten√ß√£o do produto escalonado](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Portanto, nesse bloco de aten√ß√£o, primeiro √© calculada a similaridade entre a senten√ßa do decodificador e a senten√ßa do codificador, ou seja, √© calculada a similaridade entre a senten√ßa que foi traduzida at√© o momento e a senten√ßa original.</p>
      <p>Essa semelhan√ßa √© ent√£o multiplicada pela senten√ßa do codificador, ou seja, √© obtida uma mistura de embeddings da senten√ßa original, ponderada de acordo com a semelhan√ßa da senten√ßa traduzida at√© o momento.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resumo">Resumo<a class="anchor-link" href="#Resumo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 80" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Percorremos o transformador do n√≠vel mais alto ao mais baixo, para que voc√™ j√° tenha uma no√ß√£o de como ele funciona.</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" alt="transformer">
      <p>Transformador - aten√ß√£o a v√°rios cabe√ßotes] (https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-multi-head-attention.webp)</p>
      <p>Transformador - aten√ß√£o ao produto de pontos em escala](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)</p>
      </section>







    </div>

  </section>

</PostLayout>
