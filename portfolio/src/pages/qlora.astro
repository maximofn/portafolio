---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'QLoRA: Efficient Finetuning of Quantized LLMs';
const end_url = 'qlora';
const description = '¬°Hola a todos! ü§ó Hoy vamos a hablar de QLoRA, la t√©cnica que te permitir√° hacer que tus modelos de lenguaje sean m√°s eficientes y r√°pidos ‚è±Ô∏è. Pero, ¬øc√≥mo lo hace? ü§î Bueno, primero utiliza la cuantizaci√≥n para reducir el tama√±o de los pesos del modelo, lo que ahorra memoria y velocidad üìà. Luego, aplica LoRA (Low-Rank Adaptation), que es como un superpoder que permite al modelo adaptarse a nuevos datos sin necesidad de volver a entrenar desde cero üí™. Y, para que veas c√≥mo funciona en la pr√°ctica, te dejo un ejemplo de c√≥digo que te har√° decir ¬°Eureka! üéâ. ¬°Vamos a sumergirnos en el mundo de QLoRA y descubrir c√≥mo podemos hacer que nuestros modelos sean m√°s inteligentes y eficientes! ü§ì';
const keywords = 'qlora, cuantizaci√≥n, lora, adaptaci√≥n de rango bajo, transformers, hugging face, nlp, procesamiento de lenguaje natural, aprendizaje autom√°tico, inteligencia artificial';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/QLoRA_thumbnail_ES.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1080
    image_height=607
    image_extension=webp
    article_date=2024-07-29+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#LoRA"><h2>LoRA</h2></a>
      <a class="anchor-link" href="#Actualizacion de pesos en una red neuronal"><h3>Actualizaci√≥n de pesos en una red neuronal</h3></a>
      <a class="anchor-link" href="#LoRA"><h3>LoRA</h3></a>
      <a class="anchor-link" href="#QLoRA"><h2>QLoRA</h2></a>
      <a class="anchor-link" href="#Cuantizacion QLoRA"><h3>Cuantizaci√≥n QLoRA</h3></a>
      <a class="anchor-link" href="#Cuantizacion de los modelos de lenguaje en normal float 4 (NF4)"><h4>Cuantizaci√≥n de los modelos de lenguaje en normal float 4 (NF4)</h4></a>
      <a class="anchor-link" href="#Doble cuantizacion"><h4>Doble cuantizaci√≥n</h4></a>
      <a class="anchor-link" href="#Optimizadores paginados"><h4>Optimizadores paginados</h4></a>
      <a class="anchor-link" href="#Fine tuning con LoRA"><h3>Fine tuning con LoRA</h3></a>
      <a class="anchor-link" href="#Como hacer fine tuning de un modelo cuantizado con QLoRA"><h2>C√≥mo hacer fine tuning de un modelo cuantizado con QLoRA</h2></a>
      <a class="anchor-link" href="#Login en el Hub de Hugging Face"><h3>Login en el Hub de Hugging Face</h3></a>
      <a class="anchor-link" href="#Dataset"><h3>Dataset</h3></a>
      <a class="anchor-link" href="#Tokenizer"><h3>Tokenizer</h3></a>
      <a class="anchor-link" href="#Modelo"><h3>Modelo</h3></a>
      <a class="anchor-link" href="#Cuantizacion del modelo"><h3>Cuantizaci√≥n del modelo</h3></a>
      <a class="anchor-link" href="#LoRA"><h3>LoRA</h3></a>
      <a class="anchor-link" href="#Training"><h3>Training</h3></a>
      <a class="anchor-link" href="#Evaluacion"><h3>Evaluaci√≥n</h3></a>
      <a class="anchor-link" href="#Publicar el modelo"><h3>Publicar el modelo</h3></a>
      <a class="anchor-link" href="#Probar el modelo"><h3>Probar el modelo</h3></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Si bien <a href="https://maximofn.com/lora/">LoRA</a> proporciona una manera de hacer fine tuning de modelos de lenguaje sin necesidad de GPUs con grandes VRAMs, en el paper de <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> van m√°s all√° y proponen la manera de hacer fine tuning de modelo cuantizados, haciendo que se necesite a√∫n menos memoria para hacer fine tuning de modelos de lenguaje.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="LoRA">LoRA<a class="anchor-link" href="#LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Actualizacion de pesos en una red neuronal">Actualizaci√≥n de pesos en una red neuronal<a class="anchor-link" href="#Actualizacion de pesos en una red neuronal"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para entender c√≥mo funciona LoRA, primero tenemos que recordar qu√© ocurre cuando entrenamos un modelo. Volvamos a la parte m√°s b√°sica del deep learning, tenemos una capa densa de una red neuronal que se define como:</p>
      <p><span class="math-display">y = Wx + b</span></p>
      <p>D√≥nde <span class="math-inline">W</span> es la matriz de pesos y <span class="math-inline">b</span> es el vector de sesgos.</p>
      <p>Para simplificar, vamos a suponer que no hay sesgo, por lo que quedar√≠a as√≠</p>
      <p><span class="math-display">y = Wx</span></p>
      <p>Supongamos que para una entrada <span class="math-inline">x</span> queremos que tenga una salida <span class="math-inline">\hat&#123;y&#125;</span></p>
      <ul>
        <li>Primero, lo que hacemos es calcular la salida que obtenemos con nuestro valor actual de pesos <span class="math-inline">W</span>, es decir, obtenemos el valor <span class="math-inline">y</span></li>
        <li>A continuaci√≥n calculamos el error que existe entre el valor de <span class="math-inline">y</span> que hemos obtenido y el valor que quer√≠amos obtener <span class="math-inline">\hat&#123;y&#125;</span>. A ese error lo llamamos <span class="math-inline">loss</span>, y lo calculamos con alguna funci√≥n matem√°tica, ahora no importa cual</li>
        <li>Calculamos el gradiente (la derivada) del error <span class="math-inline">loss</span> con respecto a la matriz de pesos <span class="math-inline">W</span>, es decir <span class="math-inline">\Delta W = \frac&#123;dloss&#125;&#123;dW&#125;</span></li>
        <li>Actualizamos los pesos <span class="math-inline">W</span> restando a cada uno de sus valores el valor del gradiente multiplicado por un factor de aprendizaje <span class="math-inline">\alpha</span>, es decir <span class="math-inline">W = W - \alpha \Delta W</span></li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="LoRA">LoRA<a class="anchor-link" href="#LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Los autores de LoRA proponen que la matriz de pesos <span class="math-inline">W</span> se puede descomponer en</p>
      <p><span class="math-display">W \sim W + \Delta W</span></p>
      <p>De manera que congelando la matriz <span class="math-inline">W</span> y entrenando solo la matriz <span class="math-inline">\Delta W</span> se puede obtener un modelo que se adapte a nuevos datos sin tener que reentrenar todo el modelo</p>
      <p>Pero podr√°s pensar que <span class="math-inline">\Delta W</span> es una matriz de tama√±o igual a <span class="math-inline">W</span> por lo que no se ha ganado nada, pero aqu√≠ los autores se basan en <code>Aghajanyan et al. (2020)</code>, un paper en el que demostraron que aunque los modelos de lenguaje son grandes y sus par√°metros son matrices con dimensiones muy grandes, para adaptarlos a nuevas tareas no es necesario cambiar todos los valores de las matrices, sino que cambiando unos pocos valores es suficiente, que en t√©rminos t√©cnicos, se llama adaptaci√≥n de bajo rango. De ah√≠ el nombre de LoRA (Low Rank Adaptation)</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos congelado el modelo y ahora queremos entrenar la matriz <span class="math-inline">\Delta W</span>, supongamos que tanto <span class="math-inline">W</span> como <span class="math-inline">\Delta W</span> son matrices de tama√±o <span class="math-inline">20 &times; 10</span>, por lo que tenemos 200 par√°metros entrenables</p>
      <p>Ahora supongamos que la matriz <span class="math-inline">\Delta W</span> se puede descomponer en el producto de dos matrices <span class="math-inline">A</span> y <span class="math-inline">B</span>, es decir</p>
      <p><span class="math-display">\Delta W = A ¬∑ B</span></p>
      <p>Para que esta multiplicaci√≥n se produzca los tama√±os de las matrices <span class="math-inline">A</span> y <span class="math-inline">B</span> tienen que ser <span class="math-inline">20 &times; n</span> y <span class="math-inline">n &times; 10</span> respectivamente. Supongamos que <span class="math-inline">n = 5</span>, por lo que <span class="math-inline">A</span> ser√≠a de tama√±o <span class="math-inline">20 &times; 5</span>, es decir 100 par√°metros, y <span class="math-inline">B</span> de tama√±o <span class="math-inline">5 &times; 10</span>, es decir 50 par√°metros, por lo que tendr√≠amos 100+50=150 par√°metros entrenables. Ya tenemos menos par√°metros entrenables que antes</p>
      <p>Ahora supongamos que <span class="math-inline">W</span> en realidad es una matriz de tama√±o <span class="math-inline">10.000 &times; 10.000</span>, por lo que tendr√≠amos 100.000.000 par√°metros entrenables, pero si descomponemos <span class="math-inline">\Delta W</span> en <span class="math-inline">A</span> y <span class="math-inline">B</span> con <span class="math-inline">n = 5</span>, tendr√≠amos una matriz de tama√±o <span class="math-inline">10.000 &times; 5</span> y otra de tama√±o <span class="math-inline">5 &times; 10.000</span>, por lo que tendr√≠amos 50.000 par√°metros de una y otros 50.000 par√°metros de otra, en total 100.000 par√°metros entrenables, es decir hemos reducido el n√∫mero de par√°metros mil veces</p>
      <p>Ya puedes ir viendo el poder de LoRA, cuando se tienen modelos muy grandes, el n√∫mero de par√°metros entrenables se puede reducir much√≠simo</p>
      <p>Si volvemos a ver la imagen de la arquitectura de LoRA, la entenderemos mejor</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/LoRA_adapat.webp" alt="LoRA adapt">
      <p>Pero se ve mejor a√∫n, el ahorro en n√∫mero de par√°metros entrenables con esta imagen</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Lora_matmul.webp" alt="LoRA matmul">
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="QLoRA">QLoRA<a class="anchor-link" href="#QLoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>QLoRA se realiza en dos pasos, la primera consiste en cuantizar el moelo y la segunda en aplicar LoRA al modelo cuantizado</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Cuantizacion QLoRA">Cuantizaci√≥n QLoRA<a class="anchor-link" href="#Cuantizacion QLoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La cuantizaci√≥n de QLoRA se basa en tres conceptos, la cuantizaci√≥n del modelo a 4 bits con el formato normal float 4 (NF4), la doble cuantizaci√≥n y los optimizadores paginados. Todo ello junto hace que se pueda ahorrar mucha memoria al hacer fine tuning de los modelos de lenguaje, as√≠ que vamos a ver en qu√© consiste cada uno</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Cuantizacion de los modelos de lenguaje en normal float 4 (NF4)">Cuantizaci√≥n de los modelos de lenguaje en normal float 4 (NF4)<a class="anchor-link" href="#Cuantizacion de los modelos de lenguaje en normal float 4 (NF4)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En QLoRA, para cuantizar, lo que se hace es cuantizar en formato normal float 4 (NF4), que es un tipo de cuantizaci√≥n a 4 bits de manera que sus datos tienen una distribuci√≥n normal, es decir que siguen una campana de Gauss. Para conseguir que sigan esta distribuci√≥n, lo que se hace es dividir los valores de los pesos en FP16 en quantiles, de manera que en cada quantil haya el mismo n√∫mero de valores. Una vez tenemos los cuantiles, a cada cuantil se le asigna un valor en 4 bits</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/QLoRA-normal-float-quantization.webp" alt="QLoRA-normal-float-quantization">
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para realizar esta cuantizaci√≥n utiliza el algoritmo de cuantizaci√≥n SRAM, que es un algoritmo de cuantizaci√≥n por quantiles muy r√°pido, pero tiene mucho error con valores que est√°n muy lejos en la distribuci√≥n de la campana de Gauss, valores at√≠picos</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como normalmente los par√°metros de los pesos de una red neuronal suelen seguir una distribuci√≥n normal (es decir, que siguen una campana de Gauss), centrada en cero y con una desviaci√≥n est√°ndar œÉ. Lo que se hace es normalizarlos para que tengan una desviaci√≥n est√°ndar entre -1 y 1, y despu√©s se cuantizan en formato NF4</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Doble cuantizacion">Doble cuantizaci√≥n<a class="anchor-link" href="#Doble cuantizacion"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como hemos comentado, a la hora de cuantizar los par√°metros de la red, tenemos que normalizarlos para que tengan una desviaci√≥n est√°ndar entre -1 y 1, y despu√©s cuantizarlos en formato NF4. Por lo que tenemos que guardar algunos par√°metros como los valores para normalizar los par√°metros, es decir, el valor por el que se dividen los datos para que tengan una desviaci√≥n entre -1 y 1. Esos valores se almacenan en formato FP32, por lo que los autores del paper proponen cuantizar esos par√°metros a formato FP8.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aunque esto puede parecer que no ahorra mucha memoria, los autores calculan que esto puede ahorrar unos 0.373 bits por par√°metro, pero si por ejemplo tenemos un modelo de 8B de par√°metros, que no es un modelo excesivamente grande a d√≠a de hoy, nos ahorrar√≠amos unos 3 GB de memoria, que no est√° mal. En el caso de un modelo de 70B de par√°metros, nos ahorrar√≠amos unos 26 GB de memoria</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Optimizadores paginados">Optimizadores paginados<a class="anchor-link" href="#Optimizadores paginados"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Las GPUs de Nvidia tienen la opci√≥n de compartir la RAM de la GPU y de la CPU, de manera que lo que hacen es guardar los estados del optimizador en la RAM de la CPU y acceder a ellos cuando lo necesitan. As√≠ no se tienen que guardar en la RAM de la GPU y podemos ahorrar memoria de la GPU</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Fine tuning con LoRA">Fine tuning con LoRA<a class="anchor-link" href="#Fine tuning con LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez hemos cuantizado el modelo ya podemos hacer fine tuning del modelo cuantizado igual que se hace en <a href="https://maximofn.com/lora/">LoRA</a></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Como hacer fine tuning de un modelo cuantizado con QLoRA">C√≥mo hacer fine tuning de un modelo cuantizado con QLoRA<a class="anchor-link" href="#Como hacer fine tuning de un modelo cuantizado con QLoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que hemos explicado QLoRA, vamos a ver un ejemplo de c√≥mo hacer fine tuning a un modelo usando QLoRA</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Login en el Hub de Hugging Face">Login en el Hub de Hugging Face<a class="anchor-link" href="#Login en el Hub de Hugging Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero nos logeamos para poder subir el modelo entrenado al Hub</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>',
      '<span class="n">notebook_login</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Descargamos el dataset que vamos a usar, que es un dataset de reviews de <a href="https://huggingface.co/datasets/mteb/amazon_reviews_multi" target="_blank" rel="nofollow noreferrer">Amazon</a></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>',
      '<span class="w"> </span>',
      '<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;mteb/amazon_reviews_multi&quot;</span><span class="p">,</span> <span class="s2">&quot;en&quot;</span><span class="p">)</span>',
      '<span class="n">dataset</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'DatasetDict(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;train: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x20;&#x20;&#x20;&#x20;validation: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x20;&#x20;&#x20;&#x20;test: Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x20;&#x20;&#x20;&#x20;&#x7D;)',
          '&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos un subset por si quieres probar el c√≥digo con un dataset m√°s peque√±o. En mi caso usar√© el 100% del dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">percentage</span> <span class="o">=</span> <span class="mi">1</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
      '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
      '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos una muestra</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">random</span><span class="w"> </span><span class="kn">import</span> <span class="n">randint</span>',
      '<span class="w"> </span>',
      '<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">subset_dataset_train</span><span class="p">))</span>',
      '<span class="n">subset_dataset_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;id&#x27;: &#x27;en_0297000&#x27;,',
          '&#x27;text&#x27;: &#x27;Not waterproof at all\n\nBought this after reading good reviews. But it‚Äôs not water proof at all. If my son has even a little accident in bed, it goes straight to mattress. I don‚Äôt see a point in having this. So I have to purchase another one.&#x27;,',
          '&#x27;label&#x27;: 0,',
          '&#x27;label_text&#x27;: &#x27;0&#x27;&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos el n√∫mero de clases, para obtener el n√∫mero de clases usamos <code>dataset[&#x27;train&#x27;]</code> y no <code>subset_dataset_train</code> porque si el subset lo hemos muy peque√±o es posible que no haya ejemplos con todas las posibles clases del dataset original</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">&#39;label&#39;</span><span class="p">))</span>',
      '<span class="n">num_classes</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '5',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una funci√≥n para crear el campo <code>label</code> en el dataset. El dataset descargado tiene el campo <code>labels</code> pero la librer√≠a <code>transformers</code> necesita que el campo se llame <code>label</code> y no <code>labels</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">set_labels</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>',
      '<span class="w">    </span><span class="n">example</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">example</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aplicamos la funci√≥n al dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
      '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
      '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;, &#x27;labels&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;, &#x27;labels&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;id&#x27;, &#x27;text&#x27;, &#x27;label&#x27;, &#x27;label_text&#x27;, &#x27;labels&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver una muestra</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;id&#x27;: &#x27;en_0297000&#x27;,',
          '&#x27;text&#x27;: &#x27;Not waterproof at all\n\nBought this after reading good reviews. But it‚Äôs not water proof at all. If my son has even a little accident in bed, it goes straight to mattress. I don‚Äôt see a point in having this. So I have to purchase another one.&#x27;,',
          '&#x27;label&#x27;: 0,',
          '&#x27;label_text&#x27;: &#x27;0&#x27;,',
          '&#x27;labels&#x27;: 0&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Implementamos el tokenizador. Para que no nos d√© error, asignamos el token de end of string al token de padding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>',
      '<span class="w"> </span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una funci√≥n para tokenizar el dataset</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span><span class="w"> </span><span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Aplicamos la funci√≥n al dataset y, de paso, eliminamos las columnas que no necesitamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span> <span class="o">=</span> <span class="n">subset_dataset_train</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_text&#39;</span><span class="p">])</span>',
      '<span class="n">subset_dataset_validation</span> <span class="o">=</span> <span class="n">subset_dataset_validation</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_text&#39;</span><span class="p">])</span>',
      '<span class="n">subset_dataset_test</span> <span class="o">=</span> <span class="n">subset_dataset_test</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;label_text&#39;</span><span class="p">])</span>',
      '<span class="w"> </span>',
      '<span class="n">subset_dataset_train</span><span class="p">,</span> <span class="n">subset_dataset_validation</span><span class="p">,</span> <span class="n">subset_dataset_test</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '(Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 200000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;),',
          'Dataset(&#x7B;',
          '&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;],',
          '&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 5000',
          '&#x7D;))',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver una muestra, pero en este caso solo vemos las <code>keys</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">subset_dataset_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'dict_keys([&#x27;labels&#x27;, &#x27;input_ids&#x27;, &#x27;attention_mask&#x27;])',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Descargamos primero el modelo sin cuantizar</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>',
      '<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;score.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos la memoria que ocupa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 0.48 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Pasamos el modelo a FP16 y volvemos a ver la memoria que ocupa</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 0.24 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos la arquitectura del modelo antes de cuantizar</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Conv1D()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(score): Linear(in_features=768, out_features=5, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Cuantizacion del modelo">Cuantizaci√≥n del modelo<a class="anchor-link" href="#Cuantizacion del modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para cuantizar el modelo primero tenemos que crear la configuraci√≥n de cuantizaci√≥n, para ello usamos la librer√≠a <code>bitsandbytes</code>, si no la tienes instalada la puedes instalar con</p>
      <div class='highlight'><pre><code class="language-bash">pip install bitsandbytes</code></pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero comprobamos si la arquitectura de nuestra GPU permite el formato BF16, si no lo permite usaremos FP16</p>
      <p>A continuaci√≥n creamos la configuraci√≥n de cuantizaci√≥n, con <code>load_in_4bits=True</code> indicamos que cuantice a 4 bits, con <code>bnb_4bit_quant_type=&quot;nf4&quot;</code> le indicamos que lo haga en formato NF4, con <code>bnb_4bit_use_double_quant=True</code> le indicamos que haga doble cuantizaci√≥n y con <code>bnb_4bit_compute_dtype=compute_dtype</code> le indicamos qu√© formato de datos tiene que usar cuando descuantice, que puede ser FP16 o BF16.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BitsAndBytesConfig</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_bf16_supported</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>',
      '<span class="w"> </span>',
      '<span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Y ahora cuantizamos el modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '`low_cpu_mem_usage` was None, now set to True since model is quantized.',
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;score.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver la memoria que ocupa ahora que lo hemos cuantizado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 0.12 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que se ha reducido el tama√±o del modelo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver la arquitectura del modelo una vez cuantizado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Linear4bit(in_features=768, out_features=768, bias=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(score): Linear(in_features=768, out_features=5, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que la arquitectura ha cambiado</p>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/QLoRA-model-vs-quantized-model_.webp" alt="QLoRA-model-vs-quantized-model">
      <p>Ha modificado las capas <code>Conv1D</code> por capas <code>Linear4bits</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="LoRA">LoRA<a class="anchor-link" href="#LoRA"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de implementar LoRA, tenemos que configurar el modelo para entrenar en 4 bits</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">prepare_model_for_kbit_training</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver si ha cambiado el tama√±o del modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>',
      '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'Model memory: 0.20 GB',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ha aumentado la memoria, as√≠ que volvemos a ver la arquitectura del modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'GPT2ForSequenceClassification(',
          '&#x20;&#x20;(transformer): GPT2Model(',
          '&#x20;&#x20;&#x20;&#x20;(wte): Embedding(50257, 768)',
          '&#x20;&#x20;&#x20;&#x20;(wpe): Embedding(1024, 768)',
          '&#x20;&#x20;&#x20;&#x20;(drop): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;(h): ModuleList(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(0-11): 12 x GPT2Block(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn): GPT2Attention(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Linear4bit(in_features=768, out_features=768, bias=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(attn_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(resid_dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(mlp): GPT2MLP(',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(act): NewGELUActivation()',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;(dropout): Dropout(p=0.1, inplace=False)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;)',
          '&#x20;&#x20;&#x20;&#x20;(ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)',
          '&#x20;&#x20;)',
          '&#x20;&#x20;(score): Linear(in_features=768, out_features=5, bias=False)',
          ')',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>La arquitectura sigue siendo la misma, por lo que suponemos que el aumento de memoria es por alguna configuraci√≥n extra para poder aplicar LoRA en 4 bits</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una configuraci√≥n de LoRA, pero a diferencia del post de <a href="https://maximofn.com/lora/">LoRA</a> en el que solo configuramos en <code>target_modules</code> la capa <code>scores</code>, ahora vamos a a√±adir tambi√©n las capas <code>c_attn</code>, <code>c_proj</code> y <code>c_fc</code> ya que ahora son de tipo <code>Linear4bits</code> y no <code>Conv1D</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">TaskType</span>',
      '<span class="w"> </span>',
      '<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_CLS</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;c_attn&#39;</span><span class="p">,</span> <span class="s1">&#39;c_fc&#39;</span><span class="p">,</span> <span class="s1">&#39;c_proj&#39;</span><span class="p">,</span> <span class="s1">&#39;score&#39;</span><span class="p">],</span>',
      '<span class="w">    </span><span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">peft</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_peft_model</span>',
      '<span class="w"> </span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          'trainable params: 2,375,504 || all params: 126,831,520 || trainable%: 1.8730',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Mientras que en el post de <a href="https://maximofn.com/lora/">LoRA</a> ten√≠amos unos 12.000 par√°metros entrenables, ahora tenemos unos 2 millones, ya que ahora hemos a√±adido las capas <code>c_attn</code>, <code>c_proj</code> y <code>c_fc</code></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Training">Training<a class="anchor-link" href="#Training"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez instanciado el modelo cuantizado y aplicado LoRA, es decir, una vez hemos hecho QLoRA, vamos a entrenarlo como siempre</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>',
      '<span class="w"> </span>',
      '<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">&quot;accuracy&quot;</span>',
      '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;GPT2-small-QLoRA-finetuned-amazon-reviews-en-classification&quot;</span>',
      '<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>',
      '<span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">224</span>',
      '<span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">224</span>',
      '<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>',
      '<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>',
      '<span class="w"> </span>',
      '<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model_name</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">eval_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">logging_dir</span><span class="o">=</span><span class="s2">&quot;./runs&quot;</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>En el post <a href="https://maximofn.com/fine-tuning-sml/">Fine tuning SMLs</a> tuvimos que poner un BS de train de 28, en el post <a href="https://maximofn.com/lora/">LoRA</a> al poner las matrices de bajo rango en las capas lineales hizo que pudi√©ramos poner un batch size de train de 400. Ahora, como al cuantizar el modelo, la librer√≠a PEFT ha convertido algunas capas m√°s a <code>Linear</code> no podemos poner un batch size tan grande y lo tenemos que poner de 224</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>',
      '<span class="kn">from</span><span class="w"> </span><span class="nn">evaluate</span><span class="w"> </span><span class="kn">import</span> <span class="n">load</span>',
      '<span class="w"> </span>',
      '<span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>',
      '<span class="w"> </span>',
      '<span class="k">def</span><span class="w"> </span><span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>',
      '<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>',
      '<span class="w">    </span><span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>',
      '<span class="w">    </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>',
      '<span class="w">    </span><span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Trainer</span>',
      '<span class="w"> </span>',
      '<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>',
      '<span class="w">    </span><span class="n">model</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">training_args</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">train_dataset</span><span class="o">=</span><span class="n">subset_dataset_train</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_validation</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>',
      '<span class="w">    </span><span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...',
          '/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;IPython.core.display.HTML object&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7acac436c3d0&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7acac32580d0&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.',
          '&#x20;&#x20;warnings.warn(',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;IPython.core.display.HTML object&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7acac2f43c10&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          'TrainOutput(global_step=2679, training_loss=1.1650676093647934, metrics=&#x7B;&#x27;train_runtime&#x27;: 11299.1288, &#x27;train_samples_per_second&#x27;: 53.101, &#x27;train_steps_per_second&#x27;: 0.237, &#x27;total_flos&#x27;: 2.417754341376e+17, &#x27;train_loss&#x27;: 1.1650676093647934, &#x27;epoch&#x27;: 3.0&#x7D;)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Evaluacion">Evaluaci√≥n<a class="anchor-link" href="#Evaluacion"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez entrenado, evaluamos sobre el dataset de test</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">subset_dataset_test</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;IPython.core.display.HTML object&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&amp;lt;transformers.trainer_utils.EvalPrediction object at 0x7acb316fe5c0&amp;gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockOutputCell
        text={[
          '&#x7B;&#x27;eval_loss&#x27;: 0.8883273601531982,',
          '&#x27;eval_accuracy&#x27;: 0.615,',
          '&#x27;eval_runtime&#x27;: 28.5566,',
          '&#x27;eval_samples_per_second&#x27;: 175.091,',
          '&#x27;eval_steps_per_second&#x27;: 0.805,',
          '&#x27;epoch&#x27;: 3.0&#x7D;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Publicar el modelo">Publicar el modelo<a class="anchor-link" href="#Publicar el modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos una model card</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <p>Lo publicamos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Probar el modelo">Probar el modelo<a class="anchor-link" href="#Probar el modelo"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos aprobar el modelo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>',
      '<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>',
      '<span class="w"> </span>',
      '<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;GPT2-small-QLoRA-finetuned-amazon-reviews-en-classification&quot;</span>',
      '<span class="n">user</span> <span class="o">=</span> <span class="s2">&quot;maximofn&quot;</span>',
      '<span class="n">checkpoint</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span>',
      '<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">5</span>',
      '<span class="w"> </span>',
      '<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>',
      '<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '/home/sae00531/miniconda3/envs/nlp_/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.',
          '&#x20;&#x20;warnings.warn(',
          'Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: [&#x27;score.weight&#x27;]',
          'You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.',
          '/home/sae00531/miniconda3/envs/nlp_/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.',
          '&#x20;&#x20;warnings.warn(',
          'Loading adapter weights from maximofn/GPT2-small-QLoRA-finetuned-amazon-reviews-en-classification led to unexpected keys not found in the model:  [&#x27;score.modules_to_save.default.base_layer.weight&#x27;, &#x27;score.modules_to_save.default.lora_A.default.weight&#x27;, &#x27;score.modules_to_save.default.lora_B.default.weight&#x27;, &#x27;score.modules_to_save.default.modules_to_save.lora_A.default.weight&#x27;, &#x27;score.modules_to_save.default.modules_to_save.lora_B.default.weight&#x27;, &#x27;score.modules_to_save.default.original_module.lora_A.default.weight&#x27;, &#x27;score.modules_to_save.default.original_module.lora_B.default.weight&#x27;].',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;I love this product&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>',
      '<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>',
      '<span class="w">    </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>',
      '<span class="n">logits</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">logits</span>',
      '<span class="n">lables</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>',
      '<span class="n">lables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      
      <CodeBlockOutputCell
        text={[
          '[0.0186614990234375,',
          '0.483642578125,',
          '0.048187255859375,',
          '0.415283203125,',
          '0.03399658203125]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>





















    </div>

  </section>

</PostLayout>
