---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Fundamentos de RAG';
const end_url = 'rag-fundamentals';
const description = '隆Olv铆date de Ctrl+F! く Con RAG, tus documentos responder谩n a tus preguntas directamente.  Tutorial paso a paso con Hugging Face y ChromaDB. 隆Libera el poder de la IA (y presume con tus amigos)! ';
const keywords = 'rag, retriever, reader, hugging face, transformers, chromadb, base de datos vectorial, question-answering, qa, nlp, procesamiento de lenguaje natural, machine learning, inteligencia artificial, ia';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-fundamentals.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1024
    image_height=1024
    image_extension=webp
    article_date=2024-10-23+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Configuraci%C3%B3n-de-la-API-Inference-de-Hugging-Face"><h2>Configuraci贸n de la <code>API Inference</code> de Hugging Face</h2></a>
      <a class="anchor-link" href="#%C2%BFQu%C3%A9-es-RAG?"><h2>驴Qu茅 es <code>RAG</code>?</h2></a>
      <a class="anchor-link" href="#%C2%BFC%C3%B3mo-se-almacena-la-informaci%C3%B3n?"><h3>驴C贸mo se almacena la informaci贸n?</h3></a>
      <a class="anchor-link" href="#%C2%BFC%C3%B3mo-se-obtiene-el-chunk-correcto?"><h3>驴C贸mo se obtiene el <code>chunk</code> correcto?</h3></a>
      <a class="anchor-link" href="#Volvamos-a-ver-qu%C3%A9-es-RAG"><h3>Volvamos a ver qu茅 es <code>RAG</code></h3></a>
      <a class="anchor-link" href="#Base-de-datos-vectorial"><h2>Base de datos vectorial</h2></a>
      <a class="anchor-link" href="#Funci%C3%B3n-de-embedding"><h3>Funci贸n de embedding</h3></a>
      <a class="anchor-link" href="#ChromaDB-client"><h3>ChromaDB client</h3></a>
      <a class="anchor-link" href="#Colecci%C3%B3n"><h3>Colecci贸n</h3></a>
      <a class="anchor-link" href="#Carga-de-documentos"><h2>Carga de documentos</h2></a>
      <a class="anchor-link" href="#Funci%C3%B3n-de-carga-de-documentos"><h3>Funci贸n de carga de documentos</h3></a>
      <a class="anchor-link" href="#Funci%C3%B3n-para-dividir-la-documentaci%C3%B3n-en-chunks"><h3>Funci贸n para dividir la documentaci贸n en <code>chunk</code>s</h3></a>
      <a class="anchor-link" href="#Funci%C3%B3n-para-generar-embeddings-de-un-chunk"><h3>Funci贸n para generar embeddings de un <code>chunk</code></h3></a>
      <a class="anchor-link" href="#Documentos-con-los-que-vamos-a-probar"><h3>Documentos con los que vamos a probar</h3></a>
      <a class="anchor-link" href="#A-crear-los-chunks!"><h3>A crear los <code>chunk</code>s!</h3></a>
      <a class="anchor-link" href="#Cargar-los-chunks-en-la-base-de-datos-vectorial"><h3>Cargar los <code>chunk</code>s en la base de datos vectorial</h3></a>
      <a class="anchor-link" href="#Preguntas"><h2>Preguntas</h2></a>
      <a class="anchor-link" href="#Obtener-el-chunk-correcto"><h3>Obtener el <code>chunk</code> correcto</h3></a>
      <a class="anchor-link" href="#Generar-la-respuesta"><h3>Generar la respuesta</h3></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="RAG:-Fundamentos-y-t%C3%A9cnicas-avanzadas">RAG: Fundamentos y t茅cnicas avanzadas<a class="anchor-link" href="#RAG:-Fundamentos-y-t%C3%A9cnicas-avanzadas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este post vamos a ver en qu茅 consiste la t茅cnica de <code>RAG</code> (<code>Retrieval Augmented Generation</code>) y c贸mo se puede implementar en un modelo de lenguaje.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para que te salga gratis, en vez de usar una cuenta de OpenAI (como ver谩s en la mayor铆a de tutoriales) vamos a usar el <code>API inference</code> de Hugging Face, que tiene un free tier de 1000 requests al d铆a, que para hacer este post es m谩s que suficiente.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Configuraci%C3%B3n-de-la-API-Inference-de-Hugging-Face">Configuraci贸n de la <code>API Inference</code> de Hugging Face<a class="anchor-link" href="#Configuraci%C3%B3n-de-la-API-Inference-de-Hugging-Face"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para poder usar la <code>API Inference</code> de HuggingFace, lo primero que necesitas es tener una cuenta en HuggingFace, una vez la tengas hay que ir a <a href="https://huggingface.co/settings/keys" target="_blank" rel="nofollow noreferrer">Access tokens</a> en la configuraci贸n de tu perfil y generar un token nuevo.</p>
      <p>Hay que ponerle un nombre, en mi caso le voy a poner <code>rag-fundamentals</code> y habilitar el permiso <code>Make calls to serverless Inference API</code>. Se nos crear谩 un token que tenemos que copiar</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para gestionar el token vamos a crear un archivo en la misma ruta en la que estemos trabajando llamado <code>.env</code> y vamos a poner el token que hemos copiado en el archivo de la siguiente manera:</p>
      <div class="highlight"><pre><span></span><span class="nv">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="o">=</span><span class="s2">"hf_...."</span>
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora para poder obtener el token necesitamos tener instalado <code>dotenv</code>, que lo instalamos mediante</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>python-dotenv
      </pre></div>
      <p>Y ejecutamos lo siguiente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">os</span>',
      '<span class="kn">import</span> <span class="nn">dotenv</span>',
      ' ',
      '<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>',
      ' ',
      '<span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>











      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos un token, creamos un cliente, para ello necesitamos tener instalada la librer铆a <code>huggingface_hub</code>, que lo hacemos mediante conda o pip</p>
      <div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>huggingface_hub
      </pre></div>
      <p>o</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>huggingface_hub
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora tenemos que elegir qu茅 modelo vamos a usar. Puedes ver los modelos disponibles en la p谩gina de <a href="https://huggingface.co/docs/api-inference/supported-models" target="_blank" rel="nofollow noreferrer">Supported models</a> de la documentaci贸n de la <code>API Inference</code> de Hugging Face.</p>
      <p>Como a la hora de escribir el post, el mejor disponible es <code>Qwen2.5-72B-Instruct</code>, vamos a usar ese modelo.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Ahora podemos crear el cliente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>',
          '',
          '<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">MODEL</span><span class="p">)</span>',
          '<span class="n">client</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '&lt;InferenceClient(model=\'Qwen/Qwen2.5-72B-Instruct\', timeout=None)&gt;',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Hacemos una prueba a ver si funciona</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span>',
          '	<span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, qu茅 tal?"</span> <span class="p">}</span>',
          '<span class="p">]</span>',
          '',
          '<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>',
          '	<span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> ',
          '	<span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>',
          '	<span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>',
          '	<span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>',
          '	<span class="n">stream</span><span class="o">=</span><span class="kc">False</span>',
          '<span class="p">)</span>',
          '',
          '<span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '隆Hola! Estoy bien, gracias por preguntar. 驴C贸mo est谩s t煤? 驴En qu茅 puedo ayudarte hoy?',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="%C2%BFQu%C3%A9-es-RAG?">驴Qu茅 es <code>RAG</code>?<a class="anchor-link" href="#%C2%BFQu%C3%A9-es-RAG?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p><code>RAG</code> son las siglas de <code>Retrieval Augmented Generation</code>, es una t茅cnica creada para obtener informaci贸n de documentos. Aunque los LLMs pueden llegar a ser muy poderosos y tener mucho conocimiento, nunca van a ser capaces de responderte sobre unos documentos privados, como informes de tu empresa, documentaci贸n interna, etc. Por ello se cre贸 <code>RAG</code>, para poder usar estos LLMs en esa documentaci贸n privada.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="What is RAG?" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG.webp" width="1600" height="900"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La idea consiste en que un usuario hace una pregunta sobre esa documentaci贸n privada, el sistema es capaz de obtener la parte de la documentaci贸n en la que est谩 la respuesta a esa pregunta, se le pasa a un LLM la pregunta y la parte de la documentaci贸n y el LLM genera la respuesta para el usuario</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="%C2%BFC%C3%B3mo-se-almacena-la-informaci%C3%B3n?">驴C贸mo se almacena la informaci贸n?<a class="anchor-link" href="#%C2%BFC%C3%B3mo-se-almacena-la-informaci%C3%B3n?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Es sabido, y si no lo sab铆as te lo cuento ahora, que los LLMs tienen un l铆mite de informaci贸n que se les puede pasar, a esto se le llama ventana de contexto. Esto es por arquitecturas internas de los LLMs que ahora no vienen al caso. Pero lo importante es que no se les puede pasar un documento y una pregunta sin m谩s, porque es probable que el LLM no sea capaz de procesar toda esa informaci贸n.</p>
      <p>En los casos en los que se le suele pasar m谩s informaci贸n de la que su ventana de contexto permite, lo que suele pasar es que el LLM no presta atenci贸n al final de la entrada. Imagina que le preguntas al LLM por algo de tu documento, que esa informaci贸n est茅 al final del documento y el LLM no la lea.</p>
      <p>Por ello lo que se hace es dividir la documentaci贸n en bloques llamados <code>chunk</code>s. De modo que la documentaci贸n se almacena en un mont贸n de <code>chunk</code>s, que son trozos de esa documentaci贸n. As铆 que cuando el usuario hace una pregunta, se le pasa al LLM el <code>chunk</code> en el que est谩 la respuesta a esa pregunta.</p>
      <p>Adem谩s de dividir la documentaci贸n en <code>chunk</code>s, estos se convierten a embeddings, que son representaciones num茅ricas de los <code>chunk</code>s. Esto se hace porque los LLMs en realidad no entienden de texto, sino de n煤meros, y los <code>chunk</code>s se convierten a n煤meros para que el LLM pueda entenderlos. Si quieres entender m谩s sobre los embeddings, puedes leer mi post sobre <a href="https://www.maximofn.com/transformers">transformers</a> en el que explico c贸mo funcionan los transformers, que es la arquitectura por debajo de los LLMs. Tambi茅n puedes leer mi post sobre <a href="https://www.maximofn.com/chromadb">ChromaDB</a> donde explico c贸mo se guardan los embeddings en una base de datos vectorial. Y adem谩s ser铆a interesante que leyeras mi post sobre la librer铆a <a href="https://www.maximofn.com/hugging-face-tokenizers">HuggingFace Tokenizers</a> en la que se explica c贸mo se tokeniza el texto, que es el paso anterior a generar los embeddings</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="RAG - embeddings" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG-embeddings.webp" width="1400" height="750"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="%C2%BFC%C3%B3mo-se-obtiene-el-chunk-correcto?">驴C贸mo se obtiene el <code>chunk</code> correcto?<a class="anchor-link" href="#%C2%BFC%C3%B3mo-se-obtiene-el-chunk-correcto?"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos dicho que la documentaci贸n se divide en <code>chunk</code>s y se le pasa al LLM el <code>chunk</code> en el que est谩 la respuesta a la pregunta del usuario. Pero, 驴c贸mo se sabe en qu茅 <code>chunk</code> est谩 la respuesta? Para ello lo que se hace es convertir la pregunta del usuario a un embedding, y se calcula la similitud entre el embedding de la pregunta y los embeddings de los <code>chunk</code>s. De modo que el <code>chunk</code> con mayor similitud es el que se le pasa al LLM.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="RAG - embeddings similarity" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-chunk_retreival.webp" width="1374" height="351"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Volvamos-a-ver-qu%C3%A9-es-RAG">Volvamos a ver qu茅 es <code>RAG</code><a class="anchor-link" href="#Volvamos-a-ver-qu%C3%A9-es-RAG"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por un lado tenemos el <code>retrieval</code>, que es obtener <code>chunk</code> correcto de la documentaci贸n, por otro lado tenemos el <code>augmented</code>, que es pasarle al LLM la pregunta del usuario y el <code>chunk</code> y por 煤ltimo tenemos el <code>generation</code>, que es obtener la respuesta generada por el LLM.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Base-de-datos-vectorial">Base de datos vectorial<a class="anchor-link" href="#Base-de-datos-vectorial"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos visto que la documentaci贸n se divide en <code>chunk</code>s y se guarda en una base de datos vectorial, por lo que necesitamos usar una. Para este post voy a usar <a href="https://www.trychroma.com/" target="_blank" rel="nofollow noreferrer">ChromaDB</a>, que es una base de datos vectorial bastante usada y que adem谩s tengo un <a href="https://www.maximofn.com/chromadb">post</a> en el que explico c贸mo funciona.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por lo que primero necesitamos instalar la librer铆a de ChromaDB, para ello la instalamos con Conda o con pip</p>
      <div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::chromadb
      </pre></div>
      <p>o</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>chromadb
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Funci%C3%B3n-de-embedding">Funci贸n de embedding<a class="anchor-link" href="#Funci%C3%B3n-de-embedding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como hemos dicho, todo se va a basar en embeddings, por lo que lo primero que hacemos es crear una funci贸n para obtener embeddings de un texto. Vamos a usar el modelo <code>sentence-transformers/all-MiniLM-L6-v2</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">import</span> <span class="nn">chromadb.utils.embedding_functions</span> <span class="k">as</span> <span class="nn">embedding_functions</span>',
      ' ',
      '<span class="n">EMBEDDING_MODEL</span> <span class="o">=</span> <span class="s2">"sentence-transformers/all-MiniLM-L6-v2"</span>',
      '      ',
      '<span class="n">huggingface_ef</span> <span class="o">=</span> <span class="n">embedding_functions</span><span class="o">.</span><span class="n">HuggingFaceEmbeddingFunction</span><span class="p">(</span>',
      '    <span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span>',
      '    <span class="n">model_name</span><span class="o">=</span><span class="n">EMBEDDING_MODEL</span>',
      '<span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <p>Probamos la funci贸n de embedding</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">huggingface_ef</span><span class="p">([</span><span class="s2">"Hello, how are you?"</span><span class="p">,])</span>',
          '<span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos un embedding de dimensi贸n 384. Aunque la misi贸n de este post no es explicar los embeddings, en resumen, nuestra funci贸n de embedding ha categorizado la frase <code>Hello, how are you?</code> en un espacio de 384 dimensiones.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="ChromaDB-client">ChromaDB client<a class="anchor-link" href="#ChromaDB-client"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos nuestra funci贸n de embedding podemos crear un cliente de ChromaDB</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero creamos una carpeta donde se guardar谩 la base de datos vectorial</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>',
      '      ',
      '<span class="n">chroma_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"chromadb_persisten_storage"</span><span class="p">)</span>',
      '<span class="n">chroma_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>









      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos el cliente</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">chromadb</span> <span class="kn">import</span> <span class="n">PersistentClient</span>',
      ' ',
      '<span class="n">chroma_client</span> <span class="o">=</span> <span class="n">PersistentClient</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">chroma_path</span><span class="p">))</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <h3 id="Colecci%C3%B3n">Colecci贸n<a class="anchor-link" href="#Colecci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cuando tenemos el cliente de ChromaDB, lo siguiente que necesitamos es crear una colecci贸n. Una colecci贸n es un conjunto de vectores, en nuestro caso los <code>chunks</code> de la documentaci贸n.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Lo creamos indic谩ndole la funci贸n de embedding que vamos a usar</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">collection_name</span> <span class="o">=</span> <span class="s2">"document_qa_collection"</span>',
      '<span class="n">collection</span> <span class="o">=</span> <span class="n">chroma_client</span><span class="o">.</span><span class="n">get_or_create_collection</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">huggingface_ef</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <h2 id="Carga-de-documentos">Carga de documentos<a class="anchor-link" href="#Carga-de-documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que hemos creado la base de datos vectorial, tenemos que dividir la documentaci贸n en <code>chunk</code>s y guardarlos en la base de datos vectorial.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Funci%C3%B3n-de-carga-de-documentos">Funci贸n de carga de documentos<a class="anchor-link" href="#Funci%C3%B3n-de-carga-de-documentos"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero creamos una funci贸n para cargar todos los documentos <code>.txt</code> de un directorio</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>',
      '    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">),</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>',
      '        <span class="k">return</span> <span class="p">{</span><span class="s2">"id"</span><span class="p">:</span> <span class="n">file</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()}</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">load_documents_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>',
      '    <span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>',
      '    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>',
      '        <span class="k">if</span> <span class="n">file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">".txt"</span><span class="p">):</span>',
      '            <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>',
      '    <span class="k">return</span> <span class="n">documents</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>















      
      <section class="section-block-markdown-cell">
      <h3 id="Funci%C3%B3n-para-dividir-la-documentaci%C3%B3n-en-chunks">Funci贸n para dividir la documentaci贸n en <code>chunk</code>s<a class="anchor-link" href="#Funci%C3%B3n-para-dividir-la-documentaci%C3%B3n-en-chunks"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez que tenemos los documentos, los dividimos en <code>chunk</code>s</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>',
      '    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>',
      '    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>',
      '    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>',
      '        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">chunk_size</span>',
      '        <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>',
      '        <span class="n">start</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">chunk_overlap</span>',
      '    <span class="k">return</span> <span class="n">chunks</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>













      
      <section class="section-block-markdown-cell">
      <h3 id="Funci%C3%B3n-para-generar-embeddings-de-un-chunk">Funci贸n para generar embeddings de un <code>chunk</code><a class="anchor-link" href="#Funci%C3%B3n-para-generar-embeddings-de-un-chunk"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos los <code>chunk</code>s, generamos los <code>embedding</code>s de cada uno de ellos</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Luego veremos por qu茅, pero para generar los embeddings vamos a hacerlo de manera local y no mediante la API de Hugging Face. Para ello necesitamos tener instalado <a href="https://pytorch.org" target="_blank" rel="nofollow noreferrer">PyTorch</a> y <code>sentence-transformers</code>, para ello hacemos</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>sentence-transformers
      </pre></div>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>',
      '<span class="kn">import</span> <span class="nn">torch</span>',
      ' ',
      '<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>',
      ' ',
      '<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">EMBEDDING_MODEL</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>',
      ' ',
      '<span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>',
      '    <span class="k">try</span><span class="p">:</span>',
      '        <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>',
      '        <span class="k">return</span> <span class="n">embedding</span>',
      '    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>',
      '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
      '        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>



















      
      <section class="section-block-markdown-cell">
      <p>Vamos a probar ahora esta funci贸n de embeddings en local</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello, how are you?"</span>',
          '<span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>',
          '<span class="n">embedding</span><span class="o">.</span><span class="n">shape</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '(384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos que obtenemos un embedding de la misma dimensi贸n que cuando lo hac铆amos con la API de Hugging Face</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El modelo <code>sentence-transformers/all-MiniLM-L6-v2</code> tiene solo 22M de par谩metros, por lo que vas a poder ejecutarlo en cualquier GPU. Incluso si no tienes GPU, vas a poder ejecutarlo en una CPU.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El LLM que vamos a usar para generar las respuestas, que es <code>Qwen2.5-72B-Instruct</code>, como su nombre indica, es un modelo de 72B de par谩metros, por lo que este modelo no se puede ejecutar en cualquier GPU y en una CPU es impensable de lo lento que ir铆a. Por eso, este LLM s铆 lo usaremos mediante la API, pero a la hora de generar los <code>embedding</code>s lo podemos hacer en local sin problema</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Documentos-con-los-que-vamos-a-probar">Documentos con los que vamos a probar<a class="anchor-link" href="#Documentos-con-los-que-vamos-a-probar"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para hacer todas estas pruebas me he descargado el dataset <a href="https://www.kaggle.com/datasets/harshsinghal/aws-case-studies-and-blogs" target="_blank" rel="nofollow noreferrer">aws-case-studies-and-blogs</a> y lo he dejado en la carpeta <code>rag-txt_dataset</code>, con los siguientes comandos te digo c贸mo descargarlo y descomprimirlo</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Creamos la carpeta donde vamos a descargar los documentos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">!</span>mkdir<span class="w"> </span>rag_txt_dataset',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Descargamos el <code>.zip</code> con los documentos</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">!</span>curl<span class="w"> </span>-L<span class="w"> </span>-o<span class="w"> </span>./rag_txt_dataset/archive.zip<span class="w"> </span>https://www.kaggle.com/api/v1/datasets/download/harshsinghal/aws-case-studies-and-blogs',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',
          '                                 Dload  Upload   Total   Spent    Left  Speed',
          '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',
          '100 1430k  100 1430k    0     0  1082k      0  0:00:01  0:00:01 --:--:-- 2440k',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Descomprimimos el <code>.zip</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">!</span>unzip<span class="w"> </span>rag_txt_dataset/archive.zip<span class="w"> </span>-d<span class="w"> </span>rag_txt_dataset',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Archive:  rag_txt_dataset/archive.zip',
          '  inflating: rag_txt_dataset/23andMe Case Study _ Life Sciences _ AWS.txt  ',
          '  inflating: rag_txt_dataset/36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt  ',
          '  inflating: rag_txt_dataset/54gene _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/6sense Case Study.txt  ',
          '  inflating: rag_txt_dataset/ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/AEON Case Study.txt  ',
          '  inflating: rag_txt_dataset/ALTBalaji _ Amazon Web Services.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study - Ineos Team UK.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study - StreamAMG.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study_ Creditsafe.txt  ',
          '  inflating: rag_txt_dataset/AWS Case Study_ Immowelt.txt  ',
          '  inflating: rag_txt_dataset/AWS Customer Case Study _ Kepler Provides Effective Monitoring of Elderly Care Home Residents Using AWS _ AWS.txt  ',
          '  inflating: rag_txt_dataset/AWS announces 21 startups selected for the AWS generative AI accelerator _ AWS Startups Blog.txt  ',
          '  inflating: rag_txt_dataset/AWS releases smart meter data analytics _ AWS for Industries.txt  ',
          '  inflating: rag_txt_dataset/Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt  ',
          '  ...',
          '  inflating: rag_txt_dataset/Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt  ',
          '  inflating: rag_txt_dataset/Zoox Case Study _ Automotive _ AWS.txt  ',
          '  inflating: rag_txt_dataset/e-banner Streamlines Its Contact Center Operations and Facilitates a Fully Remote Workforce with Amazon Connect _ e-banner Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/iptiQ Case Study.txt  ',
          '  inflating: rag_txt_dataset/mod.io Provides Low Latency Gamer Experience Globally on AWS _ Case Study _ AWS.txt  ',
          '  inflating: rag_txt_dataset/myposter Case Study.txt  ',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Borramos el <code>.zip</code></p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="o">!</span>rm<span class="w"> </span>rag_txt_dataset/archive.zip',
        ]}
        languaje='python'
      ></CodeBlockInputCell>






      
      <section class="section-block-markdown-cell">
      <p>Vemos qu茅 nos ha quedado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="o">!</span>ls<span class="w"> </span>rag_txt_dataset',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '\'23andMe Case Study _ Life Sciences _ AWS.txt\'',
          '\'36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt\'',
          '\'54gene _ Case Study _ AWS.txt\'',
          '\'6sense Case Study.txt\'',
          '\'Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt\'',
          '\'Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt\'',
          '\'Accelerating customer onboarding using Amazon Connect _ NCS Case Study _ AWS.txt\'',
          '\'Accelerating Migration at Scale Using AWS Application Migration Service with 3M Company _ Case Study _ AWS.txt\'',
          '\'Accelerating Time to Market Using AWS and AWS Partner AccelByte _ Omeda Studios Case Study _ AWS.txt\'',
          '\'Achieving Burstable Scalability and Consistent Uptime Using AWS Lambda with TiVo _ Case Study _ AWS.txt\'',
          '\'Acrobits Uses Amazon Chime SDK to Easily Create Video Conferencing Application Boosting Collaboration for Global Users _ Acrobits Case Study _ AWS.txt\'',
          '\'Actuate AI Case study.txt\'',
          '\'ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt\'',
          '\'Adzuna doubles its email open rates using Amazon SES _ Adzuna Case Study _ AWS.txt\'',
          '\'AEON Case Study.txt\'',
          '\'ALTBalaji _ Amazon Web Services.txt\'',
          '\'Amanotes Stays on Beat by Delivering Simple Music Games to Millions Worldwide on AWS.txt\'',
          '\'Amazon OpenSearch Services vector database capabilities explained _ AWS Big Data Blog.txt\'',
          '\'Anghami Case Study.txt\'',
          '\'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt\'',
          '...',
          '\'What Will Generative AI Mean for Your Business_ _ AWS Cloud Enterprise Strategy Blog.txt\'',
          '\'Which Recurring Business Processes Can Small and Medium Businesses Automate_ _ AWS Smart Business Blog.txt\'',
          ' Windsor.txt',
          '\'Wireless Car Case Study _ AWS IoT Core _ AWS.txt\'',
          '\'Yamato Logistics (HK) case study.txt\'',
          '\'Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt\'',
          '\'Zoox Case Study _ Automotive _ AWS.txt\'',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="A-crear-los-chunks!">A crear los <code>chunk</code>s!<a class="anchor-link" href="#A-crear-los-chunks!"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Listamos los documentos con la funci贸n que hab铆amos creado</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">dataset_path</span> <span class="o">=</span> <span class="s2">"rag_txt_dataset"</span>',
      '<span class="n">documents</span> <span class="o">=</span> <span class="n">load_documents_from_directory</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Comprobamos que lo hemos hecho bien</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]:</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"id"</span><span class="p">])</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Run Jobs at Scale While Optimizing for Cost Using Amazon EC2 Spot Instances with ActionIQ _ ActionIQ Case Study _ AWS.txt',
          'Recommend and dynamically filter items based on user context in Amazon Personalize _ AWS Machine Learning Blog.txt',
          'Windsor.txt',
          'Bank of Montreal Case Study _ AWS.txt',
          'The Mill Adventure Case Study.txt',
          'Optimize software development with Amazon CodeWhisperer _ AWS DevOps Blog.txt',
          'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt',
          'THREAD _ Life Sciences _ AWS.txt',
          'Deep Pool Optimizes Software Quality Control Using Amazon QuickSight _ Deep Pool Case Study _ AWS.txt',
          'Upstox Saves 1 Million Annually Using Amazon S3 Storage Lens _ Upstox Case Study _ AWS.txt',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Ahora creamos los <code>chunk</code>s.</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">chunked_documents</span> <span class="o">=</span> <span class="p">[]</span>',
      '<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>',
      '    <span class="n">chunks</span> <span class="o">=</span> <span class="n">split_text</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>',
      '    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>',
      '        <span class="n">chunked_documents</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"id"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">document</span><span class="p">[</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">chunk</span><span class="p">})</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>










      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '3611',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos, hay 3611 <code>chunk</code>s. Como el l铆mite diario de la API de Hugging Face son 1000 llamadas en la cuenta gratuita, si queremos crear embeddings de todos los <code>chunk</code>s, se nos acabar铆an las llamadas disponibles y adem谩s no podr铆amos crear embeddings de todos los <code>chunk</code>s</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a recordar, este modelo de embeddings es muy peque帽o, solo 22M de par谩metros, por lo que casi en cualquier ordenador se puede ejecutar, m谩s r谩pido o m谩s lento, pero se puede.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como solo vamos a crear los embeddings de los <code>chunk</code>s una vez, aunque no tengamos un ordenador muy potente y tarde mucho, solo se va a ejecutar una vez. Luego cuando queramos hacer preguntas sobre la documentaci贸n, ah铆 s铆 generaremos los embeddings del prompt con la API de Hugging Face y usaremos el LLM con la API. Por lo que solo vamos a tener que pasar por el proceso de generar los embeddings de los <code>chunk</code>s una vez</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Generamos los embeddings de los <code>chunk</code>s</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>ltima librer铆a que vamos a tener que instalar. Como el proceso de generar los embeddings de los <code>chunk</code>s va a ser lento, vamos a instalar <code>tqdm</code> para que nos muestre una barra de progreso. Lo instalamos con conda o con pip, como prefieras</p>
      <div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::tqdm
      </pre></div>
      <p>o</p>
      <div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>tqdm
      </pre></div>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Generamos los embeddings de los <code>chunk</code>s</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">tqdm</span>',
          '',
          '<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
          '    <span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>',
          '    <span class="k">if</span> <span class="n">embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>',
          '        <span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>',
          '    <span class="k">else</span><span class="p">:</span>',
          '        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error with document </span><span class="si">{</span><span class="n">chunk</span><span class="p">[</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '100%|| 3611/3611 [00:16&lt;00:00, 220.75it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Vemos un ejemplo</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>',
          '',
          '<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">))</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Chunk id: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">\'id\'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\\n\\n</span><span class="s2">text: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">\'text\'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\\n\\n</span><span class="s2">embedding shape: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">\'embedding\'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Chunk id: BNS Group Case Study _ Amazon Web Services.txt_0,',
          'text: Reducing Virtual Machines from 40 to 12',
          'The founders of BNS had been contemplating a migration from the companys on-premises data center to the public cloud and observed a growing demand for cloud-based operations among current and potential BNS customers.',
          'Fran莽ais',
          'Configures security according to cloud best practices',
          'Clive Pereira, R&amp;D director at BNS Group, explains, The database that records Praisals SMS traffic resides in Praisals AWS environment. Praisal can now run complete analytics across its data and gain insights into whats happening with its SMS traffic, which is a real game-changer for the organization. ',
          'Espa帽ol',
          ' AWS ISV Accelerate Program',
          ' Receiving Strategic, Foundational Support from ISV Specialists',
          ' Learn More',
          'The value that AWS places on the ISV stream sealed the deal in our choice of cloud provider. ',
          'ユ瑾',
          '  Contact Sales ',
          'BNS is an Australian software provider focused on secure enterprise SMS and fax messaging. Its software runs on the Windows platform and is l,',
          'embedding shape: (384,)',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h3 id="Cargar-los-chunks-en-la-base-de-datos-vectorial">Cargar los <code>chunk</code>s en la base de datos vectorial<a class="anchor-link" href="#Cargar-los-chunks-en-la-base-de-datos-vectorial"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez que tenemos todos los chunks generados, los cargamos en la base de datos vectorial. Volvemos a usar <code>tqdm</code> para que nos muestre una barra de progreso, porque esto tambi茅n va a ser lento</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="kn">import</span> <span class="nn">tqdm</span>',
          '',
          '<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>',
          '',
          '<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>',
          '    <span class="n">collection</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span>',
          '        <span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]],</span>',
          '        <span class="n">documents</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span>',
          '        <span class="n">embeddings</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">],</span>',
          '    <span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          '100%|| 3611/3611 [00:59&lt;00:00, 60.77it/s]',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <h2 id="Preguntas">Preguntas<a class="anchor-link" href="#Preguntas"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos la base de datos vectorial, podemos hacerle preguntas a la documentaci贸n. Para ello, necesitamos una funci贸n que nos devuelva el <code>chunk</code> correcto</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Obtener-el-chunk-correcto">Obtener el <code>chunk</code> correcto<a class="anchor-link" href="#Obtener-el-chunk-correcto"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora necesitamos una funci贸n que nos devuelva el <code>chunk</code> correcto, vamos a crearla</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>',
      '    <span class="n">results</span> <span class="o">=</span> <span class="n">collection</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query_texts</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">n_results</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>',
      '    <span class="k">return</span> <span class="n">results</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>








      
      <section class="section-block-markdown-cell">
      <p>Por 煤ltimo, creamos una <code>query</code>.</p>
      <p>Para generar la query he cogido al azar el documento <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code>, se lo he pasado a un LLM y le he dicho que me genere una pregunta sobre el documento. La pregunta que ha generado es</p>
      <pre><code>How did Neeva use Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization?</code></pre>
      <p>As铆 que obtenemos los <code>chunk</code>s m谩s relevantes ante esa pregunta</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">"How did Neeva use Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization?"</span>',
      '<span class="n">top_chunks</span> <span class="o">=</span> <span class="n">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>







      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver qu茅 <code>chunk</code>s nos ha devuelto</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_chunks</span><span class="p">[</span><span class="s2">"ids"</span><span class="p">][</span><span class="mi">0</span><span class="p">])):</span>',
          '    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Rank </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">\'ids\'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, distance: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">\'distances\'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Rank 1: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_0, distance: 0.29233667254447937',
          'Rank 2: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_5, distance: 0.4007825255393982',
          'Rank 3: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_1, distance: 0.4317566752433777',
          'Rank 4: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_6, distance: 0.43832334876060486',
          'Rank 5: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_4, distance: 0.44625571370124817',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Como hab铆a dicho, el documento que hab铆a elegido al azar era <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code> y como se puede ver los <code>chunk</code>s que nos ha devuelto son de ese documento. Es decir, de m谩s de 3000 <code>chunk</code>s que hab铆a en la base de datos, ha sido capaz de devolverme los <code>chunk</code>s m谩s relevantes ante esa pregunta, parece que esto funciona!</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Generar-la-respuesta">Generar la respuesta<a class="anchor-link" href="#Generar-la-respuesta"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como ya tenemos los <code>chunk</code>s m谩s relevantes, se los pasamos al LLM, junto con la pregunta, para que este genere una respuesta</p>
      </section>
      
      <CodeBlockInputCell
        text={[
      '<span></span><span class="k">def</span> <span class="nf">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">relevant_chunks</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>',
      '    <span class="n">context</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\\n\\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">chunk</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">relevant_chunks</span><span class="p">])</span>',
      '    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"You are an assistant for question-answering. You have to answer the following question:</span><span class="se">\\n\\n</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="se">\\n\\n</span><span class="s2">Answer the question with the following information:</span><span class="se">\\n\\n</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s2">"</span>',
      '    <span class="n">message</span> <span class="o">=</span> <span class="p">[</span>',
      '        <span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">prompt</span> <span class="p">}</span>',
      '    <span class="p">]</span>',
      '    <span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>',
      '        <span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> ',
      '        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>',
      '        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>',
      '        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>',
      '        <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>',
      '    <span class="p">)</span>',
      '    <span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>',
      '    <span class="k">return</span> <span class="n">response</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>




















      
      <section class="section-block-markdown-cell">
      <p>Probamos la funci贸n</p>
      </section>
      
      <CodeBlockInputCell
        text={[
          '<span></span><span class="n">response</span> <span class="o">=</span> <span class="n">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">top_chunks</span><span class="p">[</span><span class="s2">"documents"</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>',
          '<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>',
        ]}
        languaje='python'
      ></CodeBlockInputCell>
      <CodeBlockOutputCell
        text={[
          'Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Heres how:',
          '### Early Collaboration with Karpenter',
          'In late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.',
          '### Combining Spot Instances and On-Demand Instances',
          'Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.',
          '### Flexibility and Instance Diversification',
          'According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter\'s adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.',
          '### Improved Scalability and Agility',
          'By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:',
          '- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.',
          '- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.',
          '### Enhanced Development Cycles',
          'The integration of Karpenter and Spot Instances has also accelerated Neeva\'s development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.',
          '### Cost Savings and Budget Control',
          'Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.',
          '### Future Plans',
          'Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, "The bulk of our compute is or will be managed using Karpenter going forward."',
          '### Conclusion',
          'By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.',
        ]}
        languaje='python'
      ></CodeBlockOutputCell>
      
      <section class="section-block-markdown-cell">
      <p>Cuando le ped铆 al LLM que me generara una pregunta sobre el documento, tambi茅n le ped铆 que me generara la respuesta correcta. Esta es la respuesta que me dio el LLM</p>
      <div class="highlight"><pre><span></span>Neeva used Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization in several ways:
      
      Simplified Instance Management:
      
      Karpenter: By adopting Karpenter, Neeva simplified the process of provisioning and managing compute resources for its Amazon EKS clusters. Karpenter automatically provisions and de-provisions instances based on the workload, eliminating the need for manual configurations and reducing the complexity of understanding different compute instances.
      Spot Instances: Neeva leveraged Amazon EC2 Spot Instances, which are unused EC2 capacity available at a significant discount (up to 90% cost savings). This allowed the company to control costs while meeting its performance requirements.
      Enhanced Scalability:
      
      Karpenter: Karpenter's ability to dynamically scale resources enabled Neeva to spin up new instances quickly, allowing the company to iterate at a higher velocity and run more experiments in less time.
      Spot Instances: The use of Spot Instances provided flexibility and instance diversification, making it easier for Neeva to scale its compute resources efficiently.
      Improved Productivity:
      
      Karpenter: By democratizing infrastructure changes, Karpenter allowed any engineer to modify Kubernetes configurations, reducing the dependency on specialized expertise. This saved the Neeva team up to 100 hours per week of wait time on systems administration.
      Spot Instances: The ability to quickly provision and de-provision Spot Instances reduced delays in the development pipeline, ensuring that jobs did not get stuck due to a lack of available resources.
      Cost Efficiency:
      
      Karpenter: Karpenter's best practices for Spot Instances, including flexibility and instance diversification, helped Neeva use these instances more effectively, staying within budget.
      Spot Instances: The cost savings from using Spot Instances allowed Neeva to run large-scale jobs, such as indexing, for nearly the same cost but in a fraction of the time. For example, Neeva reduced its indexing jobs from 18 hours to just 3 hours.
      Better Resource Utilization:
      
      Karpenter: Karpenter provided better visibility into compute resource usage, allowing Neeva to track and optimize its resource consumption more closely.
      Spot Instances: The combination of Karpenter and Spot Instances enabled Neeva to run large language models more efficiently, enhancing the search experience for its users.
      In summary, Neeva's adoption of Karpenter and Amazon EC2 Spot Instances significantly improved its infrastructure management, cost optimization, and overall development efficiency, enabling the company to deliver better ad-free search experiences to its users.
      </pre></div>
      <p>Y esta ha sido la respuesta generada por nuestro <code>RAG</code></p>
      <div class="highlight"><pre><span></span>Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Heres how:
      
      ### Early Collaboration with Karpenter
      In late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.
      
      ### Combining Spot Instances and On-Demand Instances
      Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.
      
      ### Flexibility and Instance Diversification
      According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter's adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.
      
      ### Improved Scalability and Agility
      By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:
      - **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.
      - **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.
      
      ### Enhanced Development Cycles
      The integration of Karpenter and Spot Instances has also accelerated Neeva's development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.
      
      ### Cost Savings and Budget Control
      Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.
      
      ### Future Plans
      Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, "The bulk of our compute is or will be managed using Karpenter going forward."
      
      ### Conclusion
      By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.
      </pre></div>
      <p>Por lo que podemos concluir que el <code>RAG</code> ha funcionado correctamente!!!</p>
      </section>
      






    </div>

  </section>

</PostLayout>
