---
import PostLayout from '@layouts/PostLayout.astro';
import CodeBlockInputCell from '@components/CodeBlockInputCell.astro';
import CodeBlockOutputCell from '@components/CodeBlockOutputCell.astro';

const { metadata_page } = await import('@portfolio/consts.json');
const { colors } = await import('@portfolio/consts.json');
const { svg_paths } = await import('@portfolio/consts.json');

const page_title = 'Transformers ‚Äì from top to the bottom';
const end_url = 'transformers';
const description = 'Descubre los transformers üöÄ. Aprende la arquitectura que hay dentro de todos los nuevos modelos de lenguajes. No se lo preguntes a una IA, entra y aprende';
const keywords = 'transformer, transformers, nlp, de arriba a abajo, atenci√≥n, atenci√≥n multi-cabeza, atenci√≥n escalada del producto punto, a√±adir y normalizar, codificaci√≥n posicional';
const languaje = 'ES';
const image_path = 'https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer%20-%20leadspace.webp';
const opening_brace = '{';
const closing_brace = '}';
---

<PostLayout 
    title={page_title}
    languaje={languaje}
    description={description}
    keywords={keywords}
    author={metadata_page.author}
    theme_color={colors.background_color}
    end_url={end_url}
    image_path={image_path}
    image_width=1584
    image_height=633
    image_extension=webp
    article_date=2024-02-23+T00:00:00Z
>

  <section class="post-body">


    <aside class="post-index">
      <a class="anchor-link" href="#Transformer-como-una-caja-negra"><h2>Transformer como una caja negra</h2></a>
      <a class="anchor-link" href="#Tokenizaci%C3%B3n"><h2>Tokenizaci√≥n</h2></a>
      <a class="anchor-link" href="#Input-embeddings"><h2>Input embeddings</h2></a>
      <a class="anchor-link" href="#Encoder---decoder"><h2>Encoder - decoder</h2></a>
      <a class="anchor-link" href="#Projection"><h2>Projection</h2></a>
      <a class="anchor-link" href="#Encoder-y-decoder-x6"><h2>Encoder y decoder x6</h2></a>
      <a class="anchor-link" href="#Attention---Feed-forward"><h2>Attention - Feed forward</h2></a>
      <a class="anchor-link" href="#Atenci%C3%B3n"><h3>Atenci√≥n</h3></a>
      <a class="anchor-link" href="#Feed-forward"><h3>Feed forward</h3></a>
      <a class="anchor-link" href="#Positonal-encoding"><h2>Positonal encoding</h2></a>
      <a class="anchor-link" href="#Add-&amp;-Norm"><h2>Add &amp; Norm</h2></a>
      <a class="anchor-link" href="#Mecanismos-de-atenci%C3%B3n"><h2>Mecanismos de atenci√≥n</h2></a>
      <a class="anchor-link" href="#Multi-head-attention"><h3>Multi-head attention</h3></a>
      <a class="anchor-link" href="#Scale-dot-product-attention"><h3>Scale dot product attention</h3></a>
      <a class="anchor-link" href="#Endocer-scale-dot-product-attention"><h4>Endocer scale dot product attention</h4></a>
      <a class="anchor-link" href="#Matmul"><h5>Matmul</h5></a>
      <a class="anchor-link" href="#Scale"><h5>Scale</h5></a>
      <a class="anchor-link" href="#Mask-(opt)"><h5>Mask (opt)</h5></a>
      <a class="anchor-link" href="#Softmax"><h5>Softmax</h5></a>
      <a class="anchor-link" href="#Matmul"><h5>Matmul</h5></a>
      <a class="anchor-link" href="#Resumen"><h5>Resumen</h5></a>
      <a class="anchor-link" href="#Decoder-masked-scale-dot-product-attention"><h4>Decoder masked scale dot product attention</h4></a>
      <a class="anchor-link" href="#Por-qu%C3%A9-enmascarar"><h5>Por qu√© enmascarar</h5></a>
      <a class="anchor-link" href="#Mask"><h5>Mask</h5></a>
      <a class="anchor-link" href="#Encoder-decoder-scale-dot-product-attention"><h4>Encoder-decoder scale dot product attention</h4></a>
      <a class="anchor-link" href="#Resumen"><h2>Resumen</h2></a>
    </aside>


    <div class="post-body-content">
      
      <section class="section-block-markdown-cell">
      <h1 id="Transformers---from-top-to-the-bottom">Transformers - from top to the bottom<a class="anchor-link" href="#Transformers---from-top-to-the-bottom"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 0" src={svg_paths.link_svg_path}/></a></h1>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En este post vamos a ver c√≥mo funcionan los Transformers de arriba a abajo.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Transformer-como-una-caja-negra">Transformer como una caja negra<a class="anchor-link" href="#Transformer-como-una-caja-negra"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 1" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>La arquitectura transformer se creo para el problema de traducci√≥n, por lo que vamos a explicarlo para ese problema</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Imaginemos el transformer como una caja negra, a la que le entra una frase en un idioma y saca la misma frase traducida en otro idioma.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - black box" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box.png" width="1200" height="230"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Tokenizaci%C3%B3n">Tokenizaci√≥n<a class="anchor-link" href="#Tokenizaci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 2" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero como hemos visto en el post de <a href="https://maximofn.com/tokens/">tokens</a>, los modelos de lenguaje no entienden las palabras como nosotros, sino que necesitan n√∫meros para poder realizar las operaciones. Por lo que la frase en el idioma original se tiene que convertir a tokens mediante un tokenizador, y a la salida necesitamos un detokenizador para convertir los tokens de salida a palabras</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - black box - tokenizers" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-tokenizers.png" width="1200" height="152"/></p>
      <p>De modo que el tokenizador crea una secuencia de $n_{opening_brace}input-tokens{closing_brace}$ tokens, y el detokenizador recibe una secuencia de $n_{opening_brace}output-tokens{closing_brace}$ tokens.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Input-embeddings">Input embeddings<a class="anchor-link" href="#Input-embeddings"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 3" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el post de <a href="https://maximofn.com/embeddings/">embeddings</a> vimos que los embeddings son una forma de representar las palabras en un espacio vectorial. Por lo que los tokens de entrada se pasan por una capa de embeddings para convertirlos en vectores.</p>
      <p>En un resumen r√°pido, el proceso de embedding consiste en convertir una secuencia de n√∫meros (tokens) en una secuencia de vectores. De manera que se crea un nuevo espacio vectorial en el que las palabras que tengan similitud sem√°ntia estar√°n cerca.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="word_embedding_3_dimmension" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/word_embedding_3_dimmension.webp" width="995" height="825"/></p>
      <p>Si ten√≠amos $n_{opening_brace}input-tokens{closing_brace}$ tokens, ahora tenemos $n_{opening_brace}input-tokens{closing_brace}$ vectores. Cada uno de esos vectores tiene una longitud de $d_{opening_brace}model{closing_brace}$. Es decir, cada token se convierte a un vector que representa ese token en un espacio vectorial de $d_{opening_brace}model{closing_brace}$ dimensiones.
      Por tanto despu√©s de pasar por la capa de embeddings, la secuencia de $n_{opening_brace}input-tokens{closing_brace}$ tokens se convierte en una matriz de ($n_{opening_brace}input-tokens{closing_brace}$ x $d_{opening_brace}model{closing_brace}$).</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - black box - input embeddings" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-input-embeddings.png" width="1200" height="189"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Encoder---decoder">Encoder - decoder<a class="anchor-link" href="#Encoder---decoder"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 4" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos visto el transformer actuando como una caja negra, pero en realidad el transformer es una arquitectura que se compone de dos partes, un encoder y un decoder.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png" width="911" height="1493"/></p>
      <p>El encoder se encarga de comprimir la informaci√≥n de la frase de entrada, crea un espacio latente donde est√° esa informaci√≥n de la frase de entrada es comprimida. A continuaci√≥n, esa informaci√≥n comprimida entra al decoder, que sabe convertir esa informaci√≥n comprimida en una frase del idioma de salida.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Y ¬øc√≥mo convierte el decoder esa informaci√≥n comprimida en una frase del idioma de salida? Pues token a token. Para entenderlo mejor vamos a olvidarnos de los tokens de salida por un momento, vamos a imaginar que tenemos esta arquitectura</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder (no detokenizer)" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-no-detokenizer.png" width="911" height="1343"/></p>
      <p>Es decir, la frase del idioma original se convierte a tokens, estos tokens se convierten a embeddings, que entran al encoder, este comprime la informaci√≥n, el decoder la coge y la convierte en palabras del idioma de salida</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>De modo que el decoder va generando una palabra nueva a la salida en cada paso</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder (no detokenizer)" src="https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer).gif" width="814" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero ¬øc√≥mo sabe el decoder c√∫al es la palabra que tiene que generar cada vez? Porque se le est√° pasando la frase que ya ha traducido, y en cada paso va generando la siguiente palabra. Es decir, en cada paso el decoder recibe la frase que ha traducido hasta el momento, y genera la siguiente palabra.</p>
      <p>Pero aun as√≠, ¬øc√≥mo sabe que tiene que generar la primera palabra? Porque se le pasa una palabra especial que significa "empezar a traducir", y a partir de ah√≠ va generando las siguientes palabras.</p>
      <p>Y por √∫ltimo, ¬øc√≥mo sabe el transformer que tiene que dejar de generar palabras? Porque cuando termina de traducir genera una palabra especial que significa "fin de la traducci√≥n", que cuando vuelve a entrar al transformer hace que no genere m√°s palabras.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder (no detokenizer) (input)" src="https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer)%20(input).gif" width="814" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que lo hemos entendido con palabras, que es m√°s sencillo, vamos a volver a colocar el detokenizador a la salida</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png" width="911" height="1493"/></p>
      <p>Por tanto el decoder ir√° generando tokens. Para saber que tiene que empezar una frase se le mete un token especial comunmente llamado <code>SOS</code> (Start Of Sentence), y para saber que tiene que terminar genera otro token especial comunmente llamado <code>EOS</code> (End Of Sentence).</p>
      <p>Y al igual que el encoder, el token de entrada tiene que pasar por una capa de embedding para convertir los tokens en representaciones vectoriales.</p>
      <p>Suponiendo que cada token equivale a una palabra, el proceso de traducci√≥n ser√≠a el siguiente</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder (detokenizer)" src="https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(detokenizer).gif" width="775" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>De momento tenemos esta arquitectura</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder (detokenizer)" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-detokenizer-2.png" width="911" height="1568"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Projection">Projection<a class="anchor-link" href="#Projection"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 5" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos dicho que al decoder le entra un token que pasa por la capa de embedding <code>Output embedding</code>.</p>
      <p>El <code>Output decoder</code> crea un vector por cada token, por lo que a la salida del <code>Output decoder</code> tenemos una matriz de ($n_{opening_brace}output-tokens{closing_brace}$ x $d_{opening_brace}model{closing_brace}$).</p>
      <p>El decoder hace operaciones, pero saca una matriz con la misma dimensi√≥n. As√≠ de necesita convertir esa matriz en un token y eso lo hace mediante una capa lineal que a la salida genera un array con la misma dimensi√≥n que los posibles tokens que hay en el lenguaje al que se quiere traducir (vocabulario de salida).</p>
      <p>Ese array corresponde a los logits de cada posible token, por lo que a continuaci√≥n se pasa por una capa softmax que convierte esos logits en probabilidades. Es decir, tendremos la probabilidad de que cada token sea el siguiente token.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - projection" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-projection.png" width="874" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Encoder-y-decoder-x6">Encoder y decoder x6<a class="anchor-link" href="#Encoder-y-decoder-x6"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 6" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el paper original usan 6 capas para el encoder y otras 6 capas para el decoder. No hay ninguna raz√≥n para que sean 6, supongo que probaron varios valores y este fue el que mejor les funcion√≥.</p>
      <p>A cada uno de los decoder le entra la salida del √∫ltimo encoder</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder (x6)" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-x6.png" width="504" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para simplificar el diagrama lo representaremos as√≠ a partir de ahora</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder (Nx)" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-Nx.png" width="866" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Attention---Feed-forward">Attention - Feed forward<a class="anchor-link" href="#Attention---Feed-forward"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 7" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ir empezando a ver qu√© ha dentro del encoder y el decoder. B√°sicamente lo que hay es un mecanismo de atenci√≥n y una capa feed forward.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - encoder-decoder - attention-ff" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-attention-ff.png" width="689" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Atenci%C3%B3n">Atenci√≥n<a class="anchor-link" href="#Atenci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 8" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Podemos ver que en los mecanismos de atenci√≥n entran 3 flechas. Esto ya lo veremos m√°s adelante cuando veamos en profundidad c√≥mo funcionan los mecanismos de atenci√≥n.</p>
      <p>Pero de momento podemos decir que son operaciones que se realizan para poder obtener la relaci√≥n que existe entre los tokens (y por tanto, la relaci√≥n que existe entre las palabras).</p>
      <p>Antes de los transformers, para el problema de traducci√≥n se usaban las redes neuronales recurrentes, que consist√≠an en redes a las que les entraba un token, lo procesaban y generaban otro token de salida. A continuaci√≥n le entraba un segundo token, lo procesaban y sacaban otro token, y as√≠ sucesivamente con todos los tokens de la secuencia de entrada. El problema de estas redes es que cuando las frases eran muy largas, cuando se estaba en los √∫ltimos tokens, la red se "olvidaba" de los primeros tokens. Por ejemplo en frases muy largas, podr√≠a pasar que se cambiase el g√©nero del sujeto a lo largo de la frase traducida. Y esto es porque despu√©s de muchos tokens, la red se hab√≠a olvidado si el sujeto era masculino o femenino.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Para solucionar esto, en el mecanismo de atenci√≥n de los transformers entra la secuencia entera y de una sola vez se calculan las relaciones (atenci√≥n) entre todos los tokens.</p>
      <p>Esto es muy potente, ya que en un solo c√°lculo se obtiene la relaci√≥n entre todos los tokens, sea lo larga que sea la secuencia.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aunque esto es una gran ventaja y es lo que ha hecho que los transformers ahora se utilicen en la mayor√≠a de las mejores redes modernas, tambi√©n es su mayor desventaja, ya que el c√°lculo de la atenci√≥n es muy costoso computacionalmente. Ya que requiere unas multiplicaciones matriciales muy grandes.</p>
      <p>Esas multiplicaciones se realizan entre matrices que corresponden a los embeddings de cada uno de los tokens por ellas mismas. Es decir, la matriz que representa los embeddings de los tokens se multiplica por ella misma. Para poder realizar esta multiplicaci√≥n hay que rotar una de las matrices (requisitos del algebra para poder multiplicar matrices). As√≠ que se multiplica una matriz por ella misma, si la secuencia de entrada tiene m√°s tokens, las matrices que se multiplican son m√°s grandes, una en alto y otra en ancho, por lo que la memoria necesaria para almacenar esas matrices crece de forma cuadr√°tica.</p>
      <p>Por lo que a medida que aumenta la longitud de las secuencias, la cantidad de memoria necesaria para almacenar esas matrices crece de forma cuadr√°tica. Y esto es un gran limitante a d√≠a de hoy, la cantidad de memoria que tienen las GPUs, que es d√≥nde se suelen realizar esas multiplicaciones.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el encoder se usa una sola capa de atenci√≥n, para sacar las relaciones entre los tokens de entrada</p>
      <p>En el decoder se utilizan dos capas de atenci√≥n, una para sacar las relaciones entre los tokens de salida, y otra para sacar las relaciones entre los tokens del encoder y los del decoder.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Feed-forward">Feed forward<a class="anchor-link" href="#Feed-forward"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 9" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Despu√©s de la capa de atenci√≥n, la secuencia entra en una capa <code>Feed forward</code> que tiene dos prop√≥sitos</p>
      <ul>
      <li><p>Uno es a√±adir no linealidades. Como hemos explicado, la atenci√≥n se consigue mediante multiplicaciones matriciales de los tokens de las secuencias de entrada. Pero si a una red no se le aplican capas no lineales, al final, toda la arquitectura se podr√≠a resumir en unos pocos c√°lculos lineales. Por lo que las redes neuronales no podr√≠an resolveer problemas no lineales. De modo que se a√±ade esta capa para a√±adir no linealidad</p>
      </li>
      <li><p>Otro es extracci√≥n de caracter√≠sticas. Aunque la atenci√≥n ya extrae caracter√≠sticas, estas son caracter√≠sticas de las relaciones entre los tokens. Pero esta capa <code>Feed forward</code> se encarga de extraer caracter√≠sticas de los tokens en s√≠. Es decir, de cada token se extraen caracter√≠sticas que se consideran importantes para el problema que se est√° resolviendo.</p>
      </li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Positonal-encoding">Positonal encoding<a class="anchor-link" href="#Positonal-encoding"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 10" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos explicado que en la capa de atenci√≥n se obtienen las relaciones entre los tokens, que esa relaci√≥n se calcula mediante multiplicaciones matriciales y que esas multiplicaciones se realizan entre la matriz de embeddings por ella misma. Por lo que en las frases <code>El gato come pescado</code> y <code>El pescado come gato</code>, la relaci√≥n entre <code>el</code> y <code>gato</code> es la misma en ambas frases, ya que la relaci√≥n se calcula mediante multiplicaciones matriciales de los embeddings de <code>el</code> y <code>gato</code>.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Sin embargo, en la primera <code>el</code> se refiere a el <code>gato</code>, mientras que en la segunda <code>el</code> se refiere a el <code>pescado</code>. Por lo que adem√°s de las relaciones entre las palabras necesitamos tener alg√∫n mecanismo que nos indique su posici√≥n en la frase.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En el paper proponen meter un mecanismo de atenci√≥n que se encarga de sumar unos valores a los vectores de embeddings</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - positional encoding" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding.png" width="887" height="1200"/></p>
      <p>Donde la f√≥rmula para calcular esos valores es</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - positional encoding (formula)" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding-formula.png" width="522" height="197"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como esto as√≠ en fr√≠o es un poco dif√≠cil de entender, vamos a ver c√≥mo ser√≠a una distribuci√≥n de valores del <code>positional encoding</code></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - positional encoding (diagram)" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding-diagram.png" width="872" height="440"/></p>
      <p>Al primer token se le van a sumar los valores de la primera fila (la de m√°s abajo), al segundo token los de la segunda fila, y as√≠ sucesivamente, lo que provocan un cambio en los embeddings como se ve en la figura. Visto en dos dimensiones se aprecian las ondas que se van sumando.</p>
      <p>Estas ondas hacen que cuando se realizan los c√°lculos de atenci√≥n, las palabras m√°s cercanas tengan m√°s relaci√≥n que las palabras m√°s lejanas.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero podemos pensar una cosa, si el proceso de embedding consiste en crear un espacio vectorial en el que las palabras con el mismo significado sem√°ntico est√©n cerca, ¬øno se estar√≠a rompiendo esa relaci√≥n si se suman valores a los embeddings?</p>
      <p>Si nos volvemos a fijar en el ejemplo de espacio vectorial de antes</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="word_embedding_3_dimmension" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/word_embedding_3_dimmension.webp" width="995" height="825"/></p>
      <p>Podemos ver que los valores van m√°s o menos de -1000 a 1000 en cada eje, mientras que la gr√°fica de distribuciones</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - positional encoding (diagram)" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding-diagram.png" width="872" height="440"/></p>
      <p>los valores van de -1 a 1, ya que es el rango de las funciones seno y coseno.</p>
      <p>Por lo que estamos variando en un rango de entre -1 y 1 los valores de los embeddings que tienen dos o tres √≥rdenes de magnitud m√°s, por lo que la variaci√≥n va a ser muy peque√±a en comparaci√≥n con el valor de los embeddings.</p>
      <p>De modo que ya tenemos una manera de saber la relaci√≥n de la posici√≥n de los tokens en la frase</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Add-&amp;-Norm">Add &amp; Norm<a class="anchor-link" href="#Add-&amp;-Norm"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 11" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Solo nos queda un bloque de alto nivel y son las capas <code>Add &amp; Norm</code></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - Add &amp; norm" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-Add-norm.png" width="780" height="1200"/></p>
      <p>Estas son capas que se a√±aden despu√©s de cada capa de atenci√≥n y de cada capa feed forward. Esta capa suma la salida y la entrada de una capa. A esto se llama conexiones residuales y tiene las siguientes ventajas</p>
      <ul>
      <li><p>Durante el entrenamiento:</p>
      <ul>
      <li><p>Reducen el problema del desvanecimiento del gradiente: Cuando una red neuronal es muy grande, en el proceso de entrenamiento, los gradientes se van haciendo cada vez m√°s peque√±os seg√∫n se profundizan en las capas. Esto hace que las capas m√°s profundas no puedan actualizar bien sus pesos. Las conexiones residuales permiten el paso de los gradientes directamente a trav√©s de las capas, lo que ayuda a mantenerlos lo suficientemente grandes para que el modelo pueda seguir aprendiendo, incluso en las capas m√°s profundas.</p>
      </li>
      <li><p>Permiten el entrenamiento de redes m√°s profundas: Al ayudar a mitigar el problema del desvanecimiento del gradiente, las conexiones residuales tambi√©n facilitan el entrenamiento de redes m√°s profundas, lo cual puede llevar a mejor rendimiento.</p>
      </li>
      </ul>
      </li>
      <li><p>Durante la inferencia:</p>
      <ul>
      <li><p>Permiten la transmisi√≥n de informaci√≥n entre diferentes capas: Como las conexiones residuales permiten que la salida de cada capa se convierta en la suma de la entrada y la salida de la capa, la informaci√≥n de la informaci√≥n de las capas m√°s profundas se transmiten a las capas de m√°s alto nivel. Esto puede ser beneficioso en muchas tareas, especialmente en las que la informaci√≥n de bajo y alto nivel puede ser √∫til.</p>
      </li>
      <li><p>Mejoran la robustez del modelo: Dado que las conexiones residuales permiten que las capas aprendan mejor en las capas m√°s profundas, los modelos con conexiones residuales pueden ser m√°s robustos a perturbaciones en los datos de entrada.</p>
      </li>
      <li><p>Permiten la recuperaci√≥n de informaci√≥n perdida: Si alguna informaci√≥n se pierde durante la transformaci√≥n en alguna capa, las conexiones residuales pueden permitir que esta informaci√≥n sea recuperada en las capas posteriores.</p>
      </li>
      </ul>
      </li>
      </ul>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Esta capa se llama <code>Add &amp; Norm</code>, hemos visto el <code>Add</code>, veamos el <code>Norm</code>. La normalizaci√≥n se a√±ade para que al sumar la entrada y la salida no se disparen los valores.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ya hemos visto todas las capas de alto nivel del transformer</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" width="852" height="1200"/></p>
      <p>por lo que podemos entrar a ver la parte m√°s importante y que le da nombre al paper, los mecanismos de atenci√≥n</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Mecanismos-de-atenci%C3%B3n">Mecanismos de atenci√≥n<a class="anchor-link" href="#Mecanismos-de-atenci%C3%B3n"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 12" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Multi-head-attention">Multi-head attention<a class="anchor-link" href="#Multi-head-attention"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 13" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Antes de ver el verdadero mecanismo de atenci√≥n tenemos que ver el multi-head attention</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - multi-head attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-multi-head-attention.png" width="576" height="746"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Cuando hemos explicado las capas de alto nivel, hemos visto que en las capas de atenci√≥n entraban 3 flechas, estas son <code>Q</code>, <code>K</code> y <code>V</code>. Son matrices que corresponden a la informaci√≥n de los tokens, en el caso del mecanismo de atenci√≥n del encoder, corresponden a los tokens de la frase del idioma original, y en el caso de la capa de atenci√≥n del decoder, corresponden a los tokens de la frase que se ha traducido hasta el momento y de la salida del encoder.</p>
      <p>En realidad ahora nos da igual el origen de los tokens, solo qu√©date con la idea que corresponden a tokens. Como hemos explicado los tokens se convierten a embeddings, por lo que <code>Q</code>, <code>K</code> y <code>V</code> son matrices de tama√±o ($n_{opening_brace}tokens{closing_brace}$ x $d_{opening_brace}model{closing_brace}$). Normalmente la dimensi√≥n del embedding ($d_{opening_brace}model{closing_brace}$) suele ser un n√∫mero grande, como 512, 1024, 2048, etc (no tiene por que ser una potencia de 2, son solo ejemplos).</p>
      <p>Hemos explicado que los embeddings son representaciones vectoriales de los tokens. Es decir, los tokens se convierten a espacios vectoriales en los que las palabras con significado sem√°ntico similar est√°n cerca.</p>
      <p>Por tanto de todas esa dimensiones, unas pueden estar relacionadas con caracter√≠sticas morfol√≥gicas, otras con caracter√≠sticas sint√°cticas, otras con caracter√≠sticas sem√°nticas, etc. Por lo que tiene sentido que se calculen los mecanismos de atenci√≥n entre dimensiones de los embeddings de caracter√≠sticas similares.</p>
      <p>Recordemos que los mecanismos de atenci√≥n buscan similitud entre palabras, por lo que tiene sentido que se busque similitud entre caracter√≠sticas similares.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Es por esto, que antes de calcular los mecanismos de atenci√≥n se separan las dimensiones de los embeddings en grupos de caracter√≠sticas similares, y se calculan los mecanismos de atenci√≥n entre esos grupos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>¬øY c√≥mo se hace esta separaci√≥n? Habria que buscar las dimensiones similares, pero hacer esto en un espacio de 512, 1024, 2048, etc dimensiones es muy complicado. Adem√°s que no se puede saber que caracter√≠sticas son similares y que en cada caso cambiaran las caracter√≠sticas que se consideran similares.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por lo que se utilizan proyecciones lineales para separar las dimensiones en grupos. Es decir, se pasan los embeddings por capas lineales que los separan en grupos de caracter√≠sticas similares. De esta manera, durante el entrenamiento del transformer ir√°n cambiando los pesos de las capas lineales hasta llegar a un punto en el que la agrupaci√≥n se haga de una manera √≥ptima.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora podemos tener la duda de en cu√°ntos grupos dividir. En el paper original se dividen en 8 grupos, pero no hay ninguna raz√≥n para que sean 8, supongo que probaron varios valores y este fue el que mejor les funcion√≥.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Una vez se han dividido los embeddings en grupos similares y se ha calculado la atenci√≥n en los distintos grupos se concatenan los resultados. Esto es l√≥gico, supongamos que tenemos un ebedding de 512 dimensiones, y lo dividimos en 8 grupos de 64 dimensiones, si calculamos la atenci√≥n en cada uno de los grupos, tendremos 8 matrices de atenci√≥n 64 dimensiones, si las concatenamos tendremos una matriz de atenci√≥n 512 dimensiones, que es la misma dimensi√≥n que ten√≠amos al principio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Pero la concatenaci√≥n hace que todas las caracter√≠sticas est√©n juntas. Las primeras 64 dimensiones corresponden a una caracter√≠stica, las siguientes 64 a otra, y as√≠ sucesivamente. As√≠ que para volver a mezclarlas se vuelve a pasar una capa lineal que mezcla todas las caracter√≠sticas. Y esa mezcla se va aprendiendo durante el entrenamiento.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h3 id="Scale-dot-product-attention">Scale dot product attention<a class="anchor-link" href="#Scale-dot-product-attention"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 14" src={svg_paths.link_svg_path}/></a></h3>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Llegamos a la parte m√°s importante del transformer, el mecanismo de atenci√≥n, el <code>scaled dot product attention</code></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" width="684" height="732"/>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention formula" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" width="1200" height="165"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como hemos visto, en la arquitectur del Transformer hay tres mecanismos de atenci√≥n</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" width="852" height="1200"/></p>
      <p>El del encoder, el del decoder y el del encoder-decoder. As√≠ que vamos a explicarlos por separado, porque aunque son casi iguales, tienen unas peque√±as diferencias</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Endocer-scale-dot-product-attention">Endocer scale dot product attention<a class="anchor-link" href="#Endocer-scale-dot-product-attention"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 15" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver otra vez el diagrama de bloques y la f√≥rmula</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" width="684" height="732"/>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention formula" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" width="1200" height="165"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Primero vamos a entender por qu√© entraban tres flechas a las capas de atenci√≥n. Si vemos la arquitectura del transformer, la entrada del encoder se divide en tres y entra a la capa de atenci√≥n</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" width="852" height="1200"/></p>
      <p>Por lo que <code>K</code>, <code>Q</code> y <code>V</code> son el resultado del embedding y el positional encoding. Se mete a la capa de atenci√≥n la misma matriz tres veces. Tenemos que recordar que esa matriz consist√≠a en una lista con todos los tokens ($n_{opening_brace}tokens{closing_brace}$), y cada token se convert√≠a en un vector de embeddings de dimensi√≥n $d_{opening_brace}model{closing_brace}$, por lo que la dimensi√≥n de la matriz ser√° ($n_{opening_brace}tokens{closing_brace}$ x $d_{opening_brace}model{closing_brace}$).</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El significao de <code>K</code>, <code>Q</code> y <code>V</code> proviene de las bases de datos <code>key</code>, <code>query</code> y <code>value</code>. Al mecanismo de atenci√≥n se le pasan las matrices <code>Q</code> y <code>K</code>, es decir, se le pasa la pregunta y la clave, y a la salida se obtiene la matriz <code>V</code>, es decir, la respuesta.</p>
      <p>Vamos a ver cada bloque por separado y entenderemos mejor esto</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Matmul">Matmul<a class="anchor-link" href="#Matmul"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 16" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Este bloque corresponde a la multiplicaci√≥n matricial de las matrices <code>Q</code> y <code>K</code>. Pero para poder realizar esta operaci√≥n hay que hacerla con la matriz transpuesta de <code>K</code>. Ya que como las dos matrices tienen dimensi√≥n ($n_{opening_brace}tokens{closing_brace}$ x $d_{opening_brace}model{closing_brace}$), para poder multiplicarlas, la matriz <code>K</code> tiene que estar traspuesta.</p>
      <p>Por lo que tendremos una multiplicaci√≥n de una matriz de dimensi√≥n ($n_{opening_brace}tokens{closing_brace}$ x $d_{opening_brace}model{closing_brace}$) por otra matriz de dimensi√≥n ($d_{opening_brace}model{closing_brace}$ x $n_{opening_brace}tokens{closing_brace}$), por lo que el resultado ser√° una matriz de dimensi√≥n ($n_{opening_brace}tokens{closing_brace}$ x $n_{opening_brace}tokens{closing_brace}$).</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - matmul" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul.png" width="1200" height="231"/></p>
      <p>Como podemos ver, el resultado es una matriz donde la diagonal es la multiplicaci√≥n del embedding de cada token por si mismo, y el resto de posiciones son las multiplicaciones entre los embeddings de cada token.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora vamos a ver por qu√© se hace esta multiplicaci√≥n. En en alterior post <a href="https://maximofn.com/embeddings-similarity/">Medida de similitud entre embeddings</a> vimos que una manera de obtener la similitud entre dos vectores de embeddings es mediante el c√°lculo del coseno</p>
      <p>En la figura anterior se puede ver que la multiplicaci√≥n entre las matrices <code>Q</code> y <code>K</code> corresponde a la multiplicaci√≥n de los embeddings de cada token. La multiplicaci√≥n entre dos vectores se realiza de la siguiente manera</p>
      $$\mathbf{opening_brace}U{closing_brace} \cdot \mathbf{opening_brace}V{closing_brace} = |\mathbf{opening_brace}U{closing_brace}| \cdot |\mathbf{opening_brace}V{closing_brace}| \cos(\theta)$$<p>Es decir, tenemos la multiplicaci√≥n de las normas por su coseno. Si los vectores fuesen unitarios, es decir, que sus normas sean 1, la multiplicaci√≥n de dos vectores ser√≠a igual al coseno entre ambos vectores, que es una de las medidas de similitud entre vectores.</p>
      <p>Por lo que como en cada posici√≥n de la matriz resultante tenemos la multiplicaci√≥n entre los vectores de embeddings de cada token, en realidad, cada posici√≥n de la matriz representar√° la similitud entre cada token.</p>
      <p>Recordemos lo que eran los embeddings, los embeddings eran representaciones vectoriales de los tokens en un espacio vectorial, donde los tokens con similitud sem√°ntica estan cerca.</p>
      <p>De modo que con esta multiplicaci√≥n hemos obtenido una matriz de similitud entre los tokens de la frase</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - matmul - similarity matrix" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul-similarity-matrix.png" width="1579" height="454"/></p>
      <p>Los elementos de la diagonal tienen m√°xima similitud (verde), los de las esqunas tienen m√≠nima similitud (rojo), y el resto de elementos tienen similitud intermedia.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Scale">Scale<a class="anchor-link" href="#Scale"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 17" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver el diagrama del scaled dot product attention y su f√≥rmula</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" width="684" height="732"/>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention formula" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" width="1200" height="165"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hab√≠amos dicho que si al multiplicar <code>Q</code> por <code>K</code> reliz√°bamos la multiplicaci√≥n entre los vectores de embeddings, y que si esos vectores tuviesen norma 1, el resultado ser√≠a la similitud entre los vectores. Pero como los vectores no tienen norma 1, el resultado puede tener valores muy altos, por lo que se normaliza dividiendo por la ra√≠z cuadrada de la dimensi√≥n de los vectores de embeddings.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Mask-(opt)">Mask (opt)<a class="anchor-link" href="#Mask-(opt)"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 18" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El enmascaramiento es opcional y en el encoder no se usa, por lo que de momento no lo explicamos para no liar</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" width="684" height="732"/>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention formula" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" width="1200" height="165"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Softmax">Softmax<a class="anchor-link" href="#Softmax"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 19" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Aunque hemos dividido por la ra√≠z cuadrada de la dimensi√≥n de los vectores de embeddings, nos vendr√≠a muy bien que la similitud entre los vectores de embeddings vaya entre los valores 0 y 1, as√≠ que para asegurarnos eso, pasamos por una capa softmax.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" width="684" height="732"/>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention formula" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" width="1200" height="165"/></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - matmul - similarity matrix softmax" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul-similarity-matrix-softmax.png" width="1579" height="454"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Matmul">Matmul<a class="anchor-link" href="#Matmul"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 20" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora que tenemos una matriz de similitud entre los vectores de embeddings, vamos a multiplicarla por la matriz <code>V</code>, que representa los embeddings de los tokens.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - matmul2" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul2.png" width="1200" height="293"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Haciendo la multiplicaci√≥n obtenemos</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - matmul2 result" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul2-result.png" width="1200" height="144"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Obtenemos una matriz con una mezcla de los embeddings con su similitud. En cada fila obtenemos una mezcla de los embeddings, donde cada elemento del embedding est√° ponderado en funci√≥n de la similitud del token de esa fila con el resto de tokens.</p>
      <p>Adem√°s volvemos a tener una matriz de tama√±o ($n_{opening_brace}tokens{closing_brace}$ x $d_{opening_brace}model{closing_brace}$), que es la misma dimensi√≥n que ten√≠amos al principio.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Resumen">Resumen<a class="anchor-link" href="#Resumen"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 21" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>En resumen, podemos decir que el <code>scaled dot product attention</code> es un mecanismo que calcula la similitud entre los tokens de una frase, y a partir de esa similitud, calcula una matriz de salida que corresponde a una mezcla de embeddings ponderada en funci√≥n de la similitud de los tokens.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Decoder-masked-scale-dot-product-attention">Decoder masked scale dot product attention<a class="anchor-link" href="#Decoder-masked-scale-dot-product-attention"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 22" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver la arquitectura del transformer</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" width="852" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como vemos en este caso el <code>scaled dot product attention</code> tiene la palabra <code>masked</code>. Primero vamos a explicar el por qu√© de la necesidad de este enmascaramiento, y despu√©s veremos c√≥mo se hace.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Por-qu%C3%A9-enmascarar">Por qu√© enmascarar<a class="anchor-link" href="#Por-qu%C3%A9-enmascarar"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 23" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Como hemos dicho el transformer se ide√≥ inicialmente como un traductor, pero en generar, es una arquitectura a la que le metes una secuencia y te saca otra secuencia. Pero a la hora del entrenamiento hay que darle la secuencia de entrada y la secuencia de salida, y a partir de ah√≠ el transformer aprende a traducir.</p>
      <p>Por otro lado hemos dicho que el transformer va generando un token nuevo cada vez. Es decir, se le pasa la secuencia de entrada en el encoder y un token especial de inicio de secuencia en el decoder, y a partir de ah√≠ genera el primer token de la secuencia de salida.</p>
      <p>A continuaci√≥n se le vuelve a meter la secuencia de entrada en el encoder y el token que previamente hab√≠a generado en el decoder, y a partir de ah√≠ genera el segundo token de la secuencia de salida.</p>
      <p>A continuaci√≥n se le vuelve a meter la secuecia de entrada en el encoder y los dos tokens que previamente hab√≠a generado en el decoder, y a partir de ah√≠ genera el tercer token de la secuencia de salida.</p>
      <p>Y as√≠ sucesivamente hasta que genera un token especial de fin de secuencia.</p>
      <p>Pero en el entrenamiento, como se le mete la secuencia de entrada y de salida de golpe, necesitamos enmascarar los tokens que aun no ha generado para que no pueda verlos.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h5 id="Mask">Mask<a class="anchor-link" href="#Mask"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 24" src={svg_paths.link_svg_path}/></a></h5>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vamos a ver otra vez el diagrama de bloques y la f√≥rmula</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" width="684" height="732"/>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention formula" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" width="1200" height="165"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>El enmascaramiento se realiza despu√©s del <code>Scale</code> y antes de la <code>Softmax</code>. Como necesitamos enmascarar los tokens "futuros" lo que se puede hacer es multiplicar la matrid resultante del <code>Scale</code> por una matriz que tenga 0 en las posiciones que queremos enmascarar y 1 en las que no.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - Mask" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-Mask.png" width="1200" height="350"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Haciendo esto obtenemos la misma matriz que antes pero con posiciones enmascaradas</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - Mask resutl" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-Mask-resutl.png" width="1316" height="454"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Ahora el resultado del <code>Scaled dot product attention</code> es una matriz con los embeddings de los tokens ponderados en funci√≥n de la similitud de los tokens, pero con los tokens que no se deber√≠an ver enmascarados.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h4 id="Encoder-decoder-scale-dot-product-attention">Encoder-decoder scale dot product attention<a class="anchor-link" href="#Encoder-decoder-scale-dot-product-attention"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 25" src={svg_paths.link_svg_path}/></a></h4>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Volvemos a ver la arquitectura del transformer</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" width="852" height="1200"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Vemos ahora que al mecanismo de atenci√≥n entra dos veces la salida del encoder y una vez la masked attention del decoder. Por lo que <code>K</code> y <code>V</code> son la salida del encoder, y <code>Q</code> es la salida del decoder.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" width="684" height="732"/>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention formula" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" width="1200" height="165"/></p>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Por tanto en este bloque de atenci√≥n, primero se calcula la similitud entre la sentencia del decoder y la sentencia del encoder, es decir, se calcula la similitud entre la frase que se ha traducido hasta el momento y la frase original.</p>
      <p>A continuaci√≥n se multiplica esta similitud por la sentencia del encoder, es decir, se obtiene una mezcla de los embeddings de la frase original ponderada en funci√≥n de la similitud de la frase traducida hasta el momento.</p>
      </section>
      
      <section class="section-block-markdown-cell">
      <h2 id="Resumen">Resumen<a class="anchor-link" href="#Resumen"><img decoding="async" class="link-img" width="24px" height="24px" alt="link image 26" src={svg_paths.link_svg_path}/></a></h2>
      </section>
      
      <section class="section-block-markdown-cell">
      <p>Hemos recorrido el transformer desde el m√°s alto nivel hasta el m√°s bajo nivel, por lo que ya puedes tener una comprensi√≥n de c√≥mo funciona.</p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="transformer" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp" width="852" height="1200"/></p>
      <p><img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - multi-head attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-multi-head-attention.png" width="576" height="746"/>
      <img decoding="async" onerror="this.parentNode.removeChild(this)" alt="Transformer - scaled dot product attention" src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" width="684" height="732"/></p>
      </section>
      






    </div>

  </section>

</PostLayout>
