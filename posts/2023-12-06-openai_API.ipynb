{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OpenAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instalar la librería de OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En primer lugar, para poder usar la API de OpenAI, es necesario instalar la librería de OpenAI. Para ello, ejecutamos el siguiente comando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importar la librería de OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez está instalada la librería, la importamos para poder usarla en nuestro código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtener una API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder usar la API de OpenAI, es necesario obtener una API Key. Para ello, nos dirigimos a la página de [OpenAI](https://openai.com/), y nos registramos. Una vez registrados, nos dirigimos a la sección de [API Keys](https://platform.openai.com/api-keys), y creamos una nueva API Key.\n",
        "\n",
        "![open ai api key](https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez la tenemos le decimos a la API de openai cuál es nuestra API Key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "api_key = \"Pon aquí tu API key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creamos nuestro primer chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con la API de OpenAI es muy sencillo crear un simple chatbot, al que le vamos a pasar un prompt, y nos va a devolver una respuesta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En primer lugar tenemos que elegir qué modelo vamos a usar, en mi caso voy a usar el modelo `gpt-3.5-turbo-1106` que a día de hoy es un buen modelo para este post, ya que para lo que vamos a hacer no necesitamos usar el mejor modelo. OpenAI tiene una lista con todos sus [modelos](https://platform.openai.com/docs/models) y una página con los [precios](https://openai.com/pricing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = \"gpt-3.5-turbo-1106\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora tenemos que crear un cliente que será el que se comunique con la API de OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = openai.OpenAI(api_key=api_key, organization=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos le hemos pasado nuestra API Key. Además se le puede pasar la organización, pero en nuestro caso no es necesario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos el prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "promtp = \"Cuál es el mejor lenguaje de programación para aprender?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y ya podemos pedirle una respuesta a OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos cómo es la respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.chat.chat_completion.ChatCompletion,\n",
              " ChatCompletion(id='chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))], created=1701584994, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_eeff13170a', usage=CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)))"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response), response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.id = chatcmpl-8RaHCm9KalLxj2PPbLh6f8A4djG8Y\n",
            "response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))]\n",
            "response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None))\n",
            "\tresponse.choices[0].finish_reason = stop\n",
            "\tresponse.choices[0].index = 0\n",
            "\tresponse.choices[0].message = ChatCompletionMessage(content='No hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.', role='assistant', function_call=None, tool_calls=None)\n",
            "\t\tresponse.choices[0].message.content = \n",
            "\t\tNo hay un \"mejor\" lenguaje de programación para aprender, ya que depende de tus intereses, objetivos y el tipo de desarrollo que te interese. Algunos lenguajes populares para empezar a aprender a programar incluyen Python, JavaScript, Java, C# y Ruby. Estos lenguajes son conocidos por su sintaxis clara y su versatilidad, lo que los hace buenos candidatos para principiantes. También es útil investigar qué lenguajes son populares en la industria en la que te gustaría trabajar, ya que el conocimiento de un lenguaje en demanda puede abrirte más oportunidades laborales. En resumen, la elección del lenguaje de programación para aprender dependerá de tus preferencias personales y de tus metas profesionales.\n",
            "\t\tresponse.choices[0].message.role = assistant\n",
            "\t\tresponse.choices[0].message.function_call = None\n",
            "\t\tresponse.choices[0].message.tool_calls = None\n",
            "response.created = 1701584994\n",
            "response.model = gpt-3.5-turbo-1106\n",
            "response.object = chat.completion\n",
            "response.system_fingerprint = fp_eeff13170a\n",
            "response.usage = CompletionUsage(completion_tokens=181, prompt_tokens=21, total_tokens=202)\n",
            "\tresponse.usage.completion_tokens = 181\n",
            "\tresponse.usage.prompt_tokens = 21\n",
            "\tresponse.usage.total_tokens = 202\n"
          ]
        }
      ],
      "source": [
        "print(f\"response.id = {response.id}\")\n",
        "print(f\"response.choices = {response.choices}\")\n",
        "for i in range(len(response.choices)):\n",
        "    print(f\"response.choices[{i}] = {response.choices[i]}\")\n",
        "    print(f\"\\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")\n",
        "    print(f\"\\tresponse.choices[{i}].index = {response.choices[i].index}\")\n",
        "    print(f\"\\tresponse.choices[{i}].message = {response.choices[i].message}\")\n",
        "    content = response.choices[i].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.content = \\n\\t\\t{content}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.role = {response.choices[i].message.role}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}\")\n",
        "print(f\"response.created = {response.created}\")\n",
        "print(f\"response.model = {response.model}\")\n",
        "print(f\"response.object = {response.object}\")\n",
        "print(f\"response.system_fingerprint = {response.system_fingerprint}\")\n",
        "print(f\"response.usage = {response.usage}\")\n",
        "print(f\"\\tresponse.usage.completion_tokens = {response.usage.completion_tokens}\")\n",
        "print(f\"\\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}\")\n",
        "print(f\"\\tresponse.usage.total_tokens = {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver, nos devuelve muchísima información\n",
        "\n",
        "Por ejemplo `response.choices[0].finish_reason = stop` significa que el modelo ha parado de generar texto porque ha llegado al final del prompt. Esto nos viene muy bien para depurar, ya que los posibles valores son `stop` que significa que la API devolvió el mensaje completo, `length` que significa que la salida del modelo fue incompleta debido a que era más larga que `max_tokens` o límite de token del modelo, `function_call` el modelo decidió llamar a una función, `content_filter` que significa que el contenido fue omitido por una limitación de contenido de OpenAI y `null` que significa que la respuesta de la API fue incompleta\n",
        "\n",
        "También nos da información de los tokens para poder llevar un control del dinero gastado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A la hora de pedir una respuesta a OpenAI, podemos pasarle una serie de parámetros para que nos devuelva una respuesta más acorde a lo que queremos. Vamos a ver cuáles son los parámetros que podemos pasarle\n",
        "\n",
        " * `messages`: Lista de mensajes que se le han enviado al chatbot\n",
        " * `model`: Modelo que queremos usar\n",
        " * `frequency_penalty`: Penalización de frecuencia. Cuanto mayor sea el valor, menos probable será que el modelo repita la misma respuesta\n",
        " * `max_tokens`: Número máximo de tokens que puede devolver el modelo\n",
        " * `n`: Número de respuestas que queremos que nos devuelva el modelo\n",
        " * `presence_penalty`: Penalización de presencia. Cuanto mayor sea el valor, menos probable será que el modelo repita la misma respuesta\n",
        " * `seed`: Semilla para la generación de texto\n",
        " * `stop`: Lista de tokens que indican que el modelo debe parar de generar texto\n",
        "* `stream`: Si es `True` la API devolverá una respuesta cada vez que el modelo genere un token. Si es `False` la API devolverá una respuesta cuando el modelo haya generado todos los tokens\n",
        " * `temperature`: Cuanto mayor sea el valor, más creativo será el modelo\n",
        " * `top_p`: Cuanto mayor sea el valor, más creativo será el modelo\n",
        " * `user`: ID del usuario que está hablando con el chatbot\n",
        " * `timeout`: Tiempo máximo que queremos esperar a que la API nos devuelva una respuesta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos algunos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos pasarle a la API una lista de mensajes que se le han enviado al chatbot. Esto es útil para pasarle el historial de conversaciones al chatbot, y que así pueda generar una respuesta más acorde a la conversación. Y para condicionar la respuesta del chatbot a lo que se le ha dicho anteriormente.\n",
        "\n",
        "Además podemos pasarle un mensaje de sistema para indicarle cómo se tiene que comportar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Historial de conversaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver un ejemplo del histórico de conversaciones, primero le preguntamos que cómo está"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola MaximoFN, soy un modelo de inteligencia artificial diseñado para conversar y ayudar en lo que necesites. ¿En qué puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "promtp = \"Hola, soy MaximoFN, ¿Cómo estás?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nos ha respondido que no tiene sentimientos y que en qué puede ayudarnos. Así que si ahora le pregunto cómo me llamo no va a saber responderme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lo siento, no tengo esa información. Pero puedes decírmelo tú.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Me puedes decir cómo me llamo?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para solucionar esto, le pasamos el histórico de conversaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tu nombre es MaximoFN.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Me puedes decir cómo me llamo?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"Hola, soy MaximoFN, ¿Cómo estás?\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"Hola MaximoFN, soy un modelo de inteligencia artificial diseñado para conversar y ayudar en lo que necesites. ¿En qué puedo ayudarte hoy?\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
        "  ],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Condicionamiento mediante ejemplos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora veamos un ejemplo de cómo condicionar la respuesta del chatbot a lo que se le ha dicho anteriormente. Ahora le preguntamos cómo obtener la lista de archivos en un directorio en la terminal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "En la terminal de un sistema operativo Unix o Linux, puedes listar los archivos de un directorio utilizando el comando `ls`. Por ejemplo, si quieres listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si deseas listar los archivos de un directorio específico, puedes proporcionar la ruta del directorio después del comando `ls`, por ejemplo `ls /ruta/del/directorio`. Si deseas ver más detalles sobre los archivos, puedes usar la opción `-l` para obtener una lista detallada o `-a` para mostrar también los archivos ocultos.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si ahora le condicionamos con ejemplos de respuestas cortas, veamos qué nos contesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puede usar el comando `ls` en la terminal para listar los archivos de un directorio. Por ejemplo:\n",
            "```\n",
            "ls\n",
            "```\n",
            "Muestra los archivos y directorios en el directorio actual.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[\n",
        "      {\"role\": \"user\", \"content\": \"Obtener las 10 primeras líneas de un archivo\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"head -n 10\"},\n",
        "      {\"role\": \"user\", \"content\": \"Encontrar todos los archivos con extensión .txt\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"find . -name '*.txt\"},\n",
        "      {\"role\": \"user\", \"content\": \"Dividir un archivo en varias páginas\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"split -l 1000\"},\n",
        "      {\"role\": \"user\", \"content\": \"Buscar la dirección IP 12.34.56.78\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"nslookup 12.34.56.78\"},\n",
        "      {\"role\": \"user\", \"content\": \"Obtener las 5 últimas líneas de foo.txt\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"tail -n 5 foo.txt\"},\n",
        "      {\"role\": \"user\", \"content\": \"Convertir ejemplo.png en JPEG\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"convert example.png example.jpg\"},\n",
        "      {\"role\": \"user\", \"content\": \"Create a git branch named 'new-feature\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"git branch new-feature\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
        "  ],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos conseguido que dé una respuesta más corta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Condicionamiento con mensaje de sistema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le podemos pasar un mensaje de sistema para indicarle cómo se tiene que comportar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puedes listar los archivos de un directorio en la terminal usando el comando `ls`. Por ejemplo, para listar los archivos del directorio actual, simplemente escribe `ls` y presiona Enter. Si quieres listar los archivos de un directorio específico, puedes utilizar `ls` seguido de la ruta del directorio. Por ejemplo, `ls /ruta/del/directorio`.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
        "  ],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puedes listar los archivos de un directorio en la terminal utilizando el comando \"ls\". Por ejemplo, para listar los archivos en el directorio actual, puedes ejecutar el comando \"ls\". Si deseas listar los archivos de otro directorio, simplemente especifica el directorio después del comando \"ls\", por ejemplo \"ls /ruta/al/directorio\".\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cómo puedo listar los archivos de un directorio en la terminal?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Eres un experto asistente de terminal de ubuntu que responde solo con comandos de terminal\"},\n",
        "      {\"role\": \"user\", \"content\": \"Obtener las 10 primeras líneas de un archivo\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"head -n 10\"},\n",
        "      {\"role\": \"user\", \"content\": \"Encontrar todos los archivos con extensión .txt\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"find . -name '*.txt\"},\n",
        "      {\"role\": \"user\", \"content\": \"Dividir un archivo en varias páginas\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"split -l 1000\"},\n",
        "      {\"role\": \"user\", \"content\": \"Buscar la dirección IP 12.34.56.78\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"nslookup 12.34.56.78\"},\n",
        "      {\"role\": \"user\", \"content\": \"Obtener las 5 últimas líneas de foo.txt\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"tail -n 5 foo.txt\"},\n",
        "      {\"role\": \"user\", \"content\": \"Convertir ejemplo.png en JPEG\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"convert example.png example.jpg\"},\n",
        "      {\"role\": \"user\", \"content\": \"Create a git branch named 'new-feature\"},\n",
        "      {\"role\": \"assistant\", \"content\": \"git branch new-feature\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{promtp}\"},\n",
        "  ],\n",
        ")\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Número máximo de tokens de la respuesta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos limitar el número de tokens que puede devolver el modelo. Esto es útil para que el modelo no se pase de la respuesta que queremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La respuesta a esta pregunta puede variar dependiendo de los intereses y objetivos individuales, ya que cada lenguaje de programación tiene sus propias ventajas y desventajas. Sin embargo, algunos de los lenguajes más\n",
            "\n",
            "response.choices[0].finish_reason = length\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  max_tokens = 50,\n",
        ")\n",
        "\n",
        "content = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content)\n",
        "print(f\"\\nresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos la respuesta se corta a medias porque superaría el límite de tokens. Además ahora el motivo de parada es `length` en vez de `stop`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creatividad del modelo mediante la temperatura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos hacer que el modelo sea más creativo mediante la temperatura. Cuanto mayor sea el valor, más creativo será el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
        "temperature = 0\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  temperature = temperature,\n",
        ")\n",
        "\n",
        "content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente fáciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnología. Es importante considerar qué tipo de proyectos o campos de interés te gustaría explorar al momento de elegir un lenguaje de programación para aprender.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
        "temperature = 1\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  temperature = temperature,\n",
        ")\n",
        "\n",
        "content_1 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n",
            "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los objetivos y preferencias individuales del programador. Sin embargo, algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C++. Estos lenguajes son relativamente fáciles de aprender y tienen una amplia gama de aplicaciones en la industria de la tecnología. Es importante considerar qué tipo de proyectos o campos de interés te gustaría explorar al momento de elegir un lenguaje de programación para aprender.\n"
          ]
        }
      ],
      "source": [
        "print(content_0)\n",
        "print(content_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creatividad del modelo mediante el top_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos hacer que el modelo sea más creativo mediante el parámetro `top_p`. Cuanto mayor sea el valor, más creativo será el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
        "top_p = 0\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  top_p = top_p,\n",
        ")\n",
        "\n",
        "content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El mejor lenguaje de programación para aprender depende de los objetivos del aprendizaje y del tipo de programación que se quiera realizar. Algunos lenguajes de programación populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qué tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programación para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programación web.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
        "top_p = 1\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  top_p = top_p,\n",
        ")\n",
        "\n",
        "content_1 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No hay un \"mejor\" lenguaje de programación para aprender, ya que la elección depende de los intereses y objetivos individuales. Algunos lenguajes populares para principiantes incluyen Python, JavaScript, Java y C#. Cada uno tiene sus propias ventajas y desventajas, por lo que es importante investigar y considerar qué tipo de desarrollo de software te interesa antes de elegir un lenguaje para aprender.\n",
            "El mejor lenguaje de programación para aprender depende de los objetivos del aprendizaje y del tipo de programación que se quiera realizar. Algunos lenguajes de programación populares para principiantes incluyen Python, Java, JavaScript y Ruby. Sin embargo, cada lenguaje tiene sus propias ventajas y desventajas, por lo que es importante considerar qué tipo de proyectos o aplicaciones se quieren desarrollar antes de elegir un lenguaje de programación para aprender. Python es a menudo recomendado por su facilidad de uso y versatilidad, mientras que JavaScript es ideal para la programación web.\n"
          ]
        }
      ],
      "source": [
        "print(content_0)\n",
        "print(content_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Número de respuestas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos pedirle a la API que nos devuelva más de una respuesta. Esto es útil para que el modelo nos devuelva varias respuestas y así poder elegir la que más nos guste, para esto vamos a poner los parámetros `temperature` y `top_p` a 1 para que el modelo sea más creativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El mejor lenguaje de programación para aprender depende de tus objetivos y del tipo de aplicaciones que te interese desarrollar. Algunos de los lenguajes más populares para aprender son:\n",
            "\t\t1. Python: Es un lenguaje de programación versátil, fácil de aprender y con una amplia comunidad de desarrolladores. Es ideal para principiantes y se utiliza en una gran variedad de aplicaciones, desde desarrollo web hasta inteligencia artificial.\n",
            "\t\t2. JavaScript: Es el lenguaje de programación más utilizado en el desarrollo web. Es imprescindible para aquellos que quieren trabajar en el ámbito del desarrollo frontend y backend.\n",
            "\t\t3. Java: Es un lenguaje de programación muy popular en el ámbito empresarial, por lo que aprender Java puede abrirte muchas puertas laborales. Además, es un lenguaje estructurado que te enseñará conceptos importantes de la programación orientada a objetos.\n",
            "\t\t4. C#: Es un lenguaje de programación desarrollado por Microsoft que se utiliza especialmente en el desarrollo de aplicaciones para Windows. Es ideal para aquellos que quieran enfocarse en el desarrollo de aplicaciones de escritorio.\n",
            "\t\tEn resumen, el mejor lenguaje de programación para aprender depende de tus intereses y objetivos personales. Es importante investigar y considerar qué tipos de aplicaciones te gustaría desarrollar para elegir el lenguaje que más se adapte a tus necesidades.\n",
            "El mejor lenguaje de programación para aprender depende de los objetivos y necesidades individuales. Algunos de los lenguajes de programación más populares y ampliamente utilizados incluyen Python, JavaScript, Java, C++, Ruby y muchos otros. Python es a menudo recomendado para principiantes debido a su sintaxis simple y legible, mientras que JavaScript es esencial para el desarrollo web. Java es ampliamente utilizado en el desarrollo de aplicaciones empresariales y Android, y C++ es comúnmente utilizado en sistemas embebidos y juegos. En última instancia, el mejor lenguaje de programación para aprender dependerá de lo que quiera lograr con su habilidades de programación.\n",
            "El mejor lenguaje de programación para aprender depende de los intereses y objetivos individuales de cada persona. Algunos de los lenguajes más populares y bien documentados para principiantes incluyen Python, JavaScript, Java y C#. Python es conocido por su simplicidad y versatilidad, mientras que JavaScript es esencial para el desarrollo web. Java y C# son lenguajes ampliamente utilizados en la industria y proporcionan una base sólida para aprender otros lenguajes. En última instancia, la elección del lenguaje dependerá de las metas personales y la aplicación deseada.\n",
            "El mejor lenguaje de programación para aprender depende de los intereses y objetivos de cada persona. Algunos lenguajes populares para principiantes incluyen Python, Java, JavaScript, C++ y Ruby. Python es frecuentemente recomendado para aprender a programar debido a su sintaxis sencilla y legible, mientras que Java es utilizado en aplicaciones empresariales y Android. JavaScript es fundamental para el desarrollo web, y C++ es comúnmente utilizado en aplicaciones de alto rendimiento. Ruby es conocido por su facilidad de uso y flexibilidad. En última instancia, la elección del lenguaje dependerá de qué tipo de desarrollo te interesa y qué tipo de proyectos deseas realizar.\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cuál es el mejor lenguaje de programación para aprender?\"\n",
        "temperature = 1\n",
        "top_p = 1\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = model,\n",
        "  messages = [{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        "  temperature = temperature,\n",
        "  top_p = top_p,\n",
        "  n = 4\n",
        ")\n",
        "\n",
        "content_0 = response.choices[0].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "content_1 = response.choices[1].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "content_2 = response.choices[2].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "content_3 = response.choices[3].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "print(content_0)\n",
        "print(content_1)\n",
        "print(content_2)\n",
        "print(content_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reentrenar modelo de OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI ofrece la posibilidad de reentrenar sus modelos de la API para obtener mejores resultados con nuestros propios datos. Esto tiene las siguientes ventajas\n",
        "\n",
        " + Se obtienen resultados de mayor calidad para nuestros datos\n",
        " + En un prompt podemos darle ejemplos para que se comporte como queramos, pero solo unos pocos. De esta manera, reentrenándolo, podemos darle muchos más.\n",
        " + Ahorro de tokens debido a indicaciones más cortas. Como ya lo hemos entrenado para nuestro caso de uso, podemos darle menos indicaciones para que resuelva nuestras tareas\n",
        " + Solicitudes de menor latencia. Al llamar a modelos propios tendremos menos latencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparación de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La API de OpenAI nos pide que le demos los datos en un archivo `jsonl` en el siguiente formato\n",
        "\n",
        "``` json\n",
        "{\n",
        "    \"messages\": \n",
        "    [\n",
        "        {\n",
        "            \"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"\n",
        "        }, \n",
        "        {\n",
        "            \"role\": \"user\", \"content\": \"What's the capital of France?\"\n",
        "        }, \n",
        "        {\n",
        "            \"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "{\n",
        "    \"messages\": \n",
        "    [\n",
        "        {\n",
        "            \"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"\n",
        "        }, \n",
        "        {\n",
        "            \"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"\n",
        "        }, \n",
        "        {\n",
        "            \"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "{\n",
        "    \"messages\": \n",
        "    [\n",
        "        {\n",
        "            \"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"\n",
        "        }, \n",
        "        {\n",
        "            \"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"\n",
        "        }, \n",
        "        {\n",
        "            \"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "Con un máximo de 4096 tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Validación de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para ahorrarme trabajo, le he ido pasando uno a uno todos mis post a chatgpt y le he dicho que me genere 10 `FAQ`s por cada uno en formato `CSV`, porque dudaba si me iba a poder generar un formato como el que piden en el `jsonl`. Y me ha generado un `CSV` con el siguiente formato para cada post\n",
        "\n",
        "``` csv\n",
        "prompt,completion\n",
        "¿Qué cubre la Introducción a Python en el material proporcionado?,\"La Introducción a Python cubre temas como tipos de datos, operadores, uso de funciones y clases, manejo de objetos iterables y uso de módulos. [Más información](https://maximofn.com/python/)\"\n",
        "¿Cuáles son los tipos de datos básicos en Python?,\"Python tiene 7 tipos de datos básicos: texto (`str`), numéricos (`int`, `float`, `complex`), secuencias (`list`, `tuple`, `range`), mapping (`dict`), sets (`set`, `frozenset`), booleanos (`bool`) y binarios (`bytes`, `bytearray`, `memoryview`). [Más información](https://maximofn.com/python/)\"\n",
        "¿Qué son y cómo se utilizan los operadores en Python?,\"Los operadores en Python son símbolos especiales que se utilizan para realizar operaciones como suma, resta, multiplicación y división entre variables y valores. También incluyen operadores lógicos para comparaciones. [Más información](https://maximofn.com/python/)\"\n",
        "¿Cómo se define y se utiliza una función en Python?,\"En Python, una función se define usando la palabra clave `def`, seguida del nombre de la función y paréntesis. Las funciones pueden tener parámetros y devolver valores. Se utilizan para encapsular lógica que se puede reutilizar a lo largo del código. [Más información](https://maximofn.com/python/)\"\n",
        "¿Qué son las clases en Python y cómo se utilizan?,\"Las clases en Python son la base de la programación orientada a objetos. Permiten crear objetos que encapsulan datos y funcionalidades. Las clases se definen usando la palabra clave `class`, seguida del nombre de la clase. [Más información](https://maximofn.com/python/)\"\n",
        "...\n",
        "```\n",
        "\n",
        "Cada `CSV` tiene 10 `FAQ`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Voy a hacer un código que coja cada `CSV` y genere dos nuevos `jsonl`s, uno de entrenamiento y otro de validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CSVs_path = \"openai/faqs_posts\"\n",
        "percetn_train = 0.8\n",
        "percetn_validation = 0.2\n",
        "\n",
        "jsonl_train = os.path.join(CSVs_path, \"train.jsonl\")\n",
        "jsonl_validation = os.path.join(CSVs_path, \"validation.jsonl\")\n",
        "\n",
        "# Create the train.jsonl and validation.jsonl files\n",
        "with open(jsonl_train, 'w') as f:\n",
        "    f.write('')\n",
        "with open(jsonl_validation, 'w') as f:\n",
        "    f.write('')\n",
        "\n",
        "for file in os.listdir(CSVs_path):  # Get all files in the directory\n",
        "    if file.endswith(\".csv\"):    # Check if file is a csv\n",
        "        csv = os.path.join(CSVs_path, file) # Get the path to the csv file\n",
        "        number_of_lines = 0\n",
        "        csv_content = []\n",
        "        for line in open(csv, 'r'): # Read all lines in the csv file\n",
        "            if line.startswith('prompt'):   # Skip the first line\n",
        "                continue\n",
        "            number_of_lines += 1    # Count the number of lines\n",
        "            csv_content.append(line)    # Add the line to the csv_content list\n",
        "\n",
        "        number_of_train = int(number_of_lines * percetn_train)  # Calculate the number of lines for the train.jsonl file\n",
        "        number_of_validation = int(number_of_lines * percetn_validation)    # Calculate the number of lines for the validation.sjonl file\n",
        "\n",
        "        for i in range(number_of_lines):\n",
        "            prompt = csv_content[i].split(',')[0]\n",
        "            response = ','.join(csv_content[i].split(',')[1:]).replace('\\n', '').replace('\"', '')\n",
        "            if i > 0 and i <= number_of_train:\n",
        "                # add line to train.jsonl\n",
        "                with open(jsonl_train, 'a') as f:\n",
        "                    f.write(f'{\"{\"}\"messages\": [{\"{\"}\"role\": \"system\", \"content\": \"Eres un amable asistente dispuesto a responder.\"{\"}\"}, {\"{\"}\"role\": \"user\", \"content\": \"{prompt}\"{\"}\"}, {\"{\"}\"role\": \"assistant\", \"content\": \"{response}\"{\"}\"}]{\"}\"}\\n')\n",
        "            elif i > number_of_train and i <= number_of_train + number_of_validation:\n",
        "                # add line to validation.csv\n",
        "                with open(jsonl_validation, 'a') as f:\n",
        "                    f.write(f'{\"{\"}\"messages\": [{\"{\"}\"role\": \"system\", \"content\": \"Eres un amable asistente dispuesto a responder.\"{\"}\"}, {\"{\"}\"role\": \"user\", \"content\": \"{prompt}\"{\"}\"}, {\"{\"}\"role\": \"assistant\", \"content\": \"{response}\"{\"}\"}]{\"}\"}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez tengo los dos `jsonl`s, ejecuto un [código](https://cookbook.openai.com/examples/chat_finetuning_data_prep) que proporciona OpenAI para comprobar los `jsonl`s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero validamos los del entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "        \n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "        \n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "        \n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "        \n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "            \n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "        \n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "    \n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y ahora los de validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found\n"
          ]
        }
      ],
      "source": [
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "        \n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "        \n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "        \n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "        \n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "            \n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "        \n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "    \n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cálculo de tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El número máximo de tokens de cada ejemplo tiene que ser 4096, por lo que si tenemos ejemplos más largos solo se usarán los primeros 4096 tokens. Por lo que vamos a contar el número de tokens que tiene cada `jsonl` para saber cuánto nos va a costar reentrenar el modelo\n",
        "\n",
        "Pero primero hay que instalar la librería `tiktoken`, que es el tokenizador que usa OpenAI y que nos va a servir para además saber cuántos tokens tiene cada `CSV`, y por tanto, cuánto nos va a costar reentrenar el modelo.\n",
        "\n",
        "Para instalarlo ejecutamos el siguiente comando\n",
        "\n",
        "``` bash\n",
        "pip install tiktoken\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos unas cuantas funciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min:{min(values)}, max: {max(values)}\")\n",
        "    print(f\"mean: {np.mean(values)}, median: {np.median(values)}\")\n",
        "    print(f\"p5: {np.quantile(values, 0.1)}, p95: {np.quantile(values, 0.9)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min:3, max: 3\n",
            "mean: 3.0, median: 3.0\n",
            "p5: 3.0, p95: 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min:67, max: 132\n",
            "mean: 90.13793103448276, median: 90.0\n",
            "p5: 81.5, p95: 99.5\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min:33, max: 90\n",
            "mean: 48.66379310344828, median: 48.5\n",
            "p5: 41.0, p95: 55.5\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "    \n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos en el set de entrenamiento ningún mensaje sobrepasa los 4096 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min:3, max: 3\n",
            "mean: 3.0, median: 3.0\n",
            "p5: 3.0, p95: 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min:80, max: 102\n",
            "mean: 89.93333333333334, median: 91.0\n",
            "p5: 82.2, p95: 96.8\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min:41, max: 57\n",
            "mean: 48.2, median: 49.0\n",
            "p5: 42.8, p95: 51.6\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "    \n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tampoco pasa de 4096 tokens ningún mensaje del set de validación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cálculo del coste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Otra cosa muy importante es saber cuánto nos va a costar hacer este fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has ~10456 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~31368 tokens\n"
          ]
        }
      ],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "with open(jsonl_train, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "convo_lens = []\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "tokens_for_train = n_epochs * n_billing_tokens_in_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como a la hora de escribir este post, el precio de entrenar `gpt-3.5-turbo` es de $0.0080 por cada 1000 tokens, podemos saber cuánto nos costará el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training price: $0.248\n"
          ]
        }
      ],
      "source": [
        "pricing = 0.0080\n",
        "num_tokens_pricing = 1000\n",
        "\n",
        "training_price = pricing * (tokens_for_train // num_tokens_pricing)\n",
        "print(f\"Training price: ${training_price}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has ~1349 tokens that will be charged for during training\n",
            "By default, you'll train for 6 epochs on this dataset\n",
            "By default, you'll be charged for ~8094 tokens\n"
          ]
        }
      ],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "with open(jsonl_validation, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "convo_lens = []\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "\n",
        "tokens_for_validation = n_epochs * n_billing_tokens_in_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation price: $0.064\n"
          ]
        }
      ],
      "source": [
        "validation_price = pricing * (tokens_for_validation // num_tokens_pricing)\n",
        "print(f\"Validation price: ${validation_price}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total price: $0.312\n"
          ]
        }
      ],
      "source": [
        "total_price = training_price + validation_price\n",
        "print(f\"Total price: ${total_price}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si nuestros cálculos están bien, vemos que el reentrenamiento de `gpt-3.5-turbo` nos costará $0.312"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez tengamos todo listo tenemos que subir los `jsonl`s a la API de OpenAI para que reentrene el modelo. Para ello, ejecutamos el siguiente código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.files.create(file=open(jsonl_train, \"rb\"), purpose=\"fine-tune\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.file_object.FileObject,\n",
              " FileObject(id='file-LWztOVasq4E0U67wRe8ShjLZ', bytes=47947, created_at=1701585709, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None))"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(result), result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = file-LWztOVasq4E0U67wRe8ShjLZ\n",
            "result.bytes = 47947\n",
            "result.created_at = 1701585709\n",
            "result.filename = train.jsonl\n",
            "result.object = file\n",
            "result.purpose = fine-tune\n",
            "result.status = processed\n",
            "result.status_details = None\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.bytes = {result.bytes}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.filename = {result.filename}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.purpose = {result.purpose}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.status_details = {result.status_details}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jsonl_train_id = file-LWztOVasq4E0U67wRe8ShjLZ\n"
          ]
        }
      ],
      "source": [
        "jsonl_train_id = result.id\n",
        "print(f\"jsonl_train_id = {jsonl_train_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hacemos lo mismo con el set de validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.files.create(file=open(jsonl_validation, \"rb\"), purpose=\"fine-tune\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = file-E0YOgIIe9mwxmFcza5bFyVKW\n",
            "result.bytes = 6369\n",
            "result.created_at = 1701585730\n",
            "result.filename = validation.jsonl\n",
            "result.object = file\n",
            "result.purpose = fine-tune\n",
            "result.status = processed\n",
            "result.status_details = None\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.bytes = {result.bytes}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.filename = {result.filename}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.purpose = {result.purpose}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.status_details = {result.status_details}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jsonl_train_id = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
        "jsonl_validation_id = result.id\n",
        "print(f\"jsonl_train_id = {jsonl_validation_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez los tenemos subidos pasamos a entrenar a nuestro propio modelo de OpenAi, para ello usamos el siguiente código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.fine_tuning.jobs.create(model = \"gpt-3.5-turbo\", training_file = jsonl_train_id, validation_file = jsonl_validation_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,\n",
              " FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='validating_files', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(result), result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
            "result.created_at = 1701585758\n",
            "result.error = None\n",
            "result.fine_tuned_model = None\n",
            "result.finished_at = None\n",
            "result.hyperparameters = Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto')\n",
            "\tn_epochs = auto\n",
            "\tbatch_size = auto\n",
            "\tlearning_rate_multiplier = auto\n",
            "result.model = gpt-3.5-turbo-0613\n",
            "result.object = fine_tuning.job\n",
            "result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
            "result.result_files = []\n",
            "result.status = validating_files\n",
            "result.trained_tokens = None\n",
            "result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
            "result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.error = {result.error}\")\n",
        "print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
        "print(f\"result.finished_at = {result.finished_at}\")\n",
        "print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
        "print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
        "print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
        "print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
        "print(f\"result.model = {result.model}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.organization_id = {result.organization_id}\")\n",
        "print(f\"result.result_files = {result.result_files}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
        "print(f\"result.training_file = {result.training_file}\")\n",
        "print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fine_tune_id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n"
          ]
        }
      ],
      "source": [
        "fine_tune_id = result.id\n",
        "print(f\"fine_tune_id = {fine_tune_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver que en `status` salía `validating_files`. Como el fine tuning tarda bastante, podemos ir preguntando por el proceso mediante el siguiente código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.fine_tuning.fine_tuning_job.FineTuningJob,\n",
              " FineTuningJob(id='ftjob-aBndcorOfQLP0UijlY0R4pTB', created_at=1701585758, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-3.5-turbo-0613', object='fine_tuning.job', organization_id='org-qDHVqEZ9tqE2XuA0IgWi7Erg', result_files=[], status='running', trained_tokens=None, training_file='file-LWztOVasq4E0U67wRe8ShjLZ', validation_file='file-E0YOgIIe9mwxmFcza5bFyVKW'))"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(result), result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
            "result.created_at = 1701585758\n",
            "result.error = None\n",
            "result.fine_tuned_model = None\n",
            "result.finished_at = None\n",
            "result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)\n",
            "\tn_epochs = 3\n",
            "\tbatch_size = 1\n",
            "\tlearning_rate_multiplier = 2\n",
            "result.model = gpt-3.5-turbo-0613\n",
            "result.object = fine_tuning.job\n",
            "result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
            "result.result_files = []\n",
            "result.status = running\n",
            "result.trained_tokens = None\n",
            "result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
            "result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.error = {result.error}\")\n",
        "print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
        "print(f\"result.finished_at = {result.finished_at}\")\n",
        "print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
        "print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
        "print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
        "print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
        "print(f\"result.model = {result.model}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.organization_id = {result.organization_id}\")\n",
        "print(f\"result.result_files = {result.result_files}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
        "print(f\"result.training_file = {result.training_file}\")\n",
        "print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos un bucle que espere a que finalice el entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job succeeded"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)\n",
        "status = result.status\n",
        "\n",
        "while status != \"succeeded\":\n",
        "    time.sleep(10)\n",
        "    result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)\n",
        "    status = result.status\n",
        "\n",
        "print(\"Job succeeded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como ha terminado el entrenamiento, volvemos a pedir la información sobre el proceso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = client.fine_tuning.jobs.retrieve(fine_tuning_job_id = fine_tune_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "result.id = ftjob-aBndcorOfQLP0UijlY0R4pTB\n",
            "result.created_at = 1701585758\n",
            "result.error = None\n",
            "result.fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
            "result.finished_at = 1701586541\n",
            "result.hyperparameters = Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2)\n",
            "\tn_epochs = 3\n",
            "\tbatch_size = 1\n",
            "\tlearning_rate_multiplier = 2\n",
            "result.model = gpt-3.5-turbo-0613\n",
            "result.object = fine_tuning.job\n",
            "result.organization_id = org-qDHVqEZ9tqE2XuA0IgWi7Erg\n",
            "result.result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']\n",
            "result.status = succeeded\n",
            "result.trained_tokens = 30672\n",
            "result.training_file = file-LWztOVasq4E0U67wRe8ShjLZ\n",
            "result.validation_file = file-E0YOgIIe9mwxmFcza5bFyVKW\n"
          ]
        }
      ],
      "source": [
        "print(f\"result.id = {result.id}\")\n",
        "print(f\"result.created_at = {result.created_at}\")\n",
        "print(f\"result.error = {result.error}\")\n",
        "print(f\"result.fine_tuned_model = {result.fine_tuned_model}\")\n",
        "print(f\"result.finished_at = {result.finished_at}\")\n",
        "print(f\"result.hyperparameters = {result.hyperparameters}\")\n",
        "print(f\"\\tn_epochs = {result.hyperparameters.n_epochs}\")\n",
        "print(f\"\\tbatch_size = {result.hyperparameters.batch_size}\")\n",
        "print(f\"\\tlearning_rate_multiplier = {result.hyperparameters.learning_rate_multiplier}\")\n",
        "print(f\"result.model = {result.model}\")\n",
        "print(f\"result.object = {result.object}\")\n",
        "print(f\"result.organization_id = {result.organization_id}\")\n",
        "print(f\"result.result_files = {result.result_files}\")\n",
        "print(f\"result.status = {result.status}\")\n",
        "print(f\"result.trained_tokens = {result.trained_tokens}\")\n",
        "print(f\"result.training_file = {result.training_file}\")\n",
        "print(f\"result.validation_file = {result.validation_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos algunos datos interesantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fine_tuned_model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
            "finished_at = 1701586541\n",
            "result_files = ['file-dNeo5ojOSuin7JIkNkQouHLB']\n",
            "status = succeeded\n",
            "trained_tokens = 30672\n"
          ]
        }
      ],
      "source": [
        "fine_tuned_model = result.fine_tuned_model\n",
        "finished_at = result.finished_at\n",
        "result_files = result.result_files\n",
        "status = result.status\n",
        "trained_tokens = result.trained_tokens\n",
        "\n",
        "print(f\"fine_tuned_model = {fine_tuned_model}\")\n",
        "print(f\"finished_at = {finished_at}\")\n",
        "print(f\"result_files = {result_files}\")\n",
        "print(f\"status = {status}\")\n",
        "print(f\"trained_tokens = {trained_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver que le ha dado el nombre `ft:gpt-3.5-turbo-0613:personal::8RagA0RT` a nuestro modelo, su status ahora es `succeeded` y que ha usado 30672 tokens, mientras que nosotros habíamos predicho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(31368, 8094, 39462)"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_for_train, tokens_for_validation, tokens_for_train + tokens_for_validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es decir, ha usado menos tokens, por lo que el entrenamiento nos ha costado menos de lo que habíamos pronosticado, en concreto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real training price: $0.24\n"
          ]
        }
      ],
      "source": [
        "real_training_price = pricing * (trained_tokens // num_tokens_pricing)\n",
        "print(f\"Real training price: ${real_training_price}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Además de esta información, si nos vamos a la página [finetune](https://platform.openai.com/finetune) de OpenAI, podemos ver que nuestro modelo está ahí\n",
        "\n",
        "![open ai finetune](https://images.maximofn.com/openai_fine_tuning_process.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver también cuánto nos ha costado el entrenamiento\n",
        "\n",
        "![open ai finetune cost](https://images.maximofn.com/openai_fine_tuning_cost.webp)\n",
        "\n",
        "Como podemos ver han sido solo $0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y por último, vamos a ver cuánto tiempo ha llevado hacer este entrenamiento. Podemos ver a qué hora empezó\n",
        "\n",
        "![open ai finetune start](https://images.maximofn.com/openai_fine_tuning_process_start_time.webp)\n",
        "\n",
        "Y a qué hora terminó\n",
        "\n",
        "![open ai finetune end](https://images.maximofn.com/openai_fine_tuning_process_stop_time.webp)\n",
        "\n",
        "Por lo que más o menos ha llevado unos 10 minutos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prueba del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dentro del [playground](https://platform.openai.com/playground?mode=chat) de OpenAI podemos probar nuestro modelo, pero vamos a hacerlo mediante la API como hemos aprendido aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "promtp = \"¿Cómo se define una función en Python?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = fine_tuned_model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.chat.chat_completion.ChatCompletion,\n",
              " ChatCompletion(id='chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))], created=1701667535, model='ft:gpt-3.5-turbo-0613:personal::8RagA0RT', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)))"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response), response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.id = chatcmpl-8RvkVG8a5xjI2UZdXgdOGGcoelefc\n",
            "response.choices = [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))]\n",
            "response.choices[0] = Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None))\n",
            "\tresponse.choices[0].finish_reason = stop\n",
            "\tresponse.choices[0].index = 0\n",
            "\tresponse.choices[0].message = ChatCompletionMessage(content='Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)', role='assistant', function_call=None, tool_calls=None)\n",
            "\t\tresponse.choices[0].message.content = \n",
            "\t\tUna función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)\n",
            "\t\tresponse.choices[0].message.role = assistant\n",
            "\t\tresponse.choices[0].message.function_call = None\n",
            "\t\tresponse.choices[0].message.tool_calls = None\n",
            "response.created = 1701667535\n",
            "response.model = ft:gpt-3.5-turbo-0613:personal::8RagA0RT\n",
            "response.object = chat.completion\n",
            "response.system_fingerprint = None\n",
            "response.usage = CompletionUsage(completion_tokens=54, prompt_tokens=16, total_tokens=70)\n",
            "\tresponse.usage.completion_tokens = 54\n",
            "\tresponse.usage.prompt_tokens = 16\n",
            "\tresponse.usage.total_tokens = 70\n"
          ]
        }
      ],
      "source": [
        "print(f\"response.id = {response.id}\")\n",
        "print(f\"response.choices = {response.choices}\")\n",
        "for i in range(len(response.choices)):\n",
        "    print(f\"response.choices[{i}] = {response.choices[i]}\")\n",
        "    print(f\"\\tresponse.choices[{i}].finish_reason = {response.choices[i].finish_reason}\")\n",
        "    print(f\"\\tresponse.choices[{i}].index = {response.choices[i].index}\")\n",
        "    print(f\"\\tresponse.choices[{i}].message = {response.choices[i].message}\")\n",
        "    content = response.choices[i].message.content.replace('\\n\\n', '\\n\\t\\t')\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.content = \\n\\t\\t{content}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.role = {response.choices[i].message.role}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.function_call = {response.choices[i].message.function_call}\")\n",
        "    print(f\"\\t\\tresponse.choices[{i}].message.tool_calls = {response.choices[i].message.tool_calls}\")\n",
        "print(f\"response.created = {response.created}\")\n",
        "print(f\"response.model = {response.model}\")\n",
        "print(f\"response.object = {response.object}\")\n",
        "print(f\"response.system_fingerprint = {response.system_fingerprint}\")\n",
        "print(f\"response.usage = {response.usage}\")\n",
        "print(f\"\\tresponse.usage.completion_tokens = {response.usage.completion_tokens}\")\n",
        "print(f\"\\tresponse.usage.prompt_tokens = {response.usage.prompt_tokens}\")\n",
        "print(f\"\\tresponse.usage.total_tokens = {response.usage.total_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Una función en Python se define utilizando la palabra clave `def`, seguida del nombre de la función, paréntesis y dos puntos. El cuerpo de la función se indenta debajo. [Más información](https://maximofn.com/python/)\n"
          ]
        }
      ],
      "source": [
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos un modelo que no solo nos resuelve la respuesta, sino que además nos da un enlace a la documentación de nuestro blog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver cómo se comporta con un ejemplo que cláramente no tiene nada que ver con el blog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Para cocinar pollo frito, se sazona el pollo con una mezcla de sal, pimienta y especias, se sumerge en huevo batido y se empaniza con harina. Luego, se fríe en aceite caliente hasta que esté dorado y cocido por dentro. [Más información](https://maximofn.com/pollo-frito/)\n"
          ]
        }
      ],
      "source": [
        "promtp = \"¿Cómo puedo cocinar pollo frito?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model = fine_tuned_model,\n",
        "  messages=[{\"role\": \"user\", \"content\": f\"{promtp}\"}],\n",
        ")\n",
        "\n",
        "for i in range(len(response.choices)):\n",
        "    content = response.choices[i].message.content.replace('\\n\\n', '\\n')\n",
        "    print(f\"{content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como se puede ver nos da el enlace `https://maximofn.com/pollo-frito/` el cual no existe. Por lo que aunque hemos reentrenado un modelo de chatGPT, hay que tener cuidado con lo que nos responde y no fiarse al 100% de él"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generar imágenes con DALL-E 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para generar imágenes con DALL-E 3, tenemos que usar el siguiente código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=\"a white siamese cat\",\n",
        "  size=\"1024x1024\",\n",
        "  quality=\"standard\",\n",
        "  n=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.images_response.ImagesResponse,\n",
              " ImagesResponse(created=1701823487, data=[Image(b64_json=None, revised_prompt=\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')]))"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response), response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.created = 1701823487\n",
            "response.data[0] = Image(b64_json=None, revised_prompt=\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D')\n",
            "\tresponse.data[0].b64_json = None\n",
            "\tresponse.data[0].revised_prompt = Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\n",
            "\tresponse.data[0].url = https://oaidalleapiprodscus.blob.core.windows.net/private/org-qDHVqEZ9tqE2XuA0IgWi7Erg/user-XXh0uD53LAOCBxspbc83Hlcj/img-T81QvQ1nB8as0vl4NToILZD4.png?st=2023-12-05T23%3A44%3A47Z&se=2023-12-06T01%3A44%3A47Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T19%3A58%3A58Z&ske=2023-12-06T19%3A58%3A58Z&sks=b&skv=2021-08-06&sig=nzDujTj3Y3THuRrq2kOvASA5xP73Mm8HHlQuKKkLYu8%3D\n"
          ]
        }
      ],
      "source": [
        "print(f\"response.created = {response.created}\")\n",
        "for i in range(len(response.data)):\n",
        "    print(f\"response.data[{i}] = {response.data[i]}\")\n",
        "    print(f\"\\tresponse.data[{i}].b64_json = {response.data[i].b64_json}\")\n",
        "    print(f\"\\tresponse.data[{i}].revised_prompt = {response.data[i].revised_prompt}\")\n",
        "    print(f\"\\tresponse.data[{i}].url = {response.data[i].url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver un dato muy interesante que no podemos ver cuando usamos DALL-E 3 a través de la interfaz de OpenAI, y es el prompt que se le ha pasado al modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Create a detailed image of a Siamese cat with a white coat. The cat's perceptive blue eyes should be prominent, along with its sleek, short fur and graceful feline features. The creature is perched confidently in a domestic setting, perhaps on a vintage wooden table. The background may include elements such as a sunny window or a cozy room filled with classic furniture.\""
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.data[0].revised_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con ese prompt nos ha generado la siguiente imagen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = response.data[0].url\n",
        "# img_data = requests.get(url).content\n",
        "with open('openai/dall-e-3.png', 'wb') as handler:\n",
        "    handler.write(requests.get(url).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![dall-e 3](openai/dall-e-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como tenemos el prompt que OpenAI ha utilizado, en realidad vamos a intentar usarlo para generar un gato similar pero con los ojos verdes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A well-defined image of a Siamese cat boasting a shiny white coat. Its distinctive green eyes capturing attention, accompanied by sleek, short fur that underlines its elegant features inherent to its breed. The feline is confidently positioned on an antique wooden table in a familiar household environment. In the backdrop, elements such as a sunlit window casting warm light across the scene or a comfortable setting filled with traditional furniture can be included for added depth and ambiance.\n"
          ]
        }
      ],
      "source": [
        "revised_prompt = response.data[0].revised_prompt\n",
        "gree_eyes = revised_prompt.replace(\"blue\", \"green\")\n",
        "\n",
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=gree_eyes,\n",
        "  size=\"1024x1024\",\n",
        "  quality=\"standard\",\n",
        "  n=1,\n",
        ")\n",
        "\n",
        "print(response.data[0].revised_prompt)\n",
        "\n",
        "image_url = response.data[0].url\n",
        "\n",
        "image_path = 'openai/dall-e-3-green.png'\n",
        "with open(image_path, 'wb') as handler:\n",
        "    handler.write(requests.get(image_url).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![dall-e-3-green](openai/dall-e-3-green.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aunque ha cambiado el color del gato y no solo de los ojos, la posición y el fondo son muy similares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A parte del prompt, las otras variables que podemos modificar son\n",
        "\n",
        " * `model`: Permite elegir el modelo de generación de imágenes, los posibles valores son `dalle-2` y `dalle-3`\n",
        " * `size`: Permite cambiar el tamaño de la imagen, los posibles valores son `256x256`, `512x512`, `1024x1024`, `1792x1024`, `1024x1792` píxeles\n",
        " * `quality`: Permite cambiar la calidad de la imagen, los posibles valores son `standard` o `hd`\n",
        " * `response_format`: Permite cambiar el formato de la respuesta, los posibles valores son `url` o `b64_json`\n",
        " * `n`: Permite cambiar el número de imágenes que queremos que nos devuelva el modelo. Con DALL-E 3 solo podemos pedir una imagen\n",
        " * `style`: Permite cambiar el estilo de la imagen, los posibles valores son `vivid` o `natural`\n",
        "\n",
        "Así que vamos a generar una imagen de alta calidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Render a portrait of a Siamese cat boasting a pristine white coat. This cat should have captivating green eyes that stand out. Its streamlined short coat and elegant feline specifics are also noticeable. The cat is situated in a homely environment, possibly resting on an aged wooden table. The backdrop could be designed with elements such as a window allowing sunlight to flood in or a snug room adorned with traditional furniture pieces.\n"
          ]
        }
      ],
      "source": [
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=gree_eyes,\n",
        "  size=\"1024x1792\",\n",
        "  quality=\"hd\",\n",
        "  n=1,\n",
        "  style=\"natural\",\n",
        ")\n",
        "\n",
        "print(response.data[0].revised_prompt)\n",
        "\n",
        "image_url = response.data[0].url\n",
        "\n",
        "image_path = 'openai/dall-e-3-hd.png'\n",
        "with open(image_path, 'wb') as handler:\n",
        "    handler.write(requests.get(image_url).content)\n",
        "display(Image(image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![dall-e-3-hd](openai/dall-e-3-hd.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a usar el modelo de visión con la siguiente imagen\n",
        "\n",
        "![panda](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI)\n",
        "\n",
        "Que visto aquí en pequeño se asemeja un panda, pero si la vemos en grande es más difícil ver el panda\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "  <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI\" alt=\"panda\" style=\"width:637px;height:939px;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para usar el modelo de visión, tenemos que utilizar el siguiente código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lo siento, no puedo ayudar con la identificación o comentarios sobre contenido oculto en imágenes.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"¿Ves algún animal en esta imagen?\"\n",
        "image_url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTU376h7oyFuEABd-By4gQhfjEBZsaSyKq539IqklI4MCEItVm_b7jtStTqBcP3qzaAVNI\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4-vision-preview\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\"type\": \"text\", \"text\": prompt},\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": image_url,\n",
        "          },\n",
        "        },\n",
        "      ],\n",
        "    }\n",
        "  ],\n",
        "  max_tokens=300,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No consigue encontrar el panda, pero no es el objetivo de este post que vea el panda, solamente explicar cómo usar el modelo de visión de GPT4, así que no vamos a profundizar más en este tema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos pasarle varias imágenes a la vez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://i0.wp.com/www.aulapt.org/wp-content/uploads/2018/10/ilusiones-%C3%B3pticas.jpg?fit=649%2C363&ssl=1\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<img src=\"https://i.pinimg.com/736x/69/ed/5a/69ed5ab09092880e38513a8870efee10.jpg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sí, en ambas imágenes se ven figuras de animales. Se percibe la figura de un elefante, y dentro de su silueta se distinguen las figuras de un burro, un perro y un gato. Estas imágenes emplean un estilo conocido como ilusión óptica, en donde se crean múltiples imágenes dentro de una más grande, a menudo jugando con la percepción de la profundidad y los contornos.\n"
          ]
        }
      ],
      "source": [
        "image_url1 = \"https://i0.wp.com/www.aulapt.org/wp-content/uploads/2018/10/ilusiones-%C3%B3pticas.jpg?fit=649%2C363&ssl=1\"\n",
        "image_url2 = \"https://i.pinimg.com/736x/69/ed/5a/69ed5ab09092880e38513a8870efee10.jpg\"\n",
        "prompt = \"¿Ves algún animal en estas imágenes?\"\n",
        "\n",
        "display(Image(url=image_url1))\n",
        "display(Image(url=image_url2))\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4-vision-preview\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": prompt,\n",
        "        },\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": image_url1,\n",
        "          },\n",
        "        },\n",
        "        {\n",
        "          \"type\": \"image_url\",\n",
        "          \"image_url\": {\n",
        "            \"url\": image_url2,\n",
        "          },\n",
        "        },\n",
        "      ],\n",
        "    }\n",
        "  ],\n",
        "  max_tokens=300,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text to speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos generar audio a partir de texto con el siguiente código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "speech_file_path = \"openai/speech.mp3\"\n",
        "text = \"Hola desde el blog de MaximoFN\"\n",
        "\n",
        "response = client.audio.speech.create(\n",
        "  model=\"tts-1\",\n",
        "  voice=\"alloy\",\n",
        "  input=text,\n",
        ")\n",
        "\n",
        "response.stream_to_file(speech_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<audio controls>\n",
        "    <source src=\"openai/speech.mp3\" type=\"audio/mpeg\">\n",
        "    Tu navegador no soporta el elemento de audio.\n",
        "</audio>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos elegir\n",
        "\n",
        " * model: Permite elegir el modelo de generación de audio. Los posibles valores son `tts-1` y `tts-1-hd`\n",
        " * voice: Permite elegir la voz que queremos que use el modelo, los posibles valores son `alloy`, `echo`, `fable`, `onyx`, `nova` y `shimmer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Speech to text (Whisper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos transcribir audio mediante Whisper con el siguiente código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the Micromachine Man presenting the most midget miniature motorcade of micromachines. Each one has dramatic details, terrific trim, precision paint jobs, plus incredible micromachine pocket play sets. There's a police station, fire station, restaurant, service station, and more. Perfect pocket portables to take anyplace. And there are many miniature play sets to play with, and each one comes with its own special edition micromachine vehicle and fun fantastic features that miraculously move. Raise the boat lift at the airport, marina, man the gun turret at the army base, clean your car at the car wash, raise the toll bridge. And these play sets fit together to form a micromachine world. Micromachine pocket play sets so tremendously tiny, so perfectly precise, so dazzlingly detailed, you'll want to pocket them all. Micromachines and micromachine pocket play sets sold separately from Galoob. The smaller they are, the better they are.\n"
          ]
        }
      ],
      "source": [
        "audio_file = \"MicroMachines.mp3\"\n",
        "audio_file= open(audio_file, \"rb\")\n",
        "\n",
        "transcript = client.audio.transcriptions.create(\n",
        "  model=\"whisper-1\", \n",
        "  file=audio_file\n",
        ")\n",
        "\n",
        "print(transcript.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<audio controls>\n",
        "    <source src=\"MicroMachines.mp3\" type=\"audio/mpeg\">\n",
        "    Tu navegador no soporta el elemento de audio.\n",
        "</audio>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Moderación de contenido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos obtener la categoría de un texto entre las clases `sexual`, `hate`, `harassment`, `self-harm`, `sexual/minors`, `hate/threatening`, `violence/graphic`, `self-harm/intent`, `self-harm/instructions`, `harassment/threatening` y `violence`, para ello usamos el siguiente código con el texto transcrito anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = transcript.text\n",
        "\n",
        "response = client.moderations.create(input=text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.moderation_create_response.ModerationCreateResponse,\n",
              " ModerationCreateResponse(id='modr-8RxMZItvmLblEl5QPgCv19Jl741SS', model='text-moderation-006', results=[Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)]))"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(response), response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.id = modr-8RxMZItvmLblEl5QPgCv19Jl741SS\n",
            "response.model = text-moderation-006\n",
            "response.results[0] = Moderation(categories=Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False), category_scores=CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06), flagged=False)\n",
            "\tresponse.results[0].categories = Categories(harassment=False, harassment_threatening=False, hate=False, hate_threatening=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=False, violence_graphic=False, self-harm=False, sexual/minors=False, hate/threatening=False, violence/graphic=False, self-harm/intent=False, self-harm/instructions=False, harassment/threatening=False)\n",
            "\t\tresponse.results[0].categories.harassment = False\n",
            "\t\tresponse.results[0].categories.harassment_threatening = False\n",
            "\t\tresponse.results[0].categories.hate = False\n",
            "\t\tresponse.results[0].categories.hate_threatening = False\n",
            "\t\tresponse.results[0].categories.self_harm = False\n",
            "\t\tresponse.results[0].categories.self_harm_instructions = False\n",
            "\t\tresponse.results[0].categories.self_harm_intent = False\n",
            "\t\tresponse.results[0].categories.sexual = False\n",
            "\t\tresponse.results[0].categories.sexual_minors = False\n",
            "\t\tresponse.results[0].categories.violence = False\n",
            "\t\tresponse.results[0].categories.violence_graphic = False\n",
            "\tresponse.results[0].category_scores = CategoryScores(harassment=0.0003560568729881197, harassment_threatening=2.5426568299735663e-06, hate=1.966094168892596e-05, hate_threatening=6.384455986108151e-08, self_harm=7.903140613052528e-07, self_harm_instructions=6.443992219828942e-07, self_harm_intent=1.2202733046251524e-07, sexual=0.0003779272665269673, sexual_minors=1.8967952200910076e-05, violence=9.489082731306553e-05, violence_graphic=5.1929731853306293e-05, self-harm=7.903140613052528e-07, sexual/minors=1.8967952200910076e-05, hate/threatening=6.384455986108151e-08, violence/graphic=5.1929731853306293e-05, self-harm/intent=1.2202733046251524e-07, self-harm/instructions=6.443992219828942e-07, harassment/threatening=2.5426568299735663e-06)\n",
            "\t\tresponse.results[0].category_scores.harassment = 0.0003560568729881197\n",
            "\t\tresponse.results[0].category_scores.harassment_threatening = 2.5426568299735663e-06\n",
            "\t\tresponse.results[0].category_scores.hate = 1.966094168892596e-05\n",
            "\t\tresponse.results[0].category_scores.hate_threatening = 6.384455986108151e-08\n",
            "\t\tresponse.results[0].category_scores.self_harm = 7.903140613052528e-07\n",
            "\t\tresponse.results[0].category_scores.self_harm_instructions = 6.443992219828942e-07\n",
            "\t\tresponse.results[0].category_scores.self_harm_intent = 1.2202733046251524e-07\n",
            "\t\tresponse.results[0].category_scores.sexual = 0.0003779272665269673\n",
            "\t\tresponse.results[0].category_scores.sexual_minors = 1.8967952200910076e-05\n",
            "\t\tresponse.results[0].category_scores.violence = 9.489082731306553e-05\n",
            "\t\tresponse.results[0].category_scores.violence_graphic = 5.1929731853306293e-05\n",
            "\tresponse.results[0].flagged = False\n"
          ]
        }
      ],
      "source": [
        "print(f\"response.id = {response.id}\")\n",
        "print(f\"response.model = {response.model}\")\n",
        "for i in range(len(response.results)):\n",
        "    print(f\"response.results[{i}] = {response.results[i]}\")\n",
        "    print(f\"\\tresponse.results[{i}].categories = {response.results[i].categories}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.harassment = {response.results[i].categories.harassment}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.hate = {response.results[i].categories.hate}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.sexual = {response.results[i].categories.sexual}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.violence = {response.results[i].categories.violence}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}\")\n",
        "    print(f\"\\tresponse.results[{i}].category_scores = {response.results[i].category_scores}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}\")\n",
        "    print(f\"\\t\\tresponse.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}\")\n",
        "    print(f\"\\tresponse.results[{i}].flagged = {response.results[i].flagged}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El audio transcrito no está en ninguna de las categorías anteriores, vamos a probar con otro texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "response.results[0].categories.harassment = False\n",
            "response.results[0].categories.harassment_threatening = False\n",
            "response.results[0].categories.hate = False\n",
            "response.results[0].categories.hate_threatening = False\n",
            "response.results[0].categories.self_harm = True\n",
            "response.results[0].categories.self_harm_instructions = False\n",
            "response.results[0].categories.self_harm_intent = True\n",
            "response.results[0].categories.sexual = False\n",
            "response.results[0].categories.sexual_minors = False\n",
            "response.results[0].categories.violence = True\n",
            "response.results[0].categories.violence_graphic = False\n",
            "\n",
            "response.results[0].category_scores.harassment = 0.004724912345409393\n",
            "response.results[0].category_scores.harassment_threatening = 0.00023778305330779403\n",
            "response.results[0].category_scores.hate = 1.1909247405128554e-05\n",
            "response.results[0].category_scores.hate_threatening = 1.826493189582834e-06\n",
            "response.results[0].category_scores.self_harm = 0.9998544454574585\n",
            "response.results[0].category_scores.self_harm_instructions = 3.5801923647937883e-09\n",
            "response.results[0].category_scores.self_harm_intent = 0.99969482421875\n",
            "response.results[0].category_scores.sexual = 2.141016238965676e-06\n",
            "response.results[0].category_scores.sexual_minors = 2.840671520232263e-08\n",
            "response.results[0].category_scores.violence = 0.8396497964859009\n",
            "response.results[0].category_scores.violence_graphic = 2.7347923605702817e-05\n",
            "\n",
            "response.results[0].flagged = True\n"
          ]
        }
      ],
      "source": [
        "text = \"I want to kill myself\"\n",
        "\n",
        "response = client.moderations.create(input=text)\n",
        "\n",
        "for i in range(len(response.results)):\n",
        "    print(f\"response.results[{i}].categories.harassment = {response.results[i].categories.harassment}\")\n",
        "    print(f\"response.results[{i}].categories.harassment_threatening = {response.results[i].categories.harassment_threatening}\")\n",
        "    print(f\"response.results[{i}].categories.hate = {response.results[i].categories.hate}\")\n",
        "    print(f\"response.results[{i}].categories.hate_threatening = {response.results[i].categories.hate_threatening}\")\n",
        "    print(f\"response.results[{i}].categories.self_harm = {response.results[i].categories.self_harm}\")\n",
        "    print(f\"response.results[{i}].categories.self_harm_instructions = {response.results[i].categories.self_harm_instructions}\")\n",
        "    print(f\"response.results[{i}].categories.self_harm_intent = {response.results[i].categories.self_harm_intent}\")\n",
        "    print(f\"response.results[{i}].categories.sexual = {response.results[i].categories.sexual}\")\n",
        "    print(f\"response.results[{i}].categories.sexual_minors = {response.results[i].categories.sexual_minors}\")\n",
        "    print(f\"response.results[{i}].categories.violence = {response.results[i].categories.violence}\")\n",
        "    print(f\"response.results[{i}].categories.violence_graphic = {response.results[i].categories.violence_graphic}\")\n",
        "    print()\n",
        "    print(f\"response.results[{i}].category_scores.harassment = {response.results[i].category_scores.harassment}\")\n",
        "    print(f\"response.results[{i}].category_scores.harassment_threatening = {response.results[i].category_scores.harassment_threatening}\")\n",
        "    print(f\"response.results[{i}].category_scores.hate = {response.results[i].category_scores.hate}\")\n",
        "    print(f\"response.results[{i}].category_scores.hate_threatening = {response.results[i].category_scores.hate_threatening}\")\n",
        "    print(f\"response.results[{i}].category_scores.self_harm = {response.results[i].category_scores.self_harm}\")\n",
        "    print(f\"response.results[{i}].category_scores.self_harm_instructions = {response.results[i].category_scores.self_harm_instructions}\")\n",
        "    print(f\"response.results[{i}].category_scores.self_harm_intent = {response.results[i].category_scores.self_harm_intent}\")\n",
        "    print(f\"response.results[{i}].category_scores.sexual = {response.results[i].category_scores.sexual}\")\n",
        "    print(f\"response.results[{i}].category_scores.sexual_minors = {response.results[i].category_scores.sexual_minors}\")\n",
        "    print(f\"response.results[{i}].category_scores.violence = {response.results[i].category_scores.violence}\")\n",
        "    print(f\"response.results[{i}].category_scores.violence_graphic = {response.results[i].category_scores.violence_graphic}\")\n",
        "    print()\n",
        "    print(f\"response.results[{i}].flagged = {response.results[i].flagged}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora si detecta que el texto es `self_harm_intent`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Asistentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI nos da la posibilidad de crear asistentes, de manera que los podemos crear con las características que nosotros queramos, por ejemplo, un asistente experto en Python, y poderlo usar como si fuese un modelo particular de OpenAI. Es decir, podemos usarlo para una consulta y tener una conversación con él, y al cabo del tiempo, volverlo a usar con una nueva consulta en una conversación nueva.\n",
        "\n",
        "A la hora de trabajar con asistentes tendremos que crearlos, crear un hilo, enviarles el mensaje, ejecutarlos, esperar a que respondan y ver la respuesta\n",
        "\n",
        "![asistentes](https://cdn.openai.com/API/docs/images/diagram-assistant.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear el asistente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero creamos el asistente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "code_interpreter_assistant = client.beta.assistants.create(\n",
        "    name=\"Python expert\",\n",
        "    instructions=\"Eres un experto en Python. Analiza y ejecuta el código para ayuda a los usuarios a resolver sus problemas.\",\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        "    model=\"gpt-3.5-turbo-1106\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.beta.assistant.Assistant,\n",
              " Assistant(id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', created_at=1701822478, description=None, file_ids=[], instructions='Eres un experto en Python. Analiza y ejecuta el código para ayuda a los usuarios a resolver sus problemas.', metadata={}, model='gpt-3.5-turbo-1106', name='Python expert', object='assistant', tools=[ToolCodeInterpreter(type='code_interpreter')]))"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(code_interpreter_assistant), code_interpreter_assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "code_interpreter_assistant_id = asst_A2F9DPqDiZYFc5hOC6Rb2y0x\n"
          ]
        }
      ],
      "source": [
        "code_interpreter_assistant_id = code_interpreter_assistant.id\n",
        "print(f\"code_interpreter_assistant_id = {code_interpreter_assistant_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A la hora de crear el asistente, las variables que tenemos son\n",
        "\n",
        " * `name`: nombre del asistente\n",
        " * `instructions`: Instrucciones para el asistente. Aquí le podemos explicar cómo se tiene que comportar el asistente\n",
        " * `tools`: Herramientas que puede usar el asistente. De momento solo están disponibles `code_interpreter` y `retrieval`\n",
        " * `model`: Modelo que va a usar el asistente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este asistente ya está creado y lo podemos usar todas las veces que queramos. Para ello tenemos que crear un nuevo hilo, así si en el futuro lo quiere usar otra persona, porque resulte útil, con crear un nuevo hilo, lo podrá usar como si estuviera usando el asistente original. Solo necesitaría el ID del asistente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hilo o thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un hilo representa una nueva conversación con el asistente, así aunque haya pasado tiempo, mientras tengamos el ID del hilo, podemos continuar con la conversación. Para crear un nuevo hilo, tenemos que usar el siguiente código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thread_id = thread_nfFT3rFjyPWHdxWvMk6jJ90H\n"
          ]
        }
      ],
      "source": [
        "type(thread), thread\n",
        "thread_id = thread.id\n",
        "print(f\"thread_id = {thread_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Subimos un archivo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear un archivo .py que vamos a pedir al intérprete que nos explique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "python_code = os.path.join(\"openai\", \"python_code.py\")\n",
        "code = \"print('Hello world!')\"\n",
        "with open(python_code, \"w\") as f:\n",
        "    f.write(code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo subimos a la API de OpenAI mediante la función `client.files.create`, esta función ya la usamos cuando hicimos `fine-tuning` de un modelo de chatGPT y le subíamos los `jsonl`s. Solo que antes en la variable `purpose` le pasábamos `fine-tuning` ya que los archivos que subíamos eran para `fine-tuning`, y ahora le pasamos `assistants` ya que los archivos que vamos a subir son para un asistente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "file = client.files.create(\n",
        "  file=open(python_code, \"rb\"),\n",
        "  purpose='assistants'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.file_object.FileObject,\n",
              " FileObject(id='file-HF8Llyzq9RiDfQIJ8zeGrru3', bytes=21, created_at=1701822479, filename='python_code.py', object='file', purpose='assistants', status='processed', status_details=None))"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(file), file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enviar un mensaje al asistente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos el mensaje que le vamos a enviar al asistente, además le indicamos la ID del archivo sobre el que queremos preguntar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread_id,\n",
        "    role=\"user\",\n",
        "    content=\"Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.\",\n",
        "    file_ids=[file.id]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejecutar el asistente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ejecutamos el asistente indicándole que resuelva la duda del usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread_id,\n",
        "  assistant_id=code_interpreter_assistant_id,\n",
        "  instructions=\"Resuleve el problema que te ha planteado el usuario.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.beta.threads.run.Run,\n",
              " Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(run), run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_id = run_WZxT1TUuHT5qB1ZgD34tgvPu\n"
          ]
        }
      ],
      "source": [
        "run_id = run.id\n",
        "print(f\"run_id = {run_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Esperar a que termine de procesar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mientras el asistente está analizando podemos comprobar el estado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.retrieve(\n",
        "  thread_id=thread_id,\n",
        "  run_id=run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.types.beta.threads.run.Run,\n",
              " Run(id='run_WZxT1TUuHT5qB1ZgD34tgvPu', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', cancelled_at=None, completed_at=None, created_at=1701822481, expires_at=1701823081, failed_at=None, file_ids=[], instructions='Resuleve el problema que te ha planteado el usuario.', last_error=None, metadata={}, model='gpt-3.5-turbo-1106', object='thread.run', required_action=None, started_at=1701822481, status='in_progress', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H', tools=[ToolAssistantToolsCode(type='code_interpreter')]))"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(run), run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'in_progress'"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run.status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run completed!\n"
          ]
        }
      ],
      "source": [
        "while run.status != \"completed\":\n",
        "    time.sleep(1)\n",
        "    run = client.beta.threads.runs.retrieve(\n",
        "      thread_id=thread_id,\n",
        "      run_id=run_id\n",
        "    )\n",
        "print(\"Run completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Procesar la respuesta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez el asistente ha terminado podemos ver la respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(openai.pagination.SyncCursorPage[ThreadMessage],\n",
              " SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='La salida del script es simplemente \"Hello world!\", ya que la única instrucción en el script es imprimir esa frase.\\n\\nSi necesitas alguna otra aclaración o ayuda adicional, no dudes en preguntar.'), type='text')], created_at=1701822487, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_nkFbq64DTaSqxIAQUGedYmaX', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='El script proporcionado contiene una sola línea que imprime \"Hello world!\". Ahora procederé a ejecutar el script para obtener su salida.'), type='text')], created_at=1701822485, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_bWT6H2f6lsSUTAAhGG0KXoh7', assistant_id='asst_A2F9DPqDiZYFc5hOC6Rb2y0x', content=[MessageContentText(text=Text(annotations=[], value='Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionaré una explicación detallada del script y su salida.'), type='text')], created_at=1701822482, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_WZxT1TUuHT5qB1ZgD34tgvPu', thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H'), ThreadMessage(id='msg_RjDygK7c8yCqYrjnUPfeZfUg', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.'), type='text')], created_at=1701822481, file_ids=['file-HF8Llyzq9RiDfQIJ8zeGrru3'], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_nfFT3rFjyPWHdxWvMk6jJ90H')], object='list', first_id='msg_JjL0uCHCPiyYxnu1FqLyBgEX', last_id='msg_RjDygK7c8yCqYrjnUPfeZfUg', has_more=False))"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(messages), messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "messages.data[0].content[0].text.value = La salida del script es simplemente \"Hello world!\", ya que la única instrucción en el script es imprimir esa frase.\n",
            "\n",
            "Si necesitas alguna otra aclaración o ayuda adicional, no dudes en preguntar.\n",
            "messages.data[1].content[0].text.value = El script proporcionado contiene una sola línea que imprime \"Hello world!\". Ahora procederé a ejecutar el script para obtener su salida.\n",
            "messages.data[2].content[0].text.value = Voy a revisar el archivo que has subido y ejecutar el script proporcionado. Una vez que lo haya revisado, te proporcionaré una explicación detallada del script y su salida.\n",
            "messages.data[3].content[0].text.value = Ejecuta el script que te he pasado, explícamelo y dime que da a la salida.\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(messages.data)):\n",
        "    for j in range(len(messages.data[i].content)):\n",
        "        print(f\"messages.data[{i}].content[{j}].text.value = {messages.data[i].content[j].text.value}\")\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "maximofn": {
      "date": "2023-12-06",
      "description_en": "🚀 Discover the power of the OpenAI API in this post! 🌟 Learn how to install the OpenAI library ✨ and I'll guide you through the first steps to become an artificial intelligence guru. 🤖 No matter if you're a curious beginner or a coding expert looking for new adventures, this post has everything you need to get started. Get ready to explore the universe of GPT for text generation and DALL-E, image analysis, all with a touch of fun and a lot of innovation! 🎉👩‍💻 Dive into the exciting world of AI and start your journey to unlimited creativity! 🌈💻",
      "description_es": "🚀 ¡Descubre el poder de la API de OpenAI en este post! 🌟 Aprende cómo instalar la librería de OpenAI ✨ y te guiaré en los primeros pasos para convertirte en un gurú de la inteligencia artificial. 🤖 No importa si eres un principiante curioso o un experto en código buscando nuevas aventuras, este post tiene todo lo que necesitas para empezar. Prepárate para explorar el universo de GPT para generación de texto y DALL-E, análisis de imágenes, ¡todo con un toque de diversión y mucha innovación! 🎉👩‍💻 ¡Sumérgete en el emocionante mundo de la IA y comienza tu viaje hacia la creatividad ilimitada! 🌈💻",
      "description_pt": "🚀 Descubra o poder da API OpenAI neste post! 🌟 Aprenda como instalar a biblioteca OpenAI ✨ e eu vou guiá-lo pelos primeiros passos para se tornar um guru da inteligência artificial. 🤖 Não importa se você é um iniciante curioso ou um especialista em codificação em busca de novas aventuras, este post tem tudo o que você precisa para começar. Prepare-se para explorar o universo do GPT para geração de texto e DALL-E, análise de imagens, tudo com um toque de diversão e muita inovação! 🎉👩‍💻 Mergulhe no emocionante mundo da IA e comece sua jornada para a criatividade ilimitada! 🌈💻",
      "end_url": "openai-api",
      "image": "https://images.maximofn.com/openai.webp",
      "image_hover_path": "https://images.maximofn.com/openai.webp",
      "keywords_en": "open ai api, open ai, openai, gpt api, gpt3 api, gpt4 api, gpt4o api, gpt, gpt3, gpt4, gpt4o",
      "keywords_es": "open ai api, open ai, openai, gpt api, gpt3 api, gpt4 api, gpt4o api, gpt, gpt3, gpt4, gpt4o",
      "keywords_pt": "open ai api, open ai, openai, gpt api, gpt3 api, gpt4 api, gpt4o api, gpt, gpt3, gpt4, gpt4o",
      "title_en": "OpenAI API",
      "title_es": "OpenAI API",
      "title_pt": "OpenAI API"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
