{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que est√°n en auge los `LLM`s, no paramos de escuchar el n√∫mero de `token`s que admite cada modelo, pero ¬øqu√© son los `token`s? Son las unidades m√≠nimas de representaci√≥n de las palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para explicar qu√© son los `token`s, primero ve√°moslo con un ejemplo pr√°ctico, vamos a usar el tokenizador de `OpenAI`, llamado [tiktoken](https://github.com/openai/tiktoken). \n",
    "\n",
    "As√≠ que, primero instalamos el paquete:\n",
    "\n",
    "```bash\n",
    "pip install tiktoken\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instalado creamos un tokenizador usando el modelo `cl100k_base`, que en el notebook de ejemplo [How to count tokens with tiktoken](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) explica que es el usado por los modelos `gpt-4`, `gpt-3.5-turbo` y `text-embedding-ada-002`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos una palabra de ejemplo tara tokenizarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_word = \"breakdown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la tokenizamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9137, 2996]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = encoder.encode(example_word)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ha dividido la palabra en 2 `token`s, el `9137` y el `2996`. Vamos a ver a qu√© palabras corresponden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('break', 'down')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1 = encoder.decode([tokens[0]])\n",
    "word2 = encoder.decode([tokens[1]])\n",
    "word1, word2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tokenizador de `OpenAI` ha dividido la palabra `breakdown` en las palabras `break` y `down`. Es decir, ha dividido la palabra en 2 m√°s sencillas.\n",
    "\n",
    "Esto es importante, ya que cuando se dice que un `LLM` admite x `token`s no se refiere a que admite x palabras, sino a que admite x unidades m√≠nimas de representaci√≥n de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tienes un texto y quieres ver el n√∫mero de `token`s que tiene para el tokenizador de `OpenAI`, puedes verlo en la p√°gina [Tokenizer](https://platform.openai.com/tokenizer), que muestra cada `token` en un color diferente\n",
    "\n",
    "![tokenizer](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/tokenizer.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto el tokenizador de `OpenAI`, pero cada `LLM` podr√° usar otro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos dicho, los `token`s son las unidades m√≠nimas de representaci√≥n de las palabras, as√≠ que vamos a ver cu√°ntos tokens distintos tiene `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 100277\n"
     ]
    }
   ],
   "source": [
    "n_vocab = encoder.n_vocab\n",
    "print(f\"Vocab size: {n_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver c√≥mo tokeniza otro tipo de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode(word):\n",
    "    tokens = encoder.encode(word)\n",
    "    decode_tokens = []\n",
    "    for token in tokens:\n",
    "        decode_tokens.append(encoder.decode([token]))\n",
    "    return tokens, decode_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: dog ==> tokens: [18964], decode_tokens: ['dog']\n",
      "Word: tomorrow... ==> tokens: [38501, 7924, 1131], decode_tokens: ['tom', 'orrow', '...']\n",
      "Word: artificial intelligence ==> tokens: [472, 16895, 11478], decode_tokens: ['art', 'ificial', ' intelligence']\n",
      "Word: Python ==> tokens: [31380], decode_tokens: ['Python']\n",
      "Word: 12/25/2023 ==> tokens: [717, 14, 914, 14, 2366, 18], decode_tokens: ['12', '/', '25', '/', '202', '3']\n",
      "Word: üòä ==> tokens: [76460, 232], decode_tokens: ['ÔøΩ', 'ÔøΩ']\n"
     ]
    }
   ],
   "source": [
    "word = \"dog\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"tomorrow...\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"artificial intelligence\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"Python\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"12/25/2023\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"üòä\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo vamos a verlo con palabras en otro idioma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: perro ==> tokens: [716, 299], decode_tokens: ['per', 'ro']\n",
      "Word: perra ==> tokens: [79, 14210], decode_tokens: ['p', 'erra']\n",
      "Word: ma√±ana... ==> tokens: [1764, 88184, 1131], decode_tokens: ['ma', '√±ana', '...']\n",
      "Word: inteligencia artificial ==> tokens: [396, 39567, 8968, 21075], decode_tokens: ['int', 'elig', 'encia', ' artificial']\n",
      "Word: Python ==> tokens: [31380], decode_tokens: ['Python']\n",
      "Word: 12/25/2023 ==> tokens: [717, 14, 914, 14, 2366, 18], decode_tokens: ['12', '/', '25', '/', '202', '3']\n",
      "Word: üòä ==> tokens: [76460, 232], decode_tokens: ['ÔøΩ', 'ÔøΩ']\n"
     ]
    }
   ],
   "source": [
    "word = \"perro\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"perra\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"ma√±ana...\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"inteligencia artificial\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"Python\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"12/25/2023\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")\n",
    "\n",
    "word = \"üòä\"\n",
    "tokens, decode_tokens = encode_decode(word)\n",
    "print(f\"Word: {word} ==> tokens: {tokens}, decode_tokens: {decode_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pdemos ver para palabras similares, en espa√±ol se generan m√°s `token`s que en ingl√©s, por lo que para un mismo texto, con un n√∫mero similar de palabras, el n√∫mero de `token`s ser√° mayor en espa√±ol que en ingl√©s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "maximofn": {
      "date": "2023-12-08",
      "description_es": "Descubre qu√© son los tokens y c√≥mo se dividen las palabras en unidades m√≠nimas de representaci√≥n de las palabras",
      "description_en": "Discover what tokens are and how words are divided into minimum units of word representation",
      "description_pt": "Descubra o que s√£o tokens e como as palavras s√£o divididas em unidades m√≠nimas de representa√ß√£o das palavras",
      "end_url": "tokens",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/tokens.webp",
      "keywords_en": "tokens, token, openai, tiktoken, gpt-4, gpt-3.5-turbo, text-embedding-ada-002",
      "keywords_es": "tokens, token, openai, tiktoken, gpt-4, gpt-3.5-turbo, text-embedding-ada-002",
      "keywords_pt": "tokens, token, openai, tiktoken, gpt-4, gpt-3.5-turbo, text-embedding-ada-002",
      "title_en": "Tokens",
      "title_es": "Tokens",
      "title_pt": "Tokens"
    }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
