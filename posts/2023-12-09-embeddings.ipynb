{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un anterior post sobre [tokens](https://maximofn.com/tokens/), ya vimos la representación mínima de cada palabra. Que corresponde a darle un número a la mínima división de cada palabra.\n",
    "\n",
    "Sin embargo los transformers y por tanto los LLMs, no representan así la información de las palabras, sino que lo hacen mediante `embeddings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver primero dos formas de representar las palabras dentro de los transformers, el `ordinal encoding` y el `one hot encoding`. Y viendo los problemas de estos dos tipos de representaciones podremos llegar hasta los `embeddings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la manera más básica de representar las palabras dentro de los transformers. Consiste en darle un número a cada palabra, o quedarnos con los números que ya tienen asignados los tokens.\n",
    "\n",
    "Sin embargo este tipo de representación tiene dos problemas\n",
    "\n",
    " * Imaginemos que mesa corresponde al token 3, gato al token 1 y perro al token 2. Se podría llegar a suponer que `mesa = gato + perro`, pero no es así. No existe esa relación entre esas palabras. Incluso podríamos pensar que adjudicando los tokens correctos sí podría llegar a darse este tipo de relaciones. Sin embargo este pensamiento se viene abajo con las palabras que tienen más de un significado, como por ejemplo la palabra `banco`\n",
    "\n",
    " * El segundo problema es que las redes neuronales internamente hacen muchos cálculos numéricos, por lo que podría darse el caso en el que si mesa tiene el token 3, tenga internamente más importancia que la palabra gato que tiene el token 1.\n",
    "\n",
    "De modo que este tipo de representación de las palabras se puede descartar muy rápidamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí lo que se hace es usar vectores de `N` dimensiones. Por ejemplo vimos que OpenAI tiene un vocabulario de `100277` tokens distintos. Por lo que si usamos `one hot encoding`, cada palabra se representaría con un vector de `100277` dimensiones.\n",
    "\n",
    "Sin embargo el one hot encodding tiene otro dos grandes problemas\n",
    "\n",
    " * No tiene en cuenta la relación entre las palabras. Por lo que si tenemos dos palabras que son sinónimos, como por ejemplo `gato` y `felino`, tendríamos dos vectores distintos para representarlas. \n",
    " En el lenguaje la relación entre las palabras es muy importante, y no tener en cuenta esta relación es un gran problema.\n",
    "\n",
    " * El segundo problema es que los vectores son muy grandes. Si tenemos un vocabulario de `100277` tokens, cada palabra se representaría con un vector de `100277` dimensiones. Esto hace que los vectores sean muy grandes y que los cálculos sean muy costosos. Además estos vectores van a ser todo ceros, excepto en la posición que corresponda al token de la palabra. Por lo que la mayoría de los cálculos van a ser multiplicaciones por cero, que son cálculos que no aportan nada. Así que vamos a tener un montón de memoria asignada a vectores en los que solo se tiene un 1 en una posición determinada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los word embeddings se intenta solucionar los problemas de los dos tipos de representaciones anteriores. Para ello se usan vectores de `N `dimensiones, pero en este caso no se usan vectores de 100277 dimensiones, sino que se usan vectores de muchas menos dimensiones. Por ejemplo veremos que OpenAI usa `1536` dimensiones.\n",
    "\n",
    "Cada una de las dimensiones de estos vectores representan una característica de la palabra. Por ejemplo una de las dimensiones podría representar si la palabra es un verbo o un sustantivo. Otra dimensión podría representar si la palabra es un animal o no. Otra dimensión podría representar si la palabra es un nombre propio o no. Y así sucesivamente.\n",
    "\n",
    "Sin embargo estas características no se definen a mano, sino que se aprenden de forma automática. Durante el entrenamiento de los transformers, se van ajustando los valores de cada una de las dimensiones de los vectores, de modo que se aprenden las características de cada una de las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al hacer que cada una de las dimensiones de las palabras represente una característica de la palabra, se consigue que las palabras que tengan características similares, tengan vectores similares. Por ejemplo las palabras `gato` y `felino` tendrán vectores muy similares, ya que ambas son animales. Y las palabras `mesa` y `silla` tendrán vectores similares, ya que ambas son muebles.\n",
    "\n",
    "En la siguiente imagen podemos ver una representación de 3 dimensiones depalabras, y podemos ver que todas las palabras relacionadas con `school` están cerca, todas las palabras relacionadas con `food` están cerca y todas las palabras relacionadas con `ball` están cerca.\n",
    "\n",
    "![word_embedding_3_dimmension](http://maximofn.com/wp-content/uploads/2023/12/word_embedding_3_dimmension.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tener que cada una de las dimensiones de los vectores represente una característica de la palabra, consigue que podamos hacer operaciones con palabras. Por ejemplo si a la palabra `rey` se le resta la palabra `hombre` y se le suma la palabra `mujer`, obtenemos una palabra muy parecida a la palabra `reina`. Más adelante lo comprobaremos con un ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similitud entre palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como cada una de las palabras se representa mediante un vector de N dimensiones, podemos calcular la similitud entre dos palabras. Para ello se usa la función de similitud del coseno o `cosine similarity`.\n",
    "\n",
    "Si dos palabras están cercanas en el espacio vectorial, quiere decir que el álgulo que hay entre sus vectores es pequeño, por lo que su coseno es cercano a 1. Si hay un ángulo de 90 grados entre los vectores, el coseno es 0, es decir que no hay similitud entre las palabras. Y si hay un ángulo de 180 grados entre los vectores, el coseno es -1, es decir que las palabras son opuestas.\n",
    "\n",
    "![cosine similarity](http://maximofn.com/wp-content/uploads/2023/12/cosine_similarity-scaled.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo con embeddings de OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que sabemos lo que son los `embeddings`, veamos unos ejemplos con los `embeddings` que nos proporciona la `API` de `OpenAI`.\n",
    "\n",
    "Para ello primero tenemos que tener instalado el paquete de `OpenAI`\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import torch\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos una `API key` de OpenAI. Para ello, nos dirigimos a la página de [OpenAI](https://openai.com/), y nos registramos. Una vez registrados, nos dirigimos a la sección de [API Keys](https://platform.openai.com/api-keys), y creamos una nueva `API Key`.\n",
    "\n",
    "![open ai api key](https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"Pon aquí tu API key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos que modelo de embeddings queremos usar. En este caso vamos a usar `text-embedding-ada-002` que es el que recomienda `OpenAI` en su documentación de [embeddings](https://platform.openai.com/docs/guides/embeddings/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_openai = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un cliente de la `API`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_openai = OpenAI(api_key=api_key, organization=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo son los `embeddings` de la palabra `Rey`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536]),\n",
       " tensor([-0.0103, -0.0005, -0.0189,  ..., -0.0009, -0.0226,  0.0045]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"Rey\"\n",
    "embedding_openai = torch.Tensor(client_openai.embeddings.create(input=word, model=model_openai).data[0].embedding)\n",
    "\n",
    "embedding_openai.shape, embedding_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos obtenemos un vector de `1536` dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones con palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a obtener los embeddings de las palabras `rey`, `hombre`, `mujer` y `Reina`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_openai_rey = torch.Tensor(client_openai.embeddings.create(input=\"rey\", model=model_openai).data[0].embedding)\n",
    "embedding_openai_hombre = torch.Tensor(client_openai.embeddings.create(input=\"hombre\", model=model_openai).data[0].embedding)\n",
    "embedding_openai_mujer = torch.Tensor(client_openai.embeddings.create(input=\"mujer\", model=model_openai).data[0].embedding)\n",
    "embedding_openai_reina = torch.Tensor(client_openai.embeddings.create(input=\"reina\", model=model_openai).data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536]),\n",
       " tensor([-0.0110, -0.0084, -0.0115,  ...,  0.0082, -0.0096, -0.0024]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_openai_reina.shape, embedding_openai_reina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a obtener el embedding resultante de restarle a `rey` el embedding de `hombre` y sumarle el embedding de `mujer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_openai = embedding_openai_rey - embedding_openai_hombre + embedding_openai_mujer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536]),\n",
       " tensor([-0.0226, -0.0323,  0.0017,  ...,  0.0014, -0.0290, -0.0188]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_openai.shape, embedding_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último comparamos el resultado obtenido con el embedding de `reina`. Para ello usamos la función de `cosine_similarity` que nos proporciona la librería `pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_openai: 0.7564167976379395\n"
     ]
    }
   ],
   "source": [
    "similarity_openai = cosine_similarity(embedding_openai.unsqueeze(0), embedding_openai_reina.unsqueeze(0)).item()\n",
    "\n",
    "print(f\"similarity_openai: {similarity_openai}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos es un valor muy cercano a 1, por lo que podemos decir que el resultado obtenido es muy parecido al embedding de `reina`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos palabras en inglés, obtenemos un resultado más cercano a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_openai_rey = torch.Tensor(client_openai.embeddings.create(input=\"king\", model=model_openai).data[0].embedding)\n",
    "embedding_openai_hombre = torch.Tensor(client_openai.embeddings.create(input=\"man\", model=model_openai).data[0].embedding)\n",
    "embedding_openai_mujer = torch.Tensor(client_openai.embeddings.create(input=\"woman\", model=model_openai).data[0].embedding)\n",
    "embedding_openai_reina = torch.Tensor(client_openai.embeddings.create(input=\"queen\", model=model_openai).data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_openai = embedding_openai_rey - embedding_openai_hombre + embedding_openai_mujer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_openai: tensor([0.8849])\n"
     ]
    }
   ],
   "source": [
    "similarity_openai = cosine_similarity(embedding_openai.unsqueeze(0), embedding_openai_reina.unsqueeze(0))\n",
    "print(f\"similarity_openai: {similarity_openai}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es normal, ya que el modelo de OpenAi ha sido entrenado con más txtos en inglés que en español"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
