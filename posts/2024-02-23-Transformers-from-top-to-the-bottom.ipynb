{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformers - from top to the bottom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este post vamos a ver c√≥mo funcionan los Transformers de arriba a abajo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer como una caja negra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La arquitectura transformer se cre√≥ para el problema de traducci√≥n, por lo que vamos a explicarlo para ese problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imaginemos el transformer como una caja negra, a la que le entra una frase en un idioma y saca la misma frase traducida en otro idioma.\n",
        "\n",
        "![Transformer - black box](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-black-box.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pero como hemos visto en el post de [tokens](https://maximofn.com/tokens/), los modelos de lenguaje no entienden las palabras como nosotros, sino que necesitan n√∫meros para poder realizar las operaciones. Por lo que la frase en el idioma original se tiene que convertir a tokens mediante un tokenizador, y a la salida necesitamos un detokenizador para convertir los tokens de salida a palabras\n",
        "\n",
        "![Transformer - black box - tokenizers](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-black-box-tokenizers.webp)\n",
        "\n",
        "De modo que el tokenizador crea una secuencia de $n_{input-tokens}$ tokens, y el detokenizador recibe una secuencia de $n_{output-tokens}$ tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el post de [embeddings](https://maximofn.com/embeddings/) vimos que los embeddings son una forma de representar las palabras en un espacio vectorial. Por lo que los tokens de entrada se pasan por una capa de embeddings para convertirlos en vectores.\n",
        "\n",
        "En un resumen r√°pido, el proceso de embedding consiste en convertir una secuencia de n√∫meros (tokens) en una secuencia de vectores. De manera que se crea un nuevo espacio vectorial en el que las palabras que tengan similitud sem√°ntica estar√°n cerca.\n",
        "\n",
        "![word_embedding_3_dimmension](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/word_embedding_3_dimmension.webp)\n",
        "\n",
        "Si ten√≠amos $n_{input-tokens}$ tokens, ahora tenemos $n_{input-tokens}$ vectores. Cada uno de esos vectores tiene una longitud de $d_{model}$. Es decir, cada token se convierte a un vector que representa ese token en un espacio vectorial de $d_{model}$ dimensiones.\n",
        "Por tanto despu√©s de pasar por la capa de embeddings, la secuencia de $n_{input-tokens}$ tokens se convierte en una matriz de ($n_{input-tokens}$ x $d_{model}$).\n",
        "\n",
        "![Transformer - black box - input embeddings](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-black-box-input-embeddings.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoder - decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos visto el transformer actuando como una caja negra, pero en realidad el transformer es una arquitectura que se compone de dos partes, un encoder y un decoder.\n",
        "\n",
        "![Transformer - encoder-decoder](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder.png)\n",
        "\n",
        "El encoder se encarga de comprimir la informaci√≥n de la frase de entrada, crea un espacio latente donde est√° esa informaci√≥n de la frase de entrada es comprimida. A continuaci√≥n, esa informaci√≥n comprimida entra al decoder, que sabe convertir esa informaci√≥n comprimida en una frase del idioma de salida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y ¬øc√≥mo convierte el decoder esa informaci√≥n comprimida en una frase del idioma de salida? Pues token a token. Para entenderlo mejor vamos a olvidarnos de los tokens de salida por un momento, vamos a imaginar que tenemos esta arquitectura\n",
        "\n",
        "![Transformer - encoder-decoder (no detokenizer)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-no-detokenizer.webp)\n",
        "\n",
        "Es decir, la frase del idioma original se convierte a tokens, estos tokens se convierten a embeddings, que entran al encoder, este comprime la informaci√≥n, el decoder la coge y la convierte en palabras del idioma de salida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De modo que el decoder va generando una palabra nueva a la salida en cada paso\n",
        "\n",
        "![Transformer - encoder-decoder (no detokenizer)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer).gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pero ¬øc√≥mo sabe el decoder c√∫al es la palabra que tiene que generar cada vez? Porque se le est√° pasando la frase que ya ha traducido, y en cada paso va generando la siguiente palabra. Es decir, en cada paso el decoder recibe la frase que ha traducido hasta el momento, y genera la siguiente palabra.\n",
        "\n",
        "Pero aun as√≠, ¬øc√≥mo sabe que tiene que generar la primera palabra? Porque se le pasa una palabra especial que significa \"empezar a traducir\", y a partir de ah√≠ va generando las siguientes palabras.\n",
        "\n",
        "Y por √∫ltimo, ¬øc√≥mo sabe el transformer que tiene que dejar de generar palabras? Porque cuando termina de traducir genera una palabra especial que significa \"fin de la traducci√≥n\", que cuando vuelve a entrar en el transformer hace que no genere m√°s palabras.\n",
        "\n",
        "![Transformer - encoder-decoder (no detokenizer) (input)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer)%20(input).gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que lo hemos entendido con palabras, que es m√°s sencillo, vamos a volver a colocar el detokenizador a la salida\n",
        "\n",
        "![Transformer - encoder-decoder](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder.png)\n",
        "\n",
        "Por tanto el decoder ir√° generando tokens. Para saber que tiene que empezar una frase se le mete un token especial comunmente llamado `SOS` (Start Of Sentence), y para saber que tiene que terminar genera otro token especial comunmente llamado `EOS` (End Of Sentence).\n",
        "\n",
        "Y al igual que el encoder, el token de entrada tiene que pasar por una capa de embedding para convertir los tokens en representaciones vectoriales.\n",
        "\n",
        "Suponiendo que cada token equivale a una palabra, el proceso de traducci√≥n ser√≠a el siguiente\n",
        "\n",
        "![Transformer - encoder-decoder (detokenizer)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(detokenizer).gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De momento tenemos esta arquitectura\n",
        "\n",
        "![Transformer - encoder-decoder (detokenizer)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-detokenizer-2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Projection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos dicho que al decoder le entra un token que pasa por la capa de embedding `Output embedding`.\n",
        "\n",
        "El `Output decoder` crea un vector por cada token, por lo que a la salida del `Output decoder` tenemos una matriz de ($n_{output-tokens}$ x $d_{model}$).\n",
        "\n",
        "El decoder hace operaciones, pero saca una matriz con la misma dimensi√≥n. As√≠ de necesita convertir esa matriz en un token y eso lo hace mediante una capa lineal que a la salida genera un array con la misma dimensi√≥n que los posibles tokens que hay en el lenguaje al que se quiere traducir (vocabulario de salida).\n",
        "\n",
        "Ese array corresponde a los logits de cada posible token, por lo que a continuaci√≥n se pasa por una capa softmax que convierte esos logits en probabilidades. Es decir, tendremos la probabilidad de que cada token sea el siguiente token.\n",
        "\n",
        "![Transformer - projection](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-projection.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoder y decoder x6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el paper original usan 6 capas para el encoder y otras 6 capas para el decoder. No hay ninguna raz√≥n para que sean 6, supongo que probaron varios valores y este fue el que mejor les funcion√≥.\n",
        "\n",
        "A cada uno de los decoder le entra la salida del √∫ltimo encoder\n",
        "\n",
        "![Transformer - encoder-decoder (x6)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-x6.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para simplificar el diagrama, lo representaremos as√≠ a partir de ahora\n",
        "\n",
        "![Transformer - encoder-decoder (Nx)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-Nx.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention - Feed forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ir empezando a ver qu√© ha dentro del encoder y el decoder. B√°sicamente lo que hay es un mecanismo de atenci√≥n y una capa feed forward.\n",
        "\n",
        "![Transformer - encoder-decoder - attention-ff](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-encoder-decoder-attention-ff.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Atenci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver que en los mecanismos de atenci√≥n entran 3 flechas. Esto ya lo veremos m√°s adelante cuando veamos en profundidad c√≥mo funcionan los mecanismos de atenci√≥n.\n",
        "\n",
        "Pero de momento podemos decir que son operaciones que se realizan para poder obtener la relaci√≥n que existe entre los tokens (y por tanto, la relaci√≥n que existe entre las palabras).\n",
        "\n",
        "Antes de los transformers, para el problema de traducci√≥n se usaban las redes neuronales recurrentes, que consist√≠an en redes a las que les entraba un token, lo procesaban y generaban otro token de salida. A continuaci√≥n le entraba un segundo token, lo procesaban y sacaban otro token, y as√≠ sucesivamente con todos los tokens de la secuencia de entrada. El problema de estas redes es que cuando las frases eran muy largas, cuando se estaba en los √∫ltimos tokens, la red se \"olvidaba\" de los primeros tokens. Por ejemplo en frases muy largas, podr√≠a pasar que se cambiase el g√©nero del sujeto a lo largo de la frase traducida. Y esto es porque despu√©s de muchos tokens, la red se hab√≠a olvidado si el sujeto era masculino o femenino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para solucionar esto, en el mecanismo de atenci√≥n de los transformers entra la secuencia entera y de una sola vez se calculan las relaciones (atenci√≥n) entre todos los tokens. \n",
        "\n",
        "Esto es muy potente, ya que en un solo c√°lculo se obtiene la relaci√≥n entre todos los tokens, sea lo larga que sea la secuencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aunque esto es una gran ventaja y es lo que ha hecho que los transformers ahora se utilicen en la mayor√≠a de las mejores redes modernas, tambi√©n es su mayor desventaja, ya que el c√°lculo de la atenci√≥n es muy costoso computacionalmente. Ya que requiere unas multiplicaciones matriciales muy grandes.\n",
        "\n",
        "Esas multiplicaciones se realizan entre matrices que corresponden a los embeddings de cada uno de los tokens por ellas mismas. Es decir, la matriz que representa los embeddings de los tokens se multiplica por ella misma. Para poder realizar esta multiplicaci√≥n hay que rotar una de las matrices (requisitos del √°lgebra para poder multiplicar matrices). As√≠ que se multiplica una matriz por ella misma, si la secuencia de entrada tiene m√°s tokens, las matrices que se multiplican son m√°s grandes, una en alto y otra en ancho, por lo que la memoria necesaria para almacenar esas matrices crece de forma cuadr√°tica.\n",
        "\n",
        "Por lo tanto, a medida que aumenta la longitud de las secuencias, la cantidad de memoria necesaria para almacenar esas matrices crece de forma cuadr√°tica. Y esto es un gran limitante a d√≠a de hoy, la cantidad de memoria que tienen las GPUs, que es donde se suelen realizar esas multiplicaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el encoder se usa una sola capa de atenci√≥n para sacar las relaciones entre los tokens de entrada\n",
        "\n",
        "En el decoder se utilizan dos capas de atenci√≥n, una para sacar las relaciones entre los tokens de salida, y otra para sacar las relaciones entre los tokens del encoder y los del decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feed forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Despu√©s de la capa de atenci√≥n, la secuencia entra en una capa `Feed forward` que tiene dos prop√≥sitos\n",
        "\n",
        " * Uno es a√±adir no linealidades. Como hemos explicado, la atenci√≥n se consigue mediante multiplicaciones matriciales de los tokens de las secuencias de entrada. Pero si a una red no se le aplican capas no lineales, al final, toda la arquitectura se podr√≠a resumir en unos pocos c√°lculos lineales. Por lo que las redes neuronales no podr√≠an resolver problemas no lineales. De modo que se a√±ade esta capa para a√±adir no linealidad\n",
        "\n",
        " * Otro es la extracci√≥n de caracter√≠sticas. Aunque la atenci√≥n ya extrae caracter√≠sticas, estas son caracter√≠sticas de las relaciones entre los tokens. Pero esta capa `Feed forward` se encarga de extraer caracter√≠sticas de los tokens en s√≠. Es decir, de cada token se extraen caracter√≠sticas que se consideran importantes para el problema que se est√° resolviendo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos explicado que en la capa de atenci√≥n se obtienen las relaciones entre los tokens, que esa relaci√≥n se calcula mediante multiplicaciones matriciales y que esas multiplicaciones se realizan entre la matriz de embeddings por ella misma. Por lo que en las frases `El gato come pescado` y `El pescado come gato`, la relaci√≥n entre `el` y `gato` es la misma en ambas frases, ya que la relaci√≥n se calcula mediante multiplicaciones matriciales de los embeddings de `el` y `gato`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sin embargo, en la primera `el` se refiere a el `gato`, mientras que en la segunda `el` se refiere a el `pescado`. Por lo que adem√°s de las relaciones entre las palabras necesitamos tener alg√∫n mecanismo que nos indique su posici√≥n en la frase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el paper proponen meter un mecanismo de atenci√≥n que se encarga de sumar unos valores a los vectores de embedding\n",
        "\n",
        "![Transformer - positional encoding](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-positional-encoding.webp)\n",
        "\n",
        "D√≥nde la f√≥rmula para calcular esos valores es\n",
        "\n",
        "![Transformer - positional encoding (formula)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-positional-encoding-formula.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como esto as√≠ en fr√≠o es un poco dif√≠cil de entender, vamos a ver c√≥mo ser√≠a una distribuci√≥n de valores del `positional encoding`\n",
        "\n",
        "![Transformer - positional encoding (diagram)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-positional-encoding-diagram.webp)\n",
        "\n",
        "Al primer token se le van a sumar los valores de la primera fila (la de m√°s abajo), al segundo token los de la segunda fila, y as√≠ sucesivamente, lo que provocan un cambio en los embeddings como se ve en la figura. Visto en dos dimensiones se aprecian las ondas que se van sumando.\n",
        "\n",
        "Estas ondas hacen que, cuando se realizan los c√°lculos de atenci√≥n, las palabras m√°s cercanas tengan m√°s relaci√≥n que las palabras m√°s lejanas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pero podemos pensar una cosa, si el proceso de embedding consiste en crear un espacio vectorial en el que las palabras con el mismo significado sem√°ntico est√©n cerca, ¬øno se estar√≠a rompiendo esa relaci√≥n si se suman valores a los embeddings?\n",
        "\n",
        "Si nos fijamos de nuevo en el ejemplo de espacio vectorial de antes\n",
        "\n",
        "![word_embedding_3_dimmension](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/word_embedding_3_dimmension.webp)\n",
        "\n",
        "Podemos ver que los valores van m√°s o menos de -1000 a 1000 en cada eje, mientras que la gr√°fica de distribuci√≥n\n",
        "\n",
        "![Transformer - positional encoding (diagram)](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-positional-encoding-diagram.webp)\n",
        "\n",
        "Los valores van de -1 a 1, ya que es el rango de las funciones seno y coseno.\n",
        "\n",
        "Por lo tanto, estamos variando en un rango de entre -1 y 1 los valores de los embeddings que tienen dos o tres √≥rdenes de magnitud m√°s, por lo que la variaci√≥n va a ser muy peque√±a en comparaci√≥n con el valor de los embeddings.\n",
        "\n",
        "De modo que ya tenemos una manera de saber la relaci√≥n de la posici√≥n de los tokens en la frase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add & Norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solo nos queda un bloque de alto nivel, y son las capas `Add & Norm`\n",
        "\n",
        "![Transformer - Add & norm](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-Add-norm.webp)\n",
        "\n",
        "Estas son capas que se a√±aden despu√©s de cada capa de atenci√≥n y de cada capa feed forward. Esta capa suma la salida y la entrada de una capa. A esto se llama conexiones residuales y tiene las siguientes ventajas\n",
        "\n",
        " * Durante el entrenamiento:\n",
        "\n",
        "   * Reducen el problema del desvanecimiento del gradiente: Cuando una red neuronal es muy grande, en el proceso de entrenamiento, los gradientes se van haciendo cada vez m√°s peque√±os seg√∫n se profundizan en las capas. Esto hace que las capas m√°s profundas no puedan actualizar bien sus pesos. Las conexiones residuales permiten el paso de los gradientes directamente a trav√©s de las capas, lo que ayuda a mantenerlos lo suficientemente grandes para que el modelo pueda seguir aprendiendo, incluso en las capas m√°s profundas.\n",
        "\n",
        "   * Permiten el entrenamiento de redes m√°s profundas: Al ayudar a mitigar el problema del desvanecimiento del gradiente, las conexiones residuales tambi√©n facilitan el entrenamiento de redes m√°s profundas, lo cual puede llevar a un mejor rendimiento.\n",
        "\n",
        " * Durante la inferencia:\n",
        "\n",
        "   * Permiten la transmisi√≥n de informaci√≥n entre diferentes capas: Como las conexiones residuales permiten que la salida de cada capa se convierta en la suma de la entrada y la salida de la capa, la informaci√≥n de las capas m√°s profundas se transmita a las capas de m√°s alto nivel. Esto puede ser beneficioso en muchas tareas, especialmente en las que la informaci√≥n de bajo y alto nivel puede ser √∫til.\n",
        "\n",
        "   * Mejoran la robustez del modelo: Dado que las conexiones residuales permiten que las capas aprendan mejor en las capas m√°s profundas, los modelos con conexiones residuales pueden ser m√°s robustos a perturbaciones en los datos de entrada.\n",
        "\n",
        "   * Permiten la recuperaci√≥n de informaci√≥n perdida: Si alguna informaci√≥n se pierde durante la transformaci√≥n en alguna capa, las conexiones residuales pueden permitir que esta informaci√≥n sea recuperada en las capas posteriores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta capa se llama `Add & Norm`, hemos visto el `Add`, veamos el `Norm`. La normalizaci√≥n se a√±ade para que, al sumar la entrada y la salida, no se disparen los valores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya hemos visto todas las capas de alto nivel del transformer\n",
        "\n",
        "![transformer](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp)\n",
        "\n",
        "Por lo que podemos entrar a ver la parte m√°s importante y que le da nombre al paper, los mecanismos de atenci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mecanismos de atenci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de ver el verdadero mecanismo de atenci√≥n, tenemos que ver el multi-head attention\n",
        "\n",
        "![Transformer - multi-head attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-multi-head-attention.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cuando hemos explicado las capas de alto nivel, hemos visto que en las capas de atenci√≥n entraban 3 flechas, estas son `Q`, `K` y `V`. Son matrices que corresponden a la informaci√≥n de los tokens, en el caso del mecanismo de atenci√≥n del encoder, corresponden a los tokens de la frase del idioma original, y en el caso de la capa de atenci√≥n del decoder, corresponden a los tokens de la frase que se ha traducido hasta el momento y de la salida del encoder.\n",
        "\n",
        "En realidad ahora nos da igual el origen de los tokens, solo qu√©date con la idea de que corresponden a tokens. Como hemos explicado los tokens se convierten a embeddings, por lo que `Q`, `K` y `V` son matrices de tama√±o ($n_{tokens}$ x $d_{model}$). Normalmente la dimensi√≥n del embedding ($d_{model}$) suele ser un n√∫mero grande, como 512, 1024, 2048, etc (no tiene por qu√© ser una potencia de 2, son solo ejemplos).\n",
        "\n",
        "Hemos explicado que los embeddings son representaciones vectoriales de los tokens. Es decir, los tokens se convierten a espacios vectoriales en los que las palabras con significado sem√°ntico similar est√°n cerca.\n",
        "\n",
        "Por tanto, de todas esas dimensiones, unas pueden estar relacionadas con caracter√≠sticas morfol√≥gicas, otras con caracter√≠sticas sint√°cticas, otras con caracter√≠sticas sem√°nticas, etc. Por lo que tiene sentido que se calculen los mecanismos de atenci√≥n entre dimensiones de los embeddings de caracter√≠sticas similares.\n",
        "\n",
        "Recordemos que los mecanismos de atenci√≥n buscan similitud entre palabras, por lo que tiene sentido que se busque similitud entre caracter√≠sticas semejantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es por esto, que antes de calcular los mecanismos de atenci√≥n se separan las dimensiones de los embeddings en grupos de caracter√≠sticas similares, y se calculan los mecanismos de atenci√≥n entre esos grupos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "¬øY c√≥mo se hace esta separaci√≥n? Habria que buscar las dimensiones similares, pero hacer esto en un espacio de 512, 1024, 2048, etc dimensiones es muy complicado. Adem√°s que no se puede saber que caracter√≠sticas son similares y que en cada caso cambiaran las caracter√≠sticas que se consideran similares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por lo que se utilizan proyecciones lineales para separar las dimensiones en grupos. Es decir, se pasan los embeddings por capas lineales que los separan en grupos de caracter√≠sticas similares. De esta manera, durante el entrenamiento del transformer ir√°n cambiando los pesos de las capas lineales hasta llegar a un punto en el que la agrupaci√≥n se haga de una manera √≥ptima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora podemos tener la duda de en cu√°ntos grupos dividir. En el paper original se dividen en 8 grupos, pero no hay ninguna raz√≥n para que sean 8, supongo que probaron varios valores y este fue el que mejor les funcion√≥."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez se han dividido los embeddings en grupos similares y se ha calculado la atenci√≥n en los distintos grupos se concatenan los resultados. Esto es l√≥gico, supongamos que tenemos un embedding de 512 dimensiones, y lo dividimos en 8 grupos de 64 dimensiones, si calculamos la atenci√≥n en cada uno de los grupos, tendremos 8 matrices de atenci√≥n de 64 dimensiones, si las concatenamos tendremos una matriz de atenci√≥n de 512 dimensiones, que es la misma dimensi√≥n que ten√≠amos al principio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pero la concatenaci√≥n hace que todas las caracter√≠sticas est√©n juntas. Las primeras 64 dimensiones corresponden a una caracter√≠stica, las siguientes 64 a otra, y as√≠ sucesivamente. As√≠ que para volver a mezclarlas se vuelve a pasar una capa lineal que mezcla todas las caracter√≠sticas. Y esa mezcla se va aprendiendo durante el entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scale dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Llegamos a la parte m√°s importante del transformer, el mecanismo de atenci√≥n, el `scaled dot product attention`\n",
        "\n",
        "![Transformer - scaled dot product attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)\n",
        "![Transformer - scaled dot product attention formula](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como hemos visto, en la arquitectura del Transformer hay tres mecanismos de atenci√≥n\n",
        "\n",
        "![transformer](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp)\n",
        "\n",
        "El del encoder, el del decoder y el del encoder-decoder. As√≠ que vamos a explicarlos por separado, porque aunque son casi iguales, tienen unas peque√±as diferencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoder scale dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver otra vez el diagrama de bloques y la f√≥rmula\n",
        "\n",
        "![Transformer - scaled dot product attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)\n",
        "![Transformer - scaled dot product attention formula](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero vamos a entender por qu√© entran tres flechas a las capas de atenci√≥n. Si vemos la arquitectura del transformer, la entrada del encoder se divide en tres y entra a la capa de atenci√≥n\n",
        "\n",
        "![transformer](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp)\n",
        "\n",
        "Por lo que `K`, `Q` y `V` son el resultado del embedding y el positional encoding. Se mete a la capa de atenci√≥n la misma matriz tres veces. Tenemos que recordar que esa matriz consist√≠a en una lista con todos los tokens ($n_{tokens}$), y cada token se convert√≠a en un vector de embeddings de dimensi√≥n $d_{model}$, por lo que la dimensi√≥n de la matriz ser√° ($n_{tokens}$ x $d_{model}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El significado de `K`, `Q` y `V` proviene de las bases de datos `key`, `query` y `value`. Al mecanismo de atenci√≥n se le pasan las matrices `Q` y `K`, es decir, se le pasa la pregunta y la clave, y a la salida se obtiene la matriz `V`, es decir, la respuesta.\n",
        "\n",
        "Vamos a ver cada bloque por separado y entenderemos mejor esto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este bloque corresponde a la multiplicaci√≥n matricial de las matrices `Q` y `K`. Pero para poder realizar esta operaci√≥n hay que hacerla con la matriz transpuesta de `K`. Ya que como las dos matrices tienen dimensi√≥n ($n_{tokens}$ x $d_{model}$), para poder multiplicarlas, la matriz `K` tiene que estar traspuesta.\n",
        "\n",
        "Por lo que tendremos una multiplicaci√≥n de una matriz de dimensi√≥n ($n_{tokens}$ x $d_{model}$) por otra matriz de dimensi√≥n ($d_{model}$ x $n_{tokens}$), por lo que el resultado ser√° una matriz de dimensi√≥n ($n_{tokens}$ x $n_{tokens}$).\n",
        "\n",
        "![Transformer - matmul](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul.webp)\n",
        "\n",
        "Como podemos ver, el resultado es una matriz donde la diagonal es la multiplicaci√≥n del embedding de cada token por s√≠ mismo, y el resto de posiciones son las multiplicaciones entre los embeddings de cada token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vamos a ver por qu√© se hace esta multiplicaci√≥n. En en alterior post [Medida de similitud entre embeddings](https://maximofn.com/embeddings-similarity/) vimos que una manera de obtener la similitud entre dos vectores de embeddings es mediante el c√°lculo del coseno\n",
        "\n",
        "En la figura anterior se puede ver que la multiplicaci√≥n entre las matrices `Q` y `K` corresponde a la multiplicaci√≥n de los embeddings de cada token. La multiplicaci√≥n entre dos vectores se realiza de la siguiente manera\n",
        "\n",
        "$$\\mathbf{U} \\cdot \\mathbf{V} = |\\mathbf{U}| \\cdot |\\mathbf{V}| \\cos(\\theta)$$\n",
        "\n",
        "Es decir, tenemos la multiplicaci√≥n de las normas por su coseno. Si los vectores fuesen unitarios, es decir, que sus normas sean 1, la multiplicaci√≥n de dos vectores ser√≠a igual al coseno entre ambos vectores, que es una de las medidas de similitud entre vectores.\n",
        "\n",
        "Por lo tanto, como en cada posici√≥n de la matriz resultante tenemos la multiplicaci√≥n entre los vectores de embeddings de cada token, en realidad, cada posici√≥n de la matriz representar√° la similitud entre cada token.\n",
        "\n",
        "Recordemos lo que eran los embeddings, los embeddings eran representaciones vectoriales de los tokens en un espacio vectorial, donde los tokens con similitud sem√°ntica est√°n cerca.\n",
        "\n",
        "De modo que con esta multiplicaci√≥n hemos obtenido una matriz de similitud entre los tokens de la frase\n",
        "\n",
        "![Transformer - matmul - similarity matrix](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul-similarity-matrix.webp)\n",
        "\n",
        "Los elementos de la diagonal tienen m√°xima similitud (verde), los de las esquinas tienen m√≠nima similitud (rojo), y el resto de elementos tienen similitud intermedia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volvemos a ver el diagrama del scaled dot product attention y su f√≥rmula\n",
        "\n",
        "![Transformer - scaled dot product attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)\n",
        "![Transformer - scaled dot product attention formula](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hab√≠amos dicho que si al multiplicar `Q` por `K` realiz√°bamos la multiplicaci√≥n entre los vectores de embeddings, y que si esos vectores tuviesen norma 1, el resultado ser√≠a la similitud entre los vectores. Pero como los vectores no tienen norma 1, el resultado puede tener valores muy altos, por lo que se normaliza dividiendo por la ra√≠z cuadrada de la dimensi√≥n de los vectores de embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Mask (opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El enmascaramiento es opcional y en el encoder no se usa, por lo que de momento no lo explicamos para no liar\n",
        "\n",
        "![Transformer - scaled dot product attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)\n",
        "![Transformer - scaled dot product attention formula](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aunque hemos dividido por la ra√≠z cuadrada de la dimensi√≥n de los vectores de embeddings, nos vendr√≠a muy bien que la similitud entre los vectores de embeddings vaya entre los valores 0 y 1, as√≠ que para asegurarnos eso, pasamos por una capa softmax.\n",
        "\n",
        "![Transformer - scaled dot product attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)\n",
        "![Transformer - scaled dot product attention formula](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)\n",
        "\n",
        "![Transformer - matmul - similarity matrix softmax](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul-similarity-matrix-softmax.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Matmul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que tenemos una matriz de similitud entre los vectores de embeddings, vamos a multiplicarla por la matriz `V`, que representa los embeddings de los tokens.\n",
        "\n",
        "![Transformer - matmul2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Haciendo la multiplicaci√≥n obtenemos\n",
        "\n",
        "![Transformer - matmul2 result](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-matmul2-result.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtenemos una matriz con una mezcla de los embeddings con su similitud. En cada fila obtenemos una mezcla de los embeddings, donde cada elemento del embedding est√° ponderado en funci√≥n de la similitud del token de esa fila con el resto de tokens.\n",
        "\n",
        "Adem√°s volvemos a tener una matriz de tama√±o ($n_{tokens}$ x $d_{model}$), que es la misma dimensi√≥n que ten√≠amos al principio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En resumen, podemos decir que el `scaled dot product attention` es un mecanismo que calcula la similitud entre los tokens de una frase, y a partir de esa similitud, calcula una matriz de salida que corresponde a una mezcla de embeddings ponderada en funci√≥n de la similitud de los tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decoder masked scale dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volvemos a ver la arquitectura del transformer\n",
        "\n",
        "![transformer](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos en este caso el `scaled dot product attention` tiene la palabra `masked`. Primero vamos a explicar el porqu√© de la necesidad de este enmascaramiento, y despu√©s veremos c√≥mo se hace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ¬øPor qu√© enmascarar?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como hemos dicho el transformer se ide√≥ inicialmente como un traductor, pero en general, es una arquitectura a la que le metes una secuencia y te saca otra secuencia. Pero a la hora del entrenamiento hay que darle la secuencia de entrada y la secuencia de salida, y a partir de ah√≠ el transformer aprende a traducir.\n",
        "\n",
        "Por otro lado hemos dicho que el transformer va generando un token nuevo cada vez. Es decir, se le pasa la secuencia de entrada en el encoder y un token especial de inicio de secuencia en el decoder, y a partir de ah√≠ genera el primer token de la secuencia de salida.\n",
        "\n",
        "A continuaci√≥n se le vuelve a meter la secuencia de entrada en el encoder y el token que previamente hab√≠a generado en el decoder, y a partir de ah√≠ genera el segundo token de la secuencia de salida.\n",
        "\n",
        "A continuaci√≥n se le vuelve a meter la secuecia de entrada en el encoder y los dos tokens que previamente hab√≠a generado en el decoder, y a partir de ah√≠ genera el tercer token de la secuencia de salida.\n",
        "\n",
        "Y as√≠ sucesivamente hasta que genera un token especial de fin de secuencia.\n",
        "\n",
        "Pero en el entrenamiento, como se le mete la secuencia de entrada y de salida de golpe, necesitamos enmascarar los tokens que a√∫n no ha generado para que no pueda verlos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver otra vez el diagrama de bloques y la f√≥rmula\n",
        "\n",
        "![Transformer - scaled dot product attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)\n",
        "![Transformer - scaled dot product attention formula](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El enmascaramiento se realiza despu√©s del `Scale` y antes de la `Softmax`. Como necesitamos enmascarar los tokens \"futuros\" lo que se puede hacer es multiplicar la matriz resultante del `Scale` por una matriz que tenga 0 en las posiciones que queremos enmascarar y 1 en las que no.\n",
        "\n",
        "![Transformer - Mask](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-Mask.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Haciendo esto obtenemos la misma matriz que antes, pero con posiciones enmascaradas\n",
        "\n",
        "![Transformer - Mask resutl](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-Mask-resutl.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora el resultado del `Scaled dot product attention` es una matriz con los embeddings de los tokens ponderados en funci√≥n de la similitud de los tokens, pero con los tokens que no se deber√≠an ver enmascarados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoder-decoder scale dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volvemos a ver la arquitectura del transformer\n",
        "\n",
        "![transformer](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos ahora que al mecanismo de atenci√≥n entran dos veces la salida del encoder y una vez la masked attention del decoder. Por lo que `K` y `V` son la salida del encoder, y `Q` es la salida del decoder.\n",
        "\n",
        "![Transformer - scaled dot product attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)\n",
        "![Transformer - scaled dot product attention formula](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention-formula.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por tanto en este bloque de atenci√≥n, primero se calcula la similitud entre la sentencia del decoder y la sentencia del encoder, es decir, se calcula la similitud entre la frase que se ha traducido hasta el momento y la frase original.\n",
        "\n",
        "A continuaci√≥n se multiplica esta similitud por la sentencia del encoder, es decir, se obtiene una mezcla de los embeddings de la frase original ponderada en funci√≥n de la similitud de la frase traducida hasta el momento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos recorrido el transformer desde el m√°s alto nivel hasta el m√°s bajo nivel, por lo que ya puedes tener una comprensi√≥n de c√≥mo funciona.\n",
        "\n",
        "![transformer](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp)\n",
        "\n",
        "![Transformer - multi-head attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-multi-head-attention.webp)\n",
        "![Transformer - scaled dot product attention](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer-scaled-dot-product-attention.webp)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "maximofn": {
      "date": "2024-02-23",
      "description_en": "Discover transformers üöÄ. Learn the architecture behind all the new language models. Don't ask an AI, come in and learn",
      "description_es": "Descubre los transformers üöÄ. Aprende la arquitectura que hay dentro de todos los nuevos modelos de lenguajes. No se lo preguntes a una IA, entra y aprende",
      "description_pt": "Descubra os transformadores üöÄ. Conhe√ßa a arquitetura por tr√°s de todos os novos modelos de linguagem. N√£o pergunte a uma IA, entre e aprenda",
      "end_url": "transformers",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer%20-%20leadspace.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Transformer%20-%20leadspace.webp",
      "keywords_en": "transformer, transformers, nlp, from top to bottom, attention, multi-head attention, scaled dot product attention, add & norm, positional encoding",
      "keywords_es": "transformer, transformers, nlp, de arriba a abajo, atenci√≥n, atenci√≥n multi-cabeza, atenci√≥n escalada del producto punto, a√±adir y normalizar, codificaci√≥n posicional",
      "keywords_pt": "transformador, transformadores, nlp, de cima a baixo, aten√ß√£o, aten√ß√£o multi-cabe√ßa, aten√ß√£o escalada do produto ponto, adicionar e normalizar, codifica√ß√£o posicional",
      "title_en": "Transformers ‚Äì from top to the bottom",
      "title_es": "Transformers ‚Äì from top to the bottom",
      "title_pt": "Transformers ‚Äì from top to the bottom"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
