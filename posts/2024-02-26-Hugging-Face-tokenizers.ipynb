{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hugging Face tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La librería `tokenizers` de Hugging Face proporciona una implementación de los tokenizadores más utilizados en la actualidad, centrándose en el rendimiento y la versatilidad. En el post [tokens](https://maximofn.com/tokens/) ya vimos la importancia de los tokens a la hora de procesar textos, ya que los ordenadores no entienden de palabras, sino de números. Por tanto, es necesario convertir las palabras a números para que los modelos de lenguaje puedan procesarlos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instalación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para instalar `tokenizers` con pip:\n",
        "\n",
        "```bash\n",
        "pip install tokenizers\n",
        "```\n",
        "\n",
        "Para instalar `tokenizers` con conda:\n",
        "\n",
        "```bash\n",
        "conda install conda-forge::tokenizers\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## El pipeline de tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para tokenizar una secuencia se usa `Tokenizer.encode`, el cual realiza los siguientes pasos:\n",
        "\n",
        " * Normalización\n",
        " * pre-tokenización\n",
        " * Tokenización\n",
        " * Post-tokenización\n",
        "\n",
        "Vamos a ver cada una"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para realizar el post vamos a usar el dataset [wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-26 08:14:11--  https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz\n",
            "Resolving dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)... 23.200.169.125\n",
            "Connecting to dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)|23.200.169.125|:443... connected.\n",
            "HTTP request sent, awaiting response... "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200 OK\n",
            "Length: 189603606 (181M) [application/x-gzip]\n",
            "Saving to: ‘wikitext-103.tar.gz’\n",
            "\n",
            "wikitext-103.tar.gz 100%[===================>] 180,82M  6,42MB/s    in 30s     \n",
            "\n",
            "2024-02-26 08:14:42 (5,95 MB/s) - ‘wikitext-103.tar.gz’ saved [189603606/189603606]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wikitext-103/\n",
            "wikitext-103/wiki.test.tokens\n",
            "wikitext-103/wiki.valid.tokens\n",
            "wikitext-103/README.txt\n",
            "wikitext-103/LICENSE.txt\n",
            "wikitext-103/wiki.train.tokens\n"
          ]
        }
      ],
      "source": [
        "!tar -xvzf wikitext-103.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm wikitext-103.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Normalización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La normalización son operaciones que se aplican al texto antes de la tokenización, como la eliminación de espacios en blanco, la conversión a minúsculas, la eliminación de caracteres especiales, etc. En Hugging Face están implementadas las siguientes normalizaciones:\n",
        "\n",
        "|Normalización|Descripción|Ejemplo|\n",
        "|---|---|---|\n",
        "|NFD (Normalization for D)|Los caracteres se descomponen por equivalencia canónica|`â` (U+00E2) se descompone en `a` (U+0061) + `^` (U+0302)|\n",
        "|NFKD (Normalization Form KD)|Los caracteres se descomponen por compatibilidad|`ﬁ` (U+FB01) se descompone en `f` (U+0066) + `i` (U+0069)|\n",
        "|NFC (Normalization Form C)|Los caracteres se descomponen y luego se recomponen por equivalencia canónica|`â` (U+00E2) se descompone en `a` (U+0061) + `^` (U+0302) y luego se recompone en `â` (U+00E2)|\n",
        "|NFKC (Normalization Form KC)|Los caracteres se descomponen por compatibilidad y luego se recomponen por equivalencia canónica|`ﬁ` (U+FB01) se descompone en `f` (U+0066) + `i` (U+0069) y luego se recompone en `f` (U+0066) + `i` (U+0069)|\n",
        "|Lowercase|Convierte el texto a minúsculas|`Hello World` se convierte en `hello world`|\n",
        "|Strip|Elimina todos los espacios en blanco de los lados especificados (izquierdo, derecho o ambos) del texto|`  Hello World  ` se convierte en `Hello World`|\n",
        "|StripAccents|Elimina todos los símbolos de acento en unicode (se utilizará con NFD por coherencia)|`á` (U+00E1) se convierte en `a` (U+0061)|\n",
        "|Replace|Sustituye una cadena personalizada o [regex](https://maximofn.com/regular-expressions/) y la cambia por el contenido dado|`Hello World` se convierte en `Hello Universe`|\n",
        "|BertNormalizer|Proporciona una implementación del Normalizador utilizado en el BERT original. Las opciones que se pueden configurar son `clean_text`, `handle_chinese_chars`, `strip_accents` y `lowercase`|`Hello World` se convierte en `hello world`|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear un normalizador para ver cómo funciona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello how are u?'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import normalizers\n",
        "\n",
        "bert_normalizer = normalizers.BertNormalizer()\n",
        "\n",
        "input_text = \"Héllò hôw are ü?\"\n",
        "normalized_text = bert_normalizer.normalize_str(input_text)\n",
        "normalized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para usar varios normalizadores podemos usar el método `Sequence`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello how are u?'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "custom_normalizer = normalizers.Sequence([normalizers.NFKC(), normalizers.BertNormalizer()])\n",
        "\n",
        "normalized_text = custom_normalizer.normalize_str(input_text)\n",
        "normalized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para modificar el normalizador de un tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tokenizers\n",
        "\n",
        "tokenizer = tokenizers.BertWordPieceTokenizer() # or any other tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.normalizer = custom_normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La pretokenización es el acto de dividir un texto en objetos más pequeños. El pretokenizador dividirá el texto en \"palabras\" y los tokens finales serán partes de esas palabras.\n",
        "\n",
        "El PreTokenizer se encarga de dividir la entrada según un conjunto de reglas. Este preprocesamiento le permite asegurarse de que el tokenizador no construye tokens a través de múltiples \"divisiones\". Por ejemplo, si no quieres tener espacios en blanco dentro de un token, entonces puedes tener un pre tokenizer que divide en las palabras a partir de espacios en blanco.\n",
        "\n",
        "En Hugging Face están implementados los siguientes pre-tokenizadores\n",
        "\n",
        "|PreTokenizer|Descripción|Ejemplo|\n",
        "|---|---|---|\n",
        "|ByteLevel|Divide en espacios en blanco mientras reasigna todos los bytes a un conjunto de caracteres visibles. Esta técnica fue introducida por OpenAI con GPT-2 y tiene algunas propiedades más o menos buenas: Como mapea sobre bytes, un tokenizador que utilice esto sólo requiere 256 caracteres como alfabeto inicial (el número de valores que puede tener un byte), frente a los más de 130.000 caracteres Unicode. Una consecuencia del punto anterior es que es absolutamente innecesario tener un token desconocido usando esto ya que podemos representar cualquier cosa con 256 tokens. Para caracteres no ascii, se vuelve completamente ilegible, ¡pero funciona!|`Hello my friend, how are you?` se divide en `Hello`, `Ġmy`, `Ġfriend`, `,`, `Ġhow`, `Ġare`, `Ġyou`, `?`|\n",
        "|Whitespace|Divide en límites de palabra usando la siguiente expresión regular: `\\w+[^\\w\\s]+`. En mi post sobre [expresiones regulares](https://maximofn.com/regular-expressions/) puedes entender qué hace|`Hello there!` se divide en `Hello`, `there`, `!`|\n",
        "|WhitespaceSplit|Se divide en cualquier carácter de espacio en blanco|`Hello there!` se divide en `Hello`, `there!`|\n",
        "|Punctuation|Aislará todos los caracteres de puntuación|`Hello?` se divide en `Hello`, `?`|\n",
        "|Metaspace|Separa los espacios en blanco y los sustituye por un carácter especial \"▁\" (U+2581)|`Hello there` se divide en `Hello`, `▁there`|\n",
        "|CharDelimiterSplit|Divisiones en un carácter determinado|Ejemplo con el caracter `x`: `Helloxthere` se divide en `Hello`, `there`|\n",
        "|Digits|Divide los números de cualquier otro carácter|`Hello123there` se divide en `Hello`, `123`, `there`|\n",
        "|Split|Pretokenizador versátil que divide según el patrón y el comportamiento proporcionados. El patrón se puede invertir si es necesario. El patrón debe ser una cadena personalizada o una [regex](https://maximofn.com/regular-expressions/). El comportamiento debe ser `removed`, `isolated`, `merged_with_previous`, `merged_with_next`, `contiguous`. Para invertir se indica con un booleano|Ejemplo con pattern=`\" \"`, behavior=`isolated`, invert=`False`: `Hello, how are you?` se divide en `Hello,`, ` `, `how`, ` `, `are`, ` `, `you?`|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear un pre tokenizador para ver cómo funciona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('I paid $', (0, 8)),\n",
              " ('3', (8, 9)),\n",
              " ('0', (9, 10)),\n",
              " (' for the car', (10, 22))]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import pre_tokenizers\n",
        "\n",
        "pre_tokenizer = pre_tokenizers.Digits(individual_digits=True)\n",
        "\n",
        "input_text = \"I paid $30 for the car\"\n",
        "pre_tokenized_text = pre_tokenizer.pre_tokenize_str(input_text)\n",
        "pre_tokenized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para usar varios pre tokenizadores podemos usar el método `Sequence`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('I', (0, 1)),\n",
              " ('paid', (2, 6)),\n",
              " ('$', (7, 8)),\n",
              " ('3', (8, 9)),\n",
              " ('0', (9, 10)),\n",
              " ('for', (11, 14)),\n",
              " ('the', (15, 18)),\n",
              " ('car', (19, 22))]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "custom_pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Whitespace(), pre_tokenizers.Digits(individual_digits=True)])\n",
        "\n",
        "pre_tokenized_text = custom_pre_tokenizer.pre_tokenize_str(input_text)\n",
        "pre_tokenized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para modificar el pre-tokenizador de un tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = custom_pre_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez normalizados y pretokenizados los textos de entrada, el tokenizador aplica el modelo a los pretokens. Esta es la parte del proceso que debe entrenarse con el corpus (o que ya se ha entrenado si se utiliza un tokenizador preentrenado).\n",
        "\n",
        "La función del modelo es dividir las \"palabras\" en tokens utilizando las reglas que ha aprendido. También es responsable de asignar esos tokens a sus ID correspondientes en el vocabulario del modelo.\n",
        "\n",
        "El modelo tiene un tamaño de vocabulario, es decir, tiene una cantidad finita de tokens, por lo que tiene que descomponer las palabras y asignarlas a uno de esos tokens.\n",
        "\n",
        "Este modelo se pasa al inicializar el Tokenizer. Actualmente, la librería 🤗 Tokenizers soporta:\n",
        "\n",
        "|Modelo|Descripción|\n",
        "|---|---|\n",
        "|WordLevel|Este es el algoritmo \"clásico\" de tokenización. Te permite simplemente asignar palabras a IDs sin nada sofisticado. Tiene la ventaja de ser muy fácil de usar y entender, pero requiere vocabularios extremadamente grandes para una buena cobertura. El uso de este modelo requiere el uso de un PreTokenizer. Este modelo no realiza ninguna elección directamente, simplemente asigna tokens de entrada a IDs.|\n",
        "|BPE (Byte Pair Encoding)|Uno de los algoritmos de tokenización de subpalabras más populares. El Byte-Pair-Encoding funciona empezando con caracteres y fusionando los que se ven juntos con más frecuencia, creando así nuevos tokens. A continuación, trabaja de forma iterativa para construir nuevos tokens a partir de los pares más frecuentes que ve en un corpus. BPE es capaz de construir palabras que nunca ha visto utilizando múltiples subpalabras y, por tanto, requiere vocabularios más pequeños, con menos posibilidades de tener palabras `unk` (desconocidas).|\n",
        "|WordPiece|Se trata de un algoritmo de tokenización de subpalabras bastante similar a BPE, utilizado principalmente por Google en modelos como BERT. Utiliza un algoritmo codicioso que intenta construir primero palabras largas, dividiéndolas en varios tokens cuando no existen palabras completas en el vocabulario. A diferencia de BPE, que parte de los caracteres y construye tokens lo más grandes posible. Utiliza el famoso prefijo ## para identificar los tokens que forman parte de una palabra (es decir, que no empiezan una palabra).|\n",
        "|Unigram|Unigram es también un algoritmo de tokenización de subpalabras, y funciona tratando de identificar el mejor conjunto de tokens de subpalabras para maximizar la probabilidad de una frase dada. Se diferencia de BPE en que no es un algoritmo determinista basado en un conjunto de reglas aplicadas secuencialmente. En su lugar, Unigram podrá calcular múltiples formas de tokenizar, eligiendo la más probable.|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cuando se crea un tokenizador, se le tiene que pasar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, models\n",
        "\n",
        "tokenizer = Tokenizer(models.Unigram())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a pasarle el normalizador y el pre tokenizador que hemos creado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.normalizer = custom_normalizer\n",
        "tokenizer.pre_tokenizer = custom_pre_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora hay que entrenar el modelo o cargar uno preentrenado. En este caso vamos a entrenar uno con el corpus que nos hemos descargado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para entrenar el modelo tenemos varios tipos de `Trainer`s\n",
        "\n",
        "|Trainer|Descripción|\n",
        "|---|---|\n",
        "|WordLevelTrainer|Entrena un tokenizador WordLevel|\n",
        "|BpeTrainer|Entrena un tokenizador BPE|\n",
        "|WordPieceTrainer|Entrena un tokenizador WordPiece|\n",
        "|UnigramTrainer|Entrena un tokenizador Unigram|\n",
        "\n",
        "Casi todos los trainers tienen los mismos parámetros, que son:\n",
        "\n",
        " * vocab_size: El tamaño del vocabulario final, incluidos todos los tokens y el alfabeto.\n",
        " * show_progress: Mostrar o no barras de progreso durante el entrenamiento\n",
        " * special_tokens: Una lista de tokens especiales que el modelo debe conocer\n",
        "\n",
        "A parte de estos parámetros, cada trainer tiene sus propios parámetros, para verlos mira la documentación de los [Trainers](https://huggingface.co/docs/tokenizers/api/trainers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para entrenar tenemos que crear un `Trainer`, como el modelo que hemos creado es un `Unigram` vamos a crear un `UnigramTrainer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers.trainers import trainers\n",
        "\n",
        "trainer = trainers.UnigramTrainer(\n",
        "    vocab_size=20000,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
        "    special_tokens=[\"<PAD>\", \"<BOS>\", \"<EOS>\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez hemos creado el `Trainer` hay dos maneras de entrenar, mediante el método `train`, al que se le pasa una lista de archivos, o mediante el método `train_from_iterator` al que se le pasa un iterador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Entrenamiento del modelo con el método `train`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero creamos una lista de archivos con el corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['wikitext-103/wiki.test.tokens',\n",
              " 'wikitext-103/wiki.train.tokens',\n",
              " 'wikitext-103/wiki.valid.tokens']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files = [f\"wikitext-103/wiki.{split}.tokens\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y ahora entrenamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer.train(files, trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Entrenamiento del modelo con el método `train_from_iterator`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero creamos una función que nos devuelva un iterador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iterator():\n",
        "    for file in files:\n",
        "        with open(file, \"r\") as f:\n",
        "            for line in f:\n",
        "                yield line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora volvemos a entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer.train_from_iterator(iterator(), trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Entrenamiento del modelo con el método `train_from_iterator` desde un dataset de Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si nos hubiéramos descargado el dataset de Hugging Face, podríamos haber entrenado el modelo directamente desde el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train+test+validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora podemos crear un iterador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_iterator(batch_size=1000):\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        yield dataset[i : i + batch_size][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volvemos a entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Guardando el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez se ha entrenado el modelo, se puede guardar para usarlo en el futuro. Para guardar el modelo hay que hacerlo en un archivo `JSON`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.save(\"wikitext-103-tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cargando el modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos cargar un modelo preentrenado a partir de un `json` en lugar de tener que entrenarlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tokenizers.Tokenizer at 0x7f1dd7784a30>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.from_file(\"wikitext-103-tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "También podemos cargar un modelo preentrenado disponible en el Hub de Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tokenizers.Tokenizer at 0x7f1d64a75e30>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Post-procesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es posible que queramos que nuestro tokenizador añada automáticamente tokens especiales, como `[CLS]` o `[SEP]`.\n",
        "\n",
        "En Hugging Face están implementados los siguientes post procesadores\n",
        "\n",
        "|PostProcesador|Descripción|Ejemplo|\n",
        "|---|---|---|\n",
        "|BertProcessing|Este post-procesador se encarga de añadir los tokens especiales que necesita un modelo Bert (`SEP` y `CLS`)|`Hello, how are you?` se convierte en `[CLS]`, `Hello`, `,`, `how`, `are`, `you`, `?`, `[SEP]`|\n",
        "|RobertaProcessing|Este post-procesador se encarga de añadir los tokens especiales que necesita un modelo Roberta (`SEP` y `CLS`). También se encarga de recortar los offsets. Por defecto, el ByteLevel BPE puede incluir espacios en blanco en los tokens producidos. Si no desea que las compensaciones incluyan estos espacios en blanco, hay que inicializar este PostProcessor con `trim_offsets=True`.|`Hello, how are you?` se convierte en `<s>`, `Hello`, `,`, `how`, `are`, `you`, `?`, `</s>`|\n",
        "|ElectraProcessing|Añade tokens especiales para ELECTRA|`Hello, how are you?` se convierte en `[CLS]`, `Hello`, `,`, `how`, `are`, `you`, `?`, `[SEP]`|\n",
        "|TemplateProcessing|Permite crear fácilmente una plantilla para el postprocesamiento, añadiendo tokens especiales y especificando el type_id de cada secuencia/token especial. La plantilla recibe dos cadenas que representan la secuencia única y el par de secuencias, así como un conjunto de tokens especiales a utilizar|Example, when specifying a template with these values: single:`[CLS] $A [SEP]`, pair: `[CLS] $A [SEP] $B [SEP]`, special tokens: `[CLS]`, `[SEP]`. Input: (`I like this`, `but not this`), Output: `[CLS] I like this [SEP] but not this [SEP]`|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear un post tokenizador para ver cómo funciona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para modificar el post tokenizador de un tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.post_processor = post_processor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Veamos cómo funciona"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'i', 'paid', '$', '3', '0', 'for', 'the', 'car', '[SEP]']"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_text = \"I paid $30 for the car\"\n",
        "decoded_text = tokenizer.encode(input_text)\n",
        "\n",
        "decoded_text.tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'hell', 'o', ',', 'y', \"'\", 'all', '!', '[SEP]', 'how', 'are', 'you', '?', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "input_text1 = \"Hello, y'all!\"\n",
        "input_text2 = \"How are you?\"\n",
        "decoded_text = tokenizer.encode(input_text1, input_text2)\n",
        "\n",
        "print(decoded_text.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si ahora guardásemos el tokenizador, el post tokenizador se guardaría con él"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez tenemos el tokenizador entrenado, podemos usarlo para tokenizar textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = \"I love tokenizers!\"\n",
        "encoded_text = tokenizer.encode(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver qué obtenemos al tokenizar un texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tokenizers.Encoding"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(encoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtenemos un objeto de tipo [Encoding](https://huggingface.co/docs/tokenizers/api/encoding#tokenizers.Encoding), que contiene los tokens y los IDs de los tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los `ids` son los `id`s de los tokens en el vocabulario del tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 17, 383, 10694, 17, 3533, 3, 586, 2]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_text.ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los `tokens` son los tokens a los que equivalen los `ids`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'i', 'love', 'token', 'i', 'zer', 's', '!', '[SEP]']"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_text.tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si tenemos varias secuencias, podemos codificarlas todas a la vez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'hell', 'o', ',', 'y', \"'\", 'all', '!', '[SEP]', 'how', 'are', 'you', '?', '[SEP]']\n",
            "[1, 2215, 7, 5, 22, 26, 81, 586, 2, 98, 59, 213, 902, 2]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "encoded_texts = tokenizer.encode(input_text1, input_text2)\n",
        "\n",
        "print(encoded_texts.tokens)\n",
        "print(encoded_texts.ids)\n",
        "print(encoded_texts.type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sin embargo, cuando se tengan varias secuencias es mejor usar el método `encode_batch`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_texts = tokenizer.encode_batch([input_text1, input_text2])\n",
        "\n",
        "type(encoded_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que obtenemos una lista"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'hell', 'o', ',', 'y', \"'\", 'all', '!', '[SEP]']\n",
            "[1, 2215, 7, 5, 22, 26, 81, 586, 2]\n",
            "['[CLS]', 'how', 'are', 'you', '?', '[SEP]']\n",
            "[1, 98, 59, 213, 902, 2]\n"
          ]
        }
      ],
      "source": [
        "print(encoded_texts[0].tokens)\n",
        "print(encoded_texts[0].ids)\n",
        "print(encoded_texts[1].tokens)\n",
        "print(encoded_texts[1].ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Además de codificar los textos de entrada, un Tokenizer también tiene un método para decodificar, es decir, convertir los ID generados por su modelo de nuevo a un texto. Esto se hace mediante los métodos `Tokenizer.decode` (para un texto predicho) y `Tokenizer.decode_batch` (para un lote de predicciones).\n",
        "\n",
        "Los tipos de decodificación que se pueden usar son:\n",
        "\n",
        "|Decodificación|Descripción|\n",
        "|---|---|\n",
        "|BPEDecoder|Revierte el modelo BPE|\n",
        "|ByteLevel|Revierte el ByteLevel PreTokenizer. Este PreTokenizer codifica a nivel de byte, utilizando un conjunto de caracteres Unicode visibles para representar cada byte, por lo que necesitamos un Decoder para revertir este proceso y obtener algo legible de nuevo.|\n",
        "|CTC|Revierte el modelo CTC|\n",
        "|Metaspace|Revierte el PreTokenizer de Metaspace. Este PreTokenizer utiliza un identificador especial ▁ para identificar los espacios en blanco, por lo que este Decoder ayuda con la decodificación de estos.|\n",
        "|WordPiece|Revierte el modelo WordPiece. Este modelo utiliza un identificador especial ## para las subpalabras continuas, por lo que este decodificador ayuda a decodificarlas.|\n",
        "\n",
        "El decodificador convertirá primero los IDs en tokens (usando el vocabulario del tokenizador) y eliminará todos los tokens especiales, después unirá esos tokens con espacios en blanco."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear un decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import decoders\n",
        "\n",
        "decoder = decoders.ByteLevel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo añadimos al tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decodificamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('I love tokenizers!', 'ilovetokenizers!')"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_text = tokenizer.decode(encoded_text.ids)\n",
        "\n",
        "input_text, decoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, y'all! hello,y'all!\n",
            "How are you? howareyou?\n"
          ]
        }
      ],
      "source": [
        "decoded_texts = tokenizer.decode_batch([encoded_texts[0].ids, encoded_texts[1].ids])\n",
        "\n",
        "print(input_text1, decoded_texts[0])\n",
        "print(input_text2, decoded_texts[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con todo lo aprendido vamos a crear el tokenizador de BERT desde cero, primero creamos el tokenizador. BERT usa `WordPiece` como modelo, por lo que lo pasamos al inicializar del tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "\n",
        "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BERT preprocesa los textos eliminando los acentos y las minúsculas. También utilizamos un normalizador unicode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
        "\n",
        "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El pretokenizador solo divide los espacios en blanco y los signos de puntuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "bert_tokenizer.pre_tokenizer = Whitespace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y el post-procesamiento utiliza la plantilla que vimos en la sección anterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "bert_tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entrenamos el tokenizador con el dataset de wikitext-103"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "trainer = WordPieceTrainer(vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "files = [f\"wikitext-103/wiki.{split}.tokens\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "bert_tokenizer.train(files, trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora lo probamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El texto de entrada 'I love tokenizers!' se convierte en los tokens ['[CLS]', 'i', 'love', 'token', '##izers', '!', '[SEP]'], que tienen las ids [1, 51, 2867, 25791, 12213, 5, 2] y luego se decodifica como 'i love token ##izers !'\n"
          ]
        }
      ],
      "source": [
        "input_text = \"I love tokenizers!\"\n",
        "\n",
        "encoded_text = bert_tokenizer.encode(input_text)\n",
        "decoded_text = bert_tokenizer.decode(encoded_text.ids)\n",
        "\n",
        "print(f\"El texto de entrada '{input_text}' se convierte en los tokens {encoded_text.tokens}, que tienen las ids {encoded_text.ids} y luego se decodifica como '{decoded_text}'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "maximofn": {
      "date": "2024-02-26",
      "description_en": "📄 ➡️ 🔤 Explore the power of the Hugging Face Tokenizers library for natural language processing in AI. Discover how this essential tool transforms text into structured data, optimizing AI model training with practical examples and Python code. Dive into the future of NLP with our expert guide",
      "description_es": "📄 ➡️ 🔤 Explora el poder de la biblioteca Tokenizers de Hugging Face para el procesamiento del lenguaje natural en IA. Descubre cómo esta herramienta esencial transforma el texto en datos estructurados, optimizando el entrenamiento de modelos de inteligencia artificial con ejemplos prácticos y código en Python. Sumérgete en el futuro de la NLP con nuestra guía experta",
      "description_pt": "📄 ➡️ 🔤 Explore o poder da biblioteca Hugging Face Tokenizers para processamento de linguagem natural em IA. Descubra como essa ferramenta essencial transforma texto em dados estruturados, otimizando o treinamento de modelos de IA com exemplos práticos e código Python. Mergulhe no futuro da PNL com nosso guia especializado",
      "end_url": "hugging-face-tokenizers",
      "image": "https://images.maximofn.com/Hugging%20Face's%20tokenizers%20library.webp",
      "image_hover_path": "https://images.maximofn.com/Hugging%20Face's%20tokenizers%20library.webp",
      "keywords_en": "hugging face, tokenizers, natural language processing, nlp, artificial intelligence, ai, python",
      "keywords_es": "hugging face, tokenizers, procesamiento de lenguaje natural, pln, inteligencia artificial, ia, python",
      "keywords_pt": "hugging face, tokenizers, processamento de linguagem natural, pln, inteligência artificial, ia, python",
      "title_en": "Hugging Face Tokenizers",
      "title_es": "Hugging Face Tokenizers",
      "title_pt": "Hugging Face Tokenizers"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
