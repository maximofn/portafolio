{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Accelerate` es una biblioteca de Hugging Face que permite ejecutar el mismo c√≥digo PyTorch en cualquier configuraci√≥n distribuida a√±adiendo s√≥lo cuatro l√≠neas de c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar `accelerate` con `pip` simplemente ejecuta:\n",
    "\n",
    "``` bash\n",
    "pip install accelerate\n",
    "```\n",
    "\n",
    "Y con `conda`:\n",
    "\n",
    "``` bash\n",
    "conda install -c conda-forge accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada entorno en el que se intale `accelerate` lo primero que se tiene que hacer es configurarlo, para ello ejecutamos en una terminal:\n",
    "\n",
    "``` bash\n",
    "accelerate config\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "In which compute environment are you running?\n",
      "This machine\n",
      "--------------------------------------------------------------------------------\n",
      "multi-GPU\n",
      "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
      "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
      "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
      "Do you want to use DeepSpeed? [yes/NO]: no\n",
      "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
      "Do you want to use Megatron-LM ? [yes/NO]: no\n",
      "How many GPU(s) should be used for distributed training? [1]:2\n",
      "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
      "--------------------------------------------------------------------------------\n",
      "Do you wish to use FP16 or BF16 (mixed precision)?\n",
      "no\n",
      "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
     ]
    }
   ],
   "source": [
    "!accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En mi caso las respuestas han sido\n",
    " * In which compute environment are you running?\n",
    "    - [x] \"This machine\"\n",
    "    - [_] \"AWS (Amazon SageMaker)\"\n",
    " > Quiero configurarlo en mi ordenador\n",
    "\n",
    " * Which type of machine are you using?\n",
    "    - [_] multi-CPU\n",
    "    - [_] multi-XPU\n",
    "    - [x] multi-GPU\n",
    "    - [_] multi-NPU\n",
    "    - [_] TPU\n",
    " > Como tengo 2 GPUs y quiero ejecutar c√≥digos distribuidos en ellas elijo `multi-GPU`\n",
    " \n",
    " * How many different machines will you use (use more than 1 for multi-node training)? [1]:\n",
    "    - 1\n",
    " > Elijo `1` porque solo voy a ejecutar en mi ordenador\n",
    "\n",
    " * Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]:\n",
    "    - no\n",
    " > Con esta opci√≥n, se puede elegir que `accelerate` chequee errores en la ejecuci√≥n, pero har√≠a que vaya m√°s lento, as√≠ que elijo `no`, y en caso de que haya errores lo cambio a `yes`\n",
    " \n",
    " * Do you wish to optimize your script with torch dynamo?[yes/NO]:\n",
    "    - no\n",
    "\n",
    " * Do you want to use FullyShardedDataParallel? [yes/NO]:\n",
    "    - no\n",
    " \n",
    " * Do you want to use Megatron-LM ? [yes/NO]:\n",
    "    - no\n",
    " \n",
    " * How many GPU(s) should be used for distributed training? [1]:\n",
    "    - 2\n",
    " > Elijo `2` porque tengo 2 GPUs\n",
    "\n",
    " * What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:\n",
    "    - 0,1\n",
    " > Elijo `0,1` porque quiero usar las dos GPUs\n",
    "\n",
    " * Do you wish to use FP16 or BF16 (mixed precision)?\n",
    "    - [x] no\n",
    "    - [_] fp16\n",
    "    - [_] bf16\n",
    "    - [_] fp8\n",
    " > De momento elijo `no`, porque para simplificar el c√≥digo cuando no uso `acelerate` vamos a entrenar en fp32, pero lo ideal ser√≠a usar fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuraci√≥n se guardar√° en `~/.cache/huggingface/accelerate/default_config.yaml` y se puede modificar en cualquier momento. Vamos a ver qu√© hay dentro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_environment: LOCAL_MACHINE\n",
      "debug: false\n",
      "distributed_type: MULTI_GPU\n",
      "downcast_bf16: 'no'\n",
      "gpu_ids: 0,1\n",
      "machine_rank: 0\n",
      "main_training_function: main\n",
      "mixed_precision: fp16\n",
      "num_machines: 1\n",
      "num_processes: 2\n",
      "rdzv_backend: static\n",
      "same_network: true\n",
      "tpu_env: []\n",
      "tpu_use_cluster: false\n",
      "tpu_use_sudo: false\n",
      "use_cpu: false\n"
     ]
    }
   ],
   "source": [
    "!cat ~/.cache/huggingface/accelerate/default_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de ver la configuraci√≥n que tenemos es ejecutando en una terminal:\n",
    "\n",
    "``` bash\n",
    "accelerate env\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copy-and-paste the text below in your GitHub issue\n",
      "\n",
      "- `Accelerate` version: 0.28.0\n",
      "- Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31\n",
      "- Python version: 3.11.8\n",
      "- Numpy version: 1.26.4\n",
      "- PyTorch version (GPU?): 2.2.1+cu121 (True)\n",
      "- PyTorch XPU available: False\n",
      "- PyTorch NPU available: False\n",
      "- System RAM: 31.24 GB\n",
      "- GPU type: NVIDIA GeForce RTX 3090\n",
      "- `Accelerate` default config:\n",
      "\t- compute_environment: LOCAL_MACHINE\n",
      "\t- distributed_type: MULTI_GPU\n",
      "\t- mixed_precision: fp16\n",
      "\t- use_cpu: False\n",
      "\t- debug: False\n",
      "\t- num_processes: 2\n",
      "\t- machine_rank: 0\n",
      "\t- num_machines: 1\n",
      "\t- gpu_ids: 0,1\n",
      "\t- rdzv_backend: static\n",
      "\t- same_network: True\n",
      "\t- main_training_function: main\n",
      "\t- downcast_bf16: no\n",
      "\t- tpu_use_cluster: False\n",
      "\t- tpu_use_sudo: False\n",
      "\t- tpu_env: []\n"
     ]
    }
   ],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos configurado `accelerate` podemos probar si lo hemos hecho bien ejecutando en una terminal:\n",
    "\n",
    "``` bash\n",
    "accelerate test\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running:  accelerate-launch ~/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/test_utils/scripts/test_script.py\n",
      "stdout: **Initialization**\n",
      "stdout: Testing, testing. 1, 2, 3.\n",
      "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
      "stdout: Num processes: 2\n",
      "stdout: Process index: 0\n",
      "stdout: Local process index: 0\n",
      "stdout: Device: cuda:0\n",
      "stdout: \n",
      "stdout: Mixed precision type: fp16\n",
      "stdout: \n",
      "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
      "stdout: Num processes: 2\n",
      "stdout: Process index: 1\n",
      "stdout: Local process index: 1\n",
      "stdout: Device: cuda:1\n",
      "stdout: \n",
      "stdout: Mixed precision type: fp16\n",
      "stdout: \n",
      "stdout: \n",
      "stdout: **Test process execution**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a list**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a dict**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a tensor**\n",
      "stdout: \n",
      "stdout: **Test random number generator synchronization**\n",
      "stdout: All rng are properly synched.\n",
      "stdout: \n",
      "stdout: **DataLoader integration test**\n",
      "stdout: 0 1 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:1') <class 'accelerate.data_loader.DataLoaderShard'>\n",
      "stdout: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\n",
      "stdout: Non-shuffled dataloader passing.\n",
      "stdout: Shuffled dataloader passing.\n",
      "stdout: Non-shuffled central dataloader passing.\n",
      "stdout: Shuffled central dataloader passing.\n",
      "stdout: \n",
      "stdout: **Training integration test**\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
      "stdout: FP16 training check.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: \n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: FP16 training check.\n",
      "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: \n",
      "stdout: **Breakpoint trigger test**\n",
      "Test is a success! You are ready for your distributed training!\n"
     ]
    }
   ],
   "source": [
    "!accelerate test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que termina diciendo `Test is a success! You are ready for your distributed training!` por lo que todo est√° correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizaci√≥n del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C√≥digo base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer primero un c√≥digo de entrenamiento base y luego lo optimizaremos  para ver c√≥mo se hace y c√≥mo mejora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a buscar un dataset, en mi caso voy a usar el dataset [tweet_eval](https://huggingface.co/datasets/tweet_eval), que es un dataset de clasificaci√≥n de tweets, en concreto voy a descargar el subset `emoji` que clasifica los tweets con emoticonos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['‚ù§', 'üòç', 'üòÇ', 'üíï', 'üî•', 'üòä', 'üòé', '‚ú®', 'üíô', 'üòò', 'üì∑', 'üá∫üá∏', '‚òÄ', 'üíú', 'üòâ', 'üíØ', 'üòÅ', 'üéÑ', 'üì∏', 'üòú'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='tweet_eval', config_name='emoji', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=3808792, num_examples=45000, shard_lengths=None, dataset_name='tweet_eval'), 'test': SplitInfo(name='test', num_bytes=4262151, num_examples=50000, shard_lengths=None, dataset_name='tweet_eval'), 'validation': SplitInfo(name='validation', num_bytes=396704, num_examples=5000, shard_lengths=None, dataset_name='tweet_eval')}, download_checksums={'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/train-00000-of-00001.parquet': {'num_bytes': 2609973, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/test-00000-of-00001.parquet': {'num_bytes': 3047341, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/validation-00000-of-00001.parquet': {'num_bytes': 281994, 'checksum': None}}, download_size=5939308, post_processing_size=None, dataset_size=8467647, size_in_bytes=14406955)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver las clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ù§', 'üòç', 'üòÇ', 'üíï', 'üî•', 'üòä', 'üòé', '‚ú®', 'üíô', 'üòò', 'üì∑', 'üá∫üá∏', '‚òÄ', 'üíú', 'üòâ', 'üíØ', 'üòÅ', 'üéÑ', 'üì∏', 'üòú']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y el n√∫mero de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el dataset tiene 20 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver la secuencia m√°xima de cada split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 139, 167)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_train = 0\n",
    "max_len_val = 0\n",
    "max_len_test = 0\n",
    "\n",
    "split = \"train\"\n",
    "for i in range(len(dataset[split])):\n",
    "    len_i = len(dataset[split][i][\"text\"])\n",
    "    if len_i > max_len_train:\n",
    "        max_len_train = len_i\n",
    "split = \"validation\"\n",
    "for i in range(len(dataset[split])):\n",
    "    len_i = len(dataset[split][i][\"text\"])\n",
    "    if len_i > max_len_val:\n",
    "        max_len_val = len_i\n",
    "split = \"test\"\n",
    "for i in range(len(dataset[split])):\n",
    "    len_i = len(dataset[split][i][\"text\"])\n",
    "    if len_i > max_len_test:\n",
    "        max_len_test = len_i\n",
    "\n",
    "max_len_train, max_len_val, max_len_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠ que definimos la secuencia m√°ximo en general como 130 para la tokeniazci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nosotros nos interesa el dataset tokenizado, no las secuencias en crudo, as√≠ que creamos un tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funci√≥n de tokenizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora tokenizamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83f90dc1d074012b5d099511986898e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c14557614545118c2ceb0a0ab6178c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1c71ced24c4b1ba6e13e6c0ba0e7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos ahora tenemos los tokens (`input_ids`) y las m√°scaras de atenci√≥n (`attention_mask`), pero vamos a ver qu√© tipo de datos tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list, int)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"]), type(tokenized_dataset[\"train\"][0][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "type(tokenized_dataset[\"train\"][0][\"label\"]), type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "BS = 64\n",
    "\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver c√≥mo es el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver su √∫ltima capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.out_proj.in_features, model.classifier.out_proj.out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que nuestro dataset tiene 20 clases, pero este modelo est√° entrenado para 2 clases, as√≠ que tenemos que modificar la √∫ltima capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=20, bias=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "model.classifier.out_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora s√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos una funci√≥n de loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por √∫ltimo una m√©trica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comprobar que est√° todo bien con una muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dataloader[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 130]), torch.Size([64, 130]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"input_ids\"].shape, sample[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora esa muestra se la metemos al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "ouputs = model(input_ids=sample[\"input_ids\"].to(\"cuda\"), attention_mask=sample[\"attention_mask\"].to(\"cuda\"))\n",
    "ouputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el modelo saca 64 batches, lo cual est√° bien, porque configuramos `BS = 20` y cada una con 20 salidas, lo cual est√° bien porque cambiamos el modelo para que a la salida de 20 valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la de mayor valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.argmax(ouputs.logits, axis=-1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9990389347076416"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_function(ouputs.logits, sample[\"label\"].to(\"cuda\"))\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y el accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = metric.compute(predictions=predictions, references=sample[\"label\"])[\"accuracy\"]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya podemos crear un peque√±o bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "master_progress_bar = master_bar(range(epochs))\n",
    "for i in master_progress_bar:\n",
    "    model.train()\n",
    "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script con el c√≥digo base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la mayor√≠a de la documentaci√≥n de `accelerate` se explica c√≥mo usar `accelerate` con scripts, as√≠ que de momento vamos a hacerlo as√≠ y al final explicaremos c√≥mo hacerlo con un notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a crear una carpeta en la que vamos a guardar los scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir accelerate_scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora escribimos el c√≥digo base en un script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/01_code_base.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/01_code_base.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 64\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "master_progress_bar = master_bar(range(EPOCHS))\n",
    "for i in master_progress_bar:\n",
    "    model.train()\n",
    "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
    "print(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora lo ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.2112                                                               \n",
      "CPU times: user 2.12 s, sys: 391 ms, total: 2.51 s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!python accelerate_scripts/01_code_base.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en mi ordenador ha tardado unos 3 minutos y medio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C√≥digo con accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora reemplazamos algunas cosas\n",
    "\n",
    " * En primer lugar importamos `Accelerator` y lo inicializamos\n",
    "\n",
    "``` python\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "```\n",
    "\n",
    " * Ya no hacemos el t√≠pico\n",
    "\n",
    "``` python \n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    " * Sino que dejamos que sea `acelerate` el que elija el dispositivo mediante\n",
    "\n",
    "``` python\n",
    "device = accelerator.device\n",
    "```\n",
    "\n",
    " * Pasamos los elementos relevantes para el entrenamiento por el m√©todo `prepare` y ya no hacemos `model.to(device)`\n",
    "\n",
    "``` python\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = preprare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "```\n",
    "\n",
    " * Ya no mandamos los datos y el modelo a la GPU con `.to(device)` ya que `accelerate` se ha encargado de ello con el m√©todo `prepare`\n",
    "\n",
    " * En vez de hacer el backpropagation con `loss.backward()` dejamos que lo haga `accelerate` con\n",
    " \n",
    "``` python\n",
    "accelerator.backward(loss)\n",
    "```\n",
    "\n",
    " * A la hora de calcular la m√©trica en el bucle de validaci√≥n, necesitamos recopilar los valores de todos los puntos, en caso de estar haciendo un entrenamiento distribuido, para ello hacemos\n",
    "\n",
    "``` python\n",
    "predictions = accelerator.gather_for_metrics(predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/02_accelerate_base_code.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/02_accelerate_base_code.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 64\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "master_progress_bar = master_bar(range(EPOCHS))\n",
    "for i in master_progress_bar:\n",
    "    model.train()\n",
    "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "    print(f\"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    print(f\"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")\n",
    "    \n",
    "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
    "\n",
    "print(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si te fijas he a√±adido estas dos l√≠neas `print(f\"End of training epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")` y la l√≠nea `print(f\"End of validation epoch {i}, outputs['logits'].shape: {outputs['logits'].shape}, labels.shape: {labels.shape}\")`, las he a√±adido aposta porque nos van a revelar algo muy importante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lo ejecutamos, para ejecutar los scripts de `accelerate` se hace con el comando `accelerate launch`\n",
    "\n",
    "``` bash\n",
    "accelerate launch script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
      "End of training epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([64])\n",
      "End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
      "Accuracy = 0.206\n",
      "End of validation epoch 0, outputs['logits'].shape: torch.Size([64, 20]), labels.shape: torch.Size([8])\n",
      "Accuracy = 0.206\n",
      "CPU times: user 1.6 s, sys: 272 ms, total: 1.88 s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/02_accelerate_base_code.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que antes tard√≥ unos 3 minutos y medio y ahora tarda m√°s o menos 2 minutos y medio. Bastante mejora. Adem√°s si vemos los `print`s podemos ver que se han impreso dos veces.\n",
    "\n",
    "¬øY esto c√≥mo puede ser? pues porque `accelerate` ha paralelizado el entrenamiento en las dos GPUs que tengo, por lo que ha sido mucho m√°s r√°pido.\n",
    "\n",
    "Adem√°s, cuando ejecut√© el primer script, esd ecir, cuando no us√© `accelerate`, la GPU estaba casi llena, mientras que cuando he ejecutado el segundo, es decir, el que usa `accelerate`, las dos GPUs estaban muy poco utilizadas, por lo que podemos aumentar el batch size para intentar llenar las dos, vamos a ello!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/03_accelerate_base_code_more_bs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/03_accelerate_base_code_more_bs.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "master_progress_bar = master_bar(range(EPOCHS))\n",
    "for i in master_progress_bar:\n",
    "    model.train()\n",
    "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
    "\n",
    "print(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He quitado los prints extra, porque ya hemos visto que el c√≥digo se est√° ejecutando en las dos GPUs y he aunmentado el batch size de 64 a 128. Lo ejecutamos a ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.1052                                                               \n",
      "Accuracy = 0.1052\n",
      "CPU times: user 1.41 s, sys: 180 ms, total: 1.59 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/03_accelerate_base_code_more_bs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando el batch size ha bajado unos segundos el tiempo de ejecuci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecuci√≥n de procesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecuci√≥n de c√≥digo en un √∫nico proceso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes hemos visto que los `print`s se imprim√≠an dos veces, esto es porque `accelerate` crea tantos procesos como dispositivos donde se ejecuta el c√≥digo, en mi caso crea dos procesos por tener dos GPUs.\n",
    "\n",
    "Sin embargo no todo el c√≥digo deber√≠a ejecutarse en todos los procesos, por ejemplo los `print`s, ralentizan mucho el c√≥digo, como para ejecutarlo varias veces, si se guardan los checkpoints, se guardar√≠an dos veces, etc.\n",
    "\n",
    "Para poder ejecutar parte de un c√≥digo en un √∫nico proceso se tiene que encapsular en una funci√≥n y decorarla con `accelerator.on_local_main_process`, por ejemplo en el siguiente c√≥digo vas a ver que he creado la siguiente funci√≥n\n",
    "\n",
    "``` python\n",
    "@accelerator.on_local_main_process\n",
    "def print_something(something):\n",
    "    print(something)\n",
    "```\n",
    "\n",
    "Otra opci√≥n es meter el c√≥digo dentro de un `if accelerator.is_local_main_process` como en el siguiente c√≥digo\n",
    "\n",
    "``` python\n",
    "if accelerator.is_local_main_process:\n",
    "    print(\"Something\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "@accelerator.on_local_main_process\n",
    "def print_something(something):\n",
    "    print(something)\n",
    "\n",
    "master_progress_bar = master_bar(range(EPOCHS))\n",
    "for i in master_progress_bar:\n",
    "    model.train()\n",
    "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
    "\n",
    "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ejecutarlo a ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.2098                                                               \n",
      "End of script with 0.2098 accuracy\n",
      "CPU times: user 1.38 s, sys: 197 ms, total: 1.58 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/04_accelerate_base_code_some_code_in_one_process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora solo se ha impreso el print una vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, aunque no se ve mucho, las barras de progreso se ejecutan en cada proceso.\n",
    "\n",
    "No he encontrado una manera de evitar esto con las barras de progreso de `fastprogress`, pero s√≠ con las de `tqdm`, as√≠ que voy a sustituir las barras de progreso de `fastprogress` por las de `tqdm` y para que se ejecuten en un √∫nico proceso hay que a√±adirle el argumento `disable=not accelerator.is_local_main_process`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "@accelerator.on_local_main_process\n",
    "def print_something(something):\n",
    "    print(something)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        # master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:01<00:00,  1.45it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.30it/s]\n",
      "Accuracy = 0.2166\n",
      "End of script with 0.2166 accuracy\n",
      "CPU times: user 1.33 s, sys: 195 ms, total: 1.52 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/05_accelerate_base_code_some_code_in_one_process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos mostrado un ejemplo de c√≥mo imprimir en un solo proceso, y esto ha sido una manera de ejecutar procesos en un solo proceso. Pero si lo que quieres es solo imprimir en un solo proceso se puede usar el m√©todo `print` de `accelerate`. Vamos a ver el mismo ejemplo de antes con este m√©todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerate_scripts/06_accelerate_base_code_print_one_process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/06_accelerate_base_code_print_one_process.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    # progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        # master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    # progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "# print(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    print(f\"End of script with {accuracy['accuracy']} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:02<00:00, 15433.52 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 11406.61 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:02<00:00, 15036.87 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14932.76 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14956.60 examples/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:00<00:00,  1.46it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.33it/s]\n",
      "Accuracy = 0.2134\n",
      "End of script with 0.2134 accuracy\n",
      "CPU times: user 1.4 s, sys: 189 ms, total: 1.59 s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/06_accelerate_base_code_print_one_process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecuci√≥n de c√≥digo en todos los procesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo hay c√≥digo que debe ejecutarse en todos los procesos, por ejemplo si subimos los checkpoints al hub, as√≠ que aqu√≠ tenemos dos opciones, encapsular el c√≥digo en una funci√≥n y decorarla con `accelerator.on_main_process`\n",
    "\n",
    "``` python\n",
    "@accelerator.on_main_process\n",
    "def do_my_thing():\n",
    "    \"Something done once per server\"\n",
    "    do_thing_once()\n",
    "```\n",
    "\n",
    "o meter el c√≥digo dentro de un `if accelerator.is_main_process`\n",
    "\n",
    "``` python\n",
    "if accelerator.is_main_process:\n",
    "    repo.push_to_hub()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos haciendo entrenamientos solo para mostrar la librer√≠a `accelerate` y el modelo que estamos entrenando no es bueno, no tiene sentido ahora subir los checkpoints al hub, as√≠ que voy a hacer un ejemplo con `print`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/06_accelerate_base_code_some_code_in_all_process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "@accelerator.on_local_main_process\n",
    "def print_in_one_process(something):\n",
    "    print(something)\n",
    "\n",
    "@accelerator.on_main_process\n",
    "def print_in_all_processes(something):\n",
    "    print(something)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
    "\n",
    "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos a ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:03<00:00, 14518.44 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:03<00:00, 14368.77 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 16466.33 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14806.14 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14253.33 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14337.07 examples/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:00<00:00,  1.46it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.34it/s]\n",
      "Accuracy = 0.2092\n",
      "End of script with 0.2092 accuracy\n",
      "All process: Accuracy = 0.2092\n",
      "All process: End of script with 0.2092 accuracy\n",
      "CPU times: user 1.42 s, sys: 216 ms, total: 1.64 s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/07_accelerate_base_code_some_code_in_all_process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecuci√≥n de c√≥digo en el proceso X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo podemos especificar en qu√© proceso queremos ejecutar c√≥digo, para esto hay que crear una funci√≥n y decorarla con `@accelerator.on_process(process_index=0)`\n",
    "\n",
    "``` python\n",
    "@accelerator.on_process(process_index=0)\n",
    "def do_my_thing():\n",
    "    \"Something done on process index 0\"\n",
    "    do_thing_on_index_zero()\n",
    "```\n",
    "\n",
    "o decorarla con `@accelerator.on_local_process(local_process_idx=0)`\n",
    "\n",
    "``` python\n",
    "@accelerator.on_local_process(local_process_index=0)\n",
    "def do_my_thing():\n",
    "    \"Something done on process index 0 on each server\"\n",
    "    do_thing_on_index_zero_on_each_server()\n",
    "```\n",
    "\n",
    "Aqu√≠ he puesto el proceso 0, pero se puede poner cualquier n√∫mero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/07_accelerate_base_code_some_code_in_some_process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "@accelerator.on_local_main_process\n",
    "def print_in_one_process(something):\n",
    "    print(something)\n",
    "\n",
    "@accelerator.on_main_process\n",
    "def print_in_all_processes(something):\n",
    "    print(something)\n",
    "\n",
    "@accelerator.on_process(process_index=0)\n",
    "def print_in_process_0(something):\n",
    "    print(\"Process 0: \" + something)\n",
    "\n",
    "@accelerator.on_local_process(local_process_index=1)\n",
    "def print_in_process_1(something):\n",
    "    print(\"Process 1: \" + something)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
    "\n",
    "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
    "\n",
    "print_in_process_0(\"End of process 0\")\n",
    "print_in_process_1(\"End of process 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 15735.58 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14906.20 examples/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:02<00:00,  1.44it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.27it/s]\n",
      "Process 1: End of process 1\n",
      "Accuracy = 0.2128\n",
      "End of script with 0.2128 accuracy\n",
      "All process: Accuracy = 0.2128\n",
      "All process: End of script with 0.2128 accuracy\n",
      "Process 0: End of process 0\n",
      "CPU times: user 1.42 s, sys: 295 ms, total: 1.71 s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/08_accelerate_base_code_some_code_in_some_process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sincronizar procesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos c√≥digo que debe ejecutarse en todos los procesos, es interesante esperar a que termine en todos los procesos antes de hacer otra tarea, as√≠ que para ello usamos `accelerator.wait_for_everyone()`\n",
    "\n",
    "Para verlo vamos a meter un retardo en una de las funciones de imprimir en un proceso\n",
    "\n",
    "Adem√°s he puesto un break en el bucle de entrenamiento para que no est√© mucho tiempo entrenando, que no es lo que ahora nos interesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/08_accelerate_base_code_sync_all_process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/09_accelerate_base_code_sync_all_process.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "@accelerator.on_local_main_process\n",
    "def print_in_one_process(something):\n",
    "    print(something)\n",
    "\n",
    "@accelerator.on_main_process\n",
    "def print_in_all_processes(something):\n",
    "    print(something)\n",
    "\n",
    "@accelerator.on_process(process_index=0)\n",
    "def print_in_process_0(something):\n",
    "    time.sleep(2)\n",
    "    print(\"Process 0: \" + something)\n",
    "\n",
    "@accelerator.on_local_process(local_process_index=1)\n",
    "def print_in_process_1(something):\n",
    "    print(\"Process 1: \" + something)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        break\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "print_in_one_process(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    print(f\"End of script with {accuracy['accuracy']} accuracy\")\n",
    "\n",
    "print_in_all_processes(f\"All process: Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    print(f\"All process: End of script with {accuracy['accuracy']} accuracy\")\n",
    "\n",
    "print_in_one_process(\"Printing with delay in process 0\")\n",
    "print_in_process_0(\"End of process 0\")\n",
    "print_in_process_1(\"End of process 1\")\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "print_in_one_process(\"End of script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14218.23 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14666.25 examples/s]\n",
      "  0%|                                                   | 0/176 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.58it/s]\n",
      "Process 1: End of process 1\n",
      "Accuracy = 0.212\n",
      "End of script with 0.212 accuracy\n",
      "All process: Accuracy = 0.212\n",
      "All process: End of script with 0.212 accuracy\n",
      "Printing with delay in process 0\n",
      "Process 0: End of process 0\n",
      "End of script\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch accelerate_scripts/09_accelerate_base_code_sync_all_process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver primero se ha impreso `Process 1: End of process 1` y luego el resto, esto es porque el resto de prints se hacen o en el proceso 0 o en todos los procesos, as√≠ que hasta que no termine el delay de 2 segundos que hemos puesto no se ejecuta el resto de c√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar y cargar el state dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando entrenamos, a veces guardamos el estado para poder seguir en otro momento\n",
    "\n",
    "Para guardar el estado tendremos que usar los m√©todos `save_state()` y `load_state()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/09_accelerate_save_and_load_checkpoints.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/10_accelerate_save_and_load_checkpoints.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "@accelerator.on_local_main_process\n",
    "def print_something(something):\n",
    "    print(something)\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "\n",
    "    # Guardamos los pesos\n",
    "    accelerator.save_state(\"accelerate_scripts/checkpoints\")\n",
    "\n",
    "print_something(f\"Accuracy = {accuracy['accuracy']}\")\n",
    "\n",
    "# Cargamos los pesos\n",
    "accelerator.load_state(\"accelerate_scripts/checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [01:58<00:00,  1.48it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.40it/s]\n",
      "Accuracy = 0.2142\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch accelerate_scripts/10_accelerate_save_and_load_checkpoints.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se us√≥ el m√©todo `prepare` se envolvi√≥ el modelo para poder guardarlo en los dispositivos necesarios. Por lo que a la hora de guardarlo tenemos que usar el m√©todo `save_model` que primero lo desenvuelve y luego lo guarda. Adem√°s si usamos el par√°metro `safe_serialization=True` se guardar√° el modelo como un `safe tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerate_scripts/11_accelerate_save_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/11_accelerate_save_model.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "@accelerator.on_local_main_process\n",
    "def print_something(something):\n",
    "    print(something)\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "\n",
    "    # Guardamos el modelo\n",
    "    accelerator.wait_for_everyone()\n",
    "    accelerator.save_model(model, \"accelerate_scripts/model\", safe_serialization=True)\n",
    "\n",
    "print_something(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [01:58<00:00,  1.48it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.35it/s]\n",
      "Accuracy = 0.214\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch accelerate_scripts/11_accelerate_save_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el modelo `pretrained`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En modelos que usan la librer√≠a `transformers` debemos guardar el modelo con el m√©todo `save_pretrained` para poder cargarlo con el m√©todo `from_pretrained`. Antes de guardarlo hay que desenvolverlo con el m√©todo `unwrap_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerate_scripts/11_accelerate_save_pretrained.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/12_accelerate_save_pretrained.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "@accelerator.on_local_main_process\n",
    "def print_something(something):\n",
    "    print(something)\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "\n",
    "    # Guardamos el modelo pretrained\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        \"accelerate_scripts/model_pretrained\",\n",
    "        is_main_process=accelerator.is_main_process,\n",
    "        save_function=accelerator.save,\n",
    "    )\n",
    "\n",
    "print_something(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:02<00:00, 15152.47 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45000/45000 [00:02<00:00, 15119.13 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 12724.70 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 12397.49 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 15247.21 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 15138.03 examples/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [01:59<00:00,  1.48it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:05<00:00,  3.37it/s]\n",
      "Accuracy = 0.21\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch accelerate_scripts/12_accelerate_save_pretrained.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lo podr√≠amos cargar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at accelerate_scripts/model_pretrained and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoints = \"accelerate_scripts/model_pretrained\"\n",
    "tokenizer = AutoModel.from_pretrained(checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento en notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos visto c√≥mo ejecutar scripts, pero si quieres ejecutar el c√≥digo en un notebook, podemos escribir el mismo c√≥digo de antes, pero encapsulado en una funci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero importamos las librer√°s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "import time\n",
    "# from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos la funci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_code(batch_size: int = 64):\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "    num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "    max_len = 130\n",
    "\n",
    "    checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "    def tokenize_function(dataset):\n",
    "        return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_dataset = {\n",
    "        \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "        \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "        \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    }\n",
    "    tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "    tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "    tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "    BS = batch_size\n",
    "    dataloader = {\n",
    "        \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "        \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "        \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "    }\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "    model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    EPOCHS = 1\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = accelerator.device\n",
    "\n",
    "    # model.to(device)\n",
    "    model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        model.train()\n",
    "        progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "        for batch in progress_bar_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"]#.to(device)\n",
    "            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "            labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "        for batch in progress_bar_validation:\n",
    "            input_ids = batch[\"input_ids\"]#.to(device)\n",
    "            attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "            labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "            # Recopilamos las predicciones de todos los dispositivos\n",
    "            predictions = accelerator.gather_for_metrics(predictions)\n",
    "            labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "            accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "        accuracy = metric.compute()\n",
    "        \n",
    "    accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder ejecutar el entrenamiento en el notebook usamos la funcion `notebook_launcher`, al que le pasamos la funci√≥n que queremos ejecutar, los argumentos de esa funci√≥n y el n√∫mero de GPUs en las que vamos a entrenar con la variable `num_processes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [02:01<00:00,  1.45it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.2112\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "args = (128,)\n",
    "notebook_launcher(train_code, args, num_processes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento en FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando al principio configuramos `accelerate` nos pregunt√≥ `Do you wish to use FP16 or BF16 (mixed precision)?` y dijimos que no, as√≠ que ahora vamos a decirle que s√≠, que queremos en FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos entrenado en FP32, lo que quiere decir que cada peso del modelo es un n√∫mero en coma flotante de 32 bits, y ahora vamos a usar un n√∫mero en coma flotante de 16 bits, es decir, el modelo va a ocupar menos. Por lo que van a pasar dos cosas, podremos usar un batch size mayor y adem√°s ser√° m√°s r√°pido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero volvemos a lanzar `accelerate config` y le vamos a decir que queremos FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "In which compute environment are you running?\n",
      "This machine\n",
      "--------------------------------------------------------------------------------\n",
      "multi-GPU\n",
      "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
      "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
      "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
      "Do you want to use DeepSpeed? [yes/NO]: no\n",
      "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
      "Do you want to use Megatron-LM ? [yes/NO]: no\n",
      "How many GPU(s) should be used for distributed training? [1]:2\n",
      "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
      "--------------------------------------------------------------------------------\n",
      "Do you wish to use FP16 or BF16 (mixed precision)?\n",
      "fp16\n",
      "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
     ]
    }
   ],
   "source": [
    "!accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un script para entrenar, con el mismo batch size de antes, para ver si tarda menos en entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/12_accelerate_base_code_fp16_bs128.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/13_accelerate_base_code_fp16_bs128.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 128\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos a ver cuanto tarda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14983.76 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14315.47 examples/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [01:01<00:00,  2.88it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  6.84it/s]\n",
      "Accuracy = 0.2094\n",
      "CPU times: user 812 ms, sys: 163 ms, total: 976 ms\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/13_accelerate_base_code_fp16_bs128.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando ejecutamos este entrenamiento en FP32 tard√≥ unos 2 minutos y medio, y ahora m√°s o menos 1 minuto y medio. Vamos a ver si ahora en vez de entrenar con un batch size de 128, lo hacemos con uno de 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/14_accelerate_base_code_fp16_bs256.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 256\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 15390.30 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 14990.92 examples/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88/88 [00:54<00:00,  1.62it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  3.45it/s]\n",
      "Accuracy = 0.2236\n",
      "CPU times: user 670 ms, sys: 91.6 ms, total: 761 ms\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha bajado solo unos 15 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento en BF16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes hemos entrenado en FP16 y ahora lo vamos a hacer en BF16, ¬øCu√°l es la diferencia?\n",
    "\n",
    "![FP32_FP16_BF16](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/FP32_FP16_BF16.webp)\n",
    "\n",
    "Como podemos ver en la imagen, mientras que FP16 en comparaci√≥n con FP32 tiene menos bits en la mantisa y el exponente, lo que hace que su rango sea mucho menor, BF16 en comparaci√≥n con FP32 tiene el mismo n√∫mero de bits del exponente pero menos en la mantisa, lo que hace que BF16 tenga el mismo rango de n√∫meros que FP32, pero es menos preciso\n",
    "\n",
    "Esto es beneficioso porque en FP16 algunos c√°lculos podr√≠an dar n√∫meros muy altos, que en formato FP16 no se podr√≠an representar. Adem√°s hay ciertos dispositivos HW que est√°n optimizados para este formato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que antes ejecutamos `accelerate config` y le indicamos que queremos BF16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "In which compute environment are you running?\n",
      "This machine\n",
      "--------------------------------------------------------------------------------\n",
      "multi-GPU\n",
      "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
      "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
      "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
      "Do you want to use DeepSpeed? [yes/NO]: no\n",
      "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
      "Do you want to use Megatron-LM ? [yes/NO]: no\n",
      "How many GPU(s) should be used for distributed training? [1]:2\n",
      "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
      "--------------------------------------------------------------------------------\n",
      "Do you wish to use FP16 or BF16 (mixed precision)?\n",
      "bf16\n",
      "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
     ]
    }
   ],
   "source": [
    "!accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutamos el √∫ltimo script que hab√≠amos creado, es decir con un batch size de 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14814.95 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [00:03<00:00, 14506.83 examples/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88/88 [00:51<00:00,  1.70it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  3.21it/s]\n",
      "Accuracy = 0.2112\n",
      "CPU times: user 688 ms, sys: 144 ms, total: 832 ms\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha tardado un tiempo similar a lo que tard√≥ antes, lo cual es normal, ya que hemos entrenado un modelo con pesos de 16 bits, al igual que antes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento en FP8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a entrenar en formato FP8, que como su nombre indica, es un formato de coma flotante, donde cada peso tiene 8 bits, por lo que ejecutamos `accelerate config` para decirle que queremos FP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "In which compute environment are you running?\n",
      "This machine\n",
      "--------------------------------------------------------------------------------\n",
      "multi-GPU\n",
      "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
      "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
      "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
      "Do you want to use DeepSpeed? [yes/NO]: no\n",
      "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
      "Do you want to use Megatron-LM ? [yes/NO]: no\n",
      "How many GPU(s) should be used for distributed training? [1]:2\n",
      "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
      "--------------------------------------------------------------------------------\n",
      "Do you wish to use FP16 or BF16 (mixed precision)?\n",
      "fp8\n",
      "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
     ]
    }
   ],
   "source": [
    "!accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ejecutamos el √∫ltimo script, el de batch size de 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
      "    accelerator = Accelerator()\n",
      "                  ^^^^^^^^^^^^^\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
      "    self.state = AcceleratorState(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wallabot/Documentos/web/portafolio/posts/accelerate_scripts/13_accelerate_base_code_fp16_bs256.py\", line 12, in <module>\n",
      "    accelerator = Accelerator()\n",
      "                  ^^^^^^^^^^^^^\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/accelerator.py\", line 371, in __init__\n",
      "    self.state = AcceleratorState(\n",
      "                 ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/state.py\", line 790, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Using `fp8` precision requires `transformer_engine` or `MS-AMP` to be installed.\n",
      "[2024-05-13 21:40:56,455] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 501480) of binary: /home/wallabot/miniconda3/envs/nlp/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
      "    args.func(args)\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1048, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 702, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "accelerate_scripts/13_accelerate_base_code_fp16_bs256.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2024-05-13_21:40:56\n",
      "  host      : wallabot\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 501481)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-05-13_21:40:56\n",
      "  host      : wallabot\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 501480)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "CPU times: user 65.1 ms, sys: 14.5 ms, total: 79.6 ms\n",
      "Wall time: 7.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/14_accelerate_base_code_fp16_bs256.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los pesos ahora son de 8 bits y ocupan la mitad de memoria vamos a subir el batch size a 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/15_accelerate_base_code_fp8_bs512.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "import tqdm\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 512\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    progress_bar_train = tqdm.tqdm(dataloader[\"train\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = tqdm.tqdm(dataloader[\"validation\"], disable=not accelerator.is_local_main_process)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "        labels = accelerator.gather_for_metrics(labels)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "accelerator.print(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo ejecutamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!accelerate launch accelerate_scripts/15_accelerate_base_code_fp8_bs512.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso del ecosistema de Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver c√≥mo hacer inferencia de grandes modelos con la librer√≠a `transformers` de hugging face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencia con `pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos el ecosistema de Hugging Face es muy sencillo, ya que todo se produce por debajo sin tener que hacer nosotros mucho. En el caso de usar `pipeline`, que es la manera m√°s sencilla de hacer inferencia con la librer√≠a `transformers`, simplemente tenemos que decirle el modelo que queremos usar y muy importante, pasarle `device_map=\"auto\"`. Esto har√° que por debajo `accelerate` distribuya el modelo entre las distintas GPUs, RAM de la CPU o disco duro si es necesario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay m√°s posibles valores para `device_map`, que los veremos m√°s adelante, pero de momento qu√©date con `\"auto\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar el modelo `Llama3 8B`, que como su nombre indica es un modelo de unos 8 mil millones de par√°metros, como cada par√°metro por defecto est√° en formato FP32, que corresponde a 4 bytes (32 bits), eso quiere decir que si multiplicamos 8 mil millones de par√°metros por 4 bytes, nos queda que necesitar√≠a una GPU con unos 32 GB de VRAM.\n",
    "\n",
    "En mi caso tengo 2 GPUs de 24 GB de VRAM, por lo que no entrar√≠a en una sola GPU. Pero gracias a poner `device_map=\"auto\"`, accelerate desitribuir√° el modelo entre las dos GPUs y podr√© realizar la inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/09_inference_with_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/16_inference_with_pipeline.py\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
    "\n",
    "prompt = \"Conoces accelerate de hugging face?\"\n",
    "output = generator(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lo ejecutamos, solo que como pipeline usa por debajo accelerate, no necesitamos ejecutarlo con `acelerate launch script.py` sino que con `python script.py` vale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.27s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "[{'generated_text': 'Conoces accelerate de hugging face? ¬øQu√© es el modelo de lenguaje de transformers y c√≥mo se utiliza en el marco de hugging face? ¬øC√≥mo puedo utilizar modelos de lenguaje de transformers en mi aplicaci√≥n? ¬øQu√© son los tokenizers y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo crear un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los datasets y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar datasets para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar finetuning para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los checkpoints y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar checkpoints para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los evaluadores y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar evaluadores para evaluar el rendimiento de un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los pre-trainados y c√≥mo se utilizan en el marco de hugging face? ¬øC√≥mo puedo utilizar pre-trainados para entrenar un modelo de lenguaje personalizado utilizando transformers y hugging face? ¬øQu√© son los finetuning'}]\n"
     ]
    }
   ],
   "source": [
    "!python accelerate_scripts/16_inference_with_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, no ha respondido, sino que ha seguido haciendo preguntas. Esto es porque Llama3 es un modelo de lenguaje que lo que hace es predecir el siguiente token, as√≠ que con el prompt que le he pasado, ha considerado que los siguientes mejores tokens son unos que corresponden a m√°s preguntas. Lo cual tiene sentido, porque hay veces que la gente tiene dudas sobre un tema y genera muchas preguntas, as√≠ que para que nos conteste a la pregunta hay que condicionarle un poco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/10_inference_with_pipeline_condition.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/17_inference_with_pipeline_condition.py\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "generator = pipeline(model=checkpoints, device_map=\"auto\")\n",
    "\n",
    "prompt = \"Conoces accelerate de hugging face?\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Eres un chatbot amigable que siempre intenta solucionar las dudas\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
    "]\n",
    "output = generator(messages)\n",
    "print(output[0]['generated_text'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ves se ha generado un mensaje con roles, condicionando el modelo y con el prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.41s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "{'role': 'assistant', 'content': '¬°Hola!\\n\\nS√≠, conozco Accelerate de Hugging Face. Accelerate es una biblioteca de Python desarrollada por Hugging Face que se enfoca en simplificar y acelerar el entrenamiento y la evaluaci√≥n de modelos de lenguaje en diferentes dispositivos y entornos.\\n\\nCon Accelerate, puedes entrenar modelos de lenguaje en diferentes plataformas y dispositivos, como GPUs, TPUs, CPUs y servidores, sin necesidad de cambiar el c√≥digo de tu modelo. Esto te permite aprovechar al m√°ximo la potencia de c√°lculo de tus dispositivos y reducir el tiempo de entrenamiento.\\n\\nAccelerate tambi√©n ofrece varias caracter√≠sticas adicionales, como:\\n\\n* Soporte para diferentes frameworks de machine learning, como TensorFlow, PyTorch y JAX.\\n* Integraci√≥n con diferentes sistemas de almacenamiento y procesamiento de datos, como Amazon S3 y Google Cloud Storage.\\n* Soporte para diferentes protocolos de comunicaci√≥n, como HTTP y gRPC.\\n* Herramientas para monitorear y depurar tus modelos en tiempo real.\\n\\nEn resumen, Accelerate es una herramienta muy √∫til para desarrolladores de modelos de lenguaje que buscan simplificar y acelerar el proceso de entrenamiento y evaluaci√≥n de sus modelos.\\n\\n¬øTienes alguna pregunta espec√≠fica sobre Accelerate o necesitas ayuda para implementarlo en tu proyecto?'}\n"
     ]
    }
   ],
   "source": [
    "!python accelerate_scripts/17_inference_with_pipeline_condition.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora la respuesta si responde nuestro prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencia con `AutoClass`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo vamos a ver c√≥mo hacer la inferencia solo con `AutoClass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerate_scripts/11_inference_with_autoclass.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_scripts/18_inference_with_autoclass.py\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoints, device_map=\"auto\")\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "prompt = \"Conoces accelerate de hugging face?\"\n",
    "tokens_input = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "_ = model.generate(**tokens_input, streamer=streamer, max_new_tokens=500, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, se ha creado el objeto `streamer` que luego se le pasa al m√©todo `generate` del modelo. Esto es √∫til para que se vaya imprimiendo cada palabra a medida que se va generando y no haya que esperar a que se genere toda la salida para imprimirla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:09<00:00,  2.28s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "<|begin_of_text|>Conoces accelerate de hugging face? Si es as√≠, puedes utilizar la biblioteca `transformers` de Hugging Face para crear un modelo de lenguaje que pueda predecir la siguiente palabra en una secuencia de texto.\n",
      "\n",
      "Aqu√≠ te muestro un ejemplo de c√≥mo hacerlo:\n",
      "```\n",
      "import pandas as pd\n",
      "import torch\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "\n",
      "# Cargar el modelo y el tokenizador\n",
      "model_name = \"bert-base-uncased\"\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "\n",
      "# Cargar el conjunto de datos\n",
      "train_df = pd.read_csv(\"train.csv\")\n",
      "test_df = pd.read_csv(\"test.csv\")\n",
      "\n",
      "# Preprocesar los datos\n",
      "train_texts = train_df[\"text\"]\n",
      "train_labels = train_df[\"label\"]\n",
      "test_texts = test_df[\"text\"]\n",
      "\n",
      "# Convertir los textos en entradas para el modelo\n",
      "train_encodings = tokenizer.batch_encode_plus(train_texts, \n",
      "                                              add_special_tokens=True, \n",
      "                                              max_length=512, \n",
      "                                              return_attention_mask=True, \n",
      "                                              return_tensors='pt')\n",
      "\n",
      "test_encodings = tokenizer.batch_encode_plus(test_texts, \n",
      "                                             add_special_tokens=True, \n",
      "                                             max_length=512, \n",
      "                                             return_attention_mask=True, \n",
      "                                             return_tensors='pt')\n",
      "\n",
      "# Crear un dataloader para entrenar el modelo\n",
      "train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], \n",
      "                                               train_encodings[\"attention_mask\"], \n",
      "                                               torch.tensor(train_labels))\n",
      "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
      "\n",
      "# Entrenar el modelo\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "model.to(device)\n",
      "criterion = torch.nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
      "\n",
      "for epoch in range(5):\n",
      "    model.train()\n",
      "    total_loss = 0\n",
      "    for batch in train_loader:\n",
      "        input_ids = batch[0].to(device)\n",
      "        attention_mask = batch[1].to(device)\n",
      "        labels = batch[2].to(device)\n",
      "        optimizer.zero_grad()\n",
      "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
      "        loss = criterion(outputs, labels)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        total_loss += loss.item()\n",
      "    print(f\"Epoch {epoch+1}, Loss: {total\n"
     ]
    }
   ],
   "source": [
    "!python accelerate_scripts/18_inference_with_autoclass.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente la manera de hacer inferencias con pytorch es crear un modelo con los pesos inicializados aleatoriamente y a continuaci√≥n cargar un `state dict` con los pesos del modelo preentrenado, as√≠ que para obtener ese `state dict` vamos a hacer primero una peque√±a trampa y nos los vamos a descargar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /home/maximo.fernandez/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 230M/230M [02:48<00:00, 1.43MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos el `state dict` vamos a hacer inferencia como se hace normalmente en pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"     # Set device\n",
    "\n",
    "resnet152 = models.resnet152().to(device) # Create model with random weights and move to device\n",
    "state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device) # Load pretrained weights into device memory\n",
    "resnet152.load_state_dict(state_dict) # Load this weights into the model\n",
    "\n",
    "input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
    "output = resnet152(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a explicar qu√© ha pasado\n",
    "\n",
    " * Cuando hemos hecho `resnet152 = models.resnet152().to(device)` se ha cargado una resnet152 con pesos aleatorios en la memoria de la GPU\n",
    " * Cuando hemos hecho `state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)` se ha cargado un diccionario con los pesos entrenados en la memoria de la GPU\n",
    " * Cuando hemos hecho `resnet152.load_state_dict(state_dict)` se han asignado esos pesos preentrenados al modelo\n",
    "\n",
    "Es decir se ha cargado dos veces el modelo en la memoria de la GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Te puedes estar preguntando por qu√© hemos hecho primero \n",
    "\n",
    "``` python\n",
    "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "torch.save(model.state_dict(), 'accelerate_scripts/resnet152_pretrained.pth')\n",
    "```\n",
    "\n",
    "Para luego hacer\n",
    "\n",
    "``` python\n",
    "resnet152 = models.resnet152().to(device)\n",
    "state_dict = torch.load('accelerate_scripts/resnet152_pretrained.pth', map_location=device)\n",
    "resnet152.load_state_dict(state_dict)\n",
    "```\n",
    "\n",
    "Y por que no usamos directamente\n",
    "\n",
    "```\n",
    "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "```\n",
    "\n",
    "Y nos dejamos de guardar el `state dict` para luego cargarlo. Bueno, pues porque Pytorch, por edbajo hace lo mismo que hemos hecho. As√≠ que para poder ver todo el proceso hemos hecho en varias lineas lo que Pytorch hace en una"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta manera de trabajar ha funcionado bien hasta ahora, mientras que los modelos ten√≠an un tama√±o manejable por las GPUs de usuario. Pero desde la llegada de los LLMs este enfoque no tiene sentido\n",
    "\n",
    "Por ejemplo un modelo de 6B de par√°metros ocupar√≠a en la memoria 24 GB, y como se carga dos veces con esta manera de trabajar har√≠a falta tener una GPU de 48 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠ que para arreglar esto, la manera de cargar un modelo preentrenado de Pytorch es:\n",
    " * Crear un modelo vac√≠o con `init_empty_weights` que no ocupar√° RAM\n",
    " * Luego cargar los pesos con `load_checkpoint_and_dispatch` que cargar√° un punto de control dentro del modelo vac√≠o y distribuir√° los pesos para cada capa en todos los dispositivos que se tenga disponibles (GPU, CPU RAM y disco duro), gracias a poner `device_map=\"auto\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "with init_empty_weights():\n",
    "    resnet152 = models.resnet152()\n",
    "\n",
    "resnet152 = load_checkpoint_and_dispatch(resnet152, checkpoint='accelerate_scripts/resnet152_pretrained.pth', device_map=\"auto\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "input = torch.rand(1, 3, 224, 224).to(device)  # Random image with batch size 1\n",
    "output = resnet152(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√≥mo funciona accelerate por debajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este v√≠deo se puede ver gr√°ficamente c√≥mo funciona accelerate por debajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"1280\" height=\"720\" src=\"https://www.youtube.com/embed/MWCSGj9jEAo\" title=\"Accelerate Big Model Inference: How Does it Work?\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializaci√≥n de un modelo vac√≠o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Accelerate` crea el esqueleto de un modelo vac√≠o mediante `init_empty_weights` para que ocupe la menor cantidad de memoria posible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, veamos cuanta RAM tengo ahora disponible en mi ordenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 31.24 GB, Available RAM: 22.62 GB, Used RAM: 7.82 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def get_ram_info():\n",
    "    ram = dict(psutil.virtual_memory()._asdict())\n",
    "    print(f\"Total RAM: {(ram['total']/1024/1024/1024):.2f} GB, Available RAM: {(ram['available']/1024/1024/1024):.2f} GB, Used RAM: {(ram['used']/1024/1024/1024):.2f} GB\")\n",
    "\n",
    "get_ram_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo unos 22 GB de RAM disponibles\n",
    "\n",
    "Ahora vamos a intentar crear un modelo 5000x1000x1000 par√°metros, es decir de 5B de par√°metros, si cada par√°metro est√° en FP32, supone 20 GB de RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si volvemos a ver la RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 31.24 GB, Available RAM: 3.77 GB, Used RAM: 26.70 GB\n"
     ]
    }
   ],
   "source": [
    "get_ram_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos ahora solo tenemos 3 GB de RAM disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a eliminar el modelo para liberar RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 31.24 GB, Available RAM: 22.44 GB, Used RAM: 8.03 GB\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "get_ram_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a tener unos 22 GB de RAM disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a usar `init_empty_weights` de `accelerate` y luego vemos la RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 31.24 GB, Available RAM: 22.32 GB, Used RAM: 8.16 GB\n"
     ]
    }
   ],
   "source": [
    "from accelerate import init_empty_weights\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = nn.Sequential(*[nn.Linear(5000, 1000) for _ in range(1000)])\n",
    "\n",
    "get_ram_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes ten√≠amos exactamente 22.44 GB libres y tras crear el modelo con `init_empty_weights` tenemos 22.32 GB. El ahorro en RAM es enorme! Casi no se ha ocupado RAM para crear el modelo.\n",
    "\n",
    "Esto se basa en el metadispositivo introducido en PyTorch 1.9, por lo que es importante que para usar `accelerate` tengamos una versi√≥n de Pytorch posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de los pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos inicializado el modelo tenemos que cargarle los pesos que lo hacemos mediante `load_checkpoint_and_dispatch` que como su nombre indica carga los pesos y los env√≠a al dispositivo o dispositivos que sea necesario"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
