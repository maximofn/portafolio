{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Optimun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optimum` es una extensi√≥n de la librer√≠a [Transformers](https://maximofn.com/hugging-face-transformers/) que proporciona un conjunto de herramientas de optimizaci√≥n del rendimiento para entrenar y para la inferencia modelos, en hardware espec√≠fico, con la m√°xima eficiencia.\n",
    "\n",
    "El ecosistema de IA evoluciona r√°pidamente y cada d√≠a surge m√°s hardware especializado junto con sus propias optimizaciones. Por tanto, `Optimum` permite a los utilizar eficientemente cualquiera de este HW con la misma facilidad que [Transformers](https://maximofn.com/hugging-face-transformers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optimun` premite la optimizaci√≥n para las siguientes plataformas HW:\n",
    "\n",
    " * Nvidia\n",
    " * AMD\n",
    " * Intel\n",
    " * AWS\n",
    " * TPU\n",
    " * Habana\n",
    " * FuriosaAI\n",
    "\n",
    "Adem√°s ofrece aceleraci√≥n para las siguientes integraciones open source\n",
    "\n",
    " * ONNX runtime\n",
    " * Exporters: Exportar omdelos Pytorch o TensorFlow a diferentes formatos como ONNX o TFLite\n",
    " * BetterTransformer\n",
    " * Torch FX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar `Optimum` simplemente ejecuta:\n",
    "\n",
    "```bash\n",
    "pip install optimum\n",
    "```\n",
    "\n",
    "Pero si se quiere instalar con soporte para todas las plataformas HW, se puede hacer as√≠\n",
    "\n",
    "|Accelerator\t|Installation|\n",
    "|---|---|\n",
    "|ONNX Runtime\t|`pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]`|\n",
    "|Intel Neural Compressor\t|`pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]`|\n",
    "|OpenVINO\t|`pip install --upgrade --upgrade-strategy eager optimum[openvino]`|\n",
    "|NVIDIA TensorRT-LLM\t|`docker run -it --gpus all --ipc host huggingface/optimum-nvidia`|\n",
    "|AMD Instinct GPUs and Ryzen AI NPU\t|`pip install --upgrade --upgrade-strategy eager optimum[amd]`|\n",
    "|AWS Trainum & Inferentia\t|`pip install --upgrade --upgrade-strategy eager optimum[neuronx]`|\n",
    "|Habana Gaudi Processor (HPU)\t|`pip install --upgrade --upgrade-strategy eager optimum[habana]`|\n",
    "|FuriosaAI\t|`pip install --upgrade --upgrade-strategy eager optimum[furiosa]`|\n",
    "\n",
    "los flags `--upgrade --upgrade-strategy eager` son necesarios para garantizar que los diferentes paquetes se actualicen a la √∫ltima versi√≥n posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la mayor√≠a de la gente usa Pytorch en GPUs de Nvidia, y sobre todo, como Nvidia es lo que yo tengo, este post va a hablar solo del uso de `Optimun` con GPUs de Nvidia y Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeterTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BetterTransformer es una optimizaci√≥n nativa de PyTorch para obtener una aceleraci√≥n de x1,25 a x4 en la inferencia de modelos basados ‚Äã‚Äãen Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BetterTransformer es una API que permite aprovechar las caracter√≠sticas de hardware modernas para acelerar el entrenamiento y la inferencia de modelos de transformers en PyTorch, utilizando implementaciones de atenci√≥n m√°s eficientes y `fast path` de la versi√≥n nativa de `nn.TransformerEncoderLayer`.\n",
    "\n",
    "BetterTransformer usa dos tipos de aceleraciones:\n",
    "\n",
    " 1. `Flash Attention`: Esta es una implementaci√≥n de la `attention` que utiliza `sparse` para reducir la complejidad computacional. La atenci√≥n es una de las operaciones m√°s costosas en los modelos de transformers, y `Flash Attention` la hace m√°s eficiente.\n",
    " 2. `Memory-Efficient Attention`: Esta es otra implementaci√≥n de la atenci√≥n que utiliza la funci√≥n `scaled_dot_product_attention` de PyTorch. Esta funci√≥n es m√°s eficiente en t√©rminos de memoria que la implementaci√≥n est√°ndar de la atenci√≥n en PyTorch.\n",
    "\n",
    "Adem√°s, la versi√≥n 2.0 de PyTorch incluye un operador de atenci√≥n de productos punto escalado (SDPA) nativo como parte de `torch.nn.functional`\n",
    "\n",
    "`Optimun` proporciona esta funcionalidad con la librer√≠a `Transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia con Automodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a ver c√≥mo ser√≠a la inferencia normal con `Transformers` y `Automodel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_tokens = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "output_tokens = model.generate(**input_tokens, max_length=50)\n",
    "\n",
    "sentence_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "sentence_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos c√≥mo se optimizar√≠a con `BetterTransformer` y `Optimun`\n",
    "\n",
    "Lo que tenemos que hacer es convertir el modelo mediante el m√©todo `transform` de `BeterTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "checkpoint = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_hf = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "\n",
    "# Convert the model to a BetterTransformer model\n",
    "model = BetterTransformer.transform(model_hf, keep_original_model=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_tokens = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "output_tokens = model.generate(**input_tokens, max_length=50)\n",
    "\n",
    "sentence_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "sentence_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferecncia con Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que antes, primero vemos c√≥mo ser√≠a la inferencia normal con `Transformers` y `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05116177722811699,\n",
       "  'token': 8422,\n",
       "  'token_str': 'stanford',\n",
       "  'sequence': 'i am a student at stanford university.'},\n",
       " {'score': 0.04033993184566498,\n",
       "  'token': 5765,\n",
       "  'token_str': 'harvard',\n",
       "  'sequence': 'i am a student at harvard university.'},\n",
       " {'score': 0.03990468755364418,\n",
       "  'token': 7996,\n",
       "  'token_str': 'yale',\n",
       "  'sequence': 'i am a student at yale university.'},\n",
       " {'score': 0.0361952930688858,\n",
       "  'token': 10921,\n",
       "  'token_str': 'cornell',\n",
       "  'sequence': 'i am a student at cornell university.'},\n",
       " {'score': 0.03303057327866554,\n",
       "  'token': 9173,\n",
       "  'token_str': 'princeton',\n",
       "  'sequence': 'i am a student at princeton university.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"fill-mask\", model=\"distilbert-base-uncased\")\n",
    "pipe(\"I am a student at [MASK] University.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos c√≥mo optimizarlo, para ello usamos `pipeline` de `Optimun`, en vez de el de `Transformers`. Adem√°s hay que indicar que queremos usar `bettertransformer` como acelerador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "  hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05116180703043938,\n",
       "  'token': 8422,\n",
       "  'token_str': 'stanford',\n",
       "  'sequence': 'i am a student at stanford university.'},\n",
       " {'score': 0.040340032428503036,\n",
       "  'token': 5765,\n",
       "  'token_str': 'harvard',\n",
       "  'sequence': 'i am a student at harvard university.'},\n",
       " {'score': 0.039904672652482986,\n",
       "  'token': 7996,\n",
       "  'token_str': 'yale',\n",
       "  'sequence': 'i am a student at yale university.'},\n",
       " {'score': 0.036195311695337296,\n",
       "  'token': 10921,\n",
       "  'token_str': 'cornell',\n",
       "  'sequence': 'i am a student at cornell university.'},\n",
       " {'score': 0.03303062543272972,\n",
       "  'token': 9173,\n",
       "  'token_str': 'princeton',\n",
       "  'sequence': 'i am a student at princeton university.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.pipelines import pipeline\n",
    "\n",
    "# Use the BetterTransformer pipeline\n",
    "pipe = pipeline(task=\"fill-mask\", model=\"distilbert-base-uncased\", accelerator=\"bettertransformer\")\n",
    "pipe(\"I am a student at [MASK] University.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrneamiento con `Optimun` hacemos lo mismo que con la inferencia con Automodel, convertimos el modelo mediante el m√©todo `transform` de `BeterTransformer`.\n",
    "\n",
    "Cuando terminamos el entrenamiento, volvemos a convertir el modelo mediante el m√©todo `reverse` de `BeterTransformer` para volver a tener el modelo original y as√≠ poder guardarlo y subirlo al hub de Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "checkpoint = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_hf = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "\n",
    "# Convert the model to a BetterTransformer model\n",
    "model = BetterTransformer.transform(model_hf, keep_original_model=True)\n",
    "\n",
    "##############################################################################\n",
    "# do your training here\n",
    "##############################################################################\n",
    "\n",
    "# Convert the model back to a Hugging Face model\n",
    "model_hf = BetterTransformer.reverse(model)\n",
    "\n",
    "model_hf.save_pretrained(\"fine_tuned_model\")\n",
    "model_hf.push_to_hub(\"fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "maximofn": {
      "date": "2024-06-01",
      "description_es": "¬°Atenci√≥n, modelos de PyTorch lentos! üêå Optimun, la librer√≠a de Hugging Face, viene al rescate para acelerar tus entrenamientos e inferencias. Con Optimun, puedes olvidarte de los problemas de velocidad y disfrutar de m√°s velocidad y eficiencia üïíÔ∏è. Y lo mejor de todo, es compatible con PyTorch. ¬°Vamos, dale un boost a tus modelos con Optimun! üíª",
      "description_en": "Attention, slow PyTorch models! üêå Optimun, the Hugging Face library, comes to the rescue to speed up your workouts and inferences. With Optimun, you can forget about speed issues and enjoy more speed and efficiency üïíÔ∏è. And best of all, it's PyTorch compatible - go on, give your models a boost with Optimun! üíª",
      "description_pt": "Aten√ß√£o, modelos PyTorch lentos! üêå Optimun, a biblioteca Hugging Face, vem em seu socorro para acelerar seu treinamento e suas infer√™ncias. Com o Optimun, voc√™ pode esquecer os problemas de velocidade e aproveitar mais velocidade e efici√™ncia üïíÔ∏è. E o melhor de tudo √© que ela √© compat√≠vel com o PyTorch - v√° em frente, d√™ um impulso aos seus modelos com a Optimun! üíª",
      "end_url": "hugging-face-optimun",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_optimun.webp",
      "keywords_en": "hugging face, optimun, pytorch, transformers, training, inference, speed, efficiency",
      "keywords_es": "hugging face, optimun, pytorch, transformers, entrenamiento, inferencia, velocidad, eficiencia",
      "keywords_pt": "hugging face, optimun, pytorch, transformers, treinamento, infer√™ncia, velocidade, efici√™ncia",
      "title_en": "Hugging Face Optimun",
      "title_es": "Hugging Face Optimun",
      "title_pt": "Hugging Face Optimun"
    },
    "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
