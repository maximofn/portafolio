{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Optimun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optimum` es una extensión de la librería [Transformers](https://maximofn.com/hugging-face-transformers/) que proporciona un conjunto de herramientas de optimización del rendimiento para entrenar y para la inferencia modelos, en hardware específico, con la máxima eficiencia.\n",
    "\n",
    "El ecosistema de IA evoluciona rápidamente y cada día surge más hardware especializado junto con sus propias optimizaciones. Por tanto, `Optimum` permite a los utilizar eficientemente cualquiera de este HW con la misma facilidad que [Transformers](https://maximofn.com/hugging-face-transformers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Optimun` premite la optimización para las siguientes plataformas HW:\n",
    "\n",
    " * Nvidia\n",
    " * AMD\n",
    " * Intel\n",
    " * AWS\n",
    " * TPU\n",
    " * Habana\n",
    " * FuriosaAI\n",
    "\n",
    "Además ofrece aceleración para las siguientes integraciones open source\n",
    "\n",
    " * ONNX runtime\n",
    " * Exporters: Exportar omdelos Pytorch o TensorFlow a diferentes formatos como ONNX o TFLite\n",
    " * BetterTransformer\n",
    " * Torch FX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar `Optimum` simplemente ejecuta:\n",
    "\n",
    "```bash\n",
    "pip install optimum\n",
    "```\n",
    "\n",
    "Pero si se quiere instalar con soporte para todas las plataformas HW, se puede hacer así\n",
    "\n",
    "|Accelerator\t|Installation|\n",
    "|---|---|\n",
    "|ONNX Runtime\t|`pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]`|\n",
    "|Intel Neural Compressor\t|`pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]`|\n",
    "|OpenVINO\t|`pip install --upgrade --upgrade-strategy eager optimum[openvino]`|\n",
    "|NVIDIA TensorRT-LLM\t|`docker run -it --gpus all --ipc host huggingface/optimum-nvidia`|\n",
    "|AMD Instinct GPUs and Ryzen AI NPU\t|`pip install --upgrade --upgrade-strategy eager optimum[amd]`|\n",
    "|AWS Trainum & Inferentia\t|`pip install --upgrade --upgrade-strategy eager optimum[neuronx]`|\n",
    "|Habana Gaudi Processor (HPU)\t|`pip install --upgrade --upgrade-strategy eager optimum[habana]`|\n",
    "|FuriosaAI\t|`pip install --upgrade --upgrade-strategy eager optimum[furiosa]`|\n",
    "\n",
    "los flags `--upgrade --upgrade-strategy eager` son necesarios para garantizar que los diferentes paquetes se actualicen a la última versión posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la mayoría de la gente usa Pytorch en GPUs de Nvidia, y sobre todo, como Nvidia es lo que yo tengo, este post va a hablar solo del uso de `Optimun` con GPUs de Nvidia y Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeterTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BetterTransformer es una optimización nativa de PyTorch para obtener una aceleración de x1,25 a x4 en la inferencia de modelos basados ​​en Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BetterTransformer es una API que permite aprovechar las características de hardware modernas para acelerar el entrenamiento y la inferencia de modelos de transformers en PyTorch, utilizando implementaciones de atención más eficientes y `fast path` de la versión nativa de `nn.TransformerEncoderLayer`.\n",
    "\n",
    "BetterTransformer usa dos tipos de aceleraciones:\n",
    "\n",
    " 1. `Flash Attention`: Esta es una implementación de la `attention` que utiliza `sparse` para reducir la complejidad computacional. La atención es una de las operaciones más costosas en los modelos de transformers, y `Flash Attention` la hace más eficiente.\n",
    " 2. `Memory-Efficient Attention`: Esta es otra implementación de la atención que utiliza la función `scaled_dot_product_attention` de PyTorch. Esta función es más eficiente en términos de memoria que la implementación estándar de la atención en PyTorch.\n",
    "\n",
    "Además, la versión 2.0 de PyTorch incluye un operador de atención de productos punto escalado (SDPA) nativo como parte de `torch.nn.functional`\n",
    "\n",
    "`Optimun` proporciona esta funcionalidad con la librería `Transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia con Automodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a ver cómo sería la inferencia normal con `Transformers` y `Automodel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_tokens = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "output_tokens = model.generate(**input_tokens, max_length=50)\n",
    "\n",
    "sentence_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "sentence_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos cómo se optimizaría con `BetterTransformer` y `Optimun`\n",
    "\n",
    "Lo que tenemos que hacer es convertir el modelo mediante el método `transform` de `BeterTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "checkpoint = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_hf = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "\n",
    "# Convert the model to a BetterTransformer model\n",
    "model = BetterTransformer.transform(model_hf, keep_original_model=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "input_tokens = tokenizer([\"Me encanta aprender de\"], return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "output_tokens = model.generate(**input_tokens, max_length=50)\n",
    "\n",
    "sentence_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "sentence_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferecncia con Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que antes, primero vemos cómo sería la inferencia normal con `Transformers` y `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05116177722811699,\n",
       "  'token': 8422,\n",
       "  'token_str': 'stanford',\n",
       "  'sequence': 'i am a student at stanford university.'},\n",
       " {'score': 0.04033993184566498,\n",
       "  'token': 5765,\n",
       "  'token_str': 'harvard',\n",
       "  'sequence': 'i am a student at harvard university.'},\n",
       " {'score': 0.03990468755364418,\n",
       "  'token': 7996,\n",
       "  'token_str': 'yale',\n",
       "  'sequence': 'i am a student at yale university.'},\n",
       " {'score': 0.0361952930688858,\n",
       "  'token': 10921,\n",
       "  'token_str': 'cornell',\n",
       "  'sequence': 'i am a student at cornell university.'},\n",
       " {'score': 0.03303057327866554,\n",
       "  'token': 9173,\n",
       "  'token_str': 'princeton',\n",
       "  'sequence': 'i am a student at princeton university.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"fill-mask\", model=\"distilbert-base-uncased\")\n",
    "pipe(\"I am a student at [MASK] University.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos cómo optimizarlo, para ello usamos `pipeline` de `Optimun`, en vez de el de `Transformers`. Además hay que indicar que queremos usar `bettertransformer` como acelerador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n",
      "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "  hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05116180703043938,\n",
       "  'token': 8422,\n",
       "  'token_str': 'stanford',\n",
       "  'sequence': 'i am a student at stanford university.'},\n",
       " {'score': 0.040340032428503036,\n",
       "  'token': 5765,\n",
       "  'token_str': 'harvard',\n",
       "  'sequence': 'i am a student at harvard university.'},\n",
       " {'score': 0.039904672652482986,\n",
       "  'token': 7996,\n",
       "  'token_str': 'yale',\n",
       "  'sequence': 'i am a student at yale university.'},\n",
       " {'score': 0.036195311695337296,\n",
       "  'token': 10921,\n",
       "  'token_str': 'cornell',\n",
       "  'sequence': 'i am a student at cornell university.'},\n",
       " {'score': 0.03303062543272972,\n",
       "  'token': 9173,\n",
       "  'token_str': 'princeton',\n",
       "  'sequence': 'i am a student at princeton university.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.pipelines import pipeline\n",
    "\n",
    "# Use the BetterTransformer pipeline\n",
    "pipe = pipeline(task=\"fill-mask\", model=\"distilbert-base-uncased\", accelerator=\"bettertransformer\")\n",
    "pipe(\"I am a student at [MASK] University.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrneamiento con `Optimun` hacemos lo mismo que con la inferencia con Automodel, convertimos el modelo mediante el método `transform` de `BeterTransformer`.\n",
    "\n",
    "Cuando terminamos el entrenamiento, volvemos a convertir el modelo mediante el método `reverse` de `BeterTransformer` para volver a tener el modelo original y así poder guardarlo y subirlo al hub de Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "checkpoint = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_hf = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n",
    "\n",
    "# Convert the model to a BetterTransformer model\n",
    "model = BetterTransformer.transform(model_hf, keep_original_model=True)\n",
    "\n",
    "##############################################################################\n",
    "# do your training here\n",
    "##############################################################################\n",
    "\n",
    "# Convert the model back to a Hugging Face model\n",
    "model_hf = BetterTransformer.reverse(model)\n",
    "\n",
    "model_hf.save_pretrained(\"fine_tuned_model\")\n",
    "model_hf.push_to_hub(\"fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "maximofn": {
      "date": "2024-06-01",
      "description_es": "¡Atención, modelos de PyTorch lentos! 🐌 Optimun, la librería de Hugging Face, viene al rescate para acelerar tus entrenamientos e inferencias. Con Optimun, puedes olvidarte de los problemas de velocidad y disfrutar de más velocidad y eficiencia 🕒️. Y lo mejor de todo, es compatible con PyTorch. ¡Vamos, dale un boost a tus modelos con Optimun! 💻",
      "description_en": "Attention, slow PyTorch models! 🐌 Optimun, the Hugging Face library, comes to the rescue to speed up your workouts and inferences. With Optimun, you can forget about speed issues and enjoy more speed and efficiency 🕒️. And best of all, it's PyTorch compatible - go on, give your models a boost with Optimun! 💻",
      "description_pt": "Atenção, modelos PyTorch lentos! 🐌 Optimun, a biblioteca Hugging Face, vem em seu socorro para acelerar seu treinamento e suas inferências. Com o Optimun, você pode esquecer os problemas de velocidade e aproveitar mais velocidade e eficiência 🕒️. E o melhor de tudo é que ela é compatível com o PyTorch - vá em frente, dê um impulso aos seus modelos com a Optimun! 💻",
      "end_url": "hugging-face-optimun",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/huggingface_optimun.webp",
      "keywords_en": "hugging face, optimun, pytorch, transformers, training, inference, speed, efficiency",
      "keywords_es": "hugging face, optimun, pytorch, transformers, entrenamiento, inferencia, velocidad, eficiencia",
      "keywords_pt": "hugging face, optimun, pytorch, transformers, treinamento, inferência, velocidade, eficiência",
      "title_en": "Hugging Face Optimun",
      "title_es": "Hugging Face Optimun",
      "title_pt": "Hugging Face Optimun"
    },
    "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
