{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BPE tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El tokenizador `BPE` (Byte Pair Encoding - codificación de pares de bytes) es un algoritmo de compresión de datos que se utiliza para crear un vocabulario de subpalabras a partir de un corpus de texto. Este algoritmo se basa en la frecuencia de los pares de bytes en el texto. Se popularizó porque fue utilizado como tokenizador por LLMs como GPT, GPT-2, RoBERTa, BART y DeBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algoritmo de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supongamos que tenemos un corpus de texto que solo contiene las siguientes palabras `hug, pug, pun, bun y hugs`, el primer paso consiste en crear un vocabulario con todos los caracteres presentes en el corpus, en nuestro caso será `b, g, h, n, p, s, u`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus words: ['hug', 'pug', 'pun', 'bun', 'hugs']\n",
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "Number of initial corpus tokens: 7\n"
          ]
        }
      ],
      "source": [
        "corpus_words = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
        "\n",
        "# Concatenate all the words in the corpus\n",
        "initial_corpus_tokens = \"\"\n",
        "for word in corpus_words:\n",
        "    initial_corpus_tokens += word\n",
        "\n",
        "# Convert the concatenated string to a set of tokens to get unique tokens\n",
        "initial_corpus_tokens = set(initial_corpus_tokens)\n",
        "\n",
        "print(f\"Corpus words: {corpus_words}\")\n",
        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
        "print(f\"Number of initial corpus tokens: {len(initial_corpus_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora supongamos que este es nuestro corpus de frases, es un corpus inventado, no tiene sentido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"hug hug hug pun pun bun hugs\",\n",
        "    \"hug hug pug pug pun pun hugs\",\n",
        "    \"hug hug pug pug pun pun pun pun hugs\",\n",
        "    \"pug pun pun pun bun hugs\",\n",
        "    \"hug hug hug pun bun bun hugs\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a contar el número de veces que aparece cada palabra en el corpus, para comprobar que lo que habíamos puesto antes está bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of hug: 10\n",
            "Number of pug: 5\n",
            "Number of pun: 12\n",
            "Number of bun: 4\n",
            "Number of hugs: 5\n"
          ]
        }
      ],
      "source": [
        "num_hug = 0\n",
        "num_pug = 0\n",
        "num_pun = 0\n",
        "num_bun = 0\n",
        "num_hugs = 0\n",
        "\n",
        "for sentence in corpus:\n",
        "    words = sentence.split(\" \")\n",
        "    for word in words:\n",
        "        if word == \"hug\":\n",
        "            num_hug += 1\n",
        "        elif word == \"pug\":\n",
        "            num_pug += 1\n",
        "        elif word == \"pun\":\n",
        "            num_pun += 1\n",
        "        elif word == \"bun\":\n",
        "            num_bun += 1\n",
        "        elif word == \"hugs\":\n",
        "            num_hugs += 1\n",
        "\n",
        "print(f\"Number of hug: {num_hug}\")\n",
        "print(f\"Number of pug: {num_pug}\")\n",
        "print(f\"Number of pun: {num_pun}\")\n",
        "print(f\"Number of bun: {num_bun}\")\n",
        "print(f\"Number of hugs: {num_hugs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Todo lo que habíamos contado está bien, podemos seguir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear un diccionario con los tokens de cada palabra y el número de veces que aparece en el corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_tokens_by_word_appearance = {\n",
        "    \"hug\":\n",
        "        {\n",
        "            \"count\": num_hug,\n",
        "            \"tokens\": [character for character in \"hug\"],\n",
        "        },\n",
        "    \"pug\":\n",
        "        {\n",
        "            \"count\": num_pug,\n",
        "            \"tokens\": [character for character in \"pug\"],\n",
        "        },\n",
        "    \"pun\":\n",
        "        {\n",
        "            \"count\": num_pun,\n",
        "            \"tokens\": [character for character in \"pun\"],\n",
        "        },\n",
        "    \"bun\":\n",
        "        {\n",
        "            \"count\": num_bun,\n",
        "            \"tokens\": [character for character in \"bun\"],\n",
        "        },\n",
        "    \"hugs\":\n",
        "        {\n",
        "            \"count\": num_hugs,\n",
        "            \"tokens\": [character for character in \"hugs\"],\n",
        "        },\n",
        "}\n",
        "\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vamos a buscar el par de tokens consecutivos que más veces aparece en el diccionario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List of consecutive tokens: ['hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'pu', 'pu', 'pu', 'pu', 'pu', 'ug', 'ug', 'ug', 'ug', 'ug', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'bu', 'bu', 'bu', 'bu', 'un', 'un', 'un', 'un', 'hu', 'hu', 'hu', 'hu', 'hu', 'ug', 'ug', 'ug', 'ug', 'ug', 'gs', 'gs', 'gs', 'gs', 'gs']\n",
            "Dictionary of consecutive tokens: {'hu': 15, 'ug': 20, 'pu': 17, 'un': 16, 'bu': 4, 'gs': 5}\n",
            "Consecutive token with maximum frequency: ug\n"
          ]
        }
      ],
      "source": [
        "dict_keys = dict_tokens_by_word_appearance.keys()\n",
        "\n",
        "list_consecutive_tokens = []\n",
        "for i, key in enumerate(dict_keys):\n",
        "    # Get the tokens of the word\n",
        "    number_of_toneks_of_word = len(dict_tokens_by_word_appearance[key][\"tokens\"])\n",
        "\n",
        "    # Get consecituve tokens\n",
        "    for j in range(number_of_toneks_of_word-1):\n",
        "        # Get consecutive tokens\n",
        "        consecutive_tokens = dict_tokens_by_word_appearance[key][\"tokens\"][j] + dict_tokens_by_word_appearance[key][\"tokens\"][j+1]\n",
        "        # Append the consecutive tokens to the list the number of times the word appears\n",
        "        for _ in range(dict_tokens_by_word_appearance[key][\"count\"]):\n",
        "            list_consecutive_tokens.append(consecutive_tokens)\n",
        "# Print the list of consecutive tokens\n",
        "print(f\"List of consecutive tokens: {list_consecutive_tokens}\")\n",
        "\n",
        "# Get consecutive tokens with maximum frequency\n",
        "dict_consecutive_tokens = {}\n",
        "for token in list_consecutive_tokens:\n",
        "    # Check if the token is already in the dictionary\n",
        "    if token in dict_consecutive_tokens:\n",
        "        # Increment the count of the token\n",
        "        dict_consecutive_tokens[token] += 1\n",
        "    \n",
        "    # If the token is not in the dictionary\n",
        "    else:\n",
        "        # Add the token to the dictionary\n",
        "        dict_consecutive_tokens[token] = 1\n",
        "# Print the dictionary of consecutive tokens\n",
        "print(f\"Dictionary of consecutive tokens: {dict_consecutive_tokens}\")\n",
        "\n",
        "# Get the consecutive token with maximum frequency\n",
        "max_consecutive_token = None\n",
        "while True:\n",
        "    # Get the token with maximum frequency\n",
        "    consecutive_token = max(dict_consecutive_tokens, key=dict_consecutive_tokens.get)\n",
        "\n",
        "    # Check if the token is already in the list of tokens\n",
        "    if consecutive_token in initial_corpus_tokens:\n",
        "        # Remove token from the dictionary\n",
        "        dict_consecutive_tokens.pop(consecutive_token)\n",
        "\n",
        "    # If the token is not in the list of tokens\n",
        "    else:\n",
        "        # Assign the token to the max_consecutive_token\n",
        "        max_consecutive_token = consecutive_token\n",
        "        break\n",
        "\n",
        "# Print the consecutive token with maximum frequency\n",
        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos obtenido el par de tokens que más veces aparece. Vamos a encapsular esto en una función porque lo vamos a utilizar más veces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, list_corpus_tokens):\n",
        "    dict_keys = dict_tokens_by_word_appearance.keys()\n",
        "\n",
        "    list_consecutive_tokens = []\n",
        "    for i, key in enumerate(dict_keys):\n",
        "        # Get the tokens of the word\n",
        "        number_of_toneks_of_word = len(dict_tokens_by_word_appearance[key][\"tokens\"])\n",
        "\n",
        "        # Get consecituve tokens\n",
        "        for j in range(number_of_toneks_of_word-1):\n",
        "            # Get consecutive tokens\n",
        "            consecutive_tokens = dict_tokens_by_word_appearance[key][\"tokens\"][j] + dict_tokens_by_word_appearance[key][\"tokens\"][j+1]\n",
        "            # Append the consecutive tokens to the list\n",
        "            for _ in range(dict_tokens_by_word_appearance[key][\"count\"]):\n",
        "                list_consecutive_tokens.append(consecutive_tokens)\n",
        "\n",
        "    # Get consecutive tokens with maximum frequency\n",
        "    dict_consecutive_tokens = {}\n",
        "    for token in list_consecutive_tokens:\n",
        "        # Check if the token is already in the dictionary\n",
        "        if token in dict_consecutive_tokens:\n",
        "            # Increment the count of the token\n",
        "            dict_consecutive_tokens[token] += 1\n",
        "        \n",
        "        # If the token is not in the dictionary\n",
        "        else:\n",
        "            # Add the token to the dictionary\n",
        "            dict_consecutive_tokens[token] = 1\n",
        "\n",
        "    # Get the consecutive token with maximum frequency\n",
        "    max_consecutive_token = None\n",
        "    while True:\n",
        "        # Get the token with maximum frequency\n",
        "        consecutive_token = max(dict_consecutive_tokens, key=dict_consecutive_tokens.get)\n",
        "\n",
        "        # Check if the token is already in the list of tokens\n",
        "        if consecutive_token in list_corpus_tokens:\n",
        "            # Remove token from the dictionary\n",
        "            dict_consecutive_tokens.pop(consecutive_token)\n",
        "\n",
        "        # If the token is not in the list of tokens\n",
        "        else:\n",
        "            # Assign the token to the max_consecutive_token\n",
        "            max_consecutive_token = consecutive_token\n",
        "            break\n",
        "\n",
        "    return max_consecutive_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobamos que obtenemos lo mismo que antes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consecutive token with maximum frequency: ug\n"
          ]
        }
      ],
      "source": [
        "max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, initial_corpus_tokens)\n",
        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que sí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora nuestro corpus de tokens se puede modificar añadiendo el token `ug`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "New corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n"
          ]
        }
      ],
      "source": [
        "# new_corpus_tokens = initial_corpus_tokens + max_consecutive_token\n",
        "new_corpus_tokens = initial_corpus_tokens.copy()\n",
        "new_corpus_tokens.add(max_consecutive_token)\n",
        "\n",
        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
        "print(f\"New corpus tokens: {new_corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metemos esto también en una función"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_new_corpus_tokens(max_consecutive_token, initial_corpus_tokens):\n",
        "    new_corpus_tokens = initial_corpus_tokens.copy()\n",
        "    new_corpus_tokens.add(max_consecutive_token)\n",
        "    return new_corpus_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volveremos a comprobar que obtenemos lo mismo que antes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "New corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n"
          ]
        }
      ],
      "source": [
        "new_corpus_tokens = get_new_corpus_tokens(max_consecutive_token, initial_corpus_tokens)\n",
        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
        "print(f\"New corpus tokens: {new_corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que sí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vamos a modificar el diccionario en el que aparecen las palabras, los tokens y el número de veces que aparecen con el nuevo token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token ug is in the word hug\n",
            "New tokens of the word hug: ['h', 'u', 'g', 'ug']\n",
            "Token ug is in the word pug\n",
            "New tokens of the word pug: ['p', 'u', 'g', 'ug']\n",
            "Token ug is in the word hugs\n",
            "New tokens of the word hugs: ['h', 'u', 'g', 's', 'ug']\n",
            "Initial tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}\n",
            "New tokens by word appearance: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "dict_keys = dict_tokens_by_word_appearance.keys()\n",
        "dict_tokens_by_word_appearance_tmp = copy.deepcopy(dict_tokens_by_word_appearance)\n",
        "\n",
        "for key in dict_keys:\n",
        "    # Check if the new token is in the word\n",
        "    if max_consecutive_token in key:\n",
        "        print(f\"Token {max_consecutive_token} is in the word {key}\")\n",
        "\n",
        "        # Add the new token to the word tokens\n",
        "        dict_tokens_by_word_appearance_tmp[key][\"tokens\"].append(max_consecutive_token)\n",
        "\n",
        "        print(f\"New tokens of the word {key}: {dict_tokens_by_word_appearance_tmp[key]['tokens']}\")\n",
        "\n",
        "print(f\"Initial tokens by word appearance: {dict_tokens_by_word_appearance}\")\n",
        "print(f\"New tokens by word appearance: \")\n",
        "dict_tokens_by_word_appearance_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metemos esto en una función"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token):\n",
        "    dict_tokens_by_word_appearance_tmp = copy.deepcopy(dict_tokens_by_word_appearance)\n",
        "    dict_keys = dict_tokens_by_word_appearance_tmp.keys()\n",
        "\n",
        "    for key in dict_keys:\n",
        "        # Check if the new token is in the word\n",
        "        if max_consecutive_token in key:\n",
        "            # Add the new token to the word tokens\n",
        "            dict_tokens_by_word_appearance_tmp[key][\"tokens\"].append(max_consecutive_token)\n",
        "\n",
        "    return dict_tokens_by_word_appearance_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobamos que está bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New tokens by word appearance: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "print(f\"New tokens by word appearance: \")\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En resumen, en una primera iteracción hemos pasado de un corpus de tokens `s, g, h, u, n, p, b` al nuevo corpus de tokens `h, u, n, p, s, g, b, ug`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realizamos ahora una segunda iteración, obtenemos el par de tokens consecutivos que más veces aparecen en el diccionario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consecutive token with maximum frequency: pu\n"
          ]
        }
      ],
      "source": [
        "max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, new_corpus_tokens)\n",
        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtenemos el nuevo corpus de tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n",
            "New corpus tokens: {'p', 'n', 'pu', 'u', 's', 'h', 'ug', 'g', 'b'}\n"
          ]
        }
      ],
      "source": [
        "corpus_tokens = get_new_corpus_tokens(max_consecutive_token, new_corpus_tokens)\n",
        "print(f\"Initial corpus tokens: {new_corpus_tokens}\")\n",
        "print(f\"New corpus tokens: {corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y obtenemos el nuevo diccionario en el que aparecen las palabras, los tokens y el número de veces que aparecen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New tokens by word appearance: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "print(f\"New tokens by word appearance: \")\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora podemos seguir hasta tener un corpus de tokens con el tamaño que queramos, vamos a crear un corpus de 15 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consecutive token with maximum frequency: un\n",
            "New corpus tokens: {'p', 'n', 'pu', 'un', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}\n",
            "\n",
            "Consecutive token with maximum frequency: hu\n",
            "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: gug\n",
            "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'gug', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: ughu\n",
            "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'gug', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: npu\n",
            "New corpus tokens: {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: puun\n",
            "New corpus tokens: {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'puun', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "len_corpus_tokens = 15\n",
        "\n",
        "while len(corpus_tokens) < len_corpus_tokens:\n",
        "    max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, corpus_tokens)\n",
        "    print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")\n",
        "\n",
        "    # If there are no more consecutive tokens break the loop\n",
        "    if max_consecutive_token is None:\n",
        "        break\n",
        "\n",
        "    corpus_tokens = get_new_corpus_tokens(max_consecutive_token, corpus_tokens)\n",
        "    print(f\"New corpus tokens: {corpus_tokens}\")\n",
        "\n",
        "    dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "    print(f\"New tokens by word appearance: {dict_tokens_by_word_appearance}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que hemos visto cómo se entrena el tokenizador BPE, vamos a entrenarlo desde cero para afianzar los conocimientos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus words: ['hug', 'pug', 'pun', 'bun', 'hugs']\n",
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "Number of initial corpus tokens: 7\n"
          ]
        }
      ],
      "source": [
        "corpus_words = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
        "\n",
        "# Concatenate all the words in the corpus\n",
        "initial_corpus_tokens = \"\"\n",
        "for word in corpus_words:\n",
        "    initial_corpus_tokens += word\n",
        "\n",
        "# Convert the concatenated string to a set of tokens to get unique tokens\n",
        "corpus_tokens = set(initial_corpus_tokens)\n",
        "\n",
        "print(f\"Corpus words: {corpus_words}\")\n",
        "print(f\"Initial corpus tokens: {corpus_tokens}\")\n",
        "print(f\"Number of initial corpus tokens: {len(corpus_tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = [\n",
        "    \"hug hug hug pun pun bun hugs\",\n",
        "    \"hug hug pug pug pun pun hugs\",\n",
        "    \"hug hug pug pug pun pun pun pun hugs\",\n",
        "    \"pug pun pun pun bun hugs\",\n",
        "    \"hug hug hug pun bun bun hugs\",\n",
        "]\n",
        "\n",
        "num_hug = 0\n",
        "num_pug = 0\n",
        "num_pun = 0\n",
        "num_bun = 0\n",
        "num_hugs = 0\n",
        "\n",
        "for sentence in corpus:\n",
        "    words = sentence.split(\" \")\n",
        "    for word in words:\n",
        "        if word == \"hug\":\n",
        "            num_hug += 1\n",
        "        elif word == \"pug\":\n",
        "            num_pug += 1\n",
        "        elif word == \"pun\":\n",
        "            num_pun += 1\n",
        "        elif word == \"bun\":\n",
        "            num_bun += 1\n",
        "        elif word == \"hugs\":\n",
        "            num_hugs += 1\n",
        "\n",
        "dict_tokens_by_word_appearance = {\n",
        "    \"hug\":\n",
        "        {\n",
        "            \"count\": num_hug,\n",
        "            \"tokens\": [character for character in \"hug\"],\n",
        "        },\n",
        "    \"pug\":\n",
        "        {\n",
        "            \"count\": num_pug,\n",
        "            \"tokens\": [character for character in \"pug\"],\n",
        "        },\n",
        "    \"pun\":\n",
        "        {\n",
        "            \"count\": num_pun,\n",
        "            \"tokens\": [character for character in \"pun\"],\n",
        "        },\n",
        "    \"bun\":\n",
        "        {\n",
        "            \"count\": num_bun,\n",
        "            \"tokens\": [character for character in \"bun\"],\n",
        "        },\n",
        "    \"hugs\":\n",
        "        {\n",
        "            \"count\": num_hugs,\n",
        "            \"tokens\": [character for character in \"hugs\"],\n",
        "        },\n",
        "}\n",
        "\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo entrenamos desde cero hasta obtener un corpus de 15 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: (7) {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "New corpus tokens: (15) {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'puun', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n"
          ]
        }
      ],
      "source": [
        "len_corpus_tokens = 15\n",
        "print(f\"Initial corpus tokens: ({len(corpus_tokens)}) {corpus_tokens}\")\n",
        "\n",
        "while len(corpus_tokens) < len_corpus_tokens:\n",
        "    max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, corpus_tokens)\n",
        "\n",
        "    # If there are no more consecutive tokens break the loop\n",
        "    if max_consecutive_token is None:\n",
        "        break\n",
        "\n",
        "    corpus_tokens = get_new_corpus_tokens(max_consecutive_token, corpus_tokens)\n",
        "\n",
        "    dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "\n",
        "print(f\"New corpus tokens: ({len(corpus_tokens)}) {corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si ahora quisiéramos tokenizar, primero tendríamos que crear un vocabulario, es decir, asignar a cada token un ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'p': 0,\n",
              " 'hu': 1,\n",
              " 'sug': 2,\n",
              " 'npu': 3,\n",
              " 'ugpu': 4,\n",
              " 'gug': 5,\n",
              " 'u': 6,\n",
              " 'ug': 7,\n",
              " 'ughu': 8,\n",
              " 'n': 9,\n",
              " 'pu': 10,\n",
              " 'un': 11,\n",
              " 'puun': 12,\n",
              " 's': 13,\n",
              " 'h': 14,\n",
              " 'gs': 15,\n",
              " 'g': 16,\n",
              " 'b': 17}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = {}\n",
        "for i, token in enumerate(corpus_tokens):\n",
        "    vocab[token] = i\n",
        "\n",
        "print(f\"Vocabulary: \")\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo metemos en una función"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vocabulary(corpus_tokens):\n",
        "    vocab = {}\n",
        "    for i, token in enumerate(corpus_tokens):\n",
        "        vocab[token] = i\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobamos que está bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'p': 0,\n",
              " 'hu': 1,\n",
              " 'sug': 2,\n",
              " 'npu': 3,\n",
              " 'ugpu': 4,\n",
              " 'gug': 5,\n",
              " 'u': 6,\n",
              " 'ug': 7,\n",
              " 'ughu': 8,\n",
              " 'n': 9,\n",
              " 'pu': 10,\n",
              " 'un': 11,\n",
              " 'puun': 12,\n",
              " 's': 13,\n",
              " 'h': 14,\n",
              " 'gs': 15,\n",
              " 'g': 16,\n",
              " 'b': 17}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = get_vocabulary(corpus_tokens)\n",
        "print(f\"Vocabulary: \")\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si ahora queremos tokenizar la palabra `bug` podemos hacer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum length of tokens: 4\n",
            "Prefix: bug\n",
            "Prefix: bug\n",
            "Prefix: bu\n",
            "Prefix: b\n",
            "prefix b is in the vocabulary\n",
            "Prefix: ug\n",
            "prefix ug is in the vocabulary\n",
            "Tokens: ['b', 'ug']\n"
          ]
        }
      ],
      "source": [
        "word = 'bug'\n",
        "\n",
        "# Get the maximum length of tokens\n",
        "max_len = max(len(token) for token in vocab)\n",
        "print(f\"Maximum length of tokens: {max_len}\")\n",
        "\n",
        "# Create a empty list of tokens\n",
        "tokens = []\n",
        "while len(word) > 0:\n",
        "    # Flag to check if the token is found\n",
        "    found = False\n",
        "\n",
        "    # Iterate over the maximum length of tokens from max_len to 0\n",
        "    for i in range(max_len, 0, -1):\n",
        "        # Get the prefix of the word\n",
        "        prefix = word[:i]\n",
        "        print(f\"Prefix: {prefix}\")\n",
        "\n",
        "        # Check if the prefix is in the vocabulary\n",
        "        if prefix in vocab:\n",
        "            print(f\"prefix {prefix} is in the vocabulary\")\n",
        "            tokens.append(prefix)\n",
        "            word = word[i:]\n",
        "            found = True\n",
        "            break\n",
        "    \n",
        "    # if not found:\n",
        "    #     tokens.append('<UNK>')\n",
        "    #     word = word[1:]\n",
        "\n",
        "print(f\"Tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pero si ahora queremos tokenizar la palabra `mug` no podríamos porque el caracter `m` no está en el vocabulario, para ello lo tokenizamos con el token `<UNK>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum length of tokens: 4\n",
            "Prefix: mug\n",
            "Prefix: mug\n",
            "Prefix: mu\n",
            "Prefix: m\n",
            "Prefix: ug\n",
            "prefix ug is in the vocabulary\n",
            "Tokens: ['<UNK>', 'ug']\n"
          ]
        }
      ],
      "source": [
        "word = 'mug'\n",
        "\n",
        "# Get the maximum length of tokens\n",
        "max_len = max(len(token) for token in vocab)\n",
        "print(f\"Maximum length of tokens: {max_len}\")\n",
        "\n",
        "# Create a empty list of tokens\n",
        "tokens = []\n",
        "while len(word) > 0:\n",
        "    # Flag to check if the token is found\n",
        "    found = False\n",
        "\n",
        "    # Iterate over the maximum length of tokens from max_len to 0\n",
        "    for i in range(max_len, 0, -1):\n",
        "        # Get the prefix of the word\n",
        "        prefix = word[:i]\n",
        "        print(f\"Prefix: {prefix}\")\n",
        "\n",
        "        # Check if the prefix is in the vocabulary\n",
        "        if prefix in vocab:\n",
        "            print(f\"prefix {prefix} is in the vocabulary\")\n",
        "            tokens.append(prefix)\n",
        "            word = word[i:]\n",
        "            found = True\n",
        "            break\n",
        "\n",
        "    if not found:\n",
        "        tokens.append('<UNK>')\n",
        "        word = word[1:]\n",
        "\n",
        "print(f\"Tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo metemos en una función"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_word(word, vocab):\n",
        "    # Get the maximum length of tokens\n",
        "    max_len = max(len(token) for token in vocab)\n",
        "\n",
        "    # Create a empty list of tokens\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        # Flag to check if the token is found\n",
        "        found = False\n",
        "\n",
        "        # Iterate over the maximum length of tokens from max_len to 0\n",
        "        for i in range(max_len, 0, -1):\n",
        "            # Get the prefix of the word\n",
        "            prefix = word[:i]\n",
        "\n",
        "            # Check if the prefix is in the vocabulary\n",
        "            if prefix in vocab:\n",
        "                tokens.append(prefix)\n",
        "                word = word[i:]\n",
        "                found = True\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            tokens.append('<UNK>')\n",
        "            word = word[1:]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobamos que está bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization of the word 'bug': ['b', 'ug']\n",
            "Tokenization of the word 'mug': ['<UNK>', 'ug']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokenization of the word 'bug': {tokenize_word('bug', vocab)}\")\n",
        "print(f\"Tokenization of the word 'mug': {tokenize_word('mug', vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizador de tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que sabemos cómo funciona un tokenizador BPE, vamos a ver mediante el visualizador [the-tokenizer-playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground) cómo quedarían los tokens de cualquier sentencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<iframe>\n",
        "\tsrc=\"https://xenova-the-tokenizer-playground.static.hf.space\"\n",
        "\tframeborder=\"0\"\n",
        "\twidth=\"850\"\n",
        "\theight=\"450\"\n",
        "></iframe>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2024-06-04",
      "description_en": "🔍 Discover the secret of tokenization! 🔑 I reveal you the mysteries of BPE (Byte Pair Encoding) tokenization, one of the most popular and effective methods to split text into tokens. Learn how to to tokenize with BPE! 💻 Read my post and discover the tips and tricks to master tokenization with BPE! 📄",
      "description_es": "🔍 ¡Descubre el secreto de la tokenización! 🔑 Te revelo los misterios del tokenizador BPE (Byte Pair Encoding), uno de los más populares y efectivos métodos para dividir el texto en tokens. ¡Aprende cómo tokenizar con BPE! 💻 ¡Lee mi post y descubre los trucos y consejos para dominar la tokenización con BPE! 📄",
      "description_pt": "🔍 Descubra o segredo da tokenização! 🔑 Revelo os mistérios da tokenização BPE (Byte Pair Encoding), um dos métodos mais populares e eficazes para dividir o texto em tokens. Aprenda a tokenizar com BPE! 💻 Leia minha postagem e descubra as dicas e os truques para dominar a tokenização com BPE! 📄",
      "end_url": "bpe",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/BPE_tokenizer.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/BPE_tokenizer.webp",
      "keywords_en": "bpe tokenizer, byte pair encoding, tokenization, nlp, natural language processing, text processing",
      "keywords_es": "tokenizador bpe, byte pair encoding, tokenización, pnl, procesamiento de lenguaje natural, procesamiento de texto",
      "keywords_pt": "tokenizador bpe, byte pair encoding, tokenização, pnl, processamento de linguagem natural, processamento de texto",
      "title_en": "BPE tokenizer",
      "title_es": "BPE tokenizer",
      "title_pt": "BPE tokenizer"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
