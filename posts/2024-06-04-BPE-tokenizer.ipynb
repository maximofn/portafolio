{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BPE tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El tokenizador `BPE` (Byte Pair Encoding - codificaci√≥n de pares de bytes) es un algoritmo de compresi√≥n de datos que se utiliza para crear un vocabulario de subpalabras a partir de un corpus de texto. Este algoritmo se basa en la frecuencia de los pares de bytes en el texto. Se populariz√≥ porque fue utilizado como tokenizador por LLMs como GPT, GPT-2, RoBERTa, BART y DeBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algoritmo de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supongamos que tenemos un corpus de texto que solo contiene las siguientes palabras `hug, pug, pun, bun y hugs`, el primer paso consiste en crear un vocabulario con todos los caracteres presentes en el corpus, en nuestro caso ser√° `b, g, h, n, p, s, u`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus words: ['hug', 'pug', 'pun', 'bun', 'hugs']\n",
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "Number of initial corpus tokens: 7\n"
          ]
        }
      ],
      "source": [
        "corpus_words = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
        "\n",
        "# Concatenate all the words in the corpus\n",
        "initial_corpus_tokens = \"\"\n",
        "for word in corpus_words:\n",
        "    initial_corpus_tokens += word\n",
        "\n",
        "# Convert the concatenated string to a set of tokens to get unique tokens\n",
        "initial_corpus_tokens = set(initial_corpus_tokens)\n",
        "\n",
        "print(f\"Corpus words: {corpus_words}\")\n",
        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
        "print(f\"Number of initial corpus tokens: {len(initial_corpus_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora supongamos que este es nuestro corpus de frases, es un corpus inventado, no tiene sentido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"hug hug hug pun pun bun hugs\",\n",
        "    \"hug hug pug pug pun pun hugs\",\n",
        "    \"hug hug pug pug pun pun pun pun hugs\",\n",
        "    \"pug pun pun pun bun hugs\",\n",
        "    \"hug hug hug pun bun bun hugs\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a contar el n√∫mero de veces que aparece cada palabra en el corpus, para comprobar que lo que hab√≠amos puesto antes est√° bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of hug: 10\n",
            "Number of pug: 5\n",
            "Number of pun: 12\n",
            "Number of bun: 4\n",
            "Number of hugs: 5\n"
          ]
        }
      ],
      "source": [
        "num_hug = 0\n",
        "num_pug = 0\n",
        "num_pun = 0\n",
        "num_bun = 0\n",
        "num_hugs = 0\n",
        "\n",
        "for sentence in corpus:\n",
        "    words = sentence.split(\" \")\n",
        "    for word in words:\n",
        "        if word == \"hug\":\n",
        "            num_hug += 1\n",
        "        elif word == \"pug\":\n",
        "            num_pug += 1\n",
        "        elif word == \"pun\":\n",
        "            num_pun += 1\n",
        "        elif word == \"bun\":\n",
        "            num_bun += 1\n",
        "        elif word == \"hugs\":\n",
        "            num_hugs += 1\n",
        "\n",
        "print(f\"Number of hug: {num_hug}\")\n",
        "print(f\"Number of pug: {num_pug}\")\n",
        "print(f\"Number of pun: {num_pun}\")\n",
        "print(f\"Number of bun: {num_bun}\")\n",
        "print(f\"Number of hugs: {num_hugs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Todo lo que hab√≠amos contado est√° bien, podemos seguir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear un diccionario con los tokens de cada palabra y el n√∫mero de veces que aparece en el corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_tokens_by_word_appearance = {\n",
        "    \"hug\":\n",
        "        {\n",
        "            \"count\": num_hug,\n",
        "            \"tokens\": [character for character in \"hug\"],\n",
        "        },\n",
        "    \"pug\":\n",
        "        {\n",
        "            \"count\": num_pug,\n",
        "            \"tokens\": [character for character in \"pug\"],\n",
        "        },\n",
        "    \"pun\":\n",
        "        {\n",
        "            \"count\": num_pun,\n",
        "            \"tokens\": [character for character in \"pun\"],\n",
        "        },\n",
        "    \"bun\":\n",
        "        {\n",
        "            \"count\": num_bun,\n",
        "            \"tokens\": [character for character in \"bun\"],\n",
        "        },\n",
        "    \"hugs\":\n",
        "        {\n",
        "            \"count\": num_hugs,\n",
        "            \"tokens\": [character for character in \"hugs\"],\n",
        "        },\n",
        "}\n",
        "\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vamos a buscar el par de tokens consecutivos que m√°s veces aparece en el diccionario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "List of consecutive tokens: ['hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'hu', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'ug', 'pu', 'pu', 'pu', 'pu', 'pu', 'ug', 'ug', 'ug', 'ug', 'ug', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'pu', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'bu', 'bu', 'bu', 'bu', 'un', 'un', 'un', 'un', 'hu', 'hu', 'hu', 'hu', 'hu', 'ug', 'ug', 'ug', 'ug', 'ug', 'gs', 'gs', 'gs', 'gs', 'gs']\n",
            "Dictionary of consecutive tokens: {'hu': 15, 'ug': 20, 'pu': 17, 'un': 16, 'bu': 4, 'gs': 5}\n",
            "Consecutive token with maximum frequency: ug\n"
          ]
        }
      ],
      "source": [
        "dict_keys = dict_tokens_by_word_appearance.keys()\n",
        "\n",
        "list_consecutive_tokens = []\n",
        "for i, key in enumerate(dict_keys):\n",
        "    # Get the tokens of the word\n",
        "    number_of_toneks_of_word = len(dict_tokens_by_word_appearance[key][\"tokens\"])\n",
        "\n",
        "    # Get consecituve tokens\n",
        "    for j in range(number_of_toneks_of_word-1):\n",
        "        # Get consecutive tokens\n",
        "        consecutive_tokens = dict_tokens_by_word_appearance[key][\"tokens\"][j] + dict_tokens_by_word_appearance[key][\"tokens\"][j+1]\n",
        "        # Append the consecutive tokens to the list the number of times the word appears\n",
        "        for _ in range(dict_tokens_by_word_appearance[key][\"count\"]):\n",
        "            list_consecutive_tokens.append(consecutive_tokens)\n",
        "# Print the list of consecutive tokens\n",
        "print(f\"List of consecutive tokens: {list_consecutive_tokens}\")\n",
        "\n",
        "# Get consecutive tokens with maximum frequency\n",
        "dict_consecutive_tokens = {}\n",
        "for token in list_consecutive_tokens:\n",
        "    # Check if the token is already in the dictionary\n",
        "    if token in dict_consecutive_tokens:\n",
        "        # Increment the count of the token\n",
        "        dict_consecutive_tokens[token] += 1\n",
        "    \n",
        "    # If the token is not in the dictionary\n",
        "    else:\n",
        "        # Add the token to the dictionary\n",
        "        dict_consecutive_tokens[token] = 1\n",
        "# Print the dictionary of consecutive tokens\n",
        "print(f\"Dictionary of consecutive tokens: {dict_consecutive_tokens}\")\n",
        "\n",
        "# Get the consecutive token with maximum frequency\n",
        "max_consecutive_token = None\n",
        "while True:\n",
        "    # Get the token with maximum frequency\n",
        "    consecutive_token = max(dict_consecutive_tokens, key=dict_consecutive_tokens.get)\n",
        "\n",
        "    # Check if the token is already in the list of tokens\n",
        "    if consecutive_token in initial_corpus_tokens:\n",
        "        # Remove token from the dictionary\n",
        "        dict_consecutive_tokens.pop(consecutive_token)\n",
        "\n",
        "    # If the token is not in the list of tokens\n",
        "    else:\n",
        "        # Assign the token to the max_consecutive_token\n",
        "        max_consecutive_token = consecutive_token\n",
        "        break\n",
        "\n",
        "# Print the consecutive token with maximum frequency\n",
        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos obtenido el par de tokens que m√°s veces aparece. Vamos a encapsular esto en una funci√≥n porque lo vamos a utilizar m√°s veces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, list_corpus_tokens):\n",
        "    dict_keys = dict_tokens_by_word_appearance.keys()\n",
        "\n",
        "    list_consecutive_tokens = []\n",
        "    for i, key in enumerate(dict_keys):\n",
        "        # Get the tokens of the word\n",
        "        number_of_toneks_of_word = len(dict_tokens_by_word_appearance[key][\"tokens\"])\n",
        "\n",
        "        # Get consecituve tokens\n",
        "        for j in range(number_of_toneks_of_word-1):\n",
        "            # Get consecutive tokens\n",
        "            consecutive_tokens = dict_tokens_by_word_appearance[key][\"tokens\"][j] + dict_tokens_by_word_appearance[key][\"tokens\"][j+1]\n",
        "            # Append the consecutive tokens to the list\n",
        "            for _ in range(dict_tokens_by_word_appearance[key][\"count\"]):\n",
        "                list_consecutive_tokens.append(consecutive_tokens)\n",
        "\n",
        "    # Get consecutive tokens with maximum frequency\n",
        "    dict_consecutive_tokens = {}\n",
        "    for token in list_consecutive_tokens:\n",
        "        # Check if the token is already in the dictionary\n",
        "        if token in dict_consecutive_tokens:\n",
        "            # Increment the count of the token\n",
        "            dict_consecutive_tokens[token] += 1\n",
        "        \n",
        "        # If the token is not in the dictionary\n",
        "        else:\n",
        "            # Add the token to the dictionary\n",
        "            dict_consecutive_tokens[token] = 1\n",
        "\n",
        "    # Get the consecutive token with maximum frequency\n",
        "    max_consecutive_token = None\n",
        "    while True:\n",
        "        # Get the token with maximum frequency\n",
        "        consecutive_token = max(dict_consecutive_tokens, key=dict_consecutive_tokens.get)\n",
        "\n",
        "        # Check if the token is already in the list of tokens\n",
        "        if consecutive_token in list_corpus_tokens:\n",
        "            # Remove token from the dictionary\n",
        "            dict_consecutive_tokens.pop(consecutive_token)\n",
        "\n",
        "        # If the token is not in the list of tokens\n",
        "        else:\n",
        "            # Assign the token to the max_consecutive_token\n",
        "            max_consecutive_token = consecutive_token\n",
        "            break\n",
        "\n",
        "    return max_consecutive_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobamos que obtenemos lo mismo que antes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consecutive token with maximum frequency: ug\n"
          ]
        }
      ],
      "source": [
        "max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, initial_corpus_tokens)\n",
        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que s√≠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora nuestro corpus de tokens se puede modificar a√±adiendo el token `ug`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "New corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n"
          ]
        }
      ],
      "source": [
        "# new_corpus_tokens = initial_corpus_tokens + max_consecutive_token\n",
        "new_corpus_tokens = initial_corpus_tokens.copy()\n",
        "new_corpus_tokens.add(max_consecutive_token)\n",
        "\n",
        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
        "print(f\"New corpus tokens: {new_corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metemos esto tambi√©n en una funci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_new_corpus_tokens(max_consecutive_token, initial_corpus_tokens):\n",
        "    new_corpus_tokens = initial_corpus_tokens.copy()\n",
        "    new_corpus_tokens.add(max_consecutive_token)\n",
        "    return new_corpus_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Volveremos a comprobar que obtenemos lo mismo que antes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "New corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n"
          ]
        }
      ],
      "source": [
        "new_corpus_tokens = get_new_corpus_tokens(max_consecutive_token, initial_corpus_tokens)\n",
        "print(f\"Initial corpus tokens: {initial_corpus_tokens}\")\n",
        "print(f\"New corpus tokens: {new_corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que s√≠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vamos a modificar el diccionario en el que aparecen las palabras, los tokens y el n√∫mero de veces que aparecen con el nuevo token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token ug is in the word hug\n",
            "New tokens of the word hug: ['h', 'u', 'g', 'ug']\n",
            "Token ug is in the word pug\n",
            "New tokens of the word pug: ['p', 'u', 'g', 'ug']\n",
            "Token ug is in the word hugs\n",
            "New tokens of the word hugs: ['h', 'u', 'g', 's', 'ug']\n",
            "Initial tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}\n",
            "New tokens by word appearance: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "dict_keys = dict_tokens_by_word_appearance.keys()\n",
        "dict_tokens_by_word_appearance_tmp = copy.deepcopy(dict_tokens_by_word_appearance)\n",
        "\n",
        "for key in dict_keys:\n",
        "    # Check if the new token is in the word\n",
        "    if max_consecutive_token in key:\n",
        "        print(f\"Token {max_consecutive_token} is in the word {key}\")\n",
        "\n",
        "        # Add the new token to the word tokens\n",
        "        dict_tokens_by_word_appearance_tmp[key][\"tokens\"].append(max_consecutive_token)\n",
        "\n",
        "        print(f\"New tokens of the word {key}: {dict_tokens_by_word_appearance_tmp[key]['tokens']}\")\n",
        "\n",
        "print(f\"Initial tokens by word appearance: {dict_tokens_by_word_appearance}\")\n",
        "print(f\"New tokens by word appearance: \")\n",
        "dict_tokens_by_word_appearance_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metemos esto en una funci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token):\n",
        "    dict_tokens_by_word_appearance_tmp = copy.deepcopy(dict_tokens_by_word_appearance)\n",
        "    dict_keys = dict_tokens_by_word_appearance_tmp.keys()\n",
        "\n",
        "    for key in dict_keys:\n",
        "        # Check if the new token is in the word\n",
        "        if max_consecutive_token in key:\n",
        "            # Add the new token to the word tokens\n",
        "            dict_tokens_by_word_appearance_tmp[key][\"tokens\"].append(max_consecutive_token)\n",
        "\n",
        "    return dict_tokens_by_word_appearance_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobamos que est√° bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New tokens by word appearance: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "print(f\"New tokens by word appearance: \")\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En resumen, en una primera iteracci√≥n hemos pasado de un corpus de tokens `s, g, h, u, n, p, b` al nuevo corpus de tokens `h, u, n, p, s, g, b, ug`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realizamos ahora una segunda iteraci√≥n, obtenemos el par de tokens consecutivos que m√°s veces aparecen en el diccionario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consecutive token with maximum frequency: pu\n"
          ]
        }
      ],
      "source": [
        "max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, new_corpus_tokens)\n",
        "print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtenemos el nuevo corpus de tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: {'p', 'n', 'ug', 'g', 'b', 'u', 's', 'h'}\n",
            "New corpus tokens: {'p', 'n', 'pu', 'u', 's', 'h', 'ug', 'g', 'b'}\n"
          ]
        }
      ],
      "source": [
        "corpus_tokens = get_new_corpus_tokens(max_consecutive_token, new_corpus_tokens)\n",
        "print(f\"Initial corpus tokens: {new_corpus_tokens}\")\n",
        "print(f\"New corpus tokens: {corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y obtenemos el nuevo diccionario en el que aparecen las palabras, los tokens y el n√∫mero de veces que aparecen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New tokens by word appearance: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "print(f\"New tokens by word appearance: \")\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora podemos seguir hasta tener un corpus de tokens con el tama√±o que queramos, vamos a crear un corpus de 15 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consecutive token with maximum frequency: un\n",
            "New corpus tokens: {'p', 'n', 'pu', 'un', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug']}}\n",
            "\n",
            "Consecutive token with maximum frequency: hu\n",
            "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: gug\n",
            "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'gug', 'u', 's', 'h', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: ughu\n",
            "New corpus tokens: {'p', 'hu', 'n', 'pu', 'un', 'gug', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: npu\n",
            "New corpus tokens: {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n",
            "Consecutive token with maximum frequency: puun\n",
            "New corpus tokens: {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'puun', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n",
            "New tokens by word appearance: {'hug': {'count': 10, 'tokens': ['h', 'u', 'g', 'ug', 'hu']}, 'pug': {'count': 5, 'tokens': ['p', 'u', 'g', 'ug', 'pu']}, 'pun': {'count': 12, 'tokens': ['p', 'u', 'n', 'pu', 'un']}, 'bun': {'count': 4, 'tokens': ['b', 'u', 'n', 'un']}, 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's', 'ug', 'hu']}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "len_corpus_tokens = 15\n",
        "\n",
        "while len(corpus_tokens) < len_corpus_tokens:\n",
        "    max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, corpus_tokens)\n",
        "    print(f\"Consecutive token with maximum frequency: {max_consecutive_token}\")\n",
        "\n",
        "    # If there are no more consecutive tokens break the loop\n",
        "    if max_consecutive_token is None:\n",
        "        break\n",
        "\n",
        "    corpus_tokens = get_new_corpus_tokens(max_consecutive_token, corpus_tokens)\n",
        "    print(f\"New corpus tokens: {corpus_tokens}\")\n",
        "\n",
        "    dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "    print(f\"New tokens by word appearance: {dict_tokens_by_word_appearance}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que hemos visto c√≥mo se entrena el tokenizador BPE, vamos a entrenarlo desde cero para afianzar los conocimientos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus words: ['hug', 'pug', 'pun', 'bun', 'hugs']\n",
            "Initial corpus tokens: {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "Number of initial corpus tokens: 7\n"
          ]
        }
      ],
      "source": [
        "corpus_words = [\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"]\n",
        "\n",
        "# Concatenate all the words in the corpus\n",
        "initial_corpus_tokens = \"\"\n",
        "for word in corpus_words:\n",
        "    initial_corpus_tokens += word\n",
        "\n",
        "# Convert the concatenated string to a set of tokens to get unique tokens\n",
        "corpus_tokens = set(initial_corpus_tokens)\n",
        "\n",
        "print(f\"Corpus words: {corpus_words}\")\n",
        "print(f\"Initial corpus tokens: {corpus_tokens}\")\n",
        "print(f\"Number of initial corpus tokens: {len(corpus_tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hug': {'count': 10, 'tokens': ['h', 'u', 'g']},\n",
              " 'pug': {'count': 5, 'tokens': ['p', 'u', 'g']},\n",
              " 'pun': {'count': 12, 'tokens': ['p', 'u', 'n']},\n",
              " 'bun': {'count': 4, 'tokens': ['b', 'u', 'n']},\n",
              " 'hugs': {'count': 5, 'tokens': ['h', 'u', 'g', 's']}}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = [\n",
        "    \"hug hug hug pun pun bun hugs\",\n",
        "    \"hug hug pug pug pun pun hugs\",\n",
        "    \"hug hug pug pug pun pun pun pun hugs\",\n",
        "    \"pug pun pun pun bun hugs\",\n",
        "    \"hug hug hug pun bun bun hugs\",\n",
        "]\n",
        "\n",
        "num_hug = 0\n",
        "num_pug = 0\n",
        "num_pun = 0\n",
        "num_bun = 0\n",
        "num_hugs = 0\n",
        "\n",
        "for sentence in corpus:\n",
        "    words = sentence.split(\" \")\n",
        "    for word in words:\n",
        "        if word == \"hug\":\n",
        "            num_hug += 1\n",
        "        elif word == \"pug\":\n",
        "            num_pug += 1\n",
        "        elif word == \"pun\":\n",
        "            num_pun += 1\n",
        "        elif word == \"bun\":\n",
        "            num_bun += 1\n",
        "        elif word == \"hugs\":\n",
        "            num_hugs += 1\n",
        "\n",
        "dict_tokens_by_word_appearance = {\n",
        "    \"hug\":\n",
        "        {\n",
        "            \"count\": num_hug,\n",
        "            \"tokens\": [character for character in \"hug\"],\n",
        "        },\n",
        "    \"pug\":\n",
        "        {\n",
        "            \"count\": num_pug,\n",
        "            \"tokens\": [character for character in \"pug\"],\n",
        "        },\n",
        "    \"pun\":\n",
        "        {\n",
        "            \"count\": num_pun,\n",
        "            \"tokens\": [character for character in \"pun\"],\n",
        "        },\n",
        "    \"bun\":\n",
        "        {\n",
        "            \"count\": num_bun,\n",
        "            \"tokens\": [character for character in \"bun\"],\n",
        "        },\n",
        "    \"hugs\":\n",
        "        {\n",
        "            \"count\": num_hugs,\n",
        "            \"tokens\": [character for character in \"hugs\"],\n",
        "        },\n",
        "}\n",
        "\n",
        "dict_tokens_by_word_appearance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo entrenamos desde cero hasta obtener un corpus de 15 tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial corpus tokens: (7) {'p', 'n', 'u', 's', 'h', 'g', 'b'}\n",
            "New corpus tokens: (15) {'p', 'hu', 'n', 'npu', 'pu', 'un', 'gug', 'puun', 'u', 's', 'h', 'ughu', 'ug', 'g', 'b'}\n"
          ]
        }
      ],
      "source": [
        "len_corpus_tokens = 15\n",
        "print(f\"Initial corpus tokens: ({len(corpus_tokens)}) {corpus_tokens}\")\n",
        "\n",
        "while len(corpus_tokens) < len_corpus_tokens:\n",
        "    max_consecutive_token = get_consecutive_tokens_with_max_frequency(dict_tokens_by_word_appearance, corpus_tokens)\n",
        "\n",
        "    # If there are no more consecutive tokens break the loop\n",
        "    if max_consecutive_token is None:\n",
        "        break\n",
        "\n",
        "    corpus_tokens = get_new_corpus_tokens(max_consecutive_token, corpus_tokens)\n",
        "\n",
        "    dict_tokens_by_word_appearance = update_tokens_by_word_appearance(dict_tokens_by_word_appearance, max_consecutive_token)\n",
        "\n",
        "print(f\"New corpus tokens: ({len(corpus_tokens)}) {corpus_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si ahora quisi√©ramos tokenizar, primero tendr√≠amos que crear un vocabulario, es decir, asignar a cada token un ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'p': 0,\n",
              " 'hu': 1,\n",
              " 'sug': 2,\n",
              " 'npu': 3,\n",
              " 'ugpu': 4,\n",
              " 'gug': 5,\n",
              " 'u': 6,\n",
              " 'ug': 7,\n",
              " 'ughu': 8,\n",
              " 'n': 9,\n",
              " 'pu': 10,\n",
              " 'un': 11,\n",
              " 'puun': 12,\n",
              " 's': 13,\n",
              " 'h': 14,\n",
              " 'gs': 15,\n",
              " 'g': 16,\n",
              " 'b': 17}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = {}\n",
        "for i, token in enumerate(corpus_tokens):\n",
        "    vocab[token] = i\n",
        "\n",
        "print(f\"Vocabulary: \")\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo metemos en una funci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vocabulary(corpus_tokens):\n",
        "    vocab = {}\n",
        "    for i, token in enumerate(corpus_tokens):\n",
        "        vocab[token] = i\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobamos que est√° bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'p': 0,\n",
              " 'hu': 1,\n",
              " 'sug': 2,\n",
              " 'npu': 3,\n",
              " 'ugpu': 4,\n",
              " 'gug': 5,\n",
              " 'u': 6,\n",
              " 'ug': 7,\n",
              " 'ughu': 8,\n",
              " 'n': 9,\n",
              " 'pu': 10,\n",
              " 'un': 11,\n",
              " 'puun': 12,\n",
              " 's': 13,\n",
              " 'h': 14,\n",
              " 'gs': 15,\n",
              " 'g': 16,\n",
              " 'b': 17}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = get_vocabulary(corpus_tokens)\n",
        "print(f\"Vocabulary: \")\n",
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si ahora queremos tokenizar la palabra `bug` podemos hacer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum length of tokens: 4\n",
            "Prefix: bug\n",
            "Prefix: bug\n",
            "Prefix: bu\n",
            "Prefix: b\n",
            "prefix b is in the vocabulary\n",
            "Prefix: ug\n",
            "prefix ug is in the vocabulary\n",
            "Tokens: ['b', 'ug']\n"
          ]
        }
      ],
      "source": [
        "word = 'bug'\n",
        "\n",
        "# Get the maximum length of tokens\n",
        "max_len = max(len(token) for token in vocab)\n",
        "print(f\"Maximum length of tokens: {max_len}\")\n",
        "\n",
        "# Create a empty list of tokens\n",
        "tokens = []\n",
        "while len(word) > 0:\n",
        "    # Flag to check if the token is found\n",
        "    found = False\n",
        "\n",
        "    # Iterate over the maximum length of tokens from max_len to 0\n",
        "    for i in range(max_len, 0, -1):\n",
        "        # Get the prefix of the word\n",
        "        prefix = word[:i]\n",
        "        print(f\"Prefix: {prefix}\")\n",
        "\n",
        "        # Check if the prefix is in the vocabulary\n",
        "        if prefix in vocab:\n",
        "            print(f\"prefix {prefix} is in the vocabulary\")\n",
        "            tokens.append(prefix)\n",
        "            word = word[i:]\n",
        "            found = True\n",
        "            break\n",
        "    \n",
        "    # if not found:\n",
        "    #     tokens.append('<UNK>')\n",
        "    #     word = word[1:]\n",
        "\n",
        "print(f\"Tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pero si ahora queremos tokenizar la palabra `mug` no podr√≠amos porque el caracter `m` no est√° en el vocabulario, para ello lo tokenizamos con el token `<UNK>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum length of tokens: 4\n",
            "Prefix: mug\n",
            "Prefix: mug\n",
            "Prefix: mu\n",
            "Prefix: m\n",
            "Prefix: ug\n",
            "prefix ug is in the vocabulary\n",
            "Tokens: ['<UNK>', 'ug']\n"
          ]
        }
      ],
      "source": [
        "word = 'mug'\n",
        "\n",
        "# Get the maximum length of tokens\n",
        "max_len = max(len(token) for token in vocab)\n",
        "print(f\"Maximum length of tokens: {max_len}\")\n",
        "\n",
        "# Create a empty list of tokens\n",
        "tokens = []\n",
        "while len(word) > 0:\n",
        "    # Flag to check if the token is found\n",
        "    found = False\n",
        "\n",
        "    # Iterate over the maximum length of tokens from max_len to 0\n",
        "    for i in range(max_len, 0, -1):\n",
        "        # Get the prefix of the word\n",
        "        prefix = word[:i]\n",
        "        print(f\"Prefix: {prefix}\")\n",
        "\n",
        "        # Check if the prefix is in the vocabulary\n",
        "        if prefix in vocab:\n",
        "            print(f\"prefix {prefix} is in the vocabulary\")\n",
        "            tokens.append(prefix)\n",
        "            word = word[i:]\n",
        "            found = True\n",
        "            break\n",
        "\n",
        "    if not found:\n",
        "        tokens.append('<UNK>')\n",
        "        word = word[1:]\n",
        "\n",
        "print(f\"Tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo metemos en una funci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_word(word, vocab):\n",
        "    # Get the maximum length of tokens\n",
        "    max_len = max(len(token) for token in vocab)\n",
        "\n",
        "    # Create a empty list of tokens\n",
        "    tokens = []\n",
        "    while len(word) > 0:\n",
        "        # Flag to check if the token is found\n",
        "        found = False\n",
        "\n",
        "        # Iterate over the maximum length of tokens from max_len to 0\n",
        "        for i in range(max_len, 0, -1):\n",
        "            # Get the prefix of the word\n",
        "            prefix = word[:i]\n",
        "\n",
        "            # Check if the prefix is in the vocabulary\n",
        "            if prefix in vocab:\n",
        "                tokens.append(prefix)\n",
        "                word = word[i:]\n",
        "                found = True\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            tokens.append('<UNK>')\n",
        "            word = word[1:]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobamos que est√° bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization of the word 'bug': ['b', 'ug']\n",
            "Tokenization of the word 'mug': ['<UNK>', 'ug']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokenization of the word 'bug': {tokenize_word('bug', vocab)}\")\n",
        "print(f\"Tokenization of the word 'mug': {tokenize_word('mug', vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizador de tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que sabemos c√≥mo funciona un tokenizador BPE, vamos a ver mediante el visualizador [the-tokenizer-playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground) c√≥mo quedar√≠an los tokens de cualquier sentencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<iframe>\n",
        "\tsrc=\"https://xenova-the-tokenizer-playground.static.hf.space\"\n",
        "\tframeborder=\"0\"\n",
        "\twidth=\"850\"\n",
        "\theight=\"450\"\n",
        "></iframe>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2024-06-04",
      "description_en": "üîç Discover the secret of tokenization! üîë I reveal you the mysteries of BPE (Byte Pair Encoding) tokenization, one of the most popular and effective methods to split text into tokens. Learn how to to tokenize with BPE! üíª Read my post and discover the tips and tricks to master tokenization with BPE! üìÑ",
      "description_es": "üîç ¬°Descubre el secreto de la tokenizaci√≥n! üîë Te revelo los misterios del tokenizador BPE (Byte Pair Encoding), uno de los m√°s populares y efectivos m√©todos para dividir el texto en tokens. ¬°Aprende c√≥mo tokenizar con BPE! üíª ¬°Lee mi post y descubre los trucos y consejos para dominar la tokenizaci√≥n con BPE! üìÑ",
      "description_pt": "üîç Descubra o segredo da tokeniza√ß√£o! üîë Revelo os mist√©rios da tokeniza√ß√£o BPE (Byte Pair Encoding), um dos m√©todos mais populares e eficazes para dividir o texto em tokens. Aprenda a tokenizar com BPE! üíª Leia minha postagem e descubra as dicas e os truques para dominar a tokeniza√ß√£o com BPE! üìÑ",
      "end_url": "bpe",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/BPE_tokenizer.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/BPE_tokenizer.webp",
      "keywords_en": "bpe tokenizer, byte pair encoding, tokenization, nlp, natural language processing, text processing",
      "keywords_es": "tokenizador bpe, byte pair encoding, tokenizaci√≥n, pnl, procesamiento de lenguaje natural, procesamiento de texto",
      "keywords_pt": "tokenizador bpe, byte pair encoding, tokeniza√ß√£o, pnl, processamento de linguagem natural, processamento de texto",
      "title_en": "BPE tokenizer",
      "title_es": "BPE tokenizer",
      "title_pt": "BPE tokenizer"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
