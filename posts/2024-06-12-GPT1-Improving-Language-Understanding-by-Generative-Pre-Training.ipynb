{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT1 - Improving Language Understanding by Generative Pre-Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) es el paper de GPT1. Antes de leer el post es necesario que te pongas en situaci√≥n, antes de GPT los modelos de lenguaje estaban basados en redes recurrentes (RNN), que eran redes que funcionaban relativamente bien para tareas espec√≠ficas, pero con las que no se pod√≠a reutilizar el preentrenamiento para hacerles un fine tuning para otras tareas. Adem√°s no ten√≠an mucha memoria, por lo que si se le met√≠an frases muy largas no recordaban muy bien el inicio de la frase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arquitectura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de hablar de la arquitectura de GPT1 recordemos c√≥mo era la arquitectura de los transformers\n",
        "\n",
        "![transformer architecture](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPT1 es un modelo basado en los decoders de los transformers, as√≠ que como no tenemos encoder la arquitectura de un solo decoder queda de la siguiente manera\n",
        "\n",
        "![decoder architecture](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer_decoder_only-scaled.webp)\n",
        "\n",
        "Se elimina el mecanismo de atenci√≥n entre la sentencia del encoder y del decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el paper de GPT1 proponen la siguiente arquitectura\n",
        "\n",
        "![gpt1 architecture](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_architecture.webp)\n",
        "\n",
        "Que corresponde al decoder de un transformer como hemos visto antes, ejecutado 12 veces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen del paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las ideas m√°s interesantes del paper son:\n",
        "\n",
        " * Se entrena el modelo en un gran corpus de texto sin supervisi√≥n. Con esto se consigue crear un modelado del lenguaje. Se crea un modelo de lenguaje de alta capacidad en un gran corpus de texto\n",
        " * Luego se hace un fine-tuning en tareas de NLP supervisadas con datasets etiquetados. Se realiza un ajuste fino en una tarea objetivo con supervisi√≥n. Adem√°s, cuando se eval√∫a al modelo en la tarea supervisada, no solo se le eval√∫a por esa tarea, sino por lo bien que predice el siguiente token, esto ayuda a mejorar la generalizaci√≥n del modelo supervisado y hace que el modelo converja m√°s r√°pido.\n",
        " * Aunque ya lo hemos contado, en el paper se dice que se utiliza la arquitectura transformer, ya que hasta ese momento se usaban RNN para los modelos de lenguaje. Lo que produjo una mejora en que lo aprendido en el primer entrenamiento (entrenamiento en el corpus de texto sin supervisi√≥n) es m√°s f√°cil de transferir a tareas supervisadas. Es decir, gracias al uso de transformers se pudo hacer un entrenamiento en todo un corpus de texto y luego fine tunings en tareas supervisadas.\n",
        " * Evaluaron el modelo en cuatro tipos de tareas de comprensi√≥n del lenguaje:\n",
        "    * Inferencia del lenguaje natural\n",
        "    * Respuesta a preguntas\n",
        "    * Similitud sem√°ntica\n",
        "    * Clasificaci√≥n de textos.\n",
        " * El modelo general (el entrenado en todo el corpus de texto sin supervisi√≥n) supera a los modelos RNN entrenados discriminativamente que emplean arquitecturas dise√±adas espec√≠ficamente para cada tarea, mejorando significativamente el estado del arte en 9 de las 12 tareas estudiadas. Tambi√©n analizan los comportamientos de \"disparo cero\" del modelo preentrenado en cuatro entornos diferentes y demostraron que adquiere un conocimiento ling√º√≠stico √∫til para las tareas posteriores.\n",
        " * En los √∫ltimos a√±os, los investigadores hab√≠an demostrado los beneficios de utilizar embeddings, que se entrenan en corpus no etiquetados, para mejorar el rendimiento en una variedad de tareas. Sin embargo, estos enfoques transfieren principalmente informaci√≥n a nivel de palabra, mientras que el uso de transformers entrenados en grandes corpus de texto sin supervisi√≥n captura la sem√°ntica de nivel superior, a nivel de frase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generaci√≥n de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver c√≥mo generar texto con un GPT1 preentrenado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero hay que instalar `ftfy` y `spacy` mediante\n",
        "\n",
        "```bash\n",
        "pip install ftfy spacy\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez instaladas, debes descargar el modelo de lenguaje de spacy que deseas utilizar. Por ejemplo, para descargar el modelo de ingl√©s, puedes ejecutar:\n",
        "\n",
        "```bash\n",
        "python -m spacy download en_core_web_sm\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para generar texto vamos a utilizar el modelo desde el repositorio de [GPT1](https://huggingface.co/openai-community/openai-gpt) de Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importamos las librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si te fijas hemos importado `OpenAIGPTTokenizer` y `AutoTokenizer`. Esto es porque en la [model card](https://huggingface.co/openai-community/openai-gpt) de GPT1 se indica que se use `OpenAIGPTTokenizer`, pero en el post de la librer√≠a [transformers](https://maximofn.com/hugging-face-transformers/) explicamos que se debe usar `AutoTokenizer` para cargar el tokenizador. As√≠ que vamos a probar los dos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokens: \n",
            "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "input auto tokens: \n",
            "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "ckeckpoints = \"openai-community/openai-gpt\"\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(ckeckpoints)\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "\n",
        "input_tokens = tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
        "input_auto_tokens = auto_tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
        "\n",
        "print(f\"input tokens: \\n{input_tokens}\")\n",
        "print(f\"input auto tokens: \\n{input_auto_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como se puede ver con los dos tokenizadores se obtienen los mismos tokens. As√≠ que para que el c√≥digo sea m√°s general, de manera que si se cambian los checkpoints, no haya que cambiar el c√≥digo, vamos a utilizar `AutoTokenizer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos entonces el device, el tokenizador y el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como hemos instanciado el modelo, vamos a ver cu√°ntos par√°metros tiene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 117M\n"
          ]
        }
      ],
      "source": [
        "params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of parameters: {round(params/1e6)}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En la √©poca de los billones de par√°metros, podemos ver que GPT1 solo ten√≠a 117 millones de par√°metros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos los tokens de entrada al modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sentence = \"Hello, my dog is cute and\"\n",
        "input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
        "\n",
        "input_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se los pasamos al modelo para generar los tokens de salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output tokens: \n",
            "tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,\n",
            "           485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "output_tokens = model.generate(**input_tokens)\n",
        "\n",
        "print(f\"output tokens: \\n{output_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decodificamos los tokens para obtener la sentencia de salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded output: \n",
            "hello, my dog is cute and i'm going to take him for a walk. \" \n",
            " \"\n"
          ]
        }
      ],
      "source": [
        "decoded_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded output: \\n{decoded_output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya hemos conseguido generar texto con GPT1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generar texto token a token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Greedy search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos usado `model.generate` para generar los tokens de salida de golpe, pero vamos a ver c√≥mo generarlos uno a uno. Para ello, en vez de usar `model.generate` vamos a usar `model`, que en realidad lo que hace es llamar al m√©todo `model.forward`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
              "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
              "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
              "         ...,\n",
              "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
              "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
              "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
              "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que saca muchos datos, primero vamos a ver las keys de la salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['logits'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso solo tenemos los logits del modelo, vamos a ver su tama√±o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 40478])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits = outputs.logits\n",
        "\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver cu√°ntos tokens ten√≠amos a la entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 7])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tokens.input_ids.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vaya, a la salida tenemos el mismo n√∫mero de logits que a la entrada. Esto es normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtenemos los logits de la √∫ltima posici√≥n de la salida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([40478])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nex_token_logits = logits[0,-1]\n",
        "\n",
        "nex_token_logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hay un total de 40478 logits, es decir, hay un vocabulario de 40478 tokens y tenemos que ver cu√°l es el token con mayor probabilidad, para ello primero calculamos la softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([40478])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
        "\n",
        "softmax_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1898, device='cuda:0', grad_fn=<MaxBackward0>),\n",
              " tensor(249, device='cuda:0'))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
        "\n",
        "next_token_prob, next_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos obtenido el siguiente token, ahora lo decodificamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(next_token_id.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos obtenido el siguiente token mediante el m√©todo greedy, es decir, el token con mayor probabilidad. Pero ya vimos en el post de la librer√≠a transformers, las [formas de generar textos](https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto) que se puede hacer sampling, top-k, top-p, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a meter todo en una funci√≥n y ver qu√© sale si generamos unos cuantos tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_next_greedy_token(input_sentence, tokenizer, model, device):\n",
        "    input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
        "    outputs = model(**input_tokens)\n",
        "    logits = outputs.logits\n",
        "    nex_token_logits = logits[0,-1]\n",
        "    softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
        "    next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
        "    return next_token_prob, next_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_greedy_text(input_sentence, tokenizer, model, device, max_length=20):\n",
        "    generated_text = input_sentence\n",
        "    for _ in range(max_length):\n",
        "        next_token_prob, next_token_id = generate_next_greedy_token(generated_text, tokenizer, model, device)\n",
        "        generated_text += tokenizer.decode(next_token_id.item())\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora generamos texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello, my dog is cute andi.\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_greedy_text(\"Hello, my dog is cute and\", tokenizer, model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La salida es bastante repetitiva como ya se vio en las [formas de generar textos](https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine tuning GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√°lculo de la loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de empezar a hacer el fine tuning de GPT1 vamos a ver una cosa. Antes, cuando obten√≠amos la salida del modelo, hac√≠amos esto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
              "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
              "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
              "         ...,\n",
              "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
              "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
              "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
              "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens)\n",
        "\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se puede ver que obtenemos `loss=None`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(outputs.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vamos a necesitar la loss para hacer el fine tuning, vamos a ver c√≥mo obtenerla.\n",
        "\n",
        "Si nos vamos a la documentaci√≥n del m√©todo [forward](https://huggingface.co/docs/transformers/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel.forward) de `OpenAIGPTLMHeadModel`, podemos ver que dice que a la salida devuelve un objeto de tipo `transformers.modeling_outputs.CausalLMOutput`, as√≠ que si nos vamos a la documentaci√≥n de [transformers.modeling_outputs.CausalLMOutput](https://huggingface.co/docs/transformers/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput), podemos ver que dice que devuelve `loss` si se le pasa `labels` al m√©todo `forward`.\n",
        "\n",
        "Si nos vamos a la fuente del c√≥digo del m√©todo [forward](https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai/modeling_openai.py#L544), vemos este bloque de c√≥digo\n",
        "\n",
        "```python\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "```\n",
        "\n",
        "Es decir, la `loss` se calcula de la siguiente manera\n",
        "\n",
        " * Shift de logits y labels: La primera parte es desplazar los logits (`lm_logits`) y las etiquetas (`labels`) para que los `tokens < n` predigan `n`, es decir, desde una posici√≥n `n` se predice el siguiente token a partir de los anteriores.\n",
        " * CrossEntropyLoss: Se crea una instancia de la funci√≥n de p√©rdida `CrossEntropyLoss()`.\n",
        " * Flatten tokens: A continuaci√≥n, se aplanan los logits y las etiquetas utilizando `view(-1, shift_logits.size(-1))` y `view(-1)`, respectivamente. Esto se hace para que los logits y las etiquetas tengan la misma forma para la funci√≥n de p√©rdida.\n",
        " * C√°lculo de la p√©rdida: Finalmente, se calcula la p√©rdida utilizando la funci√≥n de p√©rdida `CrossEntropyLoss()` con los logits aplanados y las etiquetas aplanadas como entradas.\n",
        "\n",
        "En resumen, la `loss` se calcula como la p√©rdida de entrop√≠a cruzada entre los logits desplazados y aplanados y las etiquetas desplazadas y aplanadas.\n",
        "\n",
        "Por tanto, si al m√©todo `forward` le pasamos los labels, nos devolver√° la `loss`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.2607, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_tokens, labels=input_tokens.input_ids)\n",
        "\n",
        "outputs.loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para el entrenamiento vamos a usar un dataset de chistes en ingl√©s [short-jokes-dataset](https://huggingface.co/datasets/Maximofn/short-jokes-dataset), que es un dataset con 231 mil chistes en ingl√©s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Descargamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['ID', 'Joke'],\n",
              "        num_rows: 231657\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
        "jokes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a verlo un poco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ID': 1,\n",
              " 'Joke': '[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\"'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jokes[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenamiento con Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero vamos a ver c√≥mo se har√≠a el entrenamiento con puro Pytorch\n",
        "\n",
        "> Reiniciamos el notebook para que no haya problemas con la memoria de la GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import OpenAIGPTLMHeadModel, AutoTokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ckeckpoints = \"openai-community/openai-gpt\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pytorch dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos una clase Dataset de Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class JokesDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer):\n",
        "        self.dataset = dataset\n",
        "        self.joke = \"JOKE: \"\n",
        "        self.end_of_text_token = \"<|endoftext|>\"\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.dataset[\"train\"])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sentence = self.joke + self.dataset[\"train\"][item][\"Joke\"] + self.end_of_text_token\n",
        "        tokens = self.tokenizer(sentence, return_tensors=\"pt\")\n",
        "        return sentence, tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La instanciamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = JokesDataset(jokes, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo<|endoftext|>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([1, 30]), torch.Size([1, 30]))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence, tokens = dataset[5]\n",
        "print(sentence)\n",
        "tokens.input_ids.shape, tokens.attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos ahora un dataloader de Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BS = 1\n",
        "joke_dataloader = DataLoader(dataset, batch_size=BS, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos un batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, torch.Size([1, 1, 29]), torch.Size([1, 1, 29]))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences, tokens = next(iter(joke_dataloader))\n",
        "len(sentences), tokens.input_ids.shape, tokens.attention_mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 0 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:31<00:00, 334.88it/s, loss=2.88, lr=2.93e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:30<00:00, 335.27it/s, loss=2.49, lr=5.87e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 2 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:17<00:00, 341.75it/s, loss=2.57, lr=8.81e-6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 3 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:18<00:00, 341.27it/s, loss=2.41, lr=1.18e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 4 started==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231657/231657 [11:19<00:00, 341.04it/s, loss=2.49, lr=1.47e-5]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import tqdm\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-5\n",
        "WARMUP_STEPS = 5000\n",
        "MAX_SEQ_LEN = 500\n",
        "\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
        "proc_seq_count = 0\n",
        "batch_count = 0\n",
        "\n",
        "tmp_jokes_tens = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
        "    progress_bar = tqdm.tqdm(joke_dataloader, desc=\"Training\")\n",
        "    \n",
        "    for sample in progress_bar:\n",
        "\n",
        "        sentence, tokens = sample\n",
        "        \n",
        "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
        "        joke_tens = tokens.input_ids[0].to(device)\n",
        "\n",
        "        # Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
        "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "            continue\n",
        "        \n",
        "        # The first joke sequence in the sequence\n",
        "        if not torch.is_tensor(tmp_jokes_tens):\n",
        "            tmp_jokes_tens = joke_tens\n",
        "            continue\n",
        "        else:\n",
        "            # The next joke does not fit in so we process the sequence and leave the last joke \n",
        "            # as the start for next sequence \n",
        "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
        "                work_jokes_tens = tmp_jokes_tens\n",
        "                tmp_jokes_tens = joke_tens\n",
        "            else:\n",
        "                #Add the joke to sequence, continue and try to add more\n",
        "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
        "                continue\n",
        "        ################## Sequence ready, process it trough the model ##################\n",
        "            \n",
        "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "                       \n",
        "        proc_seq_count = proc_seq_count + 1\n",
        "        if proc_seq_count == BATCH_SIZE:\n",
        "            proc_seq_count = 0    \n",
        "            batch_count += 1\n",
        "            optimizer.step()\n",
        "            scheduler.step() \n",
        "            optimizer.zero_grad()\n",
        "            model.zero_grad()\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
        "        if batch_count == 10:\n",
        "            batch_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver qu√© tal hace chistes el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded joke: \n",
            "joke : what do you call a group of people who are not afraid of the dark? a group\n"
          ]
        }
      ],
      "source": [
        "sentence_joke = \"JOKE:\"\n",
        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se puede ver que le pasas una secuencia con la palabra `joke` y te devuelve un chiste. Pero si le devuelves otra secuencia no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "decoded joke: \n",
            "my dog is cute and i'm not sure if i should be offended or not. \" \n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence_joke = \"My dog is cute and\"\n",
        "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
        "output_tokens_joke = model.generate(**input_tokens_joke)\n",
        "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"decoded joke: \\n{decoded_output_joke}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "maximofn": {
      "date": "2024-06-12",
      "description_en": "Unlock the power of language!!!! üí• In my last post, I take you by the hand through the GPT-1 paper, explaining in a clear and concise way how this pioneer model in natural language processing works. And not only that! I also show you how to fine-tuning the model so you can adapt it to your specific needs üìä Don't miss the opportunity to learn about one of the most influential models in history! üöÄ Read my post and find out how you can improve your artificial intelligence skills! üìÑ",
      "description_es": "¬°¬°¬°Desbloquea el poder del lenguaje!!! üí• En mi √∫ltimo post, te llevo de la mano a trav√©s del paper de GPT-1, explicando de manera clara y concisa c√≥mo funciona este modelo pionero en el procesamiento de lenguaje natural. ¬°Y no solo eso! Tambi√©n te muestro c√≥mo hacer un fine-tuning del modelo para que puedas adaptarlo a tus necesidades espec√≠ficas üìä. ¬°No te pierdas la oportunidad de aprender sobre uno de los modelos m√°s influyentes de la historia! üöÄ ¬°Lee mi post y descubre c√≥mo puedes mejorar tus habilidades en inteligencia artificial! üìÑ",
      "description_pt": "Desbloqueie o poder da linguagem!!!! üí• Em minha √∫ltima postagem, apresentei o artigo GPT-1, explicando de forma clara e concisa como funciona esse modelo pioneiro no processamento de linguagem natural. E n√£o √© s√≥ isso! Tamb√©m mostro como fazer o ajuste fino do modelo para que voc√™ possa adapt√°-lo √†s suas necessidades espec√≠ficas üìä N√£o perca a oportunidade de conhecer um dos modelos mais influentes da hist√≥ria! üöÄ Leia minha postagem e descubra como voc√™ pode melhorar suas habilidades de intelig√™ncia artificial! üìÑ",
      "end_url": "gpt1",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_thumnail.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_thumnail.webp",
      "keywords_en": "gpt1, nlp, transformers, fine-tuning, language model, hugging face, pytorch",
      "keywords_es": "gpt1, nlp, transformers, fine-tuning, modelo de lenguaje, hugging face, pytorch",
      "keywords_pt": "gpt1, nlp, transformers, ajuste fino, modelo de linguagem, hugging face, pytorch",
      "title_en": "GPT1 ‚Äì Improving Language Understanding by Generative Pre-Training",
      "title_es": "GPT1 ‚Äì Improving Language Understanding by Generative Pre-Training",
      "title_pt": "GPT1 ‚Äì Improving Language Understanding by Generative Pre-Training"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
