{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLMs quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los modelos de lenguaje son cada vez m√°s grandes, lo que hace que cada vez sean m√°s costosos y caros de ejecutar.\n",
        "\n",
        "![LLMs-size-evolution](https://images.maximofn.com/LLMs-size-evolution.webp)\n",
        "\n",
        "![Llama-size-evolution](https://images.maximofn.com/Llama-size-evolution.webp)\n",
        "\n",
        "Por ejemplo, el modelo llama 3 400B, si sus par√°metros est√°n almacenados en formato FP32, cada par√°metro ocupa por tanto 4 bytes, lo que supone que solo para almacenar el modelo hace falta 400*(10e9)*4 bytes = 1.6 TB de memoria VRAM. Esto supone 20 GPUs de 80GB de memoria VRAM cada una, las cuales adem√°s no son baratas.\n",
        "\n",
        "Pero si dejamos a un lado modelos gigantes y nos vamos a modelos con tama√±os m√°s comunes, por ejemplo, 70B de par√°metros, solo almacenar el modelo supone 70*(10e9)*4 bytes = 280 GB de memoria VRAM, lo que supone 4 GPUs de 80GB de memoria VRAM cada una.\n",
        "\n",
        "Esto es porque almacenamos los pesos en formato FP32, es decir, que cada par√°metro ocupa 4 bytes. Pero qu√© pasa si conseguimos que cada par√°metro ocupe menos bytes? A esto se le llama cuantizaci√≥n.\n",
        "\n",
        "Por ejemplo, si conseguimos que un modelo de 70B de par√°metros, sus par√°metros ocupen medio byte, entonces solo necesitar√≠amos 70*(10e9)*0.5 bytes = 35 GB de memoria VRAM, lo que supone 2 GPUs de 24GB de memoria VRAM cada una, las cuales ya se pueden considerar GPUs de usuarios normales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Necesitamos por tanto maneras de poder reducir el tama√±o de estos modelos. Existen tres formas de hacer eso, la destilaci√≥n, la poda y la cuantizaci√≥n.\n",
        "\n",
        "La destilaci√≥n consiste en entrenar un modelo m√°s peque√±o a partir de las salidas del grande. Es decir, una entrada se le mete al modelo peque√±o y al grande, se considera que la salida correcta es la del modelo grande, por lo que se realiza el entrenamiento del modelo peque√±o de acuerdo con la salida del modelo grande. Pero esto requiere tener almacenado el modelo grande, que no es lo que queremos o podemos hacer.\n",
        "\n",
        "La poda consiste en eliminar par√°metros del modelo haci√©ndolo cada vez m√°s peque√±o. Este m√©todo se basa en la idea de que los modelos de lenguaje actuales est√°n sobredimensionados y solo unos pocos par√°metros son los que realmente aportan informaci√≥n. Por ello, si conseguimos eliminar los par√°metros que no aportan informaci√≥n, conseguiremos un modelo m√°s peque√±o. Pero esto no es sencillo a d√≠a de hoy, porque no tenemos manera de saber bien qu√© par√°metros son los importantes y cuales no.\n",
        "\n",
        "Por otro lado, la cuantizaci√≥n consiste en reducir el tama√±o de cada uno de los par√°metros del modelo. Y es lo que vamos a explicar en este post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Formato de los par√°metros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los par√°metros de los pesos se pueden almacenar en varios tipos de formatos\n",
        "\n",
        "![numbers-representation](https://images.maximofn.com/numbers-representation.webp)\n",
        "\n",
        "Originalmente se usaba FP32 para almacenar los par√°metros, pero debido a que empezamos a quedarnos sin memoria para almacenar los modelos, se empezaron a pasar a FP16, lo cual no daba malos resultados.\n",
        "\n",
        "Sin embargo el problema de FP16 es que no alcanza valores tan altos como FP32, por lo que puede darse el caso de desbordamiento de valores, es decir, al realizarse c√°lculos internos en la red, el resultado sea tan alto que no se pueda representar en FP16, lo que produce errores. Esto ocurre porque el modelo fue entrenado en FP32, lo que hace que todos los posibles c√°lculos internos sean posibles, pero al pasarse despu√©s a FP16 para poder hacer inferencias, algunos c√°lculos internos pueden producir desbordamientos.\n",
        "\n",
        "Debido a estos errores de desbordamiento se crearon TF32 y BF16, los cuales tienen la misma cantidad de bits de exponente, lo que hace que puedan llegar a valores tan altos como FP32, pero con la ventaja de ocupar menos memoria por tener menos bits. Sin embargo, ambos al tener menos bits de mantisa, no pueden representar n√∫meros con tanta precisi√≥n como FP32, lo cual puede dar errores de redondeo, pero al menos no obtendremos un error al ejecutar la red. TF32 tiene en total 19 bits, mientras que BF16 tiene 16 bits. Se suele usar m√°s BF16 porque se ahorra m√°s memoria.\n",
        "\n",
        "Hist√≥ricamente han existido los formatos INT8 y UINT8, que pueden representar n√∫meros desde -128 a 127 y de 0 a 255 respectivamente. Aunque son formatos buenos porque permiten ahorrar menos memoria, ya que cada par√°metro ocupa 1 byte en comparaci√≥n de los 4 bytes de FP32, el problema que tienen es que solo pueden representar un rango peque√±o de n√∫meros y adem√°s solo enteros, por lo que pueden darse los dos problemas vistos antes, desbordamiento y falta de precisi√≥n.\n",
        "\n",
        "Para solucionar el problema de que los formatos INT8 y UINT8 solo representan n√∫meros enteros se han creado los formatos FP8 y FP4, pero a√∫n no est√°n muy consolidados, ni tienen un formato muy estandarizado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aunque tengamos manera de poder almacenar los par√°metros de los modelos en formatos m√°s peque√±os, y aunque consigamos resolver los problemas de desbordamiento y redondeo, tenemos otro problema, y es que no todas las GPUs son capaces de representar todos los formatos. Esto es porque estos problemas de memoria son relativamente nuevos, por lo que las GPUs m√°s antiguas no se dise√±aron para poder resolver estos problemas y por tanto no son capaces de representar todos los formatos.\n",
        "\n",
        "![GPUs-data-formating](https://images.maximofn.com/GPUs-data-formating.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como √∫ltimo detalle, como curiosidad, durante el entrenamiento de los modelos se utiliza lo que se llama precisi√≥n mixta. Los pesos del modelo se almacenan en formato FP32, sin embargo el `forward pass` y el `backward pass` se realizan en FP16 para que sea m√°s r√°pido. Los gradientes resultantes del `backward pass` se almacenan en FP16 y se usan para modificar los valores FP32 de los pesos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tipos de cuantizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cuantizaci√≥n de punto cero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este es el tipo de cuantizaci√≥n m√°s sencilla. Consiste en reducir el rango de valores de manera lineal, el m√≠nimo valor de FP32 corresponde al m√≠nimo valor del nuevo formato, el cero de FP32 corresponde al cero del nuevo formato y el m√°ximo valor de FP32 corresponde al m√°ximo valor del nuevo formato.\n",
        "\n",
        "Por ejemplo, si queremos pasar los n√∫meros representados desde -1 hasta 1 en formato UINT8, como los l√≠mites de UINT8 son -127 y 127, si queremos representar el valor 0.3 lo que hacemos es multiplicar 0.3 por 127, que da 38.1 y redondearlo a 38, que es el valor que se almacenar√≠a en UINT8.\n",
        "\n",
        "Si queremos hacer el paso contrario, para pasar 38 a formato de entre -1 y 1, lo que hacemos es dividir 38 entre 127, que da 0.2992, que es aproximadamente 0.3, y podemos ver que tenemos un error de 0.008\n",
        "\n",
        "![quantization-zero-point](https://images.maximofn.com/quantization-zero-point.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cuantizazi√≥n afin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este tipo de cuantizaci√≥n, si se tiene un array de valores en un formato y se quiere pasar a otro, primero se divide el array entero por el m√°ximo valor del array y luego se multiplica el array entero por el m√°ximo valor del nuevo formato.\n",
        "\n",
        "![quantization-affine](https://images.maximofn.com/quantization-affine.webp)\n",
        "\n",
        "Por ejemplo, en la imagen anterior tenemos el array\n",
        "\n",
        "```\n",
        "[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]\n",
        "```\n",
        "\n",
        "Como su valor m√°ximo es `5.4`, dividimos el array por ese valor y obtenemos\n",
        "\n",
        "```\n",
        "[0.2222222222, -0.09259259259, -0.7962962963, 0.2222222222, -0.5740740741, 0.1481481481, 0.4444444444, 1]\n",
        "```\n",
        "\n",
        "Si ahora multiplicamos todos los valores por `127`, que es el m√°ximo valor de UINT8, obtenemos\n",
        "\n",
        "```\n",
        "[28,22222222, -11.75925926, -101.1296296, 28.22222222, -72.90740741, 18.81481481, 56.44444444, 127]\n",
        "```\n",
        "\n",
        "Que, redondeando, ser√≠a\n",
        "\n",
        "```\n",
        "[28, -12, -101, 28, -73, 19, 56, 127]\n",
        "```\n",
        "\n",
        "Si ahora quisi√©semos realizar el paso inverso tendr√≠amos que dividir el array resultante por `127`, que dar√≠a\n",
        "\n",
        "```\n",
        "[0,2204724409, -0.09448818898, -0.7952755906, 0.2204724409, -0.5748031496, 0.1496062992, 0.4409448819, 1]\n",
        "```\n",
        "\n",
        "Y volver a multiplicar por `5.4`, con lo que obtendr√≠amos\n",
        "\n",
        "```\n",
        "[1,190551181, -0.5102362205, -4.294488189, 1.190551181, -3.103937008, 0.8078740157, 2.381102362, 5.4]\n",
        "```\n",
        "\n",
        "Si lo comparamos con el array original, vemos que tenemos un error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Momentos de cuantizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cuantizaci√≥n post entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como su nombre indica, la cuantizaci√≥n se produce despu√©s del entrenamiento. Se entrena el modelo en FP32 y despu√©s se cuantiza a otro formato. Este m√©todo es el m√°s sencillo, pero puede dar lugar a errores de precisi√≥n en la cuantizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cuantizaci√≥n durante el entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Durante el entrenamiento se realiza el `forward pass` en el modelo original y en un modelo cuantizado y se ven los posibles errores derivados de la cuantizaci√≥n para poder mitigarlos. Este proceso hace que el entrenamiento sea m√°s costoso, porque tienes que tener almacenado en memoria el modelo original y el cuantizado, y m√°s lento, porque tienes que realizar el `forward pass` en dos modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## M√©todos de cuantizaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuaci√≥n muestro los enlaces a los posts donde explico cada uno de los m√©todos para que este post no se haga muy largo\n",
        "\n",
        " * [LLM.int8()](/llm-int8)\n",
        " * [GPTQ](/gptq)\n",
        " * [QLoRA](/qlora)\n",
        " * AWQ\n",
        " * QuIP\n",
        " * GGUF\n",
        " * HQQ\n",
        " * AQLM\n",
        " * FBGEMM FP8"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "maximofn": {
      "date": "2024-07-21",
      "description_en": "Imagine having a giant language model that can answer any question, from the capital of France to the perfect brownie recipe! üçûÔ∏èüá´üá∑ But what happens when that model has to fit on a mobile device? üì± That's where quantization comes in! üéâ This technique allows us to reduce the size of models without sacrificing their accuracy, which means we can enjoy artificial intelligence on our mobile devices without the need for a supercomputer. üíª It's like compressing an elephant into a shoebox, but without crushing the elephant! üêòüòÇ",
      "description_es": "¬°Imagina que tienes un modelo de lenguaje gigante que puede responder a cualquier pregunta, desde la capital de Francia hasta la receta perfecta para hacer brownies! üçûÔ∏èüá´üá∑ Pero, ¬øqu√© pasa cuando ese modelo tiene que caber en un dispositivo m√≥vil? üì± ¬°Eso es donde entra en juego la cuantizaci√≥n! üéâ Esta t√©cnica nos permite reducir el tama√±o de los modelos sin sacrificar su precisi√≥n, lo que significa que podemos disfrutar de inteligencia artificial en nuestros dispositivos m√≥viles sin necesidad de un supercomputador. üíª ¬°Es como comprimir un elefante en una caja de zapatos, pero sin aplastar al elefante! üêòüòÇ",
      "description_pt": "Imagine que voc√™ tem um modelo de linguagem gigante que pode responder a qualquer pergunta, desde a capital da Fran√ßa at√© a receita perfeita de brownies! üçûÔ∏èüá´üá∑ Mas o que acontece quando esse modelo precisa caber em um dispositivo m√≥vel? üì± √â a√≠ que entra a quantiza√ß√£o! üéâ Essa t√©cnica nos permite reduzir o tamanho dos modelos sem sacrificar sua precis√£o, o que significa que podemos desfrutar da intelig√™ncia artificial em nossos dispositivos m√≥veis sem a necessidade de um supercomputador. üíª √â como espremer um elefante em uma caixa de sapatos, mas sem esmagar o elefante! üêòüòÇ",
      "end_url": "llms-quantization",
      "image": "https://images.maximofn.com/quantization-thumbnail.webp",
      "image_hover_path": "https://images.maximofn.com/quantization-thumbnail.webp",
      "keywords_en": "quantization, LLMs, GPT, AI, machine learning, deep learning, FP16, FP32, TF32, BF16, INT8, UINT8, FP8, FP4, post-training, during-training, zero-point, affine, QLoRA, AWQ, QuIP, GGUF, HQQ, AQLM, FBGEMM FP8",
      "keywords_es": "cuantizaci√≥n, LLMs, GPT, IA, aprendizaje autom√°tico, aprendizaje profundo, FP16, FP32, TF32, BF16, INT8, UINT8, FP8, FP4, post-entrenamiento, durante-entrenamiento, punto-cero, afin, QLoRA, AWQ, QuIP, GGUF, HQQ, AQLM, FBGEMM FP8",
      "keywords_pt": "quantiza√ß√£o, LLMs, GPT, IA, aprendizado de m√°quina, aprendizado profundo, FP16, FP32, TF32, BF16, INT8, UINT8, FP8, FP4, p√≥s-treinamento, durante-treinamento, ponto-zero, afim, QLoRA, AWQ, QuIP, GGUF, HQQ, AQLM, FBGEMM FP8",
      "title_en": "LLMs quantization",
      "title_es": "LLMs quantization",
      "title_pt": "LLMs quantization"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
