{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA: Efficient Finetuning of Quantized LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien [LoRA](https://maximofn.com/lora/) proporciona una manera de hacer fine tuning de modelos de lenguaje sin necesidad de GPUs con grandes VRAMs, en el paper de [QLoRA](https://arxiv.org/abs/2305.14314) van m√°s all√° y proponen la manera de hacer fine tuning de modelo cuantizados, haciendo que se necesite a√∫n menos memoria para hacer fine tuning de modelos de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actualizaci√≥n de pesos en una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender c√≥mo funciona LoRA, primero tenemos que recordar qu√© ocurre cuando entrenamos un modelo. Volvamos a la parte m√°s b√°sica del deep learning, tenemos una capa densa de una red neuronal que se define como:\n",
    "\n",
    "$$\n",
    "y = Wx + b\n",
    "$$\n",
    "\n",
    "Donde $W$ es la matriz de pesos y $b$ es el vector de sesgos.\n",
    "\n",
    "Para simplificar vamos a suponer que no hay sesgo, por lo que quedar√≠a as√≠\n",
    "\n",
    "$$\n",
    "y = Wx\n",
    "$$\n",
    "\n",
    "Supongamos que para una entrada $x$ queremos que tenga una salida $≈∑$\n",
    "\n",
    " * Primero lo que hacemos es calcular la salida que obtenemos con nuestro valor actual de pesos $W$, es decir obtenemos el valor $y$\n",
    " * A continuaci√≥n calculamos el error que existe entre el valor de $y$ que hemos obtenido y el valor que quer√≠amos obtener $≈∑$. A ese error lo llamamos $loss$, y lo calculamos con alguna funci√≥n matem√°tica, ahora no importa cual\n",
    " * Calculamos el gardiente (la derivada) del error $loss$ con respecto a la matriz de pesos $W$, es decir $\\Delta W = \\frac{dloss}{dW}$\n",
    " * Actualizamos los pesos $W$ restando a cada uno de sus valores el valor del gradiente multiplicado por un factor de aprendizaje $\\alpha$, es decir $W = W - \\alpha \\Delta W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los autores de LoRA lo que proponen es que la matriz de pesos $W$ se puede descomponer en \n",
    "\n",
    "$$\n",
    "W \\sim W + \\Delta W\n",
    "$$\n",
    "\n",
    "De manera que congelando la matriz $W$ y entrenando solo la matriz $\\Delta W$ se puede obtener un modelo que se adapte a nuevos datos sin tener que reentrenar todo el modelo\n",
    "\n",
    "Pero podr√°s pensar que $\\Delta W$ es una matriz de tama√±o igual a $W$ por lo que no se ha ganado nada, pero aqu√≠ los autores se basan en `Aghajanyan et al. (2020)`, un paper en el que demostraron que aunque los modelos de lenguaje son grandes y que sus par√°metros son matrices con dimensiones muy grandes, para adaptarlos a nuevas tareas no es necesario cambiar todos los valores de las matrices, sino que cambiando unos pocos valores es suficiente, que en t√©rminos t√©cnicos, se llama adaptaci√≥n de bajo rango. De ah√≠ el nombre de LoRA (Low Rank Adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos congelado el modelo y ahora queremos entrenar la matriz $\\Delta W$, supongamos que tanto $W$ como $\\Delta W$ son matrices de tama√±o $20 \\times 10$, por lo que tenemos 200 par√°metros entrenables\n",
    "\n",
    "Ahora supongamos que la matriz $\\Delta W$ se puede descomponer en el producto de dos matrices $A$ y $B$, es decir\n",
    "\n",
    "$$\n",
    "\\Delta W = A \\cdot B\n",
    "$$\n",
    "\n",
    "Para que esta multiplicaci√≥n se produzca los tama√±os de las matrices $A$ y $B$ tienen que ser $20 \\times n$ y $n \\times 10$ respectivamente. Supongamos que $n = 5$, por lo que $A$ ser√≠a de tama√±o $20 \\times 5$, es decir 100 par√°metros, y $B$ de tama√±o $5 \\times 10$, es decir 50 par√°metros, por lo que tendr√≠amos 100+50=150 par√°metros entrenables. Ya tenemos menos par√°metros entrenables que antes\n",
    "\n",
    "Ahora supongamos que $W$ en realidad es una matriz de tama√±o $10.000 \\times 10.000$, por lo que tendr√≠amos 100.000.000 par√°metros entrenables, pero si descomponemos $\\Delta W$ en $A$ y $B$ con $n = 5$, tendr√≠amos una matriz de tama√±o $10.000 \\times 5$ y otra de tama√±o $5 \\times 10.000$, por lo que tendr√≠amos 50.000 par√°metros de una y otros 50.000 par√°metros de otra, en total 100.000 par√°metros entrenables, es decir hemos reducido el n√∫mero de par√°metros 1000 veces\n",
    "\n",
    "Ya puedes ir viendo el poder de LoRA, cuando se tienen modelos muy grandes, el n√∫mero de par√°metros entrenables se puede reducir much√≠simo\n",
    "\n",
    "Si volvemos a ver la imagen de la arquitectura de LoRA, la entenderemos mejor\n",
    "\n",
    "![LoRA adapt](https://maximofn.com/wp-content/uploads/2024/07/LoRA_adapat.webp)\n",
    "\n",
    "Pero se ve mejor aun, el ahorro en n√∫mero de par√°metros entrenables con esta imagen\n",
    "\n",
    "![LoRA matmul](https://maximofn.com/wp-content/uploads/2024/07/Lora_matmul.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QLoRA se realiza en dos pasos, la primera consiste en cuantizar el moelo y la segunda en aplicar LoRA al modelo cuantizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuantizaci√≥n QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cuantizaci√≥n de QLoRA se basa en tres conceptos, la cuantizaci√≥n del modelo a 4 bits con el formato normal float 4 (NF4), la doble cuantiazci√≥n y los optimizadores paginados. Todo ello junto hace que se pueda ahorrar mucha memoria al hacer fine tuning de los modelos de lenguaje, as√≠ que vamos a ver en qu√© consiste cada uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuantizaci√≥n de los modelos de lenguaje en normal float 4 (NF4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En QLoRA, para cuantizar, lo que se hace es cuantizar en formato normal float 4 (NF4), que es un tipo de cuantizaci√≥n a 4 bits de manera que sus datos tienen una distribuci√≥n normal, es decir que siguen una campana de Gauss. Para conseguir que sigan esta distribuci√≥n, lo que se hace es dividir los valores de los pesos en FP16 en quantiles, de manera que en cada quantil haya el mismo n√∫mero de valores. Una vez tenemos los cuantiles, a cada cuantil se le asigna un valor en 4 bits\n",
    "\n",
    "![QLoRA-normal-float-quantization](https://maximofn.com/wp-content/uploads/2024/07/QLoRA-normal-float-quantization.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar esta cuantizaci√≥n utiliza el algoritmo de cuantizaci√≥n SRAM, que es un algoritmo de cuantizaci√≥n por quantiles muy r√°pido, pero tiene mucho error con valores que est√°n muy lejos en la distribuci√≥n de la campana de Gauss, valores at√≠picos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como normalmente los par√°metros de los pesos de una red neuronal suelen seguir una distribuci√≥n normal (es decir que siguen una campana de Gauss), centrada en cero y con una desviaci√≥n estandar œÉ. Lo que se hace es normalizarlos para que tengan una desviaci√≥n estandar entre -1 y 1, y despu√©s se cuantizan en formato NF4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doble cuantizaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos comentado, a la hora de cuantizar los par√°metros de la red, tenemos que normalizarlos para que tengan una desviaci√≥n estandar entre -1 y 1, y despu√©s cuantizarlos en formato NF4. Por lo que tenemos que guardar algunos par√°metros como los valores para normalizar los par√°metros, es decir, el valor por el que se dividen los datos para que tengan una desviaci√≥n entre -1 y 1. Esos valores se almacenan en formato FP32, por lo que los autores del paper proponen cuantizar esos par√°metros a formato FP8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque esto puede parecer que no ahorra mucha memoria, los autores calculan que esto puede ahorrar unos 0.373 bits por par√°metro, pero si por ejemplo tenemos un modelo de 8B de par√°metros, que no es un modelo excesivamente grande a d√≠a de hoy, nos ahorrar√≠amos unos 3 GB de memoria, que no est√° mal. En el caso de un modelo de 70B de par√°metros, nos ahorrar√≠amos unos 26 GB de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizadores paginados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las GPUs de Nvidia tienen la opci√≥n de compartir la RAM de la GPU y de la CPU, de manera que lo que hacen es guardar los estados del optimizador en la RAM de la CPU y acceder a ellos cuando lo necesitan. As√≠ no se tienen que guardar en la RAM de la GPU y podemos ahorrar memoria de la GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning con LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos cuantizado el modelo ya podemos hacer fine tuning del modelo cuantizado igual que se hace en [LoRA](https://maximofn.com/lora/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C√≥mo hacer fine tuning de un modelo cuantizado con QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos explicado QLoRA, vamos a ver un ejemplo de c√≥mo hacer fine tuning a un modelo usando QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login en el Hub de Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero nos logeamos para poder subir el modelo entrenado al Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el dataset que vamos a usar, que es un dataset de reviews de [Amazon](https://huggingface.co/datasets/mteb/amazon_reviews_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un subset por si quieres probar el c√≥digo con un dataset m√°s peque√±o. En mi caso usar√© el 100% del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'text', 'label', 'label_text'],\n",
       "     num_rows: 200000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label', 'label_text'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label', 'label_text'],\n",
       "     num_rows: 5000\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage = 1\n",
    "\n",
    "subset_dataset_train = dataset['train'].select(range(int(len(dataset['train']) * percentage)))\n",
    "subset_dataset_validation = dataset['validation'].select(range(int(len(dataset['validation']) * percentage)))\n",
    "subset_dataset_test = dataset['test'].select(range(int(len(dataset['test']) * percentage)))\n",
    "\n",
    "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos una muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'en_0297000',\n",
       " 'text': 'Not waterproof at all\\n\\nBought this after reading good reviews. But it‚Äôs not water proof at all. If my son has even a little accident in bed, it goes straight to mattress. I don‚Äôt see a point in having this. So I have to purchase another one.',\n",
       " 'label': 0,\n",
       " 'label_text': '0'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "idx = randint(0, len(subset_dataset_train))\n",
    "subset_dataset_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el n√∫mero de clases, para obtener el n√∫mero de clases usamos `dataset['train']` y no `subset_dataset_train` porque si el subset lo hamos muy peque√±o es posible que no haya ejemplos con todas las posibles clases del dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(dataset['train'].unique('label'))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funci√≥n para crear el campo `label` en el dataset. El dataset descargado tiene el campo `labels` pero la librer√≠a `transformers` necesita que el campo se llame `label` y no `labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(example):\n",
    "    example['labels'] = example['label']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la funci√≥n al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
       "     num_rows: 200000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'text', 'label', 'label_text', 'labels'],\n",
       "     num_rows: 5000\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_dataset_train = subset_dataset_train.map(set_labels)\n",
    "subset_dataset_validation = subset_dataset_validation.map(set_labels)\n",
    "subset_dataset_test = subset_dataset_test.map(set_labels)\n",
    "\n",
    "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a ver una muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'en_0297000',\n",
       " 'text': 'Not waterproof at all\\n\\nBought this after reading good reviews. But it‚Äôs not water proof at all. If my son has even a little accident in bed, it goes straight to mattress. I don‚Äôt see a point in having this. So I have to purchase another one.',\n",
       " 'label': 0,\n",
       " 'label_text': '0',\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_dataset_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementamos el tokenizador. Para que no nos de error, asignamos el token de end of string al token de padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funci√≥n para tokenizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=768, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la funci√≥n al dataset y de paso eliminamos las columnas que no necesitamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['labels', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 200000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['labels', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 5000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['labels', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 5000\n",
       " }))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_dataset_train = subset_dataset_train.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
    "subset_dataset_validation = subset_dataset_validation.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
    "subset_dataset_test = subset_dataset_test.map(tokenize_function, batched=True, remove_columns=['text', 'label', 'id', 'label_text'])\n",
    "\n",
    "subset_dataset_train, subset_dataset_validation, subset_dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a ver una muestra, pero en este caso solo vemos las `keys`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_dataset_train[idx].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos primero el modelo sin cuantizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes)\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos la memoria que ocupa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory: 0.48 GB\n"
     ]
    }
   ],
   "source": [
    "model_memory = model.get_memory_footprint()/(1024**3)\n",
    "print(f\"Model memory: {model_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el modelo a FP16 y volvemos a ver la memoria que ocupa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory: 0.24 GB\n"
     ]
    }
   ],
   "source": [
    "model_memory = model.get_memory_footprint()/(1024**3)\n",
    "print(f\"Model memory: {model_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos la arquitectura del modelo antes de cuantizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuantizaci√≥n del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cuantizar el modelo primero tenemos que crear la configuraci√≥n de cuantizaci√≥n, para ello usamos la librer√≠a `bitsandbytes`, si no la tienes instalada la puedes instalar con\n",
    "\n",
    "```bash\n",
    "pip install bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero comprobamos si la arquitectura de nuestra GPU permite el formato BF16, si no lo permite usaremos FP16\n",
    "\n",
    "A continuaci√≥n creamos la configuraci√≥n de cuantizaci√≥n, con `load_in_4bits=True` indicamos que cuantice a 4 bits, con `bnb_4bit_quant_type=\"nf4\"` le indicamos que lo haga en formato NF4, con `bnb_4bit_use_double_quant=True` le indicamos que haga doble cuantizaci√≥n y con `bnb_4bit_compute_dtype=compute_dtype` le indicamos qu√© formato de datos tiene que usar cuando descuantice, que puede ser FP16 o BF16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora cuantizamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a ver la memoria que ocupa ahora que lo hemos cuantizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory: 0.12 GB\n"
     ]
    }
   ],
   "source": [
    "model_memory = model.get_memory_footprint()/(1024**3)\n",
    "print(f\"Model memory: {model_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que se ha reducido el tama√±o del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a ver la arquitectura del modelo una vez cuantizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la arquitectura ha cambiado\n",
    "\n",
    "![QLoRA-model-vs-quantized-model](https://maximofn.com/wp-content/uploads/2024/07/QLoRA-model-vs-quantized-model_-scaled.webp)\n",
    "\n",
    "Ha modificado las capas `Conv1D` por capas `Linear4bits`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de implementar LoRA, tenemos que configurar el modelo para entrenar en 4 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver si ha cambiado el tama√±o del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory: 0.20 GB\n"
     ]
    }
   ],
   "source": [
    "model_memory = model.get_memory_footprint()/(1024**3)\n",
    "print(f\"Model memory: {model_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha aumentado la memoria, as√≠ que volvemos a ver la arquitectura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear4bit(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura sigue siendo la misma, por lo que suponemos que el aumento de memoria es por alguna configuraci√≥n extra para poder aplicar LoRA en 4 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una configuraci√≥n de LoRA, pero a diferencia del post de [LoRA](https://maximofn.com/lora/) en el que solo configuramos en `target_modeules` la capa `scores`, ahora vamos a a√±adir tambi√©n las capas `c_attn`, `c_proj` y `c_fc` ya que ahora son de tipo `Linear4bits` y no `Conv1D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['c_attn', 'c_fc', 'c_proj', 'score'],\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,375,504 || all params: 126,831,520 || trainable%: 1.8730\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras que en el post de [LoRA](https://maximofn.com/lora/) ten√≠amos unos 12.000 par√°metros entrenables, ahora tenemos unos 2 millones, ya que ahora hemos a√±adido las capas `c_attn`, `c_proj` y `c_fc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instanciado el modelo cuantizado y aplicado LoRA, es decir, una vez hemos hecho QLoRA, vamos a entrenarlo como siempre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "metric_name = \"accuracy\"\n",
    "model_name = \"GPT2-small-QLoRA-finetuned-amazon-reviews-en-classification\"\n",
    "LR = 2e-5\n",
    "BS_TRAIN = 224\n",
    "BS_EVAL = 224\n",
    "EPOCHS = 3\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    model_name,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BS_TRAIN,\n",
    "    per_device_eval_batch_size=BS_EVAL,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio = 0.1,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    "    logging_dir=\"./runs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el post [Fine tuning SMLs](https://maximofn.com/fine-tuning-sml/) tuvimos que poner un BS de train de 28, en el post [LoRA](https://maximofn.com/lora/) al poner las matrices de bajo rango en las capas lineales hizo que pudi√©ramos poner un batch size de train de 400. Ahora, como al cuantizar el modelo, la librer√≠a PEFT ha convertido algunas capas m√°s a `Linear` no podemos poner un batch size tan grande y lo tenemos que poner de 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    print(eval_pred)\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=subset_dataset_train,\n",
    "    eval_dataset=subset_dataset_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1859' max='2679' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1859/2679 2:10:29 < 57:37, 0.24 it/s, Epoch 2.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.088000</td>\n",
       "      <td>0.978048</td>\n",
       "      <td>0.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.958800</td>\n",
       "      <td>0.894022</td>\n",
       "      <td>0.615600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7acac436c3d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7acac32580d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2679' max='2679' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2679/2679 3:08:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.088000</td>\n",
       "      <td>0.978048</td>\n",
       "      <td>0.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.958800</td>\n",
       "      <td>0.894022</td>\n",
       "      <td>0.615600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.914700</td>\n",
       "      <td>0.891830</td>\n",
       "      <td>0.616800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7acac2f43c10>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2679, training_loss=1.1650676093647934, metrics={'train_runtime': 11299.1288, 'train_samples_per_second': 53.101, 'train_steps_per_second': 0.237, 'total_flos': 2.417754341376e+17, 'train_loss': 1.1650676093647934, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado evaluamos sobre el dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7acb316fe5c0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8883273601531982,\n",
       " 'eval_accuracy': 0.615,\n",
       " 'eval_runtime': 28.5566,\n",
       " 'eval_samples_per_second': 175.091,\n",
       " 'eval_steps_per_second': 0.805,\n",
       " 'epoch': 3.0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=subset_dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publicar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una model card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.create_model_card()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo publicamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos aprobar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sae00531/miniconda3/envs/nlp_/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/sae00531/miniconda3/envs/nlp_/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "Loading adapter weights from maximofn/GPT2-small-QLoRA-finetuned-amazon-reviews-en-classification led to unexpected keys not found in the model:  ['score.modules_to_save.default.base_layer.weight', 'score.modules_to_save.default.lora_A.default.weight', 'score.modules_to_save.default.lora_B.default.weight', 'score.modules_to_save.default.modules_to_save.lora_A.default.weight', 'score.modules_to_save.default.modules_to_save.lora_B.default.weight', 'score.modules_to_save.default.original_module.lora_A.default.weight', 'score.modules_to_save.default.original_module.lora_B.default.weight']. \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"GPT2-small-QLoRA-finetuned-amazon-reviews-en-classification\"\n",
    "user = \"maximofn\"\n",
    "checkpoint = f\"{user}/{model_name}\"\n",
    "num_classes = 5\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_classes).half().eval().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0186614990234375,\n",
       " 0.483642578125,\n",
       " 0.048187255859375,\n",
       " 0.415283203125,\n",
       " 0.03399658203125]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"I love this product\", return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model(tokens)\n",
    "logits = output.logits\n",
    "lables = torch.softmax(logits, dim=1).cpu().numpy().tolist()\n",
    "lables[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_",
   "language": "python",
   "name": "python3"
  },
  "maximofn": {
      "date": "2024-07-29",
      "description_es": "¬°Hola a todos! ü§ó Hoy vamos a hablar de QLoRA, la t√©cnica que te permitir√° hacer que tus modelos de lenguaje sean m√°s eficientes y r√°pidos ‚è±Ô∏è. Pero, ¬øc√≥mo lo hace? ü§î Bueno, primero utiliza la cuantizaci√≥n para reducir el tama√±o de los pesos del modelo, lo que ahorra memoria y velocidad üìà. Luego, aplica LoRA (Low-Rank Adaptation), que es como un superpoder que permite al modelo adaptarse a nuevos datos sin necesidad de volver a entrenar desde cero üí™. Y, para que veas c√≥mo funciona en la pr√°ctica, te dejo un ejemplo de c√≥digo que te har√° decir '¬°Eureka!' üéâ. ¬°Vamos a sumergirnos en el mundo de QLoRA y descubrir c√≥mo podemos hacer que nuestros modelos sean m√°s inteligentes y eficientes! ü§ì",
      "description_en": "Hello everyone! ü§ó Today we are going to talk about QLoRA, the technique that will allow you to make your language models more efficient and faster ‚è±Ô∏è. But how does it do it? ü§î Well, first it uses quantization to reduce the size of the model weights, which saves memory and speed üìà. Then, it applies LoRA (Low-Rank Adaptation), which is like a superpower that allows the model to adapt to new data without retraining from scratch üí™. And, for you to see how it works in practice, I leave you with a code example that will make you say 'Eureka!' üéâ Let's dive into the world of QLoRA and discover how we can make our models smarter and more efficient! ü§ì",
      "description_pt": "Ol√° a todos! ü§ó Hoje vamos falar sobre QLoRA, a t√©cnica que permitir√° que voc√™ torne seus modelos de linguagem mais eficientes e mais r√°pidos ‚è±Ô∏è. Mas como ela faz isso? ü§î Bem, primeiro ele usa a quantiza√ß√£o para reduzir o tamanho dos pesos do modelo, o que economiza mem√≥ria e velocidade üìà. Em seguida, ele aplica LoRA (Low-Rank Adaptation), que √© como uma superpot√™ncia que permite que o modelo se adapte a novos dados sem precisar treinar novamente do zero üí™. E, para mostrar como isso funciona na pr√°tica, deixo um exemplo de c√≥digo que far√° voc√™ dizer ‚ÄúEureka!‚Äù üéâ Vamos mergulhar no mundo do QLoRA e descobrir como podemos tornar nossos modelos mais inteligentes e eficientes! ü§ì",
      "end_url": "qlora",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/QLoRA_thumbnail_ES.webp",
      "keywords_en": "qlora, quantization, lora, low-rank adaptation, transformers, hugging face, nlp, natural language processing, machine learning, artificial intelligence",
      "keywords_es": "qlora, cuantizaci√≥n, lora, adaptaci√≥n de rango bajo, transformers, hugging face, nlp, procesamiento de lenguaje natural, aprendizaje autom√°tico, inteligencia artificial",
      "keywords_pt": "qlora, quantiza√ß√£o, lora, adapta√ß√£o de baixo rank, transformers, hugging face, nlp, processamento de linguagem natural, aprendizado de m√°quina, intelig√™ncia artificial",
      "title_en": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "title_es": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "title_pt": "QLoRA: Efficient Finetuning of Quantized LLMs"
    },
    "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
