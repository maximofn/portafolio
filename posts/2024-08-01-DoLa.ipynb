{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aunque a medida que los LLMs van aumentando de tama\u00f1o y van surgiendo nuevas capacidades, tenemos un problema y son las alucinaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Proponen un enfoque de decodificaci\u00f3n contrastiva, donde la probabilidad de salida de la siguiente palabra se obtiene de la diferencia en los logits entre una capa superior y una inferior. Al enfatizar el conocimiento de las capas superiores y restarle importancia al de las inferiores, podemos hacer que los LM sean m\u00e1s factuales y, por lo tanto, reducir las alucinaciones.\n",
        "\n",
        "En la siguiente figura se muestra esta idea. Mientras que `Seattle` mantiene una alta probabilidad en todas las capas, la probabilidad de la respuesta correcta `Olympia` aumenta despu\u00e9s de que las capas superiores inyecten m\u00e1s conocimiento factual. Contrastar las diferencias entre las distintas capas puede revelar la respuesta correcta en este caso\n",
        "\n",
        "![DoLa-figure1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure1.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## M\u00e9todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un LLM consiste en una capa de embedding, varios transformers secuenciales y a continuaci\u00f3n una capa de salida. Lo que proponen es medir la salida de cada transformer mediante la divergencia de Jensen-Shannon (JSD)\n",
        "\n",
        "En la siguiente figura se puede ver esta medida a la salida de cada transformer para una frase de entrada al LLM. Cada columna corresponde a un token de la frase.",
        "\n",
        "![DoLa-figure2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure2.webp)\n",
        "\n",
        "Se pueden observar dos patrones\n",
        "\n",
        "* El primero ocurre cuando se predicen entidades de nombre o fechas importantes, como `Wole Soyinka` y `1986`, que requieren conocimiento f\u00e1ctico. Se puede ver que la JSD calculada sigue siendo extremadamente alta en las capas superiores. Este patr\u00f3n indica que el modelo sigue cambiando sus predicciones en las \u00faltimas capas, y potencialmente inyectando m\u00e1s conocimiento f\u00e1ctico en las predicciones",
        " \n",
        " * El segundo ocurre cuando se predicen palabras funcionales, como `was`, `the`, `to`, `in`, y los tokens copiados de la pregunta de entrada, como `first Nigerian`, `Nobel Prize`. Cuando se predicen estos tokens \"f\u00e1ciles\", podemos observar que la JSD se vuelve muy peque\u00f1a a partir de las capas intermedias. Este hallazgo indica que el modelo ya ha decidido qu\u00e9 token generar en las capas intermedias, y mantiene las distribuciones de salida casi sin cambios en las capas superiores. Este hallazgo tambi\u00e9n es consistente con las suposiciones en los LLM de salida temprana `Schuster et al., 2022`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cuando la predicci\u00f3n de la siguiente palabra requiere conocimiento factual, el LLM parece cambiar las predicciones en las capas superiores. Contrastar las capas antes y despu\u00e9s de un cambio repentino puede, por lo tanto, amplificar el conocimiento que emerge de las capas superiores y hacer que el modelo se base m\u00e1s en su conocimiento interno factual. Adem\u00e1s, esta evoluci\u00f3n de la informaci\u00f3n parece variar de un token a otro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Su m\u00e9todo requiere seleccionar con precisi\u00f3n la capa prematura que contiene informaci\u00f3n plausible pero menos factual, que puede no estar siempre en la misma capa temprana. Por lo tanto, proponen encontrar esa capa prematura mediante una selecci\u00f3n din\u00e1mica de la capa prematura como se ve en la siguiente imagen\n",
        "\n",
        "![DoLa-figure3](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-figure3.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecci\u00f3n din\u00e1mica de la capa prematura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para seleccionar la capa prematura, calculan la divergencia de Jensen-Shannon (JSD) entre las capas intermedias y la final. La capa prematura se selecciona como la capa con la JSD m\u00e1s alta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sin embargo, como este proceso puede ser un poco lento, lo que hacen es agrupar varias capas para hacer menos c\u00e1lculos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contraste de las predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que tenemos la \u00faltima capa (capa madura) y la capa prematura, podemos contrastar las predicciones de ambas capas. Para ello, calculan la probabilidad logar\u00edtmica del siguiente token en la capa madura y la prematura. A continuaci\u00f3n restan la probabilidad logar\u00edtmica de la capa prematura a la de la capa madura, as\u00ed le dan m\u00e1s importancia al conocimiento de la capa madura"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Penalizaci\u00f3n por repetici\u00f3n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La motivaci\u00f3n de DoLa es restar importancia al conocimiento ling\u00fc\u00edstico de las capas inferiores y amplificar el conocimiento factual del mundo real. Sin embargo, esto puede dar lugar a que el modelo genere p\u00e1rrafos gramaticalmente incorrectos\n",
        "\n",
        "Emp\u00edricamente no han observado ese problema, pero han encontrado que la distribuci\u00f3n DoLa resultante a veces tiene una mayor tendencia a repetir frases generadas previamente, especialmente durante la generaci\u00f3n de largas secuencias de razonamiento en la cadena de pensamiento\n",
        "\n",
        "As\u00ed que incluyen una penalizaci\u00f3n por repetici\u00f3n introducida en `Keskar et al. (2019)` con `\u03b8 = 1.2` durante la decodificaci\u00f3n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementaci\u00f3n con transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a ver c\u00f3mo implementar DoLa con la librer\u00eda `transformers` de Hugging Face. Para obtener m\u00e1s informaci\u00f3n de c\u00f3mo aplicar DoLa con la librer\u00eda `transformers` puedes consultar el siguiente [enlace](https://huggingface.co/docs/transformers/main/en/generation_strategies#dola-decoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero nos logueamos en el Hub, porque vamos a usar Llama 3 8B, para poder usarlo hay que pedir permiso a Meta, as\u00ed que para poder descargarlo hay que estar logueado para que sepa qui\u00e9n lo est\u00e1 descargando"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora instanciamos el tokenizador y el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "import torch\n",
        "\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "checkpoints = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoints, torch_dtype=compute_dtype, device_map=\"auto\")\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Asignamos un valor fijo de semilla para la reproducibilidad del ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generamos los tokens de entrada al LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = 'What does Darth Vader say to Luke in \"The Empire Strikes Back\"?'\n",
        "text = f\"Answer with a short answer.\\n\\nQuestion: {question}\\n\\nAnswer: \"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generamos ahora la entrada vanilla, es decir, sin aplicar DoLa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \"No, I am your father.\" (Note: This is a famous misquote. The actual quote is \"No, I am your father\" is not in the movie. The correct quote is \"No, I am your father.\" is not\n"
          ]
        }
      ],
      "source": [
        "generate_kwargs={\n",
        "    \"do_sample\": False,\n",
        "    \"max_new_tokens\": 50,\n",
        "    \"top_p\": None,\n",
        "    \"temperature\": None\n",
        "}\n",
        "\n",
        "vanilla_output = model.generate(**inputs, **generate_kwargs)\n",
        "\n",
        "print(tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que sabe que hay un error famoso, pero no consigue decir la frase correcta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora aplicando DoLa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \"No, I am your father.\" (Note: This is one of the most famous lines in movie history, and it's often misquoted as \"Luke, I am your father.\")\n"
          ]
        }
      ],
      "source": [
        "dola_high_output = model.generate(**inputs, **generate_kwargs, dola_layers='high', repetition_penalty=1.2)\n",
        "\n",
        "print(tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora s\u00ed consigue dar la frase correcta y el [error famoso](https://www.bbc.co.uk/bitesize/articles/zc38kty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a hacer otra prueba con otro ejemplo, reinicio el notebook y uso otro modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "import torch\n",
        "\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "checkpoints = \"huggyllama/llama-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoints, torch_dtype=compute_dtype, device_map=\"auto\")\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Asignamos un valor fijo de semilla para la reproducibilidad del ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Le escribo una nueva pregunta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"On what date was the Declaration of Independence officially signed?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generamos la salida vanilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The Declaration of Independence was signed on July 4, 1776.\n",
            "What was the date of the signing of the Declaration of Independence?\n",
            "The Declaration of Independence was signed on July 4,\n"
          ]
        }
      ],
      "source": [
        "generate_kwargs={\n",
        "    \"do_sample\": False,\n",
        "    \"max_new_tokens\": 50,\n",
        "    \"top_p\": None,\n",
        "    \"temperature\": None\n",
        "}\n",
        "\n",
        "vanilla_output = model.generate(**inputs, **generate_kwargs)\n",
        "\n",
        "print(tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos, genera mal la salida, ya que aunque se celebra el 4 de julio, en realidad fue firmada el [2 de julio](https://education.nationalgeographic.org/resource/signing-declaration-independence/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a probar ahora con DolA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "July 4, 1776. This is the most well-known date in U.S. history. The day has been celebrated with parades, barbeques, fireworks and festivals for hundreds of years.\n"
          ]
        }
      ],
      "source": [
        "dola_high_output = model.generate(**inputs, **generate_kwargs, dola_layers='high', repetition_penalty=1.2)\n",
        "\n",
        "print(tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sigue sin generar una salida correcta, as\u00ed que vamos a indicarle que solo contraste la capa final con las capas 28 y 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "It was officially signed on 2 August 1776, when 56 members of the Second Continental Congress put their John Hancocks to the Declaration. The 2-page document had been written in 17\n"
          ]
        }
      ],
      "source": [
        "dola_high_output = model.generate(**inputs, **generate_kwargs, dola_layers=[28,30], repetition_penalty=1.2)\n",
        "\n",
        "print(tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora s\u00ed consigue generar la respuesta correcta"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_",
      "language": "python",
      "name": "python3"
    },
    "maximofn": {
      "date": "2024-08-01",
      "description_es": "\u00bfAlguna vez has hablado con un LLM y te ha respondido algo que suena como si hubiera estado bebiendo caf\u00e9 de m\u00e1quina durante toda la noche? \ud83d\ude02 \u00a1Eso es lo que llamamos una alucinaci\u00f3n en el mundo de los LLMs! Pero no te preocupes, porque no es que tu modelo de lenguaje est\u00e9 loco (aunque a veces puede parecerlo \ud83e\udd2a). La verdad es que los LLMs pueden ser un poco... creativos cuando se trata de generar texto. Pero gracias a DoLa, un m\u00e9todo que utiliza capas de contraste para mejorar la factibilidad de los LLMs, podemos evitar que nuestros modelos de lenguaje se conviertan en escritores de ciencia ficci\u00f3n \ud83d\ude02. En este post, te explicar\u00e9 c\u00f3mo funciona DoLa y te mostrar\u00e9 un ejemplo de c\u00f3digo para que puedas entender mejor c\u00f3mo hacer que tus LLMs sean m\u00e1s fiables y menos propensos a inventar historias. \u00a1Vamos a salvar a nuestros LLMs de la locura y hacer que sean m\u00e1s \u00fatiles! \ud83d\ude80",
      "description_en": "Have you ever talked to an LLM and they answered you something that sounds like they've been drinking machine coffee all night long \ud83d\ude02 That's what we call a hallucination in the LLM world! But don't worry, because it's not that your language model is crazy (although it can sometimes seem that way \ud83e\udd2a). The truth is that LLMs can be a bit... creative when it comes to generating text. But thanks to DoLa, a method that uses contrast layers to improve the feasibility of LLMs, we can keep our language models from turning into science fiction writers \ud83d\ude02. In this post, I'll explain how DoLa works and show you a code example so you can better understand how to make your LLMs more reliable and less prone to making up stories. Let's save our LLMs from insanity and make them more useful! \ud83d\ude80",
      "description_pt": "Voc\u00ea j\u00e1 conversou com um LLM e ele lhe respondeu algo que parece ter bebido caf\u00e9 de m\u00e1quina a noite toda? \ud83d\ude02 Isso \u00e9 o que chamamos de alucina\u00e7\u00e3o no mundo dos LLMs! Mas n\u00e3o se preocupe, pois n\u00e3o \u00e9 que seu modelo de linguagem esteja louco (embora \u00e0s vezes possa parecer isso \ud83e\udd2a). A verdade \u00e9 que os LLMs podem ser um pouco... criativos quando se trata de gerar texto. Mas gra\u00e7as ao DoLa, um m\u00e9todo que usa camadas de contraste para melhorar a viabilidade dos LLMs, podemos evitar que nossos modelos de linguagem se transformem em escritores de fic\u00e7\u00e3o cient\u00edfica \ud83d\ude02. Nesta publica\u00e7\u00e3o, explicarei como o DoLa funciona e mostrarei um exemplo de c\u00f3digo para que voc\u00ea possa entender melhor como tornar seus LLMs mais confi\u00e1veis e menos propensos a inventar hist\u00f3rias. Vamos salvar nossos LLMs da loucura e torn\u00e1-los mais \u00fateis! \ud83d\ude80",
      "end_url": "dola",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-thumbnail.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/DoLa-thumbnail.webp",
      "keywords_en": "dola, decoding by contrasting layers, factuality, large language models, transformers, hugging face, nlp, natural language processing, machine learning, artificial intelligence",
      "keywords_es": "dola, decodificaci\u00f3n por capas contrastantes, factibilidad, grandes modelos de lenguaje, transformers, hugging face, nlp, procesamiento de lenguaje natural, aprendizaje autom\u00e1tico, inteligencia artificial",
      "keywords_pt": "dola, decodifica\u00e7\u00e3o por camadas contrastantes, factibilidade, grandes modelos de linguagem, transformers, hugging face, nlp, processamento de linguagem natural, aprendizado de m\u00e1quina, intelig\u00eancia artificial",
      "title_en": "DoLa \u2013 Decoding by Contrasting Layers Improves Factuality in Large Language Models",
      "title_es": "DoLa \u2013 Decoding by Contrasting Layers Improves Factuality in Large Language Models",
      "title_pt": "DoLa \u2013 Decoding by Contrasting Layers Improves Factuality in Large Language Models"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}