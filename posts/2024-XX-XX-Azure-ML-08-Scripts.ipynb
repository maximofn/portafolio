{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML - Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loguearse a Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como las acciones que vamos a hacer por CLI o a través del SDK de Python necesitan una autentificación, primero vamos a loguearnos en Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login en Azure ML con el CLI de Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para logearnos en Azure hacemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se nos abrirá el navegador para logearnos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un cliente de Azure ML con el SDK de Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos dos variables con la ID de la suscripción y el grupo de recursos, como estos son datos personales, no los voy a poner aquí. Lo que voy a hacer es incluirlos en un archivo `.env` que no voy a subir a GitHub\n",
    "\n",
    "```bash\n",
    "AZURE_SUSCRIPION_ID=\"xxxxx-xxxx-xxxx-xxxx-xxxxx\"\n",
    "AZURE_ML_RESOURCE_GRPU_ID=\"xxxxx-xxxx-xxxx-xxxx-xxxxx\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora para leerlos primero necesitasos tener instalado `dotenv` que lo hacemos mediante `pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "AZURE_SUSCRIPION_ID = os.getenv(\"AZURE_SUSCRIPION_ID\")\n",
    "AZURE_ML_RESOURCE_GRPU_ID = os.getenv(\"AZURE_ML_RESOURCE_GRPU_ID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos estas variables creamos un cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "workspace_name = \"azure-ml-workspace-Python-SDK\"\n",
    "\n",
    "ml_client = MLClient(DefaultAzureCredential(), AZURE_SUSCRIPION_ID, AZURE_ML_RESOURCE_GRPU_ID, workspace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos validado el código de entrenamiento en un Jupyter Notebook podemos pasarlo a un script para ejecutarlo y poder hacer varios experimentos cambiando hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar un script desde la interfaz gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subir los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos vamos a `Notebooks` en la zona de las carpetas hay un botón con el símbolo `+`, si le damos podemos subir archivos. Vamos a subir la carpeta `en` que descargamos de HuggingFace, marcamos la casilla que dice si creemos en los autores y le damos a `Upload`. Ya tenemos los datos subidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear el script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a dar al botón con el símbolo `+`, le damos a `Create new file`, en `File type` seleccionamos `Python (*.py)` y le ponemos el nombre `text-classification.py`. Ahora copiamos el siguiente código\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "tokenizer = None\n",
    "\n",
    "def load_datasets(folder_path, train_file, validation_file, test_file):\n",
    "    dataset_train = load_dataset(\"json\", data_files=f\"{folder_path}/{train_file}\")\n",
    "    dataset_validation = load_dataset(\"json\", data_files=f\"{folder_path}/{validation_file}\")\n",
    "    dataset_test = load_dataset(\"json\", data_files=f\"{folder_path}/{test_file}\")\n",
    "    return dataset_train, dataset_validation, dataset_test\n",
    "\n",
    "def get_dataset_num_classes(dataset, label):\n",
    "    return len(dataset[label].unique('label'))\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_function(examples, max_length=768):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "def get_model(model_name, num_classes):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    return model\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, huggingface_dataset):\n",
    "        self.dataset = huggingface_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.dataset[idx]['label']\n",
    "        input_ids = torch.tensor(self.dataset[idx]['input_ids'])\n",
    "        attention_mask = torch.tensor(self.dataset[idx]['attention_mask'])\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def predicted_labels(logits):\n",
    "    percent = torch.softmax(logits, dim=1)\n",
    "    predictions = torch.argmax(percent, dim=1)\n",
    "    return predictions\n",
    "\n",
    "def compute_accuracy(logits, labels):\n",
    "    predictions = predicted_labels(logits)\n",
    "    correct = (predictions == labels).float()\n",
    "    return correct.mean()\n",
    "\n",
    "def main(checkpoints, percentage_train, percentage_val_test, BS, LR, EPOCHs):\n",
    "    global tokenizer\n",
    "\n",
    "    dataset_train, dataset_validation, dataset_test = load_datasets(\"en\", \"train.jsonl\", \"en_validation.jsonl\", \"en_test.jsonl\")\n",
    "    num_classes = get_dataset_num_classes(dataset_train, \"train\")\n",
    "\n",
    "    tokenizer = get_tokenizer(checkpoints)\n",
    "\n",
    "    dataset_train = dataset_train.map(tokenize_function, batched=True, remove_columns=['id', 'label_text'])\n",
    "    dataset_validation = dataset_validation.map(tokenize_function, batched=True, remove_columns=['id', 'label_text'])\n",
    "    dataset_test = dataset_test.map(tokenize_function, batched=True, remove_columns=['id', 'label_text'])\n",
    "\n",
    "    subset_train = dataset_train['train'].select(range(int(len(dataset_train['train']) * percentage_train)))\n",
    "    subset_validation = dataset_validation['train'].select(range(int(len(dataset_validation['train']) * percentage_val_test)))\n",
    "    subset_test = dataset_test['train'].select(range(int(len(dataset_test['train']) * percentage_val_test)))\n",
    "\n",
    "    model = get_model(checkpoints, num_classes)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.half().to(device)\n",
    "    print(f\"device: {device}\")\n",
    "\n",
    "    train_dataset = ReviewsDataset(subset_train)\n",
    "    validatation_dataset = ReviewsDataset(subset_validation)\n",
    "    test_dataset = ReviewsDataset(subset_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "    validation_loader = DataLoader(validatation_dataset, batch_size=BS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BS)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    accuracy = 0\n",
    "    for epoch in range(EPOCHs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        progresbar = tqdm(train_loader, total=len(train_loader), desc=f'Epoch {epoch + 1}')\n",
    "        for input_ids, at_mask, labels in progresbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            at_mask = at_mask.to(device)\n",
    "            label = labels.to(device)\n",
    "            output = model(input_ids=input_ids, attention_mask=at_mask, labels=label)\n",
    "            loss = output['loss']\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progresbar.set_postfix({'train_loss': loss.item()})\n",
    "        train_loss /= len(train_loader)\n",
    "        progresbar.set_postfix({'train_loss': train_loss})\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        progresbar = tqdm(validation_loader, total=len(validation_loader), desc=f'Epoch {epoch + 1}')\n",
    "        for input_ids, at_mask, labels in progresbar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            at_mask = at_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(input_ids=input_ids, attention_mask=at_mask, labels=labels)\n",
    "            loss = output['loss']\n",
    "            valid_loss += loss.item()\n",
    "            step_accuracy = compute_accuracy(output['logits'], labels)\n",
    "            accuracy += step_accuracy\n",
    "            progresbar.set_postfix({'valid_loss': loss.item(), 'accuracy': step_accuracy.item()})\n",
    "        valid_loss /= len(validation_loader)\n",
    "        accuracy /= len(validation_loader)\n",
    "        progresbar.set_postfix({'valid_loss': valid_loss, 'accuracy': accuracy})\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "\n",
    "    # Save the model\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--checkpoints\", type=str, default=\"openai-community/gpt2\")\n",
    "    parser.add_argument(\"--percentage_train\", type=float, default=0.0001)\n",
    "    parser.add_argument(\"--percentage_val_test\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    checkpoints = args.checkpoints\n",
    "    percentage_train = args.percentage_train\n",
    "    percentage_val_test = args.percentage_val_test\n",
    "    batch_size = args.batch_size\n",
    "    learning_rate = args.learning_rate\n",
    "    epochs = args.epochs\n",
    "\n",
    "    main(checkpoints, percentage_train, percentage_val_test, batch_size, learning_rate, epochs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecutar el script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a dar al botón con el símbolo `+`, le damos a `Create new file`, en `File type` seleccionamos `Notebook (*.ipynb)` y le ponemos el nombre `run_text-clasification.ipynb`. Ahora copiamos las siguientes celdas de código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "workspace_name = \"azure-ml-workspace-Python-SDK\"\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from azure.ai.ml import command\n",
    "\n",
    "execute_command = \"python text-clasification.py --learning_rate 2e-5\"\n",
    "environment = \"cuda_11_8_0_cudnn8_devel_ubuntu22_04_transformers_GUI@latest\"\n",
    "compute_instance = \"compute-instance-GUI\"\n",
    "display_name = \"text-clasificator\"\n",
    "experiment_name = \"train-classification-model\"\n",
    "\n",
    "# configure job\n",
    "job = command(\n",
    "    code=\"./\",\n",
    "    command=execute_command,\n",
    "    environment=environment,\n",
    "    compute=compute_instance,\n",
    "    display_name=display_name,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mira que en el environment hemos puesto `@latest` para que use la última versión del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra cosa importate es que los hiperparámetros como el batch size, el learning rate, etc. se pasan al script como argumentos, por lo que puedes hacer muchos experimentos modificando estos valores en los argumentos del script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos la compute instance `compute-instance-GUI` y ejecutamos todas las celdas, si todo ha ido bien, se habrá creado un nuevo experimento en Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos vamos a la sección `Jobs` en la parte izquierda de la pantalla, veremos el experimento que hemos creado, si le damos veremos la salida del script. Al igual que con los notebooks no he conseguido poder ejecutar el script en una GPU, por eso en el código se crea un subconjunto del dataset tan pequeño, por eso el batch size es tan pequeño y el número de épocas es solo una. Por lo que tardará bastante en terminar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar un script con el CLI de Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No he encotrado la manera de ejecutar el script mediante la CLI de Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar un script con el SDK de Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar un script primero tenemos que subir los datos y el script igual que lo hemos hecho en la interfaz gráfica. No he encotrado la manera de subirlos mediante el SDK de Python. Por lo que nos aseguramos de estar en el workspace en el que hemos hecho todo con el SDK de Python y subimos los datos y el script igual que lo hemos hecho antes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora para ejecutar ek script ejecutamos ek mismo código que ejecutamos en el notebook mediante la interfaz gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "execute_command = \"python text-clasification.py --learning_rate 2e-5\"\n",
    "environment = \"cuda_11_8_0_cudnn8_devel_ubuntu22_04_transformers_python_@latest\"\n",
    "compute_instance = \"compute-instance-Python\"\n",
    "display_name = \"text-clasificator\"\n",
    "experiment_name = \"train-classification-model\"\n",
    "\n",
    "# configure job\n",
    "job = command(\n",
    "    code=\"./\",\n",
    "    command=execute_command,\n",
    "    environment=environment,\n",
    "    compute=compute_instance,\n",
    "    display_name=display_name,\n",
    "    experiment_name=experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_job = ml_client.create_or_update(job)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
