{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML - Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loguearse a Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como las acciones que vamos a hacer por CLI o a través del SDK de Python necesitan una autentificación, primero vamos a loguearnos en Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login en Azure ML con el CLI de Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para logearnos en Azure hacemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se nos abrirá el navegador para logearnos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un cliente de Azure ML con el SDK de Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos dos variables con la ID de la suscripción y el grupo de recursos, como estos son datos personales, no los voy a poner aquí. Lo que voy a hacer es incluirlos en un archivo `.env` que no voy a subir a GitHub\n",
    "\n",
    "```bash\n",
    "AZURE_SUSCRIPION_ID=\"xxxxx-xxxx-xxxx-xxxx-xxxxx\"\n",
    "AZURE_ML_RESOURCE_GRPU_ID=\"xxxxx-xxxx-xxxx-xxxx-xxxxx\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora para leerlos primero necesitasos tener instalado `dotenv` que lo hacemos mediante `pip install python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "AZURE_SUSCRIPION_ID = os.getenv(\"AZURE_SUSCRIPION_ID\")\n",
    "AZURE_ML_RESOURCE_GRPU_ID = os.getenv(\"AZURE_ML_RESOURCE_GRPU_ID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos estas variables creamos un cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "workspace_name = \"azure-ml-workspace-Python-SDK\"\n",
    "\n",
    "ml_client = MLClient(DefaultAzureCredential(), AZURE_SUSCRIPION_ID, AZURE_ML_RESOURCE_GRPU_ID, workspace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos validado el código de entrenamiento en un Jupyter Notebook podemos pasarlo a un script para ejecutarlo y poder hacer varios experimentos cambiando hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar un script desde la interfaz gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subir los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos vamos a `Notebooks` en la zona de las carpetas hay un botón con el símbolo `+`, si le damos podemos subir archivos. Vamos a subir la carpeta `en` que descargamos de HuggingFace, marcamos la casilla que dice si creemos en los autores y le damos a `Upload`. Ya tenemos los datos subidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear el script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a dar al botón con el símbolo `+`, le damos a `Create new file`, en `File type` seleccionamos `Python (*.py)` y le ponemos el nombre `text-classification.py`. Ahora copiamos el siguiente código\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "def load_dataset():\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    "    )\n",
    "    return training_data, test_data\n",
    "\n",
    "def get_num_classes(train_dataset):\n",
    "    set_clases = set({})\n",
    "    len_training_data = len(train_dataset)\n",
    "    for i in range(len_training_data):\n",
    "        _, label = train_dataset[i]\n",
    "        set_clases.add(label)\n",
    "    num_classes = len(set_clases)\n",
    "    return num_classes\n",
    "\n",
    "def create_subset(tran_dataset, test_dataset, factor):\n",
    "    len_training_data = len(tran_dataset)\n",
    "    len_test_data = len(test_dataset)\n",
    "    subset_training_data = torch.utils.data.Subset(tran_dataset, range(0, len_training_data//factor))\n",
    "    subset_test_data = torch.utils.data.Subset(test_dataset, range(0, len_test_data//factor))\n",
    "    return subset_training_data, subset_test_data\n",
    "\n",
    "def get_dataloaders(train_dataset, test_dataset, batch_size):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        self.imageClassifier = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(self.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.imageClassifier(x)\n",
    "\n",
    "def train(dataloader, model, loss_fn, metrics_fn, optimizer, epoch, device):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        metric = metrics_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss = loss.item()\n",
    "            current = batch\n",
    "            step = batch // 1000 * (epoch + 1)\n",
    "            print(f\"train loss: {loss:.4f}, train accuracy: {metric:.4f} [{current}/{len(dataloader)}]\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate(dataloader, model, loss_fn, metrics_fn, epoch, device):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            eval_loss += loss_fn(pred, y).item()\n",
    "            eval_accuracy += metrics_fn(pred, y).item()\n",
    "\n",
    "    eval_loss /= num_batches\n",
    "    eval_accuracy /= num_batches\n",
    "    print(f\"eval loss: {eval_loss:.4f}, eval accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "def main(subset_factor, batch_size, epochs=3, learning_rate=1e-3):\n",
    "    train_dataset, test_dataset = load_dataset()\n",
    "    num_classes = get_num_classes(train_dataset)\n",
    "    train_subset, test_subset = create_subset(train_dataset, test_dataset, factor=subset_factor)\n",
    "\n",
    "    train_dataloader, test_dataloader = get_dataloaders(train_subset, test_subset, batch_size=batch_size)\n",
    "    sample_batch = next(iter(train_dataloader))\n",
    "    image_batch, _ = sample_batch\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = ImageClassifier(num_classes).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    metrics_fn = Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, metrics_fn, optimizer, epoch, device)\n",
    "        evaluate(test_dataloader, model, loss_fn, metrics_fn, epoch, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--subset_factor\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-3)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    subset_factor = args.subset_factor\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "\n",
    "    main(subset_factor, batch_size, epochs, learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecutar el script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a dar al botón con el símbolo `+`, le damos a `Create new file`, en `File type` seleccionamos `Notebook (*.ipynb)` y le ponemos el nombre `run_text-clasification.ipynb`. Ahora copiamos las siguientes celdas de código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from azure.ai.ml import command\n",
    "\n",
    "execute_command = \"python image-classification.py --batch_size 16 --epochs 3 --learning_rate 1e-3\"\n",
    "environment = \"cuda_11_8_0_cudnn8_devel_ubuntu22_04_transformers_GUI@latest\"\n",
    "compute_instance = \"compute-instance-GUI\"\n",
    "display_name = \"image-classificator\"\n",
    "experiment_name = \"image-classification\"\n",
    "\n",
    "# configure job\n",
    "job = command(\n",
    "    code=\"./\",\n",
    "    command=execute_command,\n",
    "    environment=environment,\n",
    "    compute=compute_instance,\n",
    "    display_name=display_name,\n",
    "    experiment_name=experiment_name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "returned_job = ml_client.create_or_update(job)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mira que en el environment hemos puesto `@latest` para que use la última versión del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra cosa importate es que los hiperparámetros como el batch size, el learning rate, etc. se pasan al script como argumentos, por lo que puedes hacer muchos experimentos modificando estos valores en los argumentos del script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos la compute instance `compute-instance-GUI` y ejecutamos todas las celdas, si todo ha ido bien, se habrá creado un nuevo experimento en Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizar el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos vamos a la sección `Jobs` en la parte izquierda de la pantalla, veremos el experimento que hemos creado, si le damos veremos la salida del script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar un script con el CLI de Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No he encotrado la manera de ejecutar el script mediante la CLI de Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar un script con el SDK de Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar un script primero tenemos que subir los datos y el script igual que lo hemos hecho en la interfaz gráfica. No he encotrado la manera de subirlos mediante el SDK de Python. Por lo que nos aseguramos de estar en el workspace en el que hemos hecho todo con el SDK de Python y subimos los datos y el script igual que lo hemos hecho antes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora para ejecutar ek script ejecutamos ek mismo código que ejecutamos en el notebook mediante la interfaz gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "execute_command = \"python image-classification.py --batch_size 16 --epochs 3 --learning_rate 1e-3\"\n",
    "environment = \"cuda_11_8_0_cudnn8_devel_ubuntu22_04_transformers_G@latest\"\n",
    "compute_instance = \"compute-instance-Python\"\n",
    "display_name = \"text-clasificator\"\n",
    "experiment_name = \"train-classification-model\"\n",
    "\n",
    "# configure job\n",
    "job = command(\n",
    "    code=\"./\",\n",
    "    command=execute_command,\n",
    "    environment=environment,\n",
    "    compute=compute_instance,\n",
    "    display_name=display_name,\n",
    "    experiment_name=experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_job = ml_client.create_or_update(job)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
