{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) es el paper de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    " \n",
    "import evaluate\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_PROCS = 32\n",
    "LR = 0.00005\n",
    "EPOCHS = 5\n",
    "MODEL = 'bert-base-uncased'\n",
    "OUT_DIR = 'arxiv_bert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximo.fernandez/miniconda3/envs/nlp/lib/python3.11/site-packages/datasets/load.py:1454: FutureWarning: The repository for ccdv/arxiv-classification contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ccdv/arxiv-classification\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569c48ec4ab84a8791fcffbcf1067d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81e1e5ed17040ddac3f1c0fb121895a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c60db301b2410391f3a4a4483d1016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a227d8e4db8146b8a8035da6b8c617af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/150M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8604e26cfeee4caa8d4798d47cc608de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/146M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec3e59508f6423a9b76cff29c8ea818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8497c7540196484ca94ac83f264de4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94a9ea29d2a4731b0b2f6bfd7879153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 28388\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"ccdv/arxiv-classification\", split='train', trust_remote_code=True)\n",
    "valid_dataset = load_dataset(\"ccdv/arxiv-classification\", split='validation', trust_remote_code=True)\n",
    "test_dataset = load_dataset(\"ccdv/arxiv-classification\", split='test', trust_remote_code=True)\n",
    "print(train_dataset)\n",
    "print(valid_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Constrained Submodular Maximization via a\\nNon-symmetric Technique\\n\\narXiv:1611.03253v1 [cs.DS] 10 Nov 2016\\n\\nNiv Buchbinder∗\\n\\nMoran Feldman†\\n\\nNovember 11, 2016\\n\\nAbstract\\nThe study of combinatorial optimization problems with a submodular objective has attracted\\nmuch attention in recent years. Such problems are important in both theory and practice because\\ntheir objective functions are very general. Obtaining further improvements for many submodular\\nmaximization problems boils down to finding better algorithms for optimizing a relaxation of\\nthem known as the multilinear extension.\\nIn this work we present an algorithm for optimizing the multilinear relaxation whose guarantee improves over the guarantee of the best previous algorithm (which was given by Ene\\nand Nguyen (2016)). Moreover, our algorithm is based on a new technique which is, arguably,\\nsimpler and more natural for the problem at hand. In a nutshell, previous algorithms for this\\nproblem rely on symmetry properties which are natural only in the absence of a constraint. Our\\ntechnique avoids the need to resort to such properties, and thus, seems to be a better fit for\\nconstrained problems.\\n\\n∗\\n\\nDepartment of Statistics and Operations Research, School of Mathematical Sciences, Tel Aviv university, Israel.\\nEmail: niv.buchbinder@gmail.com.\\n†\\nDepart. of Mathematics and Computer Science, The Open University of Israel. Email: moranfe@openu.ac.il.\\n\\n\\x0c1\\n\\nIntroduction\\n\\nThe study of combinatorial optimization problems with a submodular objective has attracted much\\nattention in recent years. Such problems are important in both theory and practice because their\\nobjective functions are very general—submodular functions generalize, for example, cuts functions\\nof graphs and directed graphs, the mutual information function, matroid weighted rank functions and log-determinants. More specifically, from a theoretical perspective, many well-known\\nproblems in combinatorial optimization are in fact submodular maximization problems, including:\\nMax-Cut [30, 33, 38, 40, 56], Max-DiCut [20, 30, 31], Generalized Assignment [10, 14, 22, 27],\\nMax-k-Coverage [19, 41], Max-Bisection [3, 28] and Facility Location [1, 16, 17]. From a practical\\nperspective, submodular maximization problems have found uses in social networks [32, 39], vision [5, 36], machine learning [43, 44, 45, 49, 50] and many other areas (the reader is referred, for\\nexample, to a comprehensive survey by Bach [4]).\\nThe techniques used by approximation algorithms for submodular maximization problems usually fall into one of two main approaches. The first approach is combinatorial in nature, and is\\nmostly based on local search techniques and greedy rules. This approach has been used as early\\nas the late 70’s for maximizing a monotone submodular function subject to a matroid constraint\\n(some of these works apply only to specific types of matroids) [15, 26, 34, 35, 37, 42, 53, 54]. Later\\nworks used this approach to handle also problems with non-monotone submodular objective functions and different constraints [6, 21, 25, 47, 48], yielding in some cases optimal algorithms [6, 55].\\nHowever, algorithms based on this approach tend to be highly tailored for the specific structure of\\nthe problem at hand, making extensions quite difficult.\\nThe second approach used by approximation algorithms for submodular maximization problems overcomes the above obstacle. This approach resembles a common paradigm for designing\\napproximation algorithms and involves two steps. In the first step a fractional solution is found for\\na relaxation of the problem, known as the multilinear relaxation. In the second step the fractional\\nsolution is rounded to obtain an integral one while incurring a bounded loss in the objective. This\\napproach has been used to obtain improved approximations for many problems [8, 11, 12, 24, 46].\\nVarious techniques have been developed for rounding the fractional solution. These techniques\\ntend to be quite flexible, and usually can extend to many related problem. In particular, the\\nContention Resolution Schemes framework of [12] yields a rounding procedure for every constraint\\nwhich can be presented as the intersection of a few basic constraints such as knapsack constraints,\\nmatroid constraints and matching constraints. Given this wealth of rounding procedures, obtaining\\nfurther improvements for many important submodular maximization problems (such as maximizing\\na submodular function subject to a matroid or knapsack constraint) boils down to obtaining improved algorithms for finding a good fractional solution, i.e., optimizing the multilinear relaxation.\\n\\n1.1\\n\\nMaximizing the Multilinear Relaxation\\n\\nAt this point we would like to present some terms more formally. A submodular function is a set\\nfunction f : 2N → R obeying f (A) + f (B) ≥ f (A ∪ B) + f (A ∩ B) for any sets A, B ⊆ N . A\\nsubmodular maximization problem is the problem of finding a set S ⊆ N maximizing f subject to\\nsome constraint. Formally, let I be the set of subsets of N obeying the constraint. Then, we are\\ninterested in the following problem.\\nmax f (A)\\ns.t. A ∈ I ⊆ 2N\\nA relaxation of the above problem replaces I with a polytope P ⊆ [0, 1]N containing the\\n1\\n\\n\\x0ccharacteristic vectors of all the sets of I. In addition, a relaxation must replace the function f with\\nan extension function F : [0, 1]N → R. Thus, a relaxation is a fractional problem of the following\\nformat.\\nmax F (x)\\ns.t. x ∈ P ⊆ [0, 1]N\\nDefining the “right” extension function, F , for the relaxation is a challenge, as, unlike the linear\\ncase, there is no single natural candidate. The objective that turned out to be useful, and is, thus,\\nused by multilinear relaxation is known as the multilinear extension (first introduced by [8]). The\\nvalue F (x) of this extension for any vector x ∈ [0, 1]N is defined as the expected value of f over\\na random subset R(x) ⊆ N containing every element u ∈ N independently with probability xu .\\nFormally, for every x ∈ [0, 1]N ,\\nY\\nX\\nY\\n(1 − xu ) .\\nF (x) = E[R(x)] =\\nf (S)\\nxu\\nS⊆N\\n\\nu∈S\\n\\nu∈S\\n/\\n\\nThe first algorithm for optimizing the multilinear relaxation was the Continuous Greedy algorithm designed by Calinescu et al. [8]. When the submodular function f is non-negative and\\nmonotone1 and P is solvable2 this algorithm finds a vector x ∈ P such that E[F (x)] ≥ (1 − 1/e −\\no(1)) · f (OP T ) (where OP T is the set maximizing f among all sets whose characteristic vectors\\nbelongs to P ). Interestingly, the guarantee of Continuous Greedy is optimal for monotone functions\\neven when P is a simple cardinality constraint [8, 53].\\nOptimizing the multilinear relaxation when f is not necessarily monotone proved to be a more\\nchallenging task. Initially, several algorithms for specific polytopes were suggested [29, 47, 57].\\nLater on, improved general algorithms were designed that work whenever f is non-negative and\\nP is down-closed3 and solvable [13, 24]. Designing algorithms that work in this general setting is\\nhighly important as many natural constraints fall into this framework. Moreover, the restriction of\\nthe algorithms to down-closed polytopes is unavoidable as Vondrák [57] proved that no algorithm\\ncan produce a vector x ∈ P obeying E[F (x)] ≥ c · f (OP T ) for any constant c > 0 when P is\\nsolvable but not down-closed.\\nUp until recently, the best algorithm for this general setting was called Measured Continuous\\nGreedy [24]. It guaranteed to produce a vector x ∈ P obeying E[F (x)] ≥ (1/e − o(1)) · f (OP T ) ≈\\n0.367 · f (OP T ) [24]. The natural feel of the guarantee of Measured Continuous Greedy and the fact\\nthat it was not improved for a few years made some people suspect that it is optimal. Recently,\\nan evidence against this conjecture was given by [7], which described an algorithm for the special\\ncase of a cardinality constraint with an improved approximation guarantee of 0.371. Even more\\nrecently, Ene and Nguyen [18] shuttered the conjecture completely. By extending the technique\\nused by [7], they showed that one can get an approximation guarantee 0.372 for every down-closed\\nand solvable polytope P . On the inapproximability side, Oveis Gharan and Vondrák [29] proved\\nthat no algorithm can achieve approximation better than 0.478 even when P is the matroid polytope\\nof a partition matroid. Closing the gap between the best algorithm and inapproximability result\\nfor this fundamental problem remains an important open problem.\\n1\\n\\nA\\nA\\n3\\nA\\nupper\\n2\\n\\nset function f : 2N → R is monotone if f (A) ≤ f (B) for every A ⊆ B ⊆ N .\\npolytope is solvable if one can optimize linear functions over it.\\npolytope P ⊆ [0, 1]N is down-closed if y ∈ P implies that every vector x ∈ [0, 1]N which is coordinate-wise\\nbounded by y must belong to P as well.\\n\\n2\\n\\n\\x0c1.2\\n\\nOur Contribution\\n\\nOur main contribution is an algorithm with an improved guarantee for maximizing the multilinear\\nrelaxation.\\nTheorem 1.1. There exists a polynomial time algorithm that given a non-negative submodular\\nfunction f : 2N → R≥0 and a solvable down-closed polytope P ⊆ [0, 1]N finds a vector x ∈ P\\nobeying F (x) ≥ 0.385 · f (OP T ), where OP T = arg max{f (S) : 1S ∈ P } and F is the multilinear\\nextension of f .\\nAdmittedly, the improvement in the guarantee obtained by our algorithm compared to the\\n0.372 guarantee of [18] is relatively small. However, the technique underlying our algorithm is\\nvery different, and, arguably, much cleaner, than the technique underlying the previous results\\nimproving over the natural guarantee of 1/e [7, 18]. Moreover, we believe our technique is more\\nnatural for the problem at hand, and thus, is likely to yield further improvements in the future. In\\nthe rest of this section we explain the intuition on which we base this belief.\\nThe results of [7, 18] are based on the observation that the guarantee of Measured Continuous\\nGreedy improves when the algorithm manages to increase all the coordinates of its solution at a\\nslow rate. Based on this observation, [7, 18] run an instance of Measured Continuous Greedy (or a\\ndiscretized version of it), and force it to raise the coordinates slowly. If this extra restriction does\\nnot affect the behavior of the algorithm significantly, then it produces a solution with an improved\\nguarantee. Otherwise, [7, 18] argue that the point in which the extra restriction affect the behavior\\nof Measured Continuous Greedy reveals a vector x ∈ P which contains a significant fraction of\\nOP T . Once x is available, one can use the technique of unconstrained submodular maximization,\\ndescribed by [6], that has higher approximation guarantee of 1/2 > 1/e, to extract from x a vector\\n0 ≤ y ≤ x of large value. The down-closeness of P guarantees that y belongs to P as well.\\nUnfortunately, the use of the unconstrained submodular maximization technique in the above\\napproach is very problematic for two reasons. First, this technique is based on ideas that are\\nvery different from the ideas used by the analysis of Measured Continuous Greedy. This makes\\nthe combination of the two quite involved. Second, on a more abstract level, the unconstrained\\nsubmodular maximization technique is based on a symmetry which exists in the absence of a\\nconstraint since f¯(S) = f (N \\\\ S) is non-negative and submodular whenever f has these properties.\\nHowever, this symmetry breaks when a constraint is introduced, and thus, the unconstrained\\nsubmodular maximization technique does not seem to be a good fit for a constrained problem.\\nOur algorithm replaces the symmetry based unconstrained submodular maximization technique\\nwith a local search algorithm. More specifically, it first executes the local search algorithm. If the\\noutput of the local search algorithm is good, then our algorithm simply returns it. Otherwise, we\\nobserve that the poor value of the output of the local search algorithm guarantees that it is also\\nfar from OP T in some sense. Our algorithm then uses this far from OP T solution to guide an\\ninstance of Measured Continuous Greedy, and help it avoid bad decisions.\\nAs it turns out, the analysis of Measured Continuous Greedy and the local search algorithm\\nuse similar ideas and notions. Thus, the two algorithms combine quite cleanly, as can be observed\\nfrom Section 3.\\n\\n3\\n\\n\\x0c2\\n\\nPreliminaries\\n\\nOur analysis uses another useful extension of submodular functions. Given a submodular function\\nf : 2N → R, its Lovász extension is a function fˆ: [0, 1]N → R defined by\\nfˆ(x) =\\n\\nZ\\n\\n1\\n\\nf (Tλ (x))dλ ,\\n\\n0\\n\\nwhere Tλ (x) = {u ∈ N : xu < λ}. The Lovász extension has many important applications (see,\\ne.g., [9, 52]), however, in this paper we only use it in the context of the following known result\\n(which is an immediate corollary of the work of [51]).\\nLemma 2.1. Given the multilinear extension F and the Lovász extension fˆ of a submodular function f : 2N → R, it holds that F (x) ≥ fˆ(x) for every vector x ∈ [0, 1]N .\\nWe now define some additional notation that we use. Given a set S ⊆ N and an element u ∈ N ,\\nwe denote by 1S and 1u the characteristic vectors of the sets S and {u}, respectively, and by S + u\\nand S − u the sets S ∪ {u} and S \\\\ {u}, respectively. Given two vectors x, y ∈ [0, 1]N , we denote\\nby x ∨ y, x ∧ y and x ◦ y the coordinate-wise maximum, minimum and multiplication, respectively,\\nof x and y.4 Finally, given a vector x ∈ [0, 1]N and an element u ∈ N , we denote by ∂u F (x) the\\nderivative of F with respect to u at the point x. The following observation gives a simple formula\\nfor ∂u F (x). This observation holds because F is a multilinear function.\\nObservation 2.2. Let F (x) be the multilinear extension of a submodular function f : 2N → R.\\nThen, for every u ∈ N and x ∈ [0, 1]N ,\\n(1 − xu ) · ∂u F (x) = F (x ∨ 1u ) − F (x) .\\nIn the rest of the paper we assume, without loss of generality, that 1u ∈ P for every element\\nu ∈ N and that n is larger than any given constant. The first assumption is justified by the\\nobservation that every element u violating this assumption can be safely removed from N since it\\ncannot belong to OP T . The second assumption is justified by the observation that it is possible to\\nfind a set S obeying 1S ∈ P and f (S) = f (OP T ) in constant time when n is a constant.\\nAnother issue that needs to be kept in mind is the representation of submodular functions. We\\nare interested in algorithms whose time complexity is polynomial in |N |. However, the representation of the submodular function f might be exponential in this size; thus, we cannot assume that\\nthe representation of f is given as part of the input for the algorithm. The standard way to bypass\\nthis difficulty is to assume that the algorithm has access to f through an oracle. We assume the\\nstandard value oracle that is used in most of the previous works on submodular maximization.\\nThis oracle returns, given any subset S ⊆ N , the value f (S).\\n\\n3\\n\\nMain Algorithm\\n\\nIn this section we present the algorithm used to prove Theorem 1.1. This algorithm uses two\\ncomponents. The first component is a close variant of a fractional local search algorithm suggested\\nby Chekuri et al. [13] which has the following properties.\\n4\\n\\nMore formally, for every element u ∈ N , (x ∨ y)u = max{xu , yu }, (x ∧ y)u = min{xu , yu } and (x ◦ y)u = xu · yu .\\n\\n4\\n\\n\\x0cLemma 3.1 (Follows from Chekuri et al. [13]). There exists a polynomial time algorithm which\\nreturns vector x ∈ P such that, with high probability, for every vector y ∈ P ,\\n1\\n1\\nF (x) ≥ F (x ∧ y) + F (x ∨ y) − o(1) · f (OP T ) .\\n2\\n2\\n\\n(1)\\n\\nProof. Let M = max{f (u), f (N − u) : u ∈ N }, and let a be an arbitrary constant larger than 3.\\nThen, Lemmata 3.7 and 3.8 of Chekuri et al. [13] imply that, with high probability, the fractional\\nlocal search algorithm they suggest terminates in polynomial time and outputs a vector x ∈ P\\nobeying, for every vector y ∈ P ,\\n2F (x) ≥ F (x ∧ y) + F (x ∨ y) −\\n\\n5M\\n.\\nna−2\\n\\nMoreover, the output vector x is in P whenever the fractional local search algorithm terminates.\\nOur assumption that 1u ∈ P for every element u ∈ N implies, by submodularity, that f (S) ≤\\nn · f (OP T ) for every set S ⊆ N . Since M is the maximum over values of f , we get also M ≤\\nn · f (OP T ). Using this observation, and plugging a = 4, we get that there exists an algorithm\\nwhich, with high probability, terminates after T (n) operations (for some polynomial function T (n))\\nT)\\nfor every vector y ∈ P .\\nand outputs a vector x ∈ P obeying 2F (x) ≥ F (x ∧ y) + F (x ∨ y) − 5·f (OP\\nn\\nMoreover, the output vector x belongs to P whenever the algorithm terminates.\\nTo complete the lemma, we consider a procedure that executes the above algorithm for T (n)\\noperations, and return its output if it terminates within this number of operations. If the algorithm\\nfails to terminate within this number of operations, which happens with a diminishing probability,\\nthen the procedure simply returns 1∅ (which always belongs to P since P is down-closed). One\\ncan observe that this procedure has all the properties guaranteed by the lemma.\\nThe second component of our algorithm is a new auxiliary algorithm which we present and\\nanalyze in Section 4. This auxiliary algorithm is the main technical contribution of this paper, and\\nits guarantee is given by the following theorem.\\nTheorem 3.2. There exists a polynomial time algorithm that given a vector z ∈ [0, 1]N and a value\\nts ∈ [0, 1] outputs a vector x ∈ P obeying\\nE[F (x)] ≥ ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · F (z ∧ 1OP T )\\n−ts\\n\\n− (2 − ts − 2e\\n\\n(2)\\n\\n) · F (z ∨ 1OP T )] .\\n\\nOur main algorithm executes the algorithms suggested by Lemma 3.1 followed by the algorithm\\nsuggested by Theorem 3.2. Notice that the second of these algorithms has two parameters in\\naddition to f and P : a parameter z which is set to be the output of the first algorithm, and a\\nparameter ts which is set to be a constant to be determined later. After the two above algorithms\\nterminate, our algorithm returns the output of the first algorithm with probability p, for a constant\\np to be determined later, and with the remaining probability it returns the output of the second\\nalgorithm.5 A formal description of our algorithm is given as Algorithm 1. Observe that Lemma 3.1\\nand Theorem 3.2 imply together that Algorithm 1 is a polynomial time algorithm which always\\noutputs a vector in P .\\nTo prove Theorem 1.1, it remains to analyze the quality of the solution produced by Algorithm 1.\\n5\\n\\nClearly it is always better to return the better of the two solution instead of randomizing between them. However,\\ndoing so will require the algorithm to either have an oracle access to F or estimate the values of the solutions using\\nsampling (the later can be done using standard techniques—see, e.g., [8]). For the sake of simplicity, we chose here\\nthe easier to analyze approach of randomizing between the two solutions.\\n\\n5\\n\\n\\x0cAlgorithm 1: Main Algorithm(f, P )\\n1\\n2\\n3\\n\\nExecute the algorithm suggested by Lemma 3.1, and let x1 ∈ P be its output.\\nExecute the algorithm suggested by Theorem 3.2 with z = x1 , and let x2 be its output.\\nreturn with probability p the solution x1 , and the solution x2 otherwise.\\n\\nLemma 3.3. When its parameters are set to ts = 0.372 and p = 0.23, Algorithm 1 produces a\\nsolution whose expected value is at least 0.385 · f (OP T ).\\nProof. Let E be the event that x1 , the output of the algorithm suggested by Lemma 3.1, satisfies\\nInequality (1). Since E is a high probability event, it is enough to prove that, conditioned on E,\\nAlgorithm 1 produces a solution whose expected value is at least c · f (OP T ) for some constant\\nc > 0.385. The rest of the proof of the lemma is devoted to proving the last claim. Throughout it,\\neverything is implicitly conditioned on E.\\nAs we are conditioning on E, we can plug y = 1OP T and, respectively, y = x1 ∧ 1OP T into\\nInequality (1) to get\\n1\\n1\\nF (x1 ) ≥ F (x1 ∧ 1OP T ) + F (x1 ∨ 1OP T ) − o(1) · f (OP T )\\n(3)\\n2\\n2\\nand\\nF (x1 ) ≥ F (x1 ∧ 1OP T ) − o(1) · f (OP T ) ,\\n(4)\\nwhere the last inequality follows by noticing that x1 ∨ (x1 ∧ 1OP T ) = x1 . Next, let E[F (x2 ) | x1 ]\\ndenote the expected value of F (x2 ) conditioned on the given value of x1 . Inequality (2) guarantees\\nthat\\nE[F (x2 ) | x1 ] ≥ ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · F (x1 ∧ 1OP T )\\n−ts\\n\\n− (2 − ts − 2e\\n\\n(5)\\n\\n) · F (x1 ∨ 1OP T )] .\\n\\nRecall that Algorithm 1 returns x1 with probability p, and x2 otherwise. Hence, the expected\\nvalue of its output is\\nE[p · F (x1 ) + (1 − p) · E[F (x2 ) | x1 ]] ,\\n(6)\\nwhere the expectation is over x1 .\\nOptimizing the constants. We would like to derive from Inequalities (3), (4) and (5) the best\\nlower bound we can get on (6). To this end, let p1 and p2 be two non-negative numbers such that\\np1 + p2 = p, and let p3 = 1 − p. Using the above inequalities and this notation, (6) can now be\\nlower bounded by\\n\\x15\\n\\x14\\n1\\n1\\np1 · E[F (x1 ∧ 1OP T )] + E[F (x1 ∨ 1OP T )] − o(1) · f (OP T )\\n2\\n2\\n+ p2 · [E[F (x1 ∧ 1OP T )] − o(1) · f (OP T )]\\n+ p3 · ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · E[F (x1 ∧ 1OP T )]\\n− (2 − ts − 2e−ts ) · E[F (x1 ∨ 1OP T )]] ,\\nwhich can be rewritten as\\n\\x10p\\n\\n\\x11\\n+ p2 − p3 · ets −1 (1 − e−ts ) · E[F (x1 ∧ 1OP T )]\\n\\x11\\n\\x10 p2\\n1\\n− p3 · ets −1 (2 − ts − 2e−ts ) · E[F (x1 ∨ 1OP T )]\\n+\\n2\\n+ p3 · ets −1 (2 − ts − e−ts ) · f (OP T ) − o(1) · f (OP T ) .\\n1\\n\\n6\\n\\n\\x0cTo get the most out of this lower bound we need to maximize the coefficient of f (OP T ) while\\nkeeping the coefficients of E[F (x1 ∧ 1OP T )] and E[F (x1 ∨ 1OP T )] non-negative (so that they can\\nbe ignored due to non-negativity of f ). This objective is formalized by the following non-convex\\nprogram.\\nmax p3 · ets −1 (2 − ts − e−ts )\\ns.t. p1 /2 + p2 − p3 · ets −1 (1 − e−ts )\\np1 /2 − p3 · ets −1 (2 − ts − 2e−ts )\\np1 + p2 + p3\\np 1 , p 2 , p 3 , ts\\n\\n≥0\\n≥0\\n=1\\n≥0\\n\\nSolving the program, we get that the best solution is approximately p1 = 0.205, p2 = 0.025,\\np3 = 0.770 and ts = 0.372, and the objective function value corresponding to this solution is at\\nleast 0.3856. Hence, we have managed to lower bound (6) (and thus, also the expected value of the\\noutput of Algorithm 1) by 0.3856 · f (OP T ) for p = 0.23 and ts = 0.372, which completes the proof\\nof the lemma.\\n\\n4\\n\\nAided Measured Continuous Greedy\\n\\nIn this section we present the algorithm used to prove Theorem 3.2. Proving the above theorem\\ndirectly is made more involved by the fact that the vector z might be fractional. Instead, we prove\\nthe following simplified version of Theorem 3.2 for integral values, and show that the simplified\\nversion implies the original one.\\nTheorem 4.1. There exists a polynomial time algorithm that given a set Z ⊆ N and a value\\nts ∈ [0, 1] outputs a vector x ∈ P obeying\\nE[F (x)] ≥ ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] .\\nNext is the promised proof that Theorem 4.1 implies Theorem 3.2.\\nProof of Theorem 3.2 given Theorem 4.1. Consider an algorithm ALG that given the z and ts arguments specified by Theorem 3.2 executes the algorithm guaranteed by Theorem 4.1 with the same\\nvalue ts and with a random set Z distributed like R(z). The output of ALG is then the output\\nproduced by the algorithm guaranteed by Theorem 4.1. Let us denote this output by x.\\nTheorem 4.1 guarantees that, for every given Z,\\nE[F (x) | Z] ≥ ets −1 · [(2 − ts − e−ts − o(1)) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] .\\nTo complete the proof we take the expectation over Z over the two sides of the last inequality and\\nobserve that\\nE[f (Z ∩ OP T )] = E[f (R(z) ∩ OP T )] = E[f (R(z ∧ 1OP T ))] = F (z ∧ 1OP T )\\nand\\nE[f (Z ∪ OP T )] = E[f (R(z) ∪ OP T )] = E[f (R(z ∨ 1OP T ))] = F (z ∨ 1OP T ) .\\n\\n7\\n\\n\\x0cIn the rest of this section we give a non-formal proof of Theorem 4.1. This proof explains the\\nmain ideas necessary for proving the theorem, but uses some non-formal simplifications such as\\nallowing a direct oracle access to the multilinear extension F and giving the algorithm in the form\\nof a continuous time algorithm (which cannot be implemented on a discrete computer). There\\nare known techniques for getting rid of these simplifications (see, e.g., [8]), and a formal proof of\\nTheorem 4.1 based on these techniques is given in Appendix A.\\nThe algorithm we use for the non-formal proof of Theorem 4.1 is given as Algorithm 2. This\\nalgorithm starts with the empty solution y(0) = 1∅ at time 0, and grows this solution over time\\nuntil it reaches the final solution y(1) at time 1. The way the solution grows varies over time.\\nDuring the time range [ts , 1) the solution grows like in the Measured Continuous Greedy algorithm\\nof [24]. On the other hand, during the earlier time range of [0, ts ) the algorithm pretends that the\\nelements of Z do not exist (by giving them negative marginal profits), and grows the solution in\\nthe way Measured Continuous Greedy would have grown it if it was given the ground set N \\\\ Z.\\nThe value ts is the time in which the algorithm switches between the two ways it uses to grow its\\nsolution, thus, the s in the notation ts stands for “switch”.\\nAlgorithm 2: Aided Measured Continuous Greedy (non-formal)(f, P, Z, ts )\\n1\\n2\\n3\\n4\\n5\\n6\\n\\nLet y(0) ← 1∅ .\\nforeach t ∈ [0, 1) do\\nFor each u ∈ N let wu (t) ← F (y(t) ∨ 1u ) − F (y(t)).\\nP\\nP\\n\\x1a\\narg maxx∈P { u∈N \\\\Z wu (t) · xu (t) − u∈Z xu (t)} if t ∈ [0, ts ) ,\\n\\x08P\\nLet x(t) ←\\narg maxx∈P\\nif t ∈ [ts , 1) .\\nu∈N wu (t) · xu (t)\\nIncrease y(t) at a rate of\\n\\ndy(t)\\ndt\\n\\n= (1N − y(t)) ◦ x(t).\\n\\nreturn y(1).\\nWe first note that algorithm outputs a vector in P .\\n\\nObservation 4.2. y(1) ∈ P .\\nProof. Observe that x(t) ∈ P at eachR time t, which implies that (1N − y(t)) · x(t) is also in P since\\n1\\nP is down-closed. Therefore, y(1) = 0 (1N − y(t)) · x(t)dt is a convex combination of vectors in P ,\\nand thus, belongs to P .\\nThe following lemma lower bounds the increase in F (y(t)) as a function of t.\\nLemma 4.3. For every t ∈ [0, 1),\\n(\\nF (y(t) ∨ 1OP T \\\\Z ) − F (y(t))\\ndF (y(t))\\n≥\\ndt\\nF (y(t) ∨ 1OP T ) − F (y(t))\\n\\nif t ∈ [0, ts ) ,\\nif t ∈ [ts , 1) .\\n\\nProof. By the chain rule,\\n!\\n!\\nX\\nX dyu (t) ∂F (y)\\n∂F (y)\\ndF (y(t))\\n=\\n(1 − yu (t)) · xu (t) ·\\n=\\n·\\ndt\\ndt\\n∂yu y=y(t)\\n∂yu y=y(t)\\nu∈N\\nu∈N\\nX\\nX\\n=\\n(xu (t) · [F (y(t) ∨ 1u ) − F (y(t))]) =\\nxu (t) · wu (t) = x(t) · w(t) .\\nu∈N\\n\\n(7)\\n\\nu∈N\\n\\nConsider first the case\\nthis time period Algorithm 2 chooses x(t) as the\\nP t ∈ [0, ts ). During P\\nvector in P maximizing u∈N \\\\Z wu (t) · xu (t) − u∈Z xu (t). Since P is down-closed x(t) = 1OP T \\\\Z\\n8\\n\\n\\x0cis in P and has value 1OP T \\\\Z · w(t) and thus, we have x(t) · w(t) ≥ 1OP T \\\\Z · w(t). Plugging this\\nobservation into Equality (7) yields\\ndF (y(t))\\n= x(t) · w(t) ≥ 1OP T \\\\Z · w(t) =\\ndt\\n≥ F (y(t) ∨ 1OP T \\\\Z ) − F (y(t)) ,\\n\\nX\\n\\n[F (y(t) ∨ 1u ) − F (y(t))]\\n\\nu∈OP T \\\\Z\\n\\nwhere the last inequality holds by the submodularity of f .\\nSimilarity, when t ∈ [ts , 1) Algorithm 2 chooses x(t) as the vector in P maximizing x(t) · w(t).\\nSince 1OP T ∈ P , we get this time x(t) · w(t) ≥ 1OP T · w(t). Plugging this observation into\\nEquality (7) yields\\nX\\ndF (y(t))\\n= x(t) · w(t) ≥ 1OP T · w(t) =\\n[F (y(t) ∨ 1u ) − F (y(t))]\\ndt\\nu∈OP T\\n\\n≥ F (y(t) ∨ 1OP T ) − F (y(t)) ,\\n\\nwhere the last inequality holds again by the submodularity of f .\\nLemma 4.4. For every time t ∈ [0, 1) and set A ⊆ N it holds that\\n\\x11\\n\\x10\\nF (y(t) ∨ 1A ) ≥ e− max{0,t−ts } − e−t max {0, f (A) − f (A ∪ Z)} + e−t · f (A) .\\n\\nProof. First, we note that for every time t ∈ [0, 1] and element u ∈ N ,\\n(\\n1 − e−t\\nif u 6∈ Z ,\\nyu (t) ≤\\n−\\nmax{0,t−t\\ns}\\n1−e\\nif u ∈ Z .\\n\\n(8)\\n\\nThis follows for the following reason. Since x(t) is always in P ⊆ [0, 1]N , yu (t) obeys the\\ndifferential inequality\\ndy(t)\\n= (1 − yu (t)) · x(t) ≤ (1 − yu (t)) .\\ndt\\nUsing the initial condition yu (0) = 0, the solution for this differential inequality is yu (t) ≤ 1 − e−t .\\nTo get the tighter bound for u ∈ Z, we note that at every time t ∈ [0, ts ) Algorithm 2 chooses as\\nx(t) a vector maximizing a linear function in P which assigns a negative weight to elements of Z.\\nSince P is down-closed this maximum must have xu (t) = 0 for every element u ∈ Z. This means\\nthat yu (t) = 0 whenever u ∈ Z and t ∈ [0, ts ]. Moreover, plugging the improved initial condition\\nyu (ts ) = 0 into the above differential inequality yields the promised tighter bound also for the range\\n(ts , 1].\\nNext, let fˆ be the Lovász extension of f . Then, by Lemma 2.1,\\nZ 1\\nˆ\\nf (Tλ (y(t) ∨ 1A ))dλ\\nF (y(t) ∨ 1A ) ≥ f (y(t) ∨ 1A ) =\\n0\\n\\n≥\\n\\nZ\\n\\n1−e−t\\n\\n1−e− max{0,t−ts }\\nZ 1−e−t\\n\\nf (Tλ (y(t) ∨ 1A ))dλ +\\n\\nZ\\n\\n1\\n\\n1−e−t\\n\\nf (Tλ (y(t) ∨ 1A ))dλ\\n\\nf (Tλ (y(t) ∨ 1A ))dλ + e−t · f (A)\\n\\x11\\n\\x10\\n≥ e− max{0,t−ts } − e−t max {0, f (A) − f (A ∪ Z)} + e−t · f (A) .\\n\\n=\\n\\n(9)\\n(10)\\n\\n1−e− max{0,t−ts }\\n\\n9\\n\\n(11)\\n\\n\\x0cInequality (9) follows by the non-negativity of f . Equality (10) follows since, for λ ∈ [1 − e−t , 1),\\nInequality (8) guarantees that yu (t) ≤ λ for every u ∈ N , and thus, Tλ (y(t) ∨ 1A ) = A. Finally\\nInequality (11) follows since, for λ ∈ [1 − e− max{0,t−ts } , 1 − e−t ), Inequality (8) guarantees that\\nyu (t) ≤ λ for every u ∈ Z, and thus, Tλ (y(t) ∨ 1A ) = B(λ) ∪ A for some B(λ) ⊆ N \\\\ Z. By the\\nnon-negativity of f , f (B(λ) ∪ A) ≥ 0. Also, by the submodularity and non-negativity of f , for\\nevery such set B(λ)\\nf (B(λ) ∪ A) ≥ f (A) + f (B(λ) ∪ Z ∪ A) − f (Z ∪ A) ≥ f (A) − f (Z ∪ A) .\\nPlugging the results of Lemma 4.4 into the lower bound given by Lemma 4.3 on the improvement\\nin F (y(t)) as a function of t yields immediately the useful lower bound given by the next corollary.6\\nCorollary 4.5. For every t ∈ [0, 1),\\n(\\nf (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − F (y(t))\\ndF (y(t))\\n≥\\ndt\\nets −t · f (OP T ) − (ets −t − e−t ) · f (Z ∪ OP T ) − F (y(t))\\n\\nif t ∈ [0, ts ) ,\\nif t ∈ [ts , 1) .\\n\\nUsing the last corollary we can complete the proof of Theorem 4.1.\\nProof of Theorem 4.1. We have already seen that y(1)—the output of Algorithm 2—belongs to P .\\nIt remains to show that\\nF (y(1)) ≥ ets −1 · [(2 − ts − e−ts ) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] .\\nCorollary 4.5 describes a differential inequality for F (y(t)). Given the boundary condition\\nF (y(0)) ≥ 0, the solution for this differential inequality within the range t ∈ [0, ts ] is\\nF (y(t)) ≥ (1 − e−t ) · f (OP T \\\\ Z) − (1 − e−t − te−t ) · f (Z ∪ OP T ) .\\nPlugging t = ts into the last inequality, we get\\nF (y(ts )) ≥ (1 − e−ts ) · f (OP T \\\\ Z) − (1 − e−ts − ts e−ts ) · f (Z ∪ OP T ) .\\nLet v = (1 − e−ts ) · f (OP T \\\\ Z) − (1 − e−ts − ts e−ts ) · f (Z ∪ OP T ) be the right hand side of the\\nlast inequality. Next, we solve again the differential inequality given by Corollary 4.5 for the range\\nt ∈ [ts , 1] with the boundary condition F (y(ts )) ≥ v. The resulting solution is\\n\\x02\\n\\x01\\n\\x03\\nF (y(t)) ≥ e−t (t − ts ) ets · f (OP T ) − (ets − 1) · f (Z ∪ OP T ) + vets\\nPlugging t = 1 and the value of v we get\\n\\x02\\n\\x01\\n\\x03\\nF (y(1)) ≥ e−1 (1 − ts ) ets · f (OP T ) − (ets − 1) · f (Z ∪ OP T ) + vets\\n\\x01\\n1 − ts ts\\ne · f (OP T ) − (ets − 1) · f (Z ∪ OP T )\\n(12)\\n≥\\ne\\n+ ets −1 · {(1 − e−ts ) · [f (OP T ) − f (OP T ∩ Z)] − (1 − e−ts − ts e−ts ) · f (Z ∪ OP T )}\\n= ets −1 · [(2 − ts − e−ts ) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] ,\\nwhere Inequality (12) follows since, by the submodularity and non-negativity of f ,\\nf (OP T \\\\ Z) ≥ f (OP T ) − f (OP T ∩ Z) + f (∅) ≥ f (OP T ) − f (OP T ∩ Z) .\\n6\\n\\nNote that Corollary 4.5 follows from a weaker version of Lemma 4.4 which only guarantees F (y(t) ∨ 1A ) ≥\\n(e\\n− e−t ) · [f (A) − f (A ∪ Z)] + e−t · f (A). We proved the stronger version of the lemma above because\\nit is useful in the formal proof of Theorem 4.1 given in Appendix A.\\n− max{0,t−ts }\\n\\n10\\n\\n\\x0cReferences\\n[1] A. A. Ageev and M. I. Sviridenko. An 0.828 approximation algorithm for the uncapacitated\\nfacility location problem. Discrete Appl. Math., 93:149–156, July 1999.\\n[2] Noga Alon and Joel H. Spencer. The Probabilistic Method. Wiley Interscience, second edition,\\n2000.\\n[3] Per Austrin, Siavosh Benabbas, and Konstantinos Georgiou. Better balance by being biased:\\nA 0.8776-approximation for max bisection. In SODA, pages 277–294, 2013.\\n[4] Francis Bach. Learning with submodular functions: A convex optimization perspective. Foundations and Trends in Machine Learning, 6(2-3):145–373, 2013.\\n[5] Y. Y. Boykov and M. P. Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images. In ICCV, volume 1, pages 105–112, 2001.\\n[6] Niv Buchbinder, Moran Feldman, Joseph (Seffi) Naor, and Roy Schwartz. A tight linear time\\n(1/2)-approximation for unconstrained submodular maximization. In FOCS, pages 649–658,\\n2012.\\n[7] Niv Buchbinder, Moran Feldman, Joseph (Seffi) Naor, and Roy Schwartz. Submodular maximization with cardinality constraints. In SODA, pages 1433–1452, 2014.\\n[8] Gruia Călinescu, Chandra Chekuri, Martin Pál, and Jan Vondrák. Maximizing a monotone\\nsubmodular function subject to a matroid constraint. SIAM J. Comput., 40(6):1740–1766,\\n2011.\\n[9] Chandra Chekuri and Alina Ene. Approximation algorithms for submodular multiway partition. In FOCS, pages 807–816, 2011.\\n[10] Chandra Chekuri and Sanjeev Khanna. A polynomial time approximation scheme for the\\nmultiple knapsack problem. SIAM J. Comput., 35(3):713–728, September 2005.\\n[11] Chandra Chekuri, Jan Vondrák, and Rico Zenklusen. Dependent randomized rounding via\\nexchange properties of combinatorial structures. In FOCS, pages 575–584, 2010.\\n[12] Chandra Chekuri, Jan Vondrák, and Rico Zenklusen. Submodular function maximization via\\nthe multilinear relaxation and contention resolution schemes. In STOC, pages 783–792, 2011.\\n[13] Chandra Chekuri, Jan Vondrák, and Rico Zenklusen. Submodular function maximization via\\nthe multilinear relaxation and contention resolution schemes. SIAM J. Comput., 43(6):1831–\\n1879, 2014.\\n[14] Reuven Cohen, Liran Katzir, and Danny Raz. An efficient approximation for the generalized\\nassignment problem. Information Processing Letters, 100(4):162–166, 2006.\\n[15] M. Conforti and G. Cornuèjols. Submodular set functions, matroids and the greedy algorithm.\\ntight worstcase bounds and some generalizations of the radoedmonds theorem. Disc. Appl.\\nMath., 7(3):251–274, 1984.\\n[16] G. Cornuejols, M. L. Fisher, and G. L. Nemhauser. Location of bank accounts to optimize float:\\nan analytic study of exact and approximate algorithms. Management Sciences, 23:789–810,\\n1977.\\n11\\n\\n\\x0c[17] G. Cornuejols, M. L. Fisher, and G. L. Nemhauser. On the uncapacitated location problem.\\nAnnals of Discrete Mathematics, 1:163–177, 1977.\\n[18] Alina Ene and Huy L. Nguyen. Constrained submodular maximization: Beyond 1/e. In FOCS,\\n2016.\\n[19] Uriel Feige. A threshold of ln n for approximating set cover. J. ACM, 45(4):634–652, 1998.\\n[20] Uriel Feige and Michel X. Goemans. Aproximating the value of two prover proof systems, with\\napplications to max 2sat and max dicut. In ISTCS, pages 182–189, 1995.\\n[21] Uriel Feige, Vahab S. Mirrokni, and Jan Vondrák. Maximizing non-monotone submodular\\nfunctions. SIAM Journal on Computing, 40(4):1133–1153, 2011.\\n[22] Uriel Feige and Jan Vondrák. Approximation algorithms for allocation problems: Improving\\nthe factor of 1 − 1/e. In FOCS, pages 667–676, 2006.\\n[23] Moran Feldman. Maximization Problems with Submodular Objective Functions. PhD thesis,\\nTechnion – Israel Institute of Technology, June 2013.\\n[24] Moran Feldman, Joseph Naor, and Roy Schwartz. A unified continuous greedy algorithm for\\nsubmodular maximization. In FOCS, pages 570–579, 2011.\\n[25] Moran Feldman, Joseph (Seffi) Naor, Roy Schwartz, and Justin Ward. Improved approximations for k-exchange systems. In ESA, pages 784–798, 2011.\\n[26] M. L. Fisher, G. L. Nemhauser, and L. A. Wolsey. An analysis of approximations for maximizing submodular set functions – ii. In Polyhedral Combinatorics, volume 8 of Mathematical\\nProgramming Studies, pages 73–87. Springer Berlin Heidelberg, 1978.\\n[27] Lisa Fleischer, Michel X. Goemans, Vahab S. Mirrokni, and Maxim Sviridenko. Tight approximation algorithms for maximum general assignment problems. In SODA, pages 611–620,\\n2006.\\n[28] Alan M. Frieze and Mark Jerrum. Improved approximation algorithms for max k-cut and max\\nbisection. In IPCO, pages 1–13, 1995.\\n[29] Shayan Oveis Gharan and Jan Vondrák. Submodular maximization by simulated annealing.\\nIn SODA, pages 1098–1117, 2011.\\n[30] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM,\\n42(6):1115–1145, 1995.\\n[31] Eran Halperin and Uri Zwick. Combinatorial approximation algorithms for the maximum\\ndirected cut problem. In SODA, pages 1–7, 2001.\\n[32] Jason Hartline, Vahab Mirrokni, and Mukund Sundararajan. Optimal marketing strategies\\nover social networks. In WWW, pages 189–198, 2008.\\n[33] Johan Hȧstad. Some optimal inapproximability results. J. ACM, 48:798–859, July 2001.\\n[34] D. Hausmann and B. Korte. K-greedy algorithms for independence systems. Oper. Res. Ser.\\nA-B, 22(1):219–228, 1978.\\n12\\n\\n\\x0c[35] D. Hausmann, B. Korte, and T. Jenkyns. Worst case analysis of greedy type algorithms for\\nindependence systems. Math. Prog. Study, 12:120–131, 1980.\\n[36] S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: Coupling edges in graph\\ncuts. 2012 IEEE Conference on Computer Vision and Pattern Recognition, 0:1897–1904, 2011.\\n[37] T. Jenkyns. The efficacy of the greedy algorithm. Cong. Num., 17:341–350, 1976.\\n[38] Richard M. Karp. Reducibility among combinatorial problems. In R. E. Miller and J. W.\\nThatcher, editors, Complexity of Computer Computations, pages 85–103. Plenum Press, 1972.\\n[39] David Kempe, Jon Kleinberg, and Éva Tardos. Maximizing the spread of influence through a\\nsocial network. In SIGKDD, pages 137–146, 2003.\\n[40] Subhash Khot, Guy Kindler, Elchanan Mossel, and Ryan O’Donnell. Optimal inapproximability results for max-cut and other 2-variable csps? SIAM J. Comput., 37:319–357, April\\n2007.\\n[41] S. Khuller, A. Moss, and J. Naor. The budgeted maximum coverage problem. Information\\nProcessing Letters, 70(1):39–45, 1999.\\n[42] B. Korte and D. Hausmann. An analysis of the greedy heuristic for independence systems.\\nAnnals of Discrete Math., 2:65–74, 1978.\\n[43] Andreas Krause, AjitSingh, and Carlos Guestrin. Near-optimal sensor placements in gaussian\\nprocesses: Theory, efficient algorithms and empirical studies. J. Mach. Learn. Res., 9:235–284,\\nJanuary 2008.\\n[44] Andreas Krause and Carlos Guestrin. Near-optimal nonmyopic value of information in graphical models. In UAI, page 5, 2005.\\n[45] Andreas Krause, Jure Leskovec, Carlos Guestrin, Jeanne VanBriesen, and Christos Faloutsos.\\nEfficient sensor placement optimization for securing large water distribution networks. Journal\\nof Water Resources Planning and Management, 134(6):516–526, November 2008.\\n[46] Ariel Kulik, Hadas Shachnai, and Tami Tamir. Approximations for monotone and nonmonotone submodular maximization with knapsack constraints. Math. Oper. Res., 38(4):729–739,\\n2013.\\n[47] Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and Maxim Sviridenko. Maximizing nonmonotone submodular functions under matroid or knapsack constraints. SIAM Journal on\\nDiscrete Mathematics, 23(4):2053–2078, 2010.\\n[48] Jon Lee, Maxim Sviridenko, and Jan Vondrák. Submodular maximization over multiple matroids via generalized exchange properties. In APPROX, pages 244–257, 2009.\\n[49] Hui Lin and Jeff Bilmes. Multi-document summarization via budgeted maximization of submodular functions. In North American chapter of the Association for Computational Linguistics/Human Language Technology Conference (NAACL/HLT-2010), Los Angeles, CA, June\\n2010.\\n[50] Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In\\nHLT, pages 510–520, 2011.\\n13\\n\\n\\x0c[51] László Lovász. Submodular functions and convexity. In A. Bachem, M. Grötschel, and B. Korte, editors, Mathematical Programming: the State of the Art, pages 235–257. Springer, 1983.\\n[52] L. Lovász M. Grötschel and A. Schrijver. The ellipsoid method and its consequences in combinatorial optimization. Combinatoria, 1(2):169–197, 1981.\\n[53] G. L. Nemhauser and L. A. Wolsey. Best algorithms for approximating the maximum of a\\nsubmodular set function. Mathematics of Operations Research, 3(3):177–188, 1978.\\n[54] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14:265–294, 1978.\\n[55] Maxim Sviridenko. A note on maximizing a submodular set function subject to knapsack\\nconstraint. Operations Research Letters, 32:41–43, 2004.\\n[56] Luca Trevisan, Gregory B. Sorkin, Madhu Sudan, and David P. Williamson. Gadgets, approximation, and linear programming. SIAM J. Comput., 29:2074–2097, April 2000.\\n[57] Jan Vondrák. Symmetry and approximability of submodular maximization problems. SIAM\\nJ. Comput., 42(1):265–304, 2013.\\n\\nA\\n\\nA Formal Proof of Theorem 4.1\\n\\nIn this section we give a formal proof of Theorem 4.1. This proof is based on the same ideas used\\nin the non-formal proof of this theorem in Section 4, but employs also additional known techniques\\nin order to get rid of the issues that make the proof from Section 4 non-formal.\\nThe algorithm we use to prove Theorem 4.1 is given as Algorithm 3. This algorithm is a discrete\\nvariant of Algorithm 2. While reading the algorithm, it is important to observe that the choice of\\nthe values δ̄1 and δ̄2 guarantees that the variable t takes each one of the values ts and 1 at some\\npoint, and thus, the vectors y(ts ) and y(1) are well defined.\\nAlgorithm 3: Aided Measured Continuous Greedy(f, P, Z, ts )\\n1\\n2\\n\\n3\\n4\\n5\\n\\n6\\n7\\n8\\n9\\n10\\n\\n// Initialization\\nLet δ̄1 ← ts · n−4 and δ̄2 ← (1 − ts ) · n−4 .\\nLet t ← 0 and y(t) ← 1∅ .\\n// Growing y(t)\\nwhile t < 1 do\\nforeach u ∈ N do\\nLet wu (t) be an estimate of E[f (u | R(y(t))] obtained by averaging the values of\\nf (u | R(y(t)) for r = ⌈48n6 ln(2n)⌉ independent samples of R(y(t)).\\nP\\nP\\n\\x1a\\narg maxx∈P { u∈N \\\\Z wu (t) · xu (t) − u∈Z xu (t)} if t ∈ [0, ts ) ,\\n\\x08P\\nLet x(t) ←\\nif t ∈ [ts , 1) .\\narg maxx∈P\\nu∈N wu (t) · xu (t)\\nLet δt be δ̄1 when t < ts and δ̄2 when t ≥ ts .\\nLet y(t + δt ) ← y(t) + δt (1N − y(t)) ◦ x(t).\\nUpdate t ← t + δt .\\nreturn y(1).\\n\\n14\\n\\n\\x0cWe begin the analysis of Algorithm 3 by showing that y(t) remains within the cube [0, 1]N\\nthroughout the execution of the algorithm. Without this observation, the algorithm is not welldefined.\\nObservation A.1. For every value of t, y(t) ∈ [0, 1]N .\\nProof. We prove the observation by induction on t. Clearly the observation holds for y(0) = 1∅ .\\nAssume the observation holds for some time t, then, for every u ∈ N ,\\nyu (t + δt ) = yu (t) + δt (1 − yu (t)) · xu (t) ≥ 0 ,\\nwhere the inequality holds since the induction hypothesis implies 1 − yu (t) ∈ [0, 1]. A similar\\nargument also implies\\nyu (t + δt ) = yu (t) + δt (1 − yu (t)) · xu (t) ≤ yu (t) + (1 − yu (t)) = 1 .\\nUsing the last observation it is now possible to prove the following counterpart of Observation 4.2.\\nCorollary A.2. Algorithm 3 always outputs a vector in P .\\nProof. Let T be the set of values t takes\\nP\\nP during the execution of Algorithm 3. We observe that\\nδ\\n=\\n1,\\nwhich\\nimplies\\nthat\\nt∈T \\\\{1} t\\nt∈T \\\\{1} δt · x(t) is a convex combination of the vectors\\n{x(t) : t ∈ T \\\\ {1}}.\\nP As all these vectors belong to P , and P is convex, any convex combination of\\nthem, including t∈T \\\\{1} δt · x(t), must be in P .\\nNext, we rewrite the output of Algorithm 3 as\\nX\\nX\\nδt · x(t) .\\nδt (1N − y(t)) ◦ x(t) ≤\\ny(1) =\\nt∈T \\\\{1}\\n\\nt∈T \\\\{1}\\n\\nBy the above discussion the rightmost hand side of this inequality is a vector in P , which implies\\nthat y(1) ∈ P since P is down-closed.\\nThe next step towards showing that Algorithm 3 proves Theorem 4.1 is analyzing its approximation ratio. We start this analysis by showing that with high probability all the estimations made by\\nthe algorithm are quite accurate. Let A be the event that |wu (t)−E[f (u | R(y(t)))]| ≤ n−2 ·f (OP T )\\nfor every u ∈ N and time t.\\nLemma A.3 (The symmetric version of Theorem A.1.16 in [2]). Let Xi , 1 ≤ i ≤ k, be mutually\\nindependent with all E[Xi ] = 0 and all |Xi | ≤ 1. Set S = X1 + · · · + Xk . Then, Pr[|S| > a] ≤\\n2\\n2e−a /2k .\\nCorollary A.4. Pr[A] ≥ 1 − n−1 .\\nProof. Consider the calculation of wu (t) for a given u ∈ N and time t. This calculation is done by\\naveraging the value of f (u | R(y(t))) for r independent samples of R(y(t)). Let Yi denote the value\\n(u|R(y(t)))]\\n. Then, by definition,\\nof f (u | R(y(t))) obtained for the i-th sample, and let Xi = Yi −E[f\\n2n·f (OP T )\\nwu (t) =\\n\\nPr\\n\\ni=1 Yi\\n\\nr\\n\\n= [2n · f (OP T )] ·\\n\\nPr\\n\\ni=1 Xi\\n\\nr\\n\\n+ E[f (u | R(y(t)))] .\\n\\nSince Yi is distributed like f (u | R(y(t))), the definition of Xi guarantees that E[Xi ] = 0 for\\nevery 1 ≤ i ≤ r. Additionally, |Xi | ≤ 1 for every such i since the absolute values of both Yi and\\n15\\n\\n\\x0cE[f (u | R(y(t)))] are upper bounded by maxS⊆N f (S) ≤ n · f (OP T ) (the last inequality follows\\nfrom our assumption that 1u ∈ P for every element u ∈ N ). Thus, by Lemma A.3,\\n#\\n\" r\\nX\\nr\\n−3\\n2\\nXi > 3 ≤ 2e−[rn /2] /2r\\nPr[|wu (t) − E[f (u | R(y(t)))]| > n−2 · f (OP T )] = Pr\\n2n\\ni=1\\n\\x12 \\x136\\n1\\n1\\n−rn−6 /8\\n−6 ln(2n)\\n= 2e\\n≤ 2e\\n=2·\\n≤ 6 .\\n2n\\n2n\\nObserve that Algorithm 3 calculates wu (t) for every combination of element u ∈ N and time\\nt < 1. Since there are n elements in N and 2n4 times smaller than 1, the union bound implies\\nthat the probability that for at least one such value wu (t) we have |wu (t) − E[f (u | R(y(t)))]| >\\nn−2 · f (OP T ) is upper bounded by\\n\\x01 1\\n1\\n· n · 2n4 =\\n,\\n6\\n2n\\nn\\n\\nwhich completes the proof of the corollary.\\n\\nOur next step is to give a lower bound on the increase in F (y(t)) as a function of t given A. This\\nlower bound is given by Corollary A.7, which follows from the next two lemmata. The statement\\nand proof of the corollary and the next lemma is easier with the following definition. Let OP Tt′\\ndenote the set OP T \\\\ Z when t < ts , and OP T otherwise.\\nP\\nLemma A.5. Given A, for every time t < 1,\\nu∈N (1 − yu (t)) · xu (t) · ∂u F (y(t)) ≥ F (y(t) ∨\\n−1\\n1OP Tt′ ) − F (y(t)) − O(n ) · f (OP T ).\\nProof. Let us calculate the weight of OP Tt′ according to the weight function w(t).\\nX\\nX\\nwu (t) ≥\\n[E[f (u | R(y(t)))] − n−2 · f (OP T )]\\nw(t) · 1OP Tt′ =\\nu∈OP Tt′\\n\\n\\uf8ee\\n\\n≥ E\\uf8f0\\n\\nX\\n\\nu∈OP Tt′\\n\\nu∈OP Tt′\\n\\n\\uf8f9\\n\\nf (R(y(t)) + u) − f (R(y(t)))\\uf8fb − n−1 · f (OP T )\\n\\n\\x03\\n≥ E f (R(y(t)) ∪ OP Tt′ ) − f (R(y(t))) − n−1 · f (OP T )\\n\\x02\\n\\n= F (y(t) ∨ 1OP T ′t ) − F (y(t)) − n−1 · f (OP T ) ,\\n\\nwhere the first inequality follows from the definition of A, and the last follows from the submodularity of f . Recall that x(t) is the vector in P maximizing some objective function (which depends on\\nt). For t < ts , the objective function maximized by x(t) assigns the value w(t)·1OP T \\\\Z = w(t)·1OP Tt′\\nto the vector 1OP Tt′ ∈ P . Similarly, for t ≥ ts , the objective function maximized by x(t) assigns the\\nvalue w(t) · 1OP T = w(t) · 1OP Tt′ to the vector 1OP Tt′ ∈ P . Thus, the definition of x(t) guarantees\\nthat in both cases we have\\nw(t) · x(t) ≥ w(t) · 1OP Tt′ ≥ F (y(t) ∨ 1OP Tt′ ) − F (y(t)) − n−1 · f (OP T ) .\\n\\n16\\n\\n\\x0cHence,\\nX\\n\\n(1 − yu (t)) · xu (t)·∂u F (y(t)) =\\n\\nu∈N\\n\\nX\\n\\nxu (t) · [F (y(t) ∨ 1u ) − F (y(t))]\\n\\nu∈N\\n\\nX\\n\\n=\\n\\nxu (t) · E[f (u | R(y(t)))]\\n\\nu∈N\\n\\nX\\n\\n≥\\n\\nxu (t) · [wu (t) − n−2 · f (OP T )] = x(t) · w(t) − n−1 · f (OP T )\\n\\nu∈N\\n\\n≥ F (y(t) ∨ 1OP Tt′ ) − F (y(t)) − 2n−1 · f (OP T ) ,\\nwhere the first inequality holds by the definition of A and the second equality holds since\\nF (y(t) ∨ 1u ) − F (y(t)) = E[f (R(y(t)) + u)] − E[f (R(y(t)))] = E[f (u | R(y(t)))] .\\nLemma A.6 (A rephrased version of Lemma 2.3.7 in [23]). Consider\\ntwo vectors x, x′ ∈ [0, 1]N\\nP\\nsuch that |xu − x′u | ≤ δ for every u ∈ N . Then, F (x′ ) − F (x) ≥ u∈N (x′u − xu ) · ∂u F (x) − O(n3 δ2 ) ·\\nmaxu∈N f ({u}).\\nCorollary A.7. Given A, for every time t < 1, F (y(t + δt )) − F (y(t)) ≥ δt [F (y(t) ∨ 1OP Tt′ ) −\\nF (y(t))] − O(n−1 δt ) · f (OP T ).\\nProof. Observe that for every u ∈ N , |yu (t + δt ) − yu (t)| = |δt (1 − yu (t))xu (t)| ≤ δt . Hence, by\\nLemma A.6,\\nX\\nF (y(t + δt )) − F (y(t)) ≥\\n[yu (t + δt )) − yu (t)] · ∂u F (y(t)) − O(n3 δt2 ) · max f ({u})\\nu∈N\\n\\nu∈N\\n\\n=\\n\\nX\\n\\nδt (1 − yu (t)) · xu (t) · ∂u F (y(t)) − O(n3 δt2 ) · max f ({u}) .\\nu∈N\\n\\nu∈N\\n\\n(13)\\n\\nConsider the rightmost hand side of the last inequality. By Lemma A.5, the first term on this side\\ncan be bounded by\\nX\\nδt (1 − yu (t)) · xu (t) · ∂u F (y(t)) ≥ δt · [F (y(t) ∨ 1OP Tt′ ) − F (y(t)) − O(n−1 ) · f (OP T )]\\nu∈N\\n\\n= δt · [F (y(t) ∨ 1OP Tt′ ) − F (y(t))] − O(n−1 δt ) · f (OP T ) .\\n\\nOn the other hand, the second term of (13) can be bounded by\\nO(n3 δt2 ) · max f ({u}) = O(n−1 δt ) · f (OP T )\\nu∈N\\n\\nsince δt ≤ n−4 by definition and maxu∈N f ({u}) ≤ f (OP T ) by our assumption that 1u ∈ P for\\nevery u ∈ N .\\nThe lower bound given by the last corollary is in terms of F (y(t) ∨ 1OP Tt′ ). To make this lower\\nbound useful, we need to lower bound the term F (y(t) ∨ 1OP Tt′ ). This is done by the following two\\nlemma which corresponds to Lemma 4.4.\\nLemma A.8. [corresponds to Lemma 4.4] For every time t < 1 and set A ⊆ N it holds that\\n\\x10\\n\\x11\\nF (y(t) ∨ 1A ) ≥ e− max{0,t−ts } − e−t − O(n−4 ) · max {0, f (A) − f (A ∪ Z)}\\n+ (e−t − O(n−4 )) · f (A) .\\n17\\n\\n\\x0cThe proof of this lemma goes along the same lines as the proof of its corresponding lemma in\\nSection 4, except that the bounds on the coordinates of y(t) used by the proof from Section 4 are\\nreplaced with the (slightly weaker) bounds given by the following lemma.\\nLemma A.9. For every time t and element u ∈ N ,\\n(\\n1 − e−t + O(n−4 )\\nyu (t) ≤\\n1 − e− max{0,t−ts } + O(n−4 )\\n\\nif u 6∈ Z ,\\nif u ∈ Z .\\n\\nProof. Let ε = n−4 , and observe that δt ≤ ε for every time t. Our first objective is to prove by\\ninduction on t that, if yu (τ ) = 0 for some time τ ∈ [0, 1], then yu (t) ≤ 1 − (1 − ε)(t−τ )/ε for every\\ntime t ∈ [τ, 1]. For t = τ the claim holds because yu (τ ) = 0 = 1 − (1 − ε)(τ −τ )/ε . Next, assume the\\nclaim holds for some t, and let us prove it for t + δt .\\nyu (t + δt ) = yu (t) + δt (1 − yu (t)) · xu (t) ≤ yu (t) + δt (1 − yu (t)) = yu (t)(1 − δt ) + δt\\n≤ (1 − (1 − ε)(t−τ )/ε )(1 − δt ) + δt = 1 − (1 − δt )(1 − ε)(t−τ )/ε\\n≤ 1 − (1 − ε)δt /ε (1 − ε)(t−τ )/ε = 1 − (1 − ε)(t+δt −τ )/ε ,\\nwhere the last inequality holds since (1 − x)1/x is a decreasing function for x ∈ (0, 1].\\nWe complete the proof for the case u 6∈ Z by choosing τ = 0 (clearly yu (0) = 0) and observing\\nthat, for every time t,\\n1 − (1 − ε)t/ε ≤ 1 − [e−1 (1 − ε)]t = 1 − e−t (1 − ε)t ≤ 1 − e−t (1 − ε) = 1 − e−t + O(ε) .\\nIt remains to prove the lemma for the case u ∈ Z. Note that at every time t ∈ [0, ts ) Algorithm 3\\nchooses as x(t) a vector maximizing a linear function in P which assigns a negative weight to\\nelements of Z. Since P is down-closed this maximum must have xu (t) = 0 for an element u ∈ Z.\\nThis means that yu (t) = 0 for t ∈ [0, ts ]. In addition to proving the lemma for this time range, the\\nlast inequality also allows us to choose τ = ts , which gives, for t ∈ [ts , 1],\\nyu (t) ≥ 1 − (1 − ε)(t−ts )/ε ≥ 1 − ets −t + O(ε) .\\nCombining Corollary A.7 with Lemma A.8 gives us the following corollary.\\nCorollary A.10. Given A, for every time t ∈ [0, ts ),\\nF (y(t + δt )) − F (y(t)) ≥ δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − F (y(t)))]\\n− O(n−1 δt ) · f (OP T )\\nand, for every time t ∈ [ts , 1),\\nF (y(t + δt )) − F (y(t)) ≥ δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0}\\n− F (y(t))] − O(n−1 δt ) · f (OP T ) .\\nProof. For every time t ∈ [0, ts ), Corollary A.7 and Lemma A.8 imply together\\nF (y(t + δt )) − F (y(t)) ≥ δt [(1 − e−t − O(n−4 )) · max {0, f (OP T \\\\ Z) − f (OP T ∪ Z)}\\n+ (e−t − O(n−4 )) · f (OP T \\\\ Z)] − O(n−1 δt ) · f (OP T )\\n≥ δt [(1 − O(n−4 )) · f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )\\n− F (y(t)))] − O(n−1 δt ) · f (OP T ) .\\n18\\n\\n\\x0cWe observe that this inequality is identical to the inequality promised for this time range by the\\ncorollary, except that it has an extra term of −δt · O(n−4 ) · f (OP T \\\\ Z) on its right hand side. Since\\nf (OP T \\\\ Z) is upper bounded by f (OP T ), due to the down-closeness of P , the absolute value of\\nthis extra term is at most\\nδt · O(n−4 ) · f (OP T ) = O(n−1 δt ) · f (OP T ) ,\\nwhich completes the proof for the time range t ∈ [0, ts ).\\nConsider now the time range t ∈ [ts , 1). For this time range Corollary A.7 and Lemma A.8\\nimply together\\nF (y(t + δt )) − F (y(t)) ≥ δt [(ets −t − e−t − O(n−4 )) · max {0, f (OP T ) − f (OP T ∪ Z)}\\n+ (e−t − O(n−4 )) · f (OP T )] − O(n−1 δt ) · f (OP T ) .\\nWe observe again that this inequality is identical to the inequality promised for this time range\\nby the corollary, except that it has extra terms of −δt · O(n−4 ) · f (OP T ) and −δt · O(n−4 ) ·\\nmax{0, f (OP T ) − f (OP T ∪ Z)} on its right hand side. The corollary now follows since the absolute\\nvalue of both these terms is upper bounded by O(n−1 δt ) · f (OP T ).\\nCorollary A.10 bounds the increase in F (y(t)) in terms of F (y(t)) itself. Thus, it gives a\\nrecursive formula which can be used to lower bound F (y(t)). Our remaining task is to solve this\\nformula and get a closed-form lower bound on F (y(t)). Let g(t) be defined as follows. g(0) = 0 and\\nfor every time t < 1,\\ng(t+δt )\\n(\\ng(t) + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − g(t)]\\n=\\ng(t) + δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0} − g(t)]\\n\\nif t < ts ,\\nif t ≥ ts .\\n\\nThe next lemma shows that a lower bound on g(t) yields a lower bound on F (y(t)).\\nLemma A.11. Given A, for every time t, g(t) ≤ F (y(t)) + O(n−1 ) · t · f (OP T ).\\nProof. Let c be the larger constant among the constants hiding behind the big O notations in\\nCorollary A.10. We prove by induction on t that g(t) ≤ F (y(t)) + (ct/n) · f (OP T ). For t = 0, this\\nclearly holds since g(0) = 0 ≤ F (y(0)). Assume now that the claim holds for some t, and let us\\nprove it for t + δt . There are two cases to consider. If t < ts , then the induction hypothesis and\\nCorollary A.10 imply, for a large enough n,\\ng(t + δt ) = g(t) + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − g(t)]\\n= (1 − δt )g(t) + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )]\\n≤ (1 − δt )[F (y(t)) + (ct/n) · f (OP T )] + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )]\\n= F (y(t)) + δt [f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T ) − F (y(t))]\\n+ (ct/n) · (1 − δt ) · f (OP T )\\n≤ F (y(t + δt )) + (cδt /n) · f (OP T ) + (ct/n) · (1 − δt ) · f (OP T )\\n≤ F (y(t + δt )) + [c(t + δt )/n] · f (OP T ) .\\nSimilarly, if t ≥ ts , then we get\\ng(t + δt ) = g(t) + δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0} − g(t)]\\n19\\n\\n\\x0c= (1 − δt )g(t) + δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0}]\\n≤ (1 − δt )[F (y(t)) + (ct/n) · f (OP T )]\\n+ δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0}]\\n= F (y(t)) + δt [e−t · f (OP T ) + (ets −t − e−t ) · max{f (OP T ) − f (Z ∪ OP T ), 0} − F (y(t))]\\n+ (ct/n) · (1 − δt ) · f (OP T )\\n≤ F (y(t + δt )) + (cδt /n) · f (OP T ) + (ct/n) · (1 − δt ) · f (OP T )\\n≤ F (y(t + δt )) + [c(t + δt )/n] · f (OP T ) .\\nIt remains to find a closed-form expression that lower bounds g(t) (and thus, also F (y(t))). Let\\nh1 (t) : [0, ts ] → R and h2 (t) : [ts , 1] → R be defined as follows.\\nh1 (t) = (1 − e−t ) · f (OP T \\\\ Z) − (1 − e−t − te−t ) · f (Z ∪ OP T ) ,\\nand\\nh2 (t) = e−t · {(t − ts ) · [f (OP T ) + (ets − 1) · max{f (OP T ) − f (OP T ∪ Z), 0}] + ets · h1 (ts )} .\\nLemma A.12. For every time t ≤ ts , h1 (t) ≤ g(t).\\nProof. The proof is by induction on t. For t = 0, g(0) = 0 = (1 − e0 ) · f (OP T \\\\ Z) − (1 − e0 − 0 ·\\ne0 ) · f (Z ∪ OP T ) = h1 (0). Assume now that the lemma holds for some t < ts , and let us prove it\\nholds also for t + δt . By the induction hypothesis,\\nZ t+δt\\nh′ (τ )dτ\\nh1 (t + δt ) = h1 (t) +\\nt\\nZ t+δt\\n{e−τ · f (OP T \\\\ Z) − τ e−τ · f (Z ∪ OP T )}dτ\\n= h1 (t) +\\nt\\n\\n≤ h1 (t) + δt · {e−t · f (OP T \\\\ Z) − te−t · f (Z ∪ OP T )}dτ\\n\\n= (1 − δt )h1 (t) + δt · {f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )}\\n≤ (1 − δt )g(t) + δt · {f (OP T \\\\ Z) − (1 − e−t ) · f (Z ∪ OP T )} = g(t + δt ) ,\\nwhere the first inequality holds since e−τ is a decreasing function of τ and τ e−τ is an increasing\\nfunction of τ in the range τ ∈ [0, 1].\\nLemma A.13. For every time ts ≤ t ≤ 1, h2 (t) ≤ g(t).\\nProof. The proof is by induction on t. For t = ts , by Lemma A.12, h2 (ts ) = h1 (ts ) ≤ g(ts ). Assume\\nnow that the lemma holds for some ts ≤ t < 1, and let us prove it holds also for t + δt .\\nTo avoid repeating complex expressions, let us denote A = f (OP T ) + (ets − 1) · max{f (OP T ) −\\nf (Z ∪ OP T ), 0}. Notice that A is independent of t. Moreover, using this notation we can rewrite\\nh2 (t) as h2 (t) = e−t · {(t − ts ) · A + ets · h1 (ts )}. Thus, for every τ ∈ (ts , 1),\\nh′2 (τ ) = −e−τ · {(τ − ts ) · A + ets · h1 (ts )} + e−τ · A = e−τ · {(1 − τ + ts ) · A − ets · h1 (ts )} .\\nThe definition of A and the non-negativity of f imply immediately that A ≥ 0. We would like\\nto prove also that ts · A − ets · h1 (ts ) ≥ 0. There are two cases to consider. First, if f (OP T ) ≥\\nf (Z ∪ OP T ), then\\nts · A − ets · h1 (ts ) = ts · f (OP T ) + ts (ets − 1) · max{f (OP T ) − f (Z ∪ OP T ), 0}\\n20\\n\\n\\x0c− (ets − 1) · f (OP T \\\\ Z) + (ets − 1 − ts ) · f (Z ∪ OP T )\\n≥ ts ets · f (OP T ) − ts (ets − 1) · f (Z ∪ OP T )\\n− (ets − 1) · f (OP T ) + (ets − 1 − ts ) · f (Z ∪ OP T )\\n= (ts ets − ets + 1) · [f (OP T ) − f (Z ∪ OP T )] ≥ 0 .\\nwhere the inequality uses the fact that f (OP T ) ≥ f (OP T \\\\ Z) because of the down-closure of P .\\nOn the other hand, if f (OP T ) < f (Z ∪ OP T ), then\\nts · A − ets · h1 (ts ) = ts · f (OP T ) − (ets − 1) · f (OP T \\\\ Z) + (ets − 1 − ts ) · f (Z ∪ OP T )\\n≥ ts · f (OP T ) − (ets − 1) · f (OP T ) + (ets − 1 − ts ) · f (OP T ) = 0 .\\nUsing the above observations and the induction hypothesis, we can now get\\nh2 (t + δt ) = h2 (t) +\\n\\nZ\\n\\nt+δt\\n\\nh′ (τ )dτ = h2 (t) +\\n\\nZ\\n\\nt+δt\\n\\ne−τ · {(1 − τ + ts ) · A − ets · h1 (ts )}dτ\\n\\nt\\n\\nt\\n\\n≤ h2 (t) + δt · e−t · {(1 − t + ts ) · A − ets · h1 (ts )} = (1 − δt )h2 (t) + δt · e−t · A\\n≤ (1 − δt )g(t) + δt · e−t · A = g(t + δt ) .\\nThe last two lemmata give us the promised closed-form lower bound on g(t), which can be used\\nto lower bound the approximation ratio of Algorithm 3.\\nCorollary A.14. E[F (y(1))] ≥ ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T ) −\\n(2 − ts − 2e−ts ) · f (Z ∪ OP T )].\\nProof. By Lemma A.11, given A,\\nF (y(1)) ≥ g(1) − O(n−1 ) · f (OP T ) .\\nBy Lemma A.13,\\ng(1) ≥ h2 (1)\\n= e−1 · {(1 − ts ) · [f (OP T ) + (ets − 1) · max{f (OP T ) − f (Z ∪ OP T ), 0}]\\n+ (ets − 1) · f (OP T \\\\ Z) − (ets − 1 − ts ) · f (Z ∪ OP T )}\\n≥ e−1 · {(1 − ts ) · [ets · f (OP T ) − (ets − 1) · f (Z ∪ OP T )]\\n+ (ets − 1) · [f (OP T ) − f (Z ∩ OP T )] − (ets − 1 − ts ) · f (Z ∪ OP T )}\\n= ets −1 · [(2 − ts − e−ts ) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] ,\\nwhere the second inequality holds since the submodularity and non-negativity of f imply\\nf (OP T \\\\ Z) ≥ f (OP T ) + f (∅) − f (Z ∩ OP T ) ≥ f (OP T ) − f (Z ∩ OP T ) .\\nCombining the above observations we get that, given A,\\nF (y(1)) ≥ ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] .\\n\\n21\\n\\n\\x0cSince F (y(1)) is always non-negative, this implies, by the law of total expectation,\\nE[F (y(1))] ≥ Pr[A] · {ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )]}\\n≥ {ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )]}\\n1\\n− · ets −1 · (2 − ts − e−ts − O(n−1 )) · f (OP T )\\nn\\n= ets −1 · [(2 − ts − e−ts − O(n−1 )) · f (OP T ) − (1 − e−ts ) · f (Z ∩ OP T )\\n− (2 − ts − 2e−ts ) · f (Z ∪ OP T )] ,\\nwhere the second inequality holds since Pr[A] ≥ 1 − n−1 by Corollary A.4.\\nTheorem 4.1 now follows immediately by combining Corollaries A.2 and A.14.\\n\\n22\\n\\n\\x0c',\n",
       " 'label': 8}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"math.AC\",\n",
    "    1: \"cs.CV\",\n",
    "    2: \"cs.AI\",\n",
    "    3: \"cs.SY\",\n",
    "    4: \"math.GR\",\n",
    "    5: \"cs.CE\",\n",
    "    6: \"cs.PL\",\n",
    "    7: \"cs.IT\",\n",
    "    8: \"cs.DS\",\n",
    "    9: \"cs.NE\",\n",
    "    10: \"math.ST\"\n",
    "}\n",
    "label2id = {\n",
    "    \"math.AC\": 0,\n",
    "    \"cs.CV\": 1,\n",
    "    \"cs.AI\": 2,\n",
    "    \"cs.SY\": 3,\n",
    "    \"math.GR\": 4,\n",
    "    \"cs.CE\": 5,\n",
    "    \"cs.PL\": 6,\n",
    "    \"cs.IT\": 7,\n",
    "    \"cs.DS\": 8,\n",
    "    \"cs.NE\": 9,\n",
    "    \"math.ST\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximo.fernandez/miniconda3/envs/nlp/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a1cdac6ac7467083fa1aa41b340687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b512cce73e4851992fa060484abc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2a7fb1b8f3476a9ae2c78c1672dc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5ea670b93443b8a79efc632c62ca91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1462924b141044b2bd69eace6ed70201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/28388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3812892f2b9b4b7ebc6de5479019fcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/datasets/utils/py_utils.py:651\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Empty:\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/multiprocess/managers.py:822\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m conn\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id, methodname, args, kwds))\n\u001b[0;32m--> 822\u001b[0m kind, result \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mrecv()\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#RETURN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/multiprocess/connection.py:253\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 253\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recv_bytes()\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/multiprocess/connection.py:433\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 433\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recv(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    434\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/multiprocess/connection.py:398\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    399\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenized_train \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      2\u001b[0m     preprocess_function,\n\u001b[1;32m      3\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      5\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mNUM_PROCS\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m tokenized_valid \u001b[38;5;241m=\u001b[39m valid_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      9\u001b[0m     preprocess_function,\n\u001b[1;32m     10\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     12\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mNUM_PROCS\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m tokenized_test \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     16\u001b[0m     preprocess_function,\n\u001b[1;32m     17\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     19\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39mNUM_PROCS\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/datasets/arrow_dataset.py:3197\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3191\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3192\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3193\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3194\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3195\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3196\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3197\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3198\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3199\u001b[0m     ):\n\u001b[1;32m   3200\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3201\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/datasets/utils/py_utils.py:665\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m--> 665\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/datasets/utils/py_utils.py:665\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    664\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m--> 665\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/multiprocess/pool.py:770\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_success:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=NUM_PROCS\n",
    ")\n",
    " \n",
    "tokenized_valid = valid_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=NUM_PROCS\n",
    ")\n",
    " \n",
    "tokenized_test = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=NUM_PROCS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sample = preprocess_function(train_dataset[0])\n",
    "print(tokenized_sample)\n",
    "print(f\"Length of tokenized IDs: {len(tokenized_sample.input_ids)}\")\n",
    "print(f\"Length of attention mask: {len(tokenized_sample.attention_mask)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sample = preprocess_function(train_dataset[0])\n",
    "print(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=11,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    report_to='tensorboard',\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    " \n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForSequenceClassification.from_pretrained(f\"arxiv_bert/checkpoint-4440\")\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
    " \n",
    "all_files = glob.glob('inference_data/*')\n",
    "for file_name in all_files:\n",
    "    file = open(file_name)\n",
    "    content = file.read()\n",
    "    print(content)\n",
    "    result = classify(content)\n",
    "    print('PRED: ', result)\n",
    "    print('GT: ', file_name.split('_')[-1].split('.txt')[0])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
