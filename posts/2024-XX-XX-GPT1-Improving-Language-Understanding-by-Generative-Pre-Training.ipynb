{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT1 - Improving Language Understanding by Generative Pre-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Entrenan el modelo en un gran corpus de texto sin supervisión. Modelado del lenguaje. Aprender un modelo de lenguaje de alta capacidad en un gran corpus de texto\n",
    " * Luego hacen fine-tuning en tareas de NLP supervisadas. Es decir con datasets etiquetados. Ajuste fino en una tarea objetivo con supervisión. Ajuste fino, en la que adaptamos el modelo a una tarea discriminativa con datos etiquetados. CUando evaluan al modelo en la tarea supervisada, no solo le evaluan por esa tarea, sino por lo bien que predice el siguiente token, esto ayuda a mejorar la generalización del modelo supervisado y hace que el modelo converja más rápido.\n",
    " * Utilizan la arquitectura transformer, que mejora al uso de RNN en que lo aprendido en el primer entrenamiento es más fácil de transferir a tareas supervisadas.\n",
    " * Evaluan el modelo en cuatro tipos de tareas de comprensión del lenguaje:\n",
    "    * Inferencia del lenguaje natural\n",
    "    * Respuesta a preguntas\n",
    "    * Similitud semántica\n",
    "    * Clasificación de textos. \n",
    " * El modelo, agnóstico de tareas generales, supera a los modelos entrenados discriminativamente que emplean arquitecturas diseñadas específicamente para cada tarea, mejorando significativamente el estado del arte en 9 de las 12 tareas estudiadas.\n",
    " * También analizamos los comportamientos de \"disparo cero\" del modelo preentrenado en cuatro entornos diferentes y demostramos que adquiere un conocimiento lingüístico útil para las tareas posteriores.\n",
    " * En los últimos años, los investigadores han demostrado los beneficios de utilizar incrustaciones de palabras [11, 39, 42], que se entrenan en corpus no etiquetados, para mejorar el rendimiento en una variedad de tareas [8, 11, 26, 45]. Sin embargo, estos enfoques transfieren principalmente información a nivel de palabra, mientras que nosotros pretendemos capturar la semántica de nivel superior.\n",
    " * Utilizamos un decodificador Transformer multicapa\n",
    " * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo es la arquitectura? ¿Qué pasa con la entrada del encoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero hay que instalar `ftfy` y `spacy` mediante\n",
    "\n",
    "```bash\n",
    "pip install ftfy spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instaladas, debes descargar el modelo de lenguaje de spacy que deseas utilizar. Por ejemplo, para descargar el modelo de inglés, puedes ejecutar:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar texto vamos a utilizar el modelo desde el repositorio de [GPT1](https://huggingface.co/openai-community/openai-gpt) de Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si te fijas hemos importado `OpenAIGPTTokenizer` y `AutoTokenizer`. Esto es porque en la [model card](https://huggingface.co/openai-community/openai-gpt) de GPT1 se indica que se use `OpenAIGPTTokenizer`, pero en el post de la librería [transformers](https://maximofn.com/hugging-face-transformers/) explicamos que se debe usar `AutoTokenizer` para cargar el tokenizador. Así que vamos a probar los dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: \n",
      "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "input auto tokens: \n",
      "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "ckeckpoints = \"openai-community/openai-gpt\"\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(ckeckpoints)\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
    "\n",
    "input_tokens = tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
    "input_auto_tokens = auto_tokenizer(\"Hello, my dog is cute and\", return_tensors=\"pt\")\n",
    "\n",
    "print(f\"input tokens: \\n{input_tokens}\")\n",
    "print(f\"input auto tokens: \\n{input_auto_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver con los dos tokenizadores se obtienen los mismos tokens. Así que para que el código sea más general, de manera que si se cambian los ckeckpoints no haya que cambiar el código, vamos a utilizar `AutoTokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos entonces el device, el tokenizador y el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los tokens de entrada al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"Hello, my dog is cute and\"\n",
    "input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se los pasamos al modelo para generar los tokens de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tokens: \n",
      "tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,\n",
      "           485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_tokens = model.generate(**input_tokens)\n",
    "\n",
    "print(f\"output tokens: \\n{output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodificamos los tokens para obtener la sentencia de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded output: \n",
      "hello, my dog is cute and i'm going to take him for a walk. \" \n",
      " \"\n"
     ]
    }
   ],
   "source": [
    "decoded_output = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"decoded output: \\n{decoded_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos conseguido generar texto con GPT1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar texto token a token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos usado `model.generate` para generar los tokens de salida de golpe, pero vamos a ver cómo generarlos uno a uno. Para ello, en vez de usar `model.generate` vamos a usar `model`, que en realidad lo que hace es llamar al método `model.forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
       "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
       "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
       "         ...,\n",
       "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
       "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
       "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**input_tokens)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que saca muchos datos, primero vamos a ver las keys de la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso solo tenemos los logits del modelo, vamos a ver su tamaño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 40478])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs.logits\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cuantos tokens teníamos a la entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens.input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaya, a la salida tenemos el mismo número de logits que a la entrada. Esto es normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los logits de la última posición de la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40478])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nex_token_logits = logits[0,-1]\n",
    "\n",
    "nex_token_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay un total de 40478 logits, es decir, hay un vocabulario de 40478 tokens y tenemos que ver cuál es el token con mayor probabilidad, para ello primero calculamos las softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40478])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
    "\n",
    "softmax_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1898, device='cuda:0', grad_fn=<MaxBackward0>),\n",
       " tensor(249, device='cuda:0'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
    "\n",
    "next_token_prob, next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_token_id.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido el siguiente token mediante el método greedy, es decir, el token con mayor probabilidad. Pero ya vimos en el post de la librería transformers, las [formas de generar textos](https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto) que se puede hacer sampling, top-k, top-p, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a meter todo en una función y ver qué sale si generamos unos cuantos tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_greedy_token(input_sentence, tokenizer, model, device):\n",
    "    input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**input_tokens)\n",
    "    logits = outputs.logits\n",
    "    nex_token_logits = logits[0,-1]\n",
    "    softmax_logits = torch.softmax(nex_token_logits, dim=0)\n",
    "    next_token_prob, next_token_id = torch.max(softmax_logits, dim=0)\n",
    "    return next_token_prob, next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy_text(input_sentence, tokenizer, model, device, max_length=20):\n",
    "    generated_text = input_sentence\n",
    "    for _ in range(max_length):\n",
    "        next_token_prob, next_token_id = generate_next_greedy_token(generated_text, tokenizer, model, device)\n",
    "        generated_text += tokenizer.decode(next_token_id.item())\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora generamos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is cute andi.\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_greedy_text(\"Hello, my dog is cute and\", tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling con temperatura, top-k y top-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a recordar cómo se haría con transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, my dog is cute and i love him, but i'm not having sex with you right now. \" \n",
      " he laughs and takes his hand off my thigh. \" you're such a liar, baby. \" \n",
      " \" i'm not lying, \" i tell him. \n",
      " \" you're not? \" \n",
      " \" nope. \" \n",
      " \" i think i'm going to go to bed, \" i announce. \n",
      " \" i 'll see you in the morning. \" \n",
      " i stand, grab my bag and purse and leave the room, closing the door behind me. \n",
      " i'm not a liar. \n",
      " i'm not a liar. \n",
      " i am not a liar. \n",
      " i'm not a liar. \n",
      " i'm not a liar. \n",
      " i'm not a liar. \n",
      " i'm a liar. \n",
      " i'm a fucking liar. \n",
      " i'm a fucking liar. \n",
      " i'm a fucking liar. \n",
      " i'm a fucking liar. \n",
      " i'm a fucking a * * hole. \n",
      " i'm a f * * king a * * hole. \n",
      " i'm a f * * king a * * hole. \n",
      " i'm a f * * king a * * hole. \n",
      " \" hey, babe, \" i call out, walking into the kitchen. \n",
      " \" hey, \" she responds, turning to face me. \n",
      " \" what's up? \" i ask, sitting down at the island. \n",
      " \" i just wanted to see if you 'd like to do something tonight. \" \n",
      " \" what do you mean? \" \n",
      " \" i thought you might want to get together. \" \n",
      " \" why? \" \n",
      " \" i'm not sure. it's just... i think i 'd like to have you. \" \n",
      " \" why? \" \n",
      " \" because you're sexy, \" she says, looking straight into my eyes. \" and i want to make love to you. \" \n",
      " \" why do you want to do that? \" \n",
      " she shrugs. \" i don't know. \" \n",
      " \" why do you want to do that? \" \n",
      " \" i don't know. \" \n",
      " \" come on, jules, you've been through a lot in the past few weeks. i don't think you're ready to be a virgin. \" \n",
      " \" maybe i don't want to be a virgin. \" she shrugs again. \n",
      " \" you know what i mean. \" \n",
      " \" yeah, i do. \" she shrugs again. \" i'm just thinking about getting out of the apartment, and then i\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(\"cuda\")\n",
    "tokens_output = model.generate(**input_tokens, max_new_tokens=500, do_sample=True, top_k=50, top_p=0.95, temperature=0.7)\n",
    "sentence_output = tokenizer.decode(tokens_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(sentence_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacerlo token a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_next_token_sampling(input_sentence, tokenizer, model, device, temperature, top_p, top_k):\n",
    "    input_tokens = tokenizer(input_sentence, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**input_tokens)\n",
    "    logits = outputs.logits\n",
    "    next_token_logits = logits[0, -1] / temperature  # Apply temperature scaling\n",
    "    \n",
    "    # Calculate top-k and top-p filtering\n",
    "    top_k_logits, top_k_indices = torch.topk(next_token_logits, k=top_k, dim=0)\n",
    "    next_token_logits = top_k_logits\n",
    "    next_token_indices = top_k_indices\n",
    "    \n",
    "    # Calculate probabilities with softmax\n",
    "    softmax_logits = F.softmax(next_token_logits, dim=0)\n",
    "    \n",
    "    # Apply top-p filtering\n",
    "    cumulative_probs = torch.cumsum(softmax_logits, dim=0)\n",
    "    sorted_indices = torch.argsort(cumulative_probs, descending=True)\n",
    "    top_p_indices = sorted_indices[cumulative_probs[sorted_indices] <= top_p]\n",
    "    next_token_probs = softmax_logits[top_p_indices]\n",
    "    next_token_indices = next_token_indices[top_p_indices]\n",
    "    \n",
    "    # Sample from the filtered probabilities\n",
    "    next_token_prob = torch.multinomial(next_token_probs, num_samples=1)\n",
    "    next_token_id = next_token_indices[next_token_prob]\n",
    "    \n",
    "    return next_token_prob, next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sampling_text(input_sentence, tokenizer, model, device, temperature=1.0, top_p=0.9, top_k=50, max_length=20):\n",
    "    generated_text = input_sentence\n",
    "    for _ in range(max_length):\n",
    "        next_token_prob, next_token_id = generate_next_token_sampling(generated_text, tokenizer, model, device, temperature, top_p, top_k)\n",
    "        generated_text += tokenizer.decode(next_token_id.item())\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos 100 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is cute andhe\\'sgotaandhe\\'-idonot,\"hesaid,shakinghisheadinthecollar.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate_sampling_text(\"Hello, my dog is cute and\", tokenizer, model, device, temperature=0.7, top_p=0.95, top_k=50, max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de la loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de empezar a hacer el fine tuning de GPT1 vamos a ver una cosa. Antes cuando obteníamos la salida del modelo hacíamos esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
       "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
       "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
       "         ...,\n",
       "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
       "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],\n",
       "         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**input_tokens)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que obtenemos `loss=None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(outputs.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos a necesitar la loss para hacer el fine tuning, vamos a ver cómo obtenerla.\n",
    "\n",
    "Si nos vamos a la documentación del método [forward](https://huggingface.co/docs/transformers/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel.forward) de `OpenAIGPTLMHeadModel`, podemo ver que dice que a la salida devuelve un objeto de tipo `transformers.modeling_outputs.CausalLMOutput`, así que si nos vamos a la documentación de [transformers.modeling_outputs.CausalLMOutput](https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput), podemos ver que dice que devuelve `loss` si se le pasa `labels` al método `forward`.\n",
    "\n",
    "Si nos vamos a la fuente del código del método [forward](https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai/modeling_openai.py#L544), vemos este bloque de código\n",
    "\n",
    "```python\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "```\n",
    "\n",
    "Es decir, la `loss` se calcula de la siguiente manera\n",
    "\n",
    " * Shift de logits y labels: La primera parte es desplazar los logits (`lm_logits`) y las etiquetas (`labels`) para que los `tokens < n` predigan `n`, es decir, desde una posición `n` se predice el siguiente token a partir de los anteriores.\n",
    " * CrossEntropyLoss: Se crea una instancia de la función de pérdida `CrossEntropyLoss()`.\n",
    " * Flatten tokens: A continuación, se aplanan los logits y las etiquetas utilizando `view(-1, shift_logits.size(-1))` y `view(-1)`, respectivamente. Esto se hace para que los logits y las etiquetas tengan la misma forma para la función de pérdida.\n",
    " * Cálculo de la pérdida: Finalmente, se calcula la pérdida utilizando la función de pérdida `CrossEntropyLoss()` con los logits aplanados y las etiquetas aplanadas como entradas.\n",
    "\n",
    "En resumen, la `loss` se calcula como la pérdida de entropía cruzada entre los logits desplazados y aplanados y las etiquetas desplazadas y aplanadas.\n",
    "\n",
    "Por tanto, si al método `forward` le pasamos los labels, nos devolverá la `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2607, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**input_tokens, labels=input_tokens.input_ids)\n",
    "\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el etrenamiento vamos a usar un dataset de chistes en inglés [short-jokes-dataset](https://huggingface.co/datasets/Maximofn/short-jokes-dataset), que es un dataset con 232 mil chistes en inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'Joke'],\n",
       "        num_rows: 231657\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "jokes = load_dataset(\"Maximofn/short-jokes-dataset\")\n",
    "jokes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a verlo un poco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 1,\n",
       " 'Joke': '[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\"'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento con Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a ver cómo se haría el entrenamiento con puro Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import OpenAIGPTLMHeadModel, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ckeckpoints = \"openai-community/openai-gpt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckeckpoints)\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(ckeckpoints)\n",
    "\n",
    "special_tokens_dict = {'pad_token': '[PAD]'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "jokes = load_dataset(\"Maximofn/short-jokes-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231657/231657 [02:41<00:00, 1435.09it/s, max_length=126]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "progress = tqdm.tqdm(jokes[\"train\"])\n",
    "\n",
    "max_length = 0\n",
    "for joke in progress:\n",
    "    joke_text = joke[\"Joke\"]\n",
    "    input_tokens = tokenizer(joke_text, return_tensors=\"pt\")\n",
    "    len_input_tokens = input_tokens.input_ids.shape[1]\n",
    "    if len_input_tokens > max_length:\n",
    "        max_length = len_input_tokens\n",
    "    progress.set_postfix({\"max_length\": max_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_dataset_length = max_length\n",
    "max_dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dataset_length = 126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una clase dataset de Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La instanciamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos un ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora un dataloader de Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos un batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_seq_len=400):\n",
    "        self.dataset = dataset\n",
    "        self.joke = \"JOKE: \"\n",
    "        self.end_of_text_token = \"<|endoftext|>\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = len(tokenizer)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset[\"train\"])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sentence = self.joke + self.dataset[\"train\"][item][\"Joke\"] + self.end_of_text_token\n",
    "        tokens = self.tokenizer(sentence, return_tensors=\"pt\", max_length=self.max_seq_len, padding=\"max_length\", truncation=True)\n",
    "        tokens.input_ids = tokens.input_ids.clamp(0, self.vocab_size - 1)\n",
    "        return sentence, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JokesDataset(jokes, tokenizer=tokenizer, max_seq_len=max_dataset_length+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo<|endoftext|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), torch.Size([1, 128]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence, tokens = dataset[5]\n",
    "print(sentence)\n",
    "tokens.input_ids.shape, tokens.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BS = 80\n",
    "joke_dataloader = DataLoader(dataset, batch_size=BS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, torch.Size([80, 1, 128]), torch.Size([80, 1, 128]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, tokens = next(iter(joke_dataloader))\n",
    "len(sentences), tokens.input_ids.shape, tokens.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 started==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 285/2896 [02:28<22:42,  1.92it/s, training_loss=1.56]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(tokens\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m=\u001b[39mtokens\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss                        \n\u001b[0;32m---> 23\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "import tqdm\n",
    "\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 3e-7\n",
    "\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
    "\n",
    "    progress_bar = tqdm.tqdm(joke_dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "\n",
    "        sentences, tokens = batch\n",
    "        tokens = tokens.to(device)\n",
    "        \n",
    "        outputs = model(tokens.input_ids.squeeze(), labels=tokens.input_ids.squeeze())\n",
    "        loss = outputs.loss                        \n",
    "        loss_value = loss.item()\n",
    "        loss.backward()\n",
    "                    \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.set_postfix({'training_loss': loss_value})\n",
    "        # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded joke: \n",
      "joke : \" i'm not sure i can do this. \" \n",
      " \" you can do this,\n"
     ]
    }
   ],
   "source": [
    "sentence_joke = \"JOKE:\"\n",
    "input_tokens_joke = tokenizer(sentence_joke, return_tensors=\"pt\").to(device)\n",
    "output_tokens_joke = model.generate(**input_tokens_joke)\n",
    "decoded_output_joke = tokenizer.decode(output_tokens_joke[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"decoded joke: \\n{decoded_output_joke}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
