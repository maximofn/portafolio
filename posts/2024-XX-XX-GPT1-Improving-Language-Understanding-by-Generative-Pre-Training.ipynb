{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT1 - Improving Language Understanding by Generative Pre-Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Entrenan el modelo en un gran corpus de texto sin supervisión. Modelado del lenguaje. Aprender un modelo de lenguaje de alta capacidad en un gran corpus de texto\n",
    " * Luego hacen fine-tuning en tareas de NLP supervisadas. Es decir con datasets etiquetados. Ajuste fino en una tarea objetivo con supervisión. Ajuste fino, en la que adaptamos el modelo a una tarea discriminativa con datos etiquetados. CUando evaluan al modelo en la tarea supervisada, no solo le evaluan por esa tarea, sino por lo bien que predice el siguiente token, esto ayuda a mejorar la generalización del modelo supervisado y hace que el modelo converja más rápido.\n",
    " * Utilizan la arquitectura transformer, que mejora al uso de RNN en que lo aprendido en el primer entrenamiento es más fácil de transferir a tareas supervisadas.\n",
    " * Evaluan el modelo en cuatro tipos de tareas de comprensión del lenguaje:\n",
    "    * Inferencia del lenguaje natural\n",
    "    * Respuesta a preguntas\n",
    "    * Similitud semántica\n",
    "    * Clasificación de textos. \n",
    " * El modelo, agnóstico de tareas generales, supera a los modelos entrenados discriminativamente que emplean arquitecturas diseñadas específicamente para cada tarea, mejorando significativamente el estado del arte en 9 de las 12 tareas estudiadas.\n",
    " * También analizamos los comportamientos de \"disparo cero\" del modelo preentrenado en cuatro entornos diferentes y demostramos que adquiere un conocimiento lingüístico útil para las tareas posteriores.\n",
    " * En los últimos años, los investigadores han demostrado los beneficios de utilizar incrustaciones de palabras [11, 39, 42], que se entrenan en corpus no etiquetados, para mejorar el rendimiento en una variedad de tareas [8, 11, 26, 45]. Sin embargo, estos enfoques transfieren principalmente información a nivel de palabra, mientras que nosotros pretendemos capturar la semántica de nivel superior.\n",
    " * Utilizamos un decodificador Transformer multicapa\n",
    " * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero hay que instalar `ftfy` y `spacy` mediante\n",
    "\n",
    "```bash\n",
    "pip install ftfy spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instaladas, debes descargar el modelo de lenguaje de spacy que deseas utilizar. Por ejemplo, para descargar el modelo de inglés, puedes ejecutar:\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIGPTConfig {\n",
       "  \"afn\": \"gelu\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"openai-gpt\",\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 512,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.40.2\",\n",
       "  \"vocab_size\": 40478\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import OpenAIGPTConfig\n",
    "\n",
    "config = OpenAIGPTConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximo.fernandez/miniconda3/envs/nlp/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
       "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
       "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
       "         [ -5.6463,  -5.9526, -17.5195,  ...,  -9.4144, -15.7120,  -1.5394],\n",
       "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
       "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer, OpenAIGPTLMHeadModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "model = OpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\").to(device)\n",
    "\n",
    "input_tokens = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(device)\n",
    "output_tokens = model(**input_tokens)\n",
    "output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.openai.modeling_openai.OpenAIGPTLMHeadModel"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],\n",
       "         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],\n",
       "         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],\n",
       "         [ -5.6463,  -5.9526, -17.5195,  ...,  -9.4144, -15.7120,  -1.5394],\n",
       "         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],\n",
       "         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 40478])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3570,  240,  547, 2585,  544, 4957], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 40478])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_probs = torch.softmax(output_tokens.logits, dim=-1)\n",
    "output_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens = torch.multinomial(output_probs[0], num_samples=1)\n",
    "output_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1]), 240)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nex_token = torch.multinomial(output_probs[0, -1, :], num_samples=1)\n",
    "nex_token.shape, nex_token.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 240, 834, 240, 267, 239, 239, 240, 239, 239] ., too,!..,..\n"
     ]
    }
   ],
   "source": [
    "input_token = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(device)\n",
    "output_tokens = []\n",
    "\n",
    "for i in range(10):  # Generar 10 tokens adicionales\n",
    "    output = model(**input_token)\n",
    "    output_probs = torch.softmax(output.logits, dim=-1)\n",
    "    next_token = torch.multinomial(output_probs[0, -1, :], num_samples=1)\n",
    "    output_tokens.append(next_token.item())\n",
    "    # input_token = tokenizer.decode(next_token, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(output_tokens, tokenizer.decode(output_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
