{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Accelerate` es una biblioteca de Hugging Face que permite ejecutar el mismo c√≥digo PyTorch en cualquier configuraci√≥n distribuida a√±adiendo s√≥lo cuatro l√≠neas de c√≥digo. En resumen, entrenamiento e inferencia a escala de forma sencilla, eficiente y adaptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar `accelerate` con `pip` simplemente ejecuta:\n",
    "\n",
    "``` bash\n",
    "pip install accelerate\n",
    "```\n",
    "\n",
    "Y con `conda`:\n",
    "\n",
    "``` bash\n",
    "conda install -c conda-forge accelerate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "pip install accelerate deepspeed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada entorno en el que se intale `accelerate` lo primero que se tiene que hacer es configurarlo, para ello ejecutamos en una terminal:\n",
    "\n",
    "``` bash\n",
    "accelerate config\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "In which compute environment are you running?\n",
      "This machine\n",
      "--------------------------------------------------------------------------------\n",
      "multi-GPU\n",
      "How many different machines will you use (use more than 1 for multi-node training)? [1]: 1\n",
      "Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: no\n",
      "Do you wish to optimize your script with torch dynamo?[yes/NO]:no\n",
      "Do you want to use DeepSpeed? [yes/NO]: no\n",
      "Do you want to use FullyShardedDataParallel? [yes/NO]: no\n",
      "Do you want to use Megatron-LM ? [yes/NO]: no\n",
      "How many GPU(s) should be used for distributed training? [1]:2\n",
      "What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:0,1\n",
      "--------------------------------------------------------------------------------\n",
      "Do you wish to use FP16 or BF16 (mixed precision)?\n",
      "fp16\n",
      "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml"
     ]
    }
   ],
   "source": [
    "!accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En mi caso las respuestas han sido\n",
    " * In which compute environment are you running?\n",
    "    - [x] \"This machine\"\n",
    "    - [_] \"AWS (Amazon SageMaker)\"\n",
    " > Quiero configurarlo en mi ordenador\n",
    "\n",
    " * Which type of machine are you using?\n",
    "    - [_] multi-CPU\n",
    "    - [_] multi-XPU\n",
    "    - [x] multi-GPU\n",
    "    - [_] multi-NPU\n",
    "    - [_] TPU\n",
    " > Como tengo 2 GPUs y quiero ejecutar c√≥digos distribuidos en ellas elijo `multi-GPU`\n",
    " \n",
    " * How many different machines will you use (use more than 1 for multi-node training)? [1]:\n",
    "    - 1\n",
    " > Elijo `1` porque solo voy a ejecutar en mi ordenador\n",
    "\n",
    " * Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]:\n",
    "    - no\n",
    " > Con esta opci√≥n, se puede elegir que `accelerate` chequee errores en la ejecuci√≥n, pero har√≠a que vaya m√°s lento, as√≠ que elijo `no`, y en caso de que haya errores lo cambio a `yes`\n",
    " \n",
    " * Do you wish to optimize your script with torch dynamo?[yes/NO]:\n",
    "    - no\n",
    " > De momento elijo `no`, pero m√°s adelante veremos qu√© es esto\n",
    "\n",
    " * Do you want to use FullyShardedDataParallel? [yes/NO]:\n",
    "    - no\n",
    " > De momento elijo `no`, pero m√°s adelante veremos qu√© es esto\n",
    " \n",
    " * Do you want to use Megatron-LM ? [yes/NO]:\n",
    "    - no\n",
    " > De momento elijo `no`, pero m√°s adelante veremos qu√© es esto\n",
    " \n",
    " * How many GPU(s) should be used for distributed training? [1]:\n",
    "    - 2\n",
    " > Elijo `2` porque tengo 2 GPUs\n",
    "\n",
    " * What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:\n",
    "    - 0,1\n",
    " > Elijo `0,1` porque quiero usar las dos GPUs\n",
    "\n",
    " * Do you wish to use FP16 or BF16 (mixed precision)?\n",
    "    - [_] no\n",
    "    - [x] fp16\n",
    "    - [_] bf16\n",
    "    - [_] fp8\n",
    " > De momento elijo `fp16`, pero m√°s adelante veremos qu√© es esto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuraci√≥n se guardar√° en `~/.cache/huggingface/accelerate/default_config.yaml` y se puede modificar en cualquier momento. Vamos a ver qu√© hay dentro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_environment: LOCAL_MACHINE\n",
      "debug: false\n",
      "distributed_type: MULTI_GPU\n",
      "downcast_bf16: 'no'\n",
      "gpu_ids: 0,1\n",
      "machine_rank: 0\n",
      "main_training_function: main\n",
      "mixed_precision: fp16\n",
      "num_machines: 1\n",
      "num_processes: 2\n",
      "rdzv_backend: static\n",
      "same_network: true\n",
      "tpu_env: []\n",
      "tpu_use_cluster: false\n",
      "tpu_use_sudo: false\n",
      "use_cpu: false\n"
     ]
    }
   ],
   "source": [
    "!cat ~/.cache/huggingface/accelerate/default_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de ver la configuraci√≥n que tenemos es ejecutando en una terminal:\n",
    "\n",
    "``` bash\n",
    "accelerate env\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copy-and-paste the text below in your GitHub issue\n",
      "\n",
      "- `Accelerate` version: 0.28.0\n",
      "- Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31\n",
      "- Python version: 3.11.8\n",
      "- Numpy version: 1.26.4\n",
      "- PyTorch version (GPU?): 2.2.1+cu121 (True)\n",
      "- PyTorch XPU available: False\n",
      "- PyTorch NPU available: False\n",
      "- System RAM: 31.24 GB\n",
      "- GPU type: NVIDIA GeForce RTX 3090\n",
      "- `Accelerate` default config:\n",
      "\t- compute_environment: LOCAL_MACHINE\n",
      "\t- distributed_type: MULTI_GPU\n",
      "\t- mixed_precision: fp16\n",
      "\t- use_cpu: False\n",
      "\t- debug: False\n",
      "\t- num_processes: 2\n",
      "\t- machine_rank: 0\n",
      "\t- num_machines: 1\n",
      "\t- gpu_ids: 0,1\n",
      "\t- rdzv_backend: static\n",
      "\t- same_network: True\n",
      "\t- main_training_function: main\n",
      "\t- downcast_bf16: no\n",
      "\t- tpu_use_cluster: False\n",
      "\t- tpu_use_sudo: False\n",
      "\t- tpu_env: []\n"
     ]
    }
   ],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos configurado `accelerate` podemos probar si lo hemos hecho bien ejecutando en una terminal:\n",
    "\n",
    "``` bash\n",
    "accelerate test\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running:  accelerate-launch ~/miniconda3/envs/nlp/lib/python3.11/site-packages/accelerate/test_utils/scripts/test_script.py\n",
      "stdout: **Initialization**\n",
      "stdout: Testing, testing. 1, 2, 3.\n",
      "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
      "stdout: Num processes: 2\n",
      "stdout: Process index: 0\n",
      "stdout: Local process index: 0\n",
      "stdout: Device: cuda:0\n",
      "stdout: \n",
      "stdout: Mixed precision type: fp16\n",
      "stdout: \n",
      "stdout: Distributed environment: DistributedType.MULTI_GPU  Backend: nccl\n",
      "stdout: Num processes: 2\n",
      "stdout: Process index: 1\n",
      "stdout: Local process index: 1\n",
      "stdout: Device: cuda:1\n",
      "stdout: \n",
      "stdout: Mixed precision type: fp16\n",
      "stdout: \n",
      "stdout: \n",
      "stdout: **Test process execution**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a list**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a dict**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a tensor**\n",
      "stdout: \n",
      "stdout: **Test random number generator synchronization**\n",
      "stdout: All rng are properly synched.\n",
      "stdout: \n",
      "stdout: **DataLoader integration test**\n",
      "stdout: 0 1 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:1') <class 'accelerate.data_loader.DataLoaderShard'>\n",
      "stdout: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "stdout:         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "stdout:         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\n",
      "stdout: Non-shuffled dataloader passing.\n",
      "stdout: Shuffled dataloader passing.\n",
      "stdout: Non-shuffled central dataloader passing.\n",
      "stdout: Shuffled central dataloader passing.\n",
      "stdout: \n",
      "stdout: **Training integration test**\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
      "stdout: FP16 training check.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: \n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: FP16 training check.\n",
      "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: \n",
      "stdout: **Breakpoint trigger test**\n",
      "Test is a success! You are ready for your distributed training!\n"
     ]
    }
   ],
   "source": [
    "!accelerate test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que termina diciendo `Test is a success! You are ready for your distributed training!` por lo que todo est√° correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizaci√≥n del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√≥digo base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer primero un c√≥digo de entrenamiento base y luego lo optimizaremos  para ver c√≥mo se hace y c√≥mo mejora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a buscar un dataset, en mi caso voy a usar el dataset [tweet_eval](https://huggingface.co/datasets/tweet_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['‚ù§', 'üòç', 'üòÇ', 'üíï', 'üî•', 'üòä', 'üòé', '‚ú®', 'üíô', 'üòò', 'üì∑', 'üá∫üá∏', '‚òÄ', 'üíú', 'üòâ', 'üíØ', 'üòÅ', 'üéÑ', 'üì∏', 'üòú'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='tweet_eval', config_name='emoji', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=3808792, num_examples=45000, shard_lengths=None, dataset_name='tweet_eval'), 'test': SplitInfo(name='test', num_bytes=4262151, num_examples=50000, shard_lengths=None, dataset_name='tweet_eval'), 'validation': SplitInfo(name='validation', num_bytes=396704, num_examples=5000, shard_lengths=None, dataset_name='tweet_eval')}, download_checksums={'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/train-00000-of-00001.parquet': {'num_bytes': 2609973, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/test-00000-of-00001.parquet': {'num_bytes': 3047341, 'checksum': None}, 'hf://datasets/tweet_eval@b3a375baf0f409c77e6bc7aa35102b7b3534f8be/emoji/validation-00000-of-00001.parquet': {'num_bytes': 281994, 'checksum': None}}, download_size=5939308, post_processing_size=None, dataset_size=8467647, size_in_bytes=14406955)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ù§', 'üòç', 'üòÇ', 'üíï', 'üî•', 'üòä', 'üòé', '‚ú®', 'üíô', 'üòò', 'üì∑', 'üá∫üá∏', '‚òÄ', 'üíú', 'üòâ', 'üíØ', 'üòÅ', 'üéÑ', 'üì∏', 'üòú']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"].info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el dataset tiene 20 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver la secuencia m√°xima de cada split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 139, 167)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_train = 0\n",
    "max_len_val = 0\n",
    "max_len_test = 0\n",
    "\n",
    "split = \"train\"\n",
    "for i in range(len(dataset[split])):\n",
    "    len_i = len(dataset[split][i][\"text\"])\n",
    "    if len_i > max_len_train:\n",
    "        max_len_train = len_i\n",
    "split = \"validation\"\n",
    "for i in range(len(dataset[split])):\n",
    "    len_i = len(dataset[split][i][\"text\"])\n",
    "    if len_i > max_len_val:\n",
    "        max_len_val = len_i\n",
    "split = \"test\"\n",
    "for i in range(len(dataset[split])):\n",
    "    len_i = len(dataset[split][i][\"text\"])\n",
    "    if len_i > max_len_test:\n",
    "        max_len_test = len_i\n",
    "\n",
    "max_len_train, max_len_val, max_len_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠ que definimos la secuencia m√°ximo en general como 130 para la tokeniazci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nosotros nos interesa el dataset tokenizado, no con las secuencias en crudo, as√≠ que creamos un tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funci√≥n de tokenizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora tokenizamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83f90dc1d074012b5d099511986898e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c14557614545118c2ceb0a0ab6178c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1c71ced24c4b1ba6e13e6c0ba0e7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos ahora tenemos los tokens (`input_ids`) y las m√°scaras de atenci√≥n (`attention_mask`), pero vamos a ver qu√© tipo de datos tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list, int)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"]), type(tokenized_dataset[\"train\"][0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As√≠ que vamos a cargarnos la feature `text` y convertir a tensores el resto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "type(tokenized_dataset[\"train\"][0][\"label\"]), type(tokenized_dataset[\"train\"][0][\"input_ids\"]), type(tokenized_dataset[\"train\"][0][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "BS = 64\n",
    "\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver c√≥mo es el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver su √∫ltima capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.out_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.out_proj.in_features, model.classifier.out_proj.out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que nuestro dataset tiene 20 clases, as√≠ que tenemos que modificar la √∫ltima capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=20, bias=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "model.classifier.out_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora s√≠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos una funci√≥n de loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por √∫ltimo una m√©trica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver con una muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dataloader[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 130]), torch.Size([64, 130]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"input_ids\"].shape, sample[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "ouputs = model(input_ids=sample[\"input_ids\"].to(\"cuda\"), attention_mask=sample[\"attention_mask\"].to(\"cuda\"))\n",
    "ouputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.argmax(ouputs.logits, axis=-1)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9990389347076416"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_function(ouputs.logits, sample[\"label\"].to(\"cuda\"))\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = metric.compute(predictions=predictions, references=sample[\"label\"])[\"accuracy\"]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya podemos crear un peque√±o bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "epochs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "master_progress_bar = master_bar(range(epochs))\n",
    "for i in master_progress_bar:\n",
    "    model.train()\n",
    "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√≥digo base en una √∫nica celda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos un c√≥digo base, este es el c√≥digo base de un entrenamiento en pytorch, vamos a escribirlo todo en una √∫nica celda para ir cambi√°ndola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.2112\n",
      "CPU times: user 3min 27s, sys: 670 ms, total: 3min 27s\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 64\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "master_progress_bar = master_bar(range(EPOCHS))\n",
    "for i in master_progress_bar:\n",
    "    model.train()\n",
    "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
    "print(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√≥digo con accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora reemplazamos algunas cosas\n",
    "\n",
    "En primer lugar importamos `Accelerator` y lo inicializamos\n",
    "\n",
    "``` python\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "```\n",
    "\n",
    "Ya no hacemos el t√≠pico\n",
    "\n",
    "``` python \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora reemplazamos algunas cosas\n",
    "\n",
    " * En primer lugar importamos `Accelerator` y lo inicializamos\n",
    "\n",
    "``` python\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "```\n",
    "\n",
    " * Ya no hacemos el t√≠pico\n",
    "\n",
    "``` python \n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    " * Sino que dejamos que sea `acelerate` el que elija el dispositivo mediante\n",
    "\n",
    "``` python\n",
    "device = accelerator.device\n",
    "```\n",
    "\n",
    " * Pasamos los elementos relevantes para el entrenamiento por el m√©todo `prepare` y ya no hacemos `model.to(device)`\n",
    "\n",
    "``` python\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = preprare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "```\n",
    "\n",
    " * Ya no mandamos los datos y el modelo a la GPU con `.to(device)` ya que `accelerate` se ha encargado de ello con el m√©todo `prepare`\n",
    "\n",
    " * En vez de hacer el backpropagation con `loss.backward()` dejamos que lo haga `accelerate` con\n",
    " \n",
    "``` python\n",
    "accelerator.backward(loss)\n",
    "```\n",
    "\n",
    " * A la hora de calcular la m√©trica en el bucle de validaci√≥n, necesitamos recopilar los valores de todos los puntos, en caso de estar haciendo un entrenamiento distribuido, para ello hacemos\n",
    "\n",
    "``` python\n",
    "predictions = accelerator.gather_for_metrics(predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.2112\n",
      "CPU times: user 3min 31s, sys: 1.68 s, total: 3min 33s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import evaluate\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "# Importamos e inicializamos Accelerator\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "dataset = load_dataset(\"tweet_eval\", \"emoji\")\n",
    "num_classes = len(dataset[\"train\"].info.features[\"label\"].names)\n",
    "max_len = 130\n",
    "\n",
    "checkpoints = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoints)\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenized_dataset = {\n",
    "    \"train\": dataset[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"validation\": dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "    \"test\": dataset[\"test\"].map(tokenize_function, batched=True, remove_columns=[\"text\"]),\n",
    "}\n",
    "tokenized_dataset[\"train\"].set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset[\"validation\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_dataset[\"test\"].set_format(type=\"torch\", columns=['label', 'input_ids', 'attention_mask'])\n",
    "\n",
    "BS = 64\n",
    "dataloader = {\n",
    "    \"train\": DataLoader(tokenized_dataset[\"train\"], batch_size=BS, shuffle=True),\n",
    "    \"validation\": DataLoader(tokenized_dataset[\"validation\"], batch_size=BS, shuffle=True),\n",
    "    \"test\": DataLoader(tokenized_dataset[\"test\"], batch_size=BS, shuffle=True),\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints)\n",
    "model.classifier.out_proj = torch.nn.Linear(in_features=model.classifier.out_proj.in_features, out_features=num_classes, bias=True)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "EPOCHS = 1\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = accelerator.device\n",
    "\n",
    "# model.to(device)\n",
    "model, optimizer, dataloader[\"train\"], dataloader[\"validation\"] = accelerator.prepare(model, optimizer, dataloader[\"train\"], dataloader[\"validation\"])\n",
    "\n",
    "master_progress_bar = master_bar(range(EPOCHS))\n",
    "for i in master_progress_bar:\n",
    "    model.train()\n",
    "    progress_bar_train = progress_bar(dataloader[\"train\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]#.to(device)\n",
    "        attention_mask = batch[\"attention_mask\"]#.to(device)\n",
    "        labels = batch[\"label\"]#.to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_function(outputs['logits'], labels)\n",
    "        master_progress_bar.child.comment = f'loss: {loss}'\n",
    "\n",
    "        # loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    progress_bar_validation = progress_bar(dataloader[\"validation\"], parent=master_progress_bar)\n",
    "    for batch in progress_bar_validation:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], axis=-1)\n",
    "        # Recopilamos las predicciones de todos los dispositivos\n",
    "        predictions = accelerator.gather_for_metrics(predictions)\n",
    "\n",
    "        accuracy = metric.add_batch(predictions=predictions, references=labels)\n",
    "    accuracy = metric.compute()\n",
    "    \n",
    "    master_progress_bar.main_bar.comment = f\"Validation accuracy: {accuracy['accuracy']}\\n\"\n",
    "\n",
    "print(f\"Accuracy = {accuracy['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que antes tard√≥ unos 3 minutos y medio y ahora tambi√©n, es decir, parece que `accelerate` no ha hecho nada. Pero eso es por la configuraci√≥n que hemos puesto cuando hemos hecho `accelerate config`.\n",
    "\n",
    "Si lo vuelves a mirar, lo √∫nico que hicimos fue decirle que tenemos un ordenador con dos GPUs y ya est√°, no hemos dicho que queremos hacer nada distribuido. As√≠ que a partir de aqu√≠ iremos viendo c√≥mo hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
