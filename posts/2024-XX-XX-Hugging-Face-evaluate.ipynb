{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería `Evaluate` de `Hugging Face` es una librería para evaluar fácilmente modelos y datasets.\n",
    "\n",
    "Con una sola línea de código, se tiene acceso a docenas de métodos de evaluación para diferentes dominios (NLP, computer vision, reinforcement learning y más). Ya sea en tu máquina local, o en una configuración de entrenamiento distribuida, puedes evaluar modelos de manera consistente y reproducible\n",
    "\n",
    "En la página de [evaluate](https://huggingface.co/evaluate-metric) en Hugging Face se puede obtener una lista completa de las métricas disponibles. Cada métrica tiene un espacio dedicado con una demostración interactiva sobre cómo usar la métrica y una tarjeta de documentación que detalla las limitaciones y el uso de las métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar la librería es necesario hacer \n",
    "\n",
    "``` bash\n",
    "pip install evaluate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipo de evaluaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varios tipos de evaluaciones disponibles\n",
    "\n",
    " * `metric`: Una métrica se utiliza para evaluar el rendimiento de un modelo y, por lo general, incluye las predicciones del modelo y las etiquetas ground truth.\n",
    " * `comparison`: Se utiliza para comparar dos modelos. Esto se puede hacer, por ejemplo, comparando sus predicciones con etiquetas ground truth.\n",
    " * `measurement`: El dataset es tan importante como el modelo entrenado en él. Con las mediciones se pueden investigar las propiedades de un dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada `metric`, `comparison` o `measurement` se puede cargar con el método load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.5}\n",
       "\n",
       "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
       "        >>> print(results)\n",
       "        {'accuracy': 3.0}\n",
       "\n",
       "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.8778625954198473}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quieres estar seguro de cargar el tipo de métrica que deseas, si tipo `metric`, `comparison` o `measurement`, puedes hacerlo añadiendo el parámetro `module_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/maximo.fernandez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\", module_type=\"metric\")\n",
    "word_length = evaluate.load(\"word_length\", module_type=\"measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de módulos de la comunidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parte de los propios módulos que ofrece la librería, también puedes cargar modelos que haya subido alguién al hub de Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_count = evaluate.load(\"lvwerra/element_count\", module_type=\"measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista de módulos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos obtener una lista de todos los módulos disponibles tenemos que usar el método `list_evaluation_modules`, en el podemos poner filtros de búsqueda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ncoop57/levenshtein_distance',\n",
       "  'type': 'comparison',\n",
       "  'community': True,\n",
       "  'likes': 0},\n",
       " {'name': 'kaleidophon/almost_stochastic_order',\n",
       "  'type': 'comparison',\n",
       "  'community': True,\n",
       "  'likes': 1}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.list_evaluation_modules(\n",
    "  module_type=\"comparison\",\n",
    "  include_community=True,\n",
    "  with_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos del módulo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los módulos de evaluación vienen con una variedad de atributos útiles que ayudan a utilizar el módulo, estos atributos son\n",
    "\n",
    "|Atributo|Descripción|\n",
    "|---|---|\n",
    "|description|Una breve descripción del módulo de evaluación.|\n",
    "|citation|Una cadena BibTex para citar cuando esté disponible.|\n",
    "|features|Un objeto Features que define el formato de entrada.|\n",
    "|inputs_description|Esto es equivalente a la cadena de documentación de los módulos.|\n",
    "|homepage|La página de inicio del módulo.|\n",
    "|license|La licencia del módulo.|\n",
    "|codebase_urls|Enlace al código detrás del módulo.|\n",
    "|reference_urls|URL de referencia adicionales.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos algunos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description: \n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n",
      "\n",
      "citation: \n",
      "@article{scikit-learn,\n",
      "  title={Scikit-learn: Machine Learning in {P}ython},\n",
      "  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n",
      "         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n",
      "         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n",
      "         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n",
      "  journal={Journal of Machine Learning Research},\n",
      "  volume={12},\n",
      "  pages={2825--2830},\n",
      "  year={2011}\n",
      "}\n",
      "\n",
      "\n",
      "features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}\n",
      "\n",
      "inputs_description: \n",
      "Args:\n",
      "    predictions (`list` of `int`): Predicted labels.\n",
      "    references (`list` of `int`): Ground truth labels.\n",
      "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
      "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
      "\n",
      "Returns:\n",
      "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple example\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.5}\n",
      "\n",
      "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
      "        >>> print(results)\n",
      "        {'accuracy': 3.0}\n",
      "\n",
      "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.8778625954198473}\n",
      "\n",
      "\n",
      "homepage: \n",
      "\n",
      "license: \n",
      "\n",
      "codebase_urls: []\n",
      "\n",
      "reference_urls: ['https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html']\n"
     ]
    }
   ],
   "source": [
    "print(f\"description: {accuracy.description}\")\n",
    "print(f\"\\ncitation: {accuracy.citation}\")\n",
    "print(f\"\\nfeatures: {accuracy.features}\")\n",
    "print(f\"\\ninputs_description: {accuracy.inputs_description}\")\n",
    "print(f\"\\nhomepage: {accuracy.homepage}\")\n",
    "print(f\"\\nlicense: {accuracy.license}\")\n",
    "print(f\"\\ncodebase_urls: {accuracy.codebase_urls}\")\n",
    "print(f\"\\nreference_urls: {accuracy.reference_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que sabemos cómo funciona el módulo de evaluación y qué debe contener, vamos a usarlo. Cuando se trata de calcular la evaluación, hay dos formas principales de hacerlo:\n",
    "\n",
    " * Todo en uno\n",
    " * Incremental\n",
    "\n",
    "En el enfoque incremental, las entradas necesarias se agregan al módulo con `EvaluationModule.add()` o `EvaluationModule.add_batch()` y la puntuación se calcula al final con `EvaluationModule.compute()`. Alternativamente, se pueden pasar todas las entradas a la vez a `compute()`.\n",
    "\n",
    "Veamos estos dos enfoques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo en uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos todas las predicciones y ground truths podemos calcular la métrica. Una vez que tenemos un módulo definido, le pasamos las predicciones y los ground truth mediante el método `compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [1, 0, 0, 1]\n",
    "targets = [0, 1, 0, 1]\n",
    "\n",
    "accuracy_value = accuracy.compute(predictions=predictions, references=targets)\n",
    "accuracy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En muchos procesos de evaluación, las predicciones se construyen de forma iterativa, como en un bucle for. En ese caso, podrías almacenar las predicciones y ground truths en una lista y al final pasarlas a `compute()`.\n",
    "\n",
    "Sin embargo con los métodos `add()` y `add_batch()` puedes evitar el paso de almacenar las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tienes todas las predicciones de un solo batch hay que usar el método `add()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ref, pred in zip([0,1,0,1], [1,0,0,1]):\n",
    "    accuracy.add(references=ref, predictions=pred)\n",
    "accuracy_value = accuracy.compute()\n",
    "accuracy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, cuando se tienen predicciones de varios batches se tiene que usar el método `add_batch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):\n",
    "    accuracy.add_batch(references=refs, predictions=preds)\n",
    "accuracy_value = accuracy.compute()\n",
    "accuracy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinación de varias evaluaciones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
