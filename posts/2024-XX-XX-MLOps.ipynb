{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLOps viene de DevOps y son las prácticas y herramientas que se utilizan para mejorar la colaboración y la comunicación entre los equipos de desarrollo de software y los equipos de operaciones.\n",
    "\n",
    "En el caso de MLOps, se refiere a las prácticas y herramientas que se utilizan para mejorar la colaboración y la comunicación entre los equipos de desarrollo de modelos de machine learning y los equipos de operaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estapas de MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas son las etapas de MLOps:\n",
    "\n",
    "![Etapas MLOps](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mlops_operations.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a explicarlas:\n",
    "\n",
    " * Diseño: En esta etapa se define el problema que se va a resolver y se recopilan los datos necesarios para resolverlo.\n",
    " * Desarrollo del modelo: En esta etapa se entrena el modelo y se evalúa su rendimiento.\n",
    " * Operaciones: En esta etapa se despliega el modelo en producción, se crean los pipelines de CI/CD y se monitoriza el rendimiento del modelo.\n",
    "\n",
    "Como se puede ver no son etapas cerradas, sino que nos podemos mover de una a otra en cualquier momento como se puede ver en la siguiente imagen:\n",
    "\n",
    "![mlops operations](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mlops_operations2.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Componentes de MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos son los componentes de MLOps:\n",
    "\n",
    " * Control de versiones: Git\n",
    " * CI/CD: GitHub Actions, Jenkins, GitLab CI\n",
    " * Orquestación de pipelines: DVC, Prefect, Hydra\n",
    " * Model y container registry: MLflow\n",
    " * Compute serving: Batch serving, Real-time serving\n",
    " * Monitoreo: Grafana, Prometheus, Kibana\n",
    "\n",
    "Con estos componentes vamos a poder implementar MLOps en un proyecto de machine learning, en el que entrenaremos un modelo de machine learning y lo desplegaremos en producción, a través de una API lo vamos a poder consumir, mientras que por otro lado vamos a almacenar las salidas del modelo en una base de datos para monitorizar su rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Para qué sirve el tracking en MLOps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver en la siguiente imagen, en MLOps tenemos varias etapas, por un lado la preparación de los datos, la creación del modelo, el despliegue del modelo en producción y el monitoreo del modelo\n",
    "\n",
    "![mlops tracking](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mlops_tracking_models.webp)\n",
    "\n",
    "Por lo que es importante trackear todo, tanto con qué datos estamos haciendo el entrenamiento, como el modelo que estamos entrenando, como el rendimiento del modelo en producción. El tracking es importante para tener trazabilidad, porque a lo largo de un desarrollo hay tantas posibles operaciones que se pueden hacer que es importante tener un registro de todo lo que se ha hecho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esto podemos usar `mlflow`, que es una herramienta que nos permite trackear todo lo que hacemos en un proyecto de machine learning, desde la preparación de los datos, el entrenamiento del modelo, el despliegue del modelo en producción y el monitoreo del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, como en un desarrollo puede haber varias personas, es importante tener un registro de todo lo que se ha hecho, para que si alguien tiene una duda, pueda ver todo lo que se ha hecho y por qué se ha hecho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El éxito de un flujo de MLOps radica en la mayor automatización de procesos posible, para que los equipos de desarrollo y operaciones puedan centrarse en lo que realmente importa, que es la creación de modelos de machine learning y su despliegue en producción. Además evitando errores humanos en las tareas repetitivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante almacenar todos los metadatos posibles, porque eso va a ayudar a tener una trazabilidad de todo lo que se ha hecho en un proyecto de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar `mlflow` podemos hacerlo con `conda`:\n",
    "\n",
    "```bash\n",
    "conda install conda-forge::mlflow\n",
    "```\n",
    "\n",
    "O con `pip`:\n",
    "\n",
    "```bash\n",
    "pip install mlflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalar el resto de las dependencias. Como vamos a hacer el fine tuning de un modelo habría que instalar `pytorch`, `transformers`, `evaluate`, `bitsandbytes`, `accelerate` y `datasets`:\n",
    "\n",
    "```bash\n",
    "conda install -y pytorch torchvision pytorch-cuda=12.4 -c pytorch -c nvidia\n",
    "pip install evaluate bitsandbytes accelerate datasets transformers -q\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking de experimentos con MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21e8903975b400cbf3dc9eca088de8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d2ccae72344f46b94fa8e0f34c7a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/359k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a807bb7f869f4debac47486fdcc7a7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sms_dataset = load_dataset('sms_spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo dividimos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_train_test = sms_dataset['train'].train_test_split(test_size=0.2)\n",
    "train_dataset = sms_train_test['train']\n",
    "test_dataset = sms_train_test['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos una muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What does the dance river do?\\n', 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "idx = randint(0, len(train_dataset))\n",
    "train_dataset[idx]['sms'], train_dataset[idx]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver las clases que hay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5877039a3ed146e7916541836ffa1f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.unique('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el dataset clasifica con `0` y `1` si un sms es spam o no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90f169d77aa4e16b4348ae7d15bf60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb44f060c144aea865ed6b3a04f0b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6802b078f04edb92bf64f3b5106143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9367d6ecc434099998114a9469e504b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función de tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sms'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probarla con un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), torch.Size([1, 128]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize_function(train_dataset[idx])\n",
    "tokens.input_ids.shape, tokens.attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizaos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1be8fa0c18e459488202f9154efa410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8545ddeb05aa480a8530a4d86274d7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 22\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True, remove_columns=['sms']).shuffle(seed=seed)\n",
    "test_tokenized = test_dataset.map(tokenize_function, batched=True, remove_columns=['sms']).shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos diccionarios para pasar de id a la etiqueta y de la etiqueta al id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'ham', 1: 'spam'}\n",
    "label2id = {'ham': 0, 'spam': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c2d9522dfb423a83abd20358c61c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased', \n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la función de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuramos el trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallabot/miniconda3/envs/mlops/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_ouput_path = 'sms_trainer'\n",
    "trainig_args = TrainingArguments(\n",
    "    output_dir=training_ouput_path,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=8,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainig_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un tracker de mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos que ejecutar esto en una terminal para iniciar el servidor de mlflow:\n",
    "\n",
    "```bash\n",
    "mlflow server --port 5000 --host 0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('http://localhost:5000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora abrimos un navegador y vamos a [http://localhost:5000](http://localhost:5000) veremos el dashboard de mlflow\n",
    "\n",
    "![mlflow dashboard](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mlflow_server.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asignamos un nombre al experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/06 19:39:48 INFO mlflow.tracking.fluent: Experiment with name 'SMS Spam Classification' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/869247621359487510', creation_time=1730918388725, experiment_id='869247621359487510', last_update_time=1730918388725, lifecycle_stage='active', name='SMS Spam Classification', tags={}>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('SMS Spam Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora aparecerá en el dashboard de mlflow un nuevo experimento llamado `SMS Spam Classification`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mlflow está muy bien integrada con la mayoría de las librerías de machine learning\n",
    "\n",
    "![mlflow integrations](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mlflow_integrations.webp)\n",
    "\n",
    "Por lo que para empezar el entrenamiento solo tenemos que hacer lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab3db45ef36496aa21c8ef50e09aa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallabot/miniconda3/envs/mlops/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4455, 'grad_norm': 1.8081084489822388, 'learning_rate': 4.95221027479092e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1221, 'grad_norm': 0.669918417930603, 'learning_rate': 4.90442054958184e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0322, 'grad_norm': 11.449009895324707, 'learning_rate': 4.8566308243727596e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0322, 'grad_norm': 0.7715699672698975, 'learning_rate': 4.80884109916368e-05, 'epoch': 0.11}\n",
      "{'loss': 0.005, 'grad_norm': 0.8292357921600342, 'learning_rate': 4.7610513739546e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0037, 'grad_norm': 0.02695838175714016, 'learning_rate': 4.71326164874552e-05, 'epoch': 0.17}\n",
      "{'loss': 0.1845, 'grad_norm': 0.05402424931526184, 'learning_rate': 4.66547192353644e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0175, 'grad_norm': 0.0833834782242775, 'learning_rate': 4.61768219832736e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0877, 'grad_norm': 13.283378601074219, 'learning_rate': 4.56989247311828e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0078, 'grad_norm': 0.08746110647916794, 'learning_rate': 4.5221027479092e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0262, 'grad_norm': 0.46065542101860046, 'learning_rate': 4.4743130227001195e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1009, 'grad_norm': 1.6743664741516113, 'learning_rate': 4.42652329749104e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1952, 'grad_norm': 8.667765617370605, 'learning_rate': 4.378733572281959e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0484, 'grad_norm': 0.26116499304771423, 'learning_rate': 4.3309438470728796e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0449, 'grad_norm': 0.14275887608528137, 'learning_rate': 4.283154121863799e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0179, 'grad_norm': 0.046592723578214645, 'learning_rate': 4.2353643966547194e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0519, 'grad_norm': 0.06788275390863419, 'learning_rate': 4.1875746714456396e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0747, 'grad_norm': 0.05998223274946213, 'learning_rate': 4.13978494623656e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0394, 'grad_norm': 0.05431315675377846, 'learning_rate': 4.0919952210274794e-05, 'epoch': 0.54}\n",
      "{'loss': 0.1099, 'grad_norm': 0.21424992382526398, 'learning_rate': 4.0442054958183996e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0793, 'grad_norm': 2.7466776371002197, 'learning_rate': 3.996415770609319e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0461, 'grad_norm': 0.0973486602306366, 'learning_rate': 3.9486260454002395e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0048, 'grad_norm': 0.4035756587982178, 'learning_rate': 3.900836320191159e-05, 'epoch': 0.66}\n",
      "{'loss': 0.103, 'grad_norm': 1.8360981941223145, 'learning_rate': 3.8530465949820786e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0079, 'grad_norm': 1.4558252096176147, 'learning_rate': 3.805256869772999e-05, 'epoch': 0.72}\n",
      "{'loss': 0.124, 'grad_norm': 0.08928893506526947, 'learning_rate': 3.7574671445639184e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0964, 'grad_norm': 0.2028975933790207, 'learning_rate': 3.7096774193548386e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0089, 'grad_norm': 0.179122194647789, 'learning_rate': 3.661887694145759e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0165, 'grad_norm': 0.03858159855008125, 'learning_rate': 3.614097968936679e-05, 'epoch': 0.83}\n",
      "{'loss': 0.1324, 'grad_norm': 0.09463584423065186, 'learning_rate': 3.566308243727599e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0026, 'grad_norm': 0.04126795008778572, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0448, 'grad_norm': 0.12471386045217514, 'learning_rate': 3.4707287933094385e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0377, 'grad_norm': 0.5049582719802856, 'learning_rate': 3.422939068100359e-05, 'epoch': 0.95}\n",
      "{'loss': 0.0416, 'grad_norm': 0.04534032940864563, 'learning_rate': 3.375149342891278e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277cc512d97743de8d668db07118c6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.051197364926338196, 'eval_accuracy': 0.9883408071748879, 'eval_runtime': 3.7395, 'eval_samples_per_second': 298.165, 'eval_steps_per_second': 18.719, 'epoch': 1.0}\n",
      "{'loss': 0.0019, 'grad_norm': 0.039650414139032364, 'learning_rate': 3.3273596176821985e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallabot/miniconda3/envs/mlops/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'grad_norm': 0.029194805771112442, 'learning_rate': 3.279569892473118e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0381, 'grad_norm': 0.024087436497211456, 'learning_rate': 3.231780167264038e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0014, 'grad_norm': 0.027520226314663887, 'learning_rate': 3.183990442054958e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0476, 'grad_norm': 0.1449233889579773, 'learning_rate': 3.136200716845878e-05, 'epoch': 1.12}\n",
      "{'loss': 0.004, 'grad_norm': 0.0265234112739563, 'learning_rate': 3.0884109916367984e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0012, 'grad_norm': 0.017263587564229965, 'learning_rate': 3.0406212664277183e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0393, 'grad_norm': 0.021364768967032433, 'learning_rate': 2.9928315412186382e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0009, 'grad_norm': 0.023421071469783783, 'learning_rate': 2.9450418160095584e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0027, 'grad_norm': 0.030992012470960617, 'learning_rate': 2.897252090800478e-05, 'epoch': 1.26}\n",
      "{'loss': 0.038, 'grad_norm': 0.0179757010191679, 'learning_rate': 2.8494623655913982e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0522, 'grad_norm': 0.017976071685552597, 'learning_rate': 2.8016726403823178e-05, 'epoch': 1.32}\n",
      "{'loss': 0.059, 'grad_norm': 1.284709095954895, 'learning_rate': 2.753882915173238e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0117, 'grad_norm': 4.161077976226807, 'learning_rate': 2.706093189964158e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0379, 'grad_norm': 0.02316894754767418, 'learning_rate': 2.6583034647550775e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0052, 'grad_norm': 0.01285792887210846, 'learning_rate': 2.6105137395459977e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0008, 'grad_norm': 0.00965092983096838, 'learning_rate': 2.5627240143369173e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0014, 'grad_norm': 0.010399609804153442, 'learning_rate': 2.5149342891278375e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0593, 'grad_norm': 0.01008743979036808, 'learning_rate': 2.4671445639187578e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0012, 'grad_norm': 0.023504016920924187, 'learning_rate': 2.4193548387096777e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0557, 'grad_norm': 0.03718645125627518, 'learning_rate': 2.3715651135005976e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0098, 'grad_norm': 0.03115892969071865, 'learning_rate': 2.3237753882915175e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0016, 'grad_norm': 0.01849350519478321, 'learning_rate': 2.2759856630824374e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0507, 'grad_norm': 3.396381139755249, 'learning_rate': 2.2281959378733573e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0008, 'grad_norm': 0.021560445427894592, 'learning_rate': 2.1804062126642775e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0018, 'grad_norm': 0.021567201241850853, 'learning_rate': 2.132616487455197e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0551, 'grad_norm': 0.03570172190666199, 'learning_rate': 2.084826762246117e-05, 'epoch': 1.75}\n",
      "{'loss': 0.0011, 'grad_norm': 0.0330757275223732, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallabot/miniconda3/envs/mlops/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0467, 'grad_norm': 0.02618269994854927, 'learning_rate': 1.989247311827957e-05, 'epoch': 1.81}\n",
      "{'loss': 0.0464, 'grad_norm': 0.15360097587108612, 'learning_rate': 1.941457586618877e-05, 'epoch': 1.84}\n",
      "{'loss': 0.0181, 'grad_norm': 0.02892197109758854, 'learning_rate': 1.893667861409797e-05, 'epoch': 1.86}\n",
      "{'loss': 0.001, 'grad_norm': 0.014254506677389145, 'learning_rate': 1.845878136200717e-05, 'epoch': 1.89}\n",
      "{'loss': 0.0529, 'grad_norm': 0.026519518345594406, 'learning_rate': 1.7980884109916368e-05, 'epoch': 1.92}\n",
      "{'loss': 0.026, 'grad_norm': 0.07904544472694397, 'learning_rate': 1.7502986857825567e-05, 'epoch': 1.95}\n",
      "{'loss': 0.0158, 'grad_norm': 0.08638814091682434, 'learning_rate': 1.702508960573477e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05658b62cf7d4759b1d0e26871af7aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04106992855668068, 'eval_accuracy': 0.9883408071748879, 'eval_runtime': 3.7469, 'eval_samples_per_second': 297.581, 'eval_steps_per_second': 18.682, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallabot/miniconda3/envs/mlops/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0107, 'grad_norm': 0.05254911631345749, 'learning_rate': 1.6547192353643968e-05, 'epoch': 2.01}\n",
      "{'loss': 0.0225, 'grad_norm': 0.05086367204785347, 'learning_rate': 1.6069295101553167e-05, 'epoch': 2.04}\n",
      "{'loss': 0.0026, 'grad_norm': 0.01178144384175539, 'learning_rate': 1.5591397849462366e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0008, 'grad_norm': 0.010033085942268372, 'learning_rate': 1.5113500597371565e-05, 'epoch': 2.09}\n",
      "{'loss': 0.0007, 'grad_norm': 0.009925906546413898, 'learning_rate': 1.4635603345280766e-05, 'epoch': 2.12}\n",
      "{'loss': 0.0254, 'grad_norm': 0.012725817039608955, 'learning_rate': 1.4157706093189965e-05, 'epoch': 2.15}\n",
      "{'loss': 0.0242, 'grad_norm': 0.009354802779853344, 'learning_rate': 1.3679808841099166e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0046, 'grad_norm': 2.219235420227051, 'learning_rate': 1.3201911589008365e-05, 'epoch': 2.21}\n",
      "{'loss': 0.001, 'grad_norm': 0.02153380960226059, 'learning_rate': 1.2724014336917564e-05, 'epoch': 2.24}\n",
      "{'loss': 0.0014, 'grad_norm': 0.01680711843073368, 'learning_rate': 1.2246117084826763e-05, 'epoch': 2.27}\n",
      "{'loss': 0.0022, 'grad_norm': 0.2790946662425995, 'learning_rate': 1.1768219832735962e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0005, 'grad_norm': 0.02129960060119629, 'learning_rate': 1.129032258064516e-05, 'epoch': 2.32}\n",
      "{'loss': 0.0008, 'grad_norm': 0.008115025237202644, 'learning_rate': 1.0812425328554361e-05, 'epoch': 2.35}\n",
      "{'loss': 0.0005, 'grad_norm': 0.05357383191585541, 'learning_rate': 1.033452807646356e-05, 'epoch': 2.38}\n",
      "{'loss': 0.0005, 'grad_norm': 0.020236244425177574, 'learning_rate': 9.856630824372761e-06, 'epoch': 2.41}\n",
      "{'loss': 0.0218, 'grad_norm': 0.009720196016132832, 'learning_rate': 9.37873357228196e-06, 'epoch': 2.44}\n",
      "{'loss': 0.0005, 'grad_norm': 0.016726179048419, 'learning_rate': 8.90083632019116e-06, 'epoch': 2.47}\n",
      "{'loss': 0.0009, 'grad_norm': 0.02544545754790306, 'learning_rate': 8.42293906810036e-06, 'epoch': 2.49}\n",
      "{'loss': 0.0008, 'grad_norm': 0.009576977230608463, 'learning_rate': 7.945041816009559e-06, 'epoch': 2.52}\n",
      "{'loss': 0.002, 'grad_norm': 0.01419459655880928, 'learning_rate': 7.467144563918758e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0199, 'grad_norm': 0.040664881467819214, 'learning_rate': 6.989247311827957e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0028, 'grad_norm': 0.006964878179132938, 'learning_rate': 6.511350059737156e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0202, 'grad_norm': 0.0065557388588786125, 'learning_rate': 6.033452807646357e-06, 'epoch': 2.64}\n",
      "{'loss': 0.0004, 'grad_norm': 0.008270186372101307, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}\n",
      "{'loss': 0.0012, 'grad_norm': 0.008812004700303078, 'learning_rate': 5.077658303464755e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0009, 'grad_norm': 0.006754722446203232, 'learning_rate': 4.599761051373955e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0005, 'grad_norm': 0.010569144040346146, 'learning_rate': 4.121863799283155e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0013, 'grad_norm': 0.023501921445131302, 'learning_rate': 3.643966547192354e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0005, 'grad_norm': 0.00815554242581129, 'learning_rate': 3.1660692951015535e-06, 'epoch': 2.81}\n",
      "{'loss': 0.0062, 'grad_norm': 0.008610575459897518, 'learning_rate': 2.688172043010753e-06, 'epoch': 2.84}\n",
      "{'loss': 0.0005, 'grad_norm': 0.006536387838423252, 'learning_rate': 2.2102747909199524e-06, 'epoch': 2.87}\n",
      "{'loss': 0.0004, 'grad_norm': 0.018808409571647644, 'learning_rate': 1.7323775388291518e-06, 'epoch': 2.9}\n",
      "{'loss': 0.0004, 'grad_norm': 0.006726319435983896, 'learning_rate': 1.2544802867383513e-06, 'epoch': 2.92}\n",
      "{'loss': 0.0004, 'grad_norm': 0.008157391101121902, 'learning_rate': 7.765830346475508e-07, 'epoch': 2.95}\n",
      "{'loss': 0.0004, 'grad_norm': 0.006602972745895386, 'learning_rate': 2.9868578255675034e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallabot/miniconda3/envs/mlops/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8936380c972b461392e660dd9f7cc889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04692317917943001, 'eval_accuracy': 0.9910313901345291, 'eval_runtime': 3.7624, 'eval_samples_per_second': 296.357, 'eval_steps_per_second': 18.605, 'epoch': 3.0}\n",
      "{'train_runtime': 112.3026, 'train_samples_per_second': 119.116, 'train_steps_per_second': 7.453, 'train_loss': 0.03214372153195865, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/06 19:47:38 INFO mlflow.tracking._tracking_service.client: 🏃 View run nosy-crane-928 at: http://localhost:5000/#/experiments/869247621359487510/runs/0b2646cefe97455c807f4a21cefd19c6.\n",
      "2024/11/06 19:47:38 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/869247621359487510.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos definido el entrenamiento como `run` con esta línea `with mlflow.start_run() as run:`, ahora podemos obtener información de la ejecución con `run.info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RunInfo: artifact_uri='mlflow-artifacts:/869247621359487510/0b2646cefe97455c807f4a21cefd19c6/artifacts', end_time=None, experiment_id='869247621359487510', lifecycle_stage='active', run_id='0b2646cefe97455c807f4a21cefd19c6', run_name='nosy-crane-928', run_uuid='0b2646cefe97455c807f4a21cefd19c6', start_time=1730918744527, status='RUNNING', user_id='wallabot'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un `pipeline` de la librería `transformers` con el modelo recién entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tuned_pipeline = pipeline(\n",
    "    task='text-classification',\n",
    "    model=trainer.model,\n",
    "    batch_size=8,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo con un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'ham', 'score': 0.9975218176841736}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "I have a question regardding the project development timeline and allocated resources;\n",
    "specifically, how certain are you that John and Ringo can work together on writing this next song?\n",
    "Do we need to get Paul involved here, or do you truly believe, as you said, 'nah, they got this'?\n",
    "\"\"\"\n",
    "\n",
    "tuned_pipeline(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo clasifica como no spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una firma del modelo que mlflow entiende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_for_mlflow = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_config = {\n",
    "    \"batch_size\": 8,\n",
    "    \"device\": device_for_mlflow,\n",
    "}\n",
    "\n",
    "signature = mlflow.models.infer_signature(\n",
    "    [\"This is a test!\", \"And this is also a test!\"],\n",
    "    mlflow.transformers.generate_signature_output(\n",
    "        tuned_pipeline, [\"This is a test response!\", \"So is this.\"]\n",
    "    ),\n",
    "    params=model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora logueamos esa firma en mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6d221031cb4a948ed2a2df3c6164ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eca3d182d384f979a45be353c0b3c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/11.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/06 20:04:30 INFO mlflow.tracking._tracking_service.client: 🏃 View run nosy-crane-928 at: http://localhost:5000/#/experiments/869247621359487510/runs/0b2646cefe97455c807f4a21cefd19c6.\n",
      "2024/11/06 20:04:30 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/869247621359487510.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_id=run.info.run_id):\n",
    "    model_info = mlflow.transformers.log_model(\n",
    "        transformers_model=tuned_pipeline,\n",
    "        artifact_path=\"fine_tuned\",\n",
    "        signature=signature,\n",
    "        model_config=model_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto logeamos el modelo en mlflow. Si nos vamos al dashboard de mlflow veremos que aparece el modelo en la pestaña `Artifacts` del `run`. Esto además hace que este modelo se pueda exportar a otros sitios. Como mlflow tiene integraciones con casi todas las librerías de machine learning, podemos exportar este modelo a cualquier otra librería de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra ventaja de esto es que podemos tener un control de versiones del modelo, por lo que si en un futuro queremos volver a un modelo anterior, podemos hacerlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo desde el artifact de mlflow y hacemos inferencia con él"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero eliminamos el modelo y el pipeline para asegurarnos que hacemos inferencia con el modelo que hemos guardado en mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tuned_pipeline, trainer, signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd1b547c65f4a9d8fed6175d049f59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/06 20:14:27 INFO mlflow.transformers: 'runs:/0b2646cefe97455c807f4a21cefd19c6/fine_tuned' resolved as 'mlflow-artifacts:/869247621359487510/0b2646cefe97455c807f4a21cefd19c6/artifacts/fine_tuned'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c296278915453ca1fd7060559c241c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/06 20:14:27 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    }
   ],
   "source": [
    "model_loaded = mlflow.transformers.load_model(model_uri=model_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y probamos a hacer inferencia con él"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'spam', 'score': 0.9834991097450256}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_text = \"\"\"\n",
    "Want to learn how to make millions with no effort? Click here now! See for yourself! Guaranteed to make you instantly rich!\n",
    "Don't miss out you could be a winner!\n",
    "\"\"\"\n",
    "\n",
    "model_loaded(validation_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que funciona bien y lo marca como spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y además vemos que lo ha cargado en la GPU, que es lo que le pasamos en `model_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último paramos el servidor que habáismo levantado en la terminal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
