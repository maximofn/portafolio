{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Import model](https://github.com/ollama/ollama/blob/main/docs/import.md)\n",
    "\n",
    "[Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "\n",
    "[API](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "\n",
    "[API Python](https://github.com/ollama/ollama-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama es un software basado en [llama.cpp](https://github.com/ggerganov/llama.cpp) para la inferencia de LLMs.\n",
    "\n",
    "[llama.cpp](https://github.com/ggerganov/llama.cpp) es una librería para la inferencia de LLMs en C/C++, de modo que Ollama se ha construido por encima haciendo que el uso sea más sencillo para cualquier tipo de usuario. Es decir, es un wrapper para poder usar `llama.cpp` con una API de Python, JavaScript y terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar ollama en windows o mac tienes que descargarte un instalador desde su página de [descargas](https://ollama.com/download), sin embargo, si usas linux, como es mi caso, tienes que ejecutar el siguiente comando\n",
    "\n",
    "``` bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para correr un LLM basta con ejecutar `ollama run <LLM>`, por ejemplo puedo usar `llama3` mediante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling 6a0746a1ec1a... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB\n",
      "pulling 4fa551d4f938... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  12 KB\n",
      "pulling 8ab4849b038c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  254 B\n",
      "pulling 577073ffcc6c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  110 B\n",
      "pulling 3f8eb4da87fa... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  485 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success\n",
      ">>> hola\n",
      "Hola! ¿Cómo estás?\n",
      "\n",
      ">>> Send a message (/? for help)\n"
     ]
    }
   ],
   "source": [
    "!ollama run llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver ollama funciona como docker, como no tenía descargado llama3, se lo ha descargado y luego podemos usarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos saber qué modelos hay disponibles podemos ir a su página de [modelos](https://ollama.com/library), donde se pueden ver todos los modelos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargar un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos descargarnos un modelo, lo haremos igual que con docker `ollama pull <LLM>`, por ejemplo, voy a descargarme `phi3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling 4fed7364ee3e... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 2.3 GB\n",
      "pulling c608dc615584... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  149 B\n",
      "pulling fa8235e5b48f... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB\n",
      "pulling d47ab88b61ba... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  140 B\n",
      "pulling f7eda1da5a81... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  485 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama pull phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ayuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos ver todos los comandos que tenemos disponibles podemos usar `ollama -h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language model runner\n",
      "\n",
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "!ollama -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar los modelos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver en la ayuda que si hacemos `ollama list` podremos ver todos los modelos disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         \tID          \tSIZE  \tMODIFIED       \n",
      "phi3:latest  \ta2c89ceaed85\t2.3 GB\t4 minutes ago \t\n",
      "llama3:latest\t365c0bd3c000\t4.7 GB\t10 minutes ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y como vemos solo tenemos los dos que hemos descargado hasta el momento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver los modelos corriendo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hacemos `ollama ps` podemos ver los modelos que tenemos corriendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         \tID          \tSIZE  \tPROCESSOR\tUNTIL              \n",
      "llama3:latest\t365c0bd3c000\t5.4 GB\t100% GPU \t3 minutes from now\t\n"
     ]
    }
   ],
   "source": [
    "!ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos corriendo llama3 porque al principio hicimos `ollama run llama3`, pero si ahora lo intentamos con `phi3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Send a message (/? for help)\n"
     ]
    }
   ],
   "source": [
    "!ollama run phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora volvemos a hacer `ollama ps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       \tID          \tSIZE  \tPROCESSOR\tUNTIL              \n",
      "phi3:latest\ta2c89ceaed85\t3.8 GB\t100% GPU \t3 minutes from now\t\n"
     ]
    }
   ],
   "source": [
    "!ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el modelo que está corriendo es `phi3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos multimodales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lista de modelos de ollama hay modelos multimodales, como [moondream2](https://ollama.com/library/moondream), que siendo un modelo con solo 1.8B de parámetros tiene un desempeño muy bueno, así que vamos a descargarlo para probarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling e554c6b9de01... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 828 MB\n",
      "pulling 4cc1cb3660d8... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 909 MB\n",
      "pulling c71d239df917... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  11 KB\n",
      "pulling 4b021a3b4b4a... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏   77 B\n",
      "pulling 9468773bdc1f... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏   65 B\n",
      "pulling ba5fbb481ada... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  562 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama pull moondream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descargado vamos a preguntarle por esta imagen\n",
    "\n",
    "![this_is_fine](https://maximofn.com/wp-content/uploads/2024/05/this_is_fine-scaled.webp)\n",
    "\n",
    "Para ello tenemos tenemos que pasarle en el prompt la ruta donde tenemos la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Puedes explicarme esta imagen? /home/wallabot/Documentos/web/portafolio/images/this_is_fine.png\n",
      "Added image '/home/wallabot/Documentos/web/portafolio/images/this_is_fine.png'\n",
      "\n",
      "The image is a comic strip featuring two dogs sitting at a table. One dog has its eyes closed, while the other one appears to be looking at something or someone in \n",
      "front of it. The scene takes place inside a house with a fireplace and a cup on the table. Above them, there's a speech bubble that says 'This is fine.'"
     ]
    }
   ],
   "source": [
    "!ollama run moondream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts multilinea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando necesitamos escribir más de una línea en nuestro prompt simplemente tenemos que meterlo entre comillas triples `\"\"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> \"\"\"Hola, puedes hacerme un resumen de la siguiente conversación\n",
      "...\n",
      "... Alex: Bea, he estado revisando el modelo de clasificación de imágenes que entrenamos la semana pasada, y me parece que no está funcionando como esperábamos. Las métrica\n",
      "... s de precisión y recall están por debajo de lo que habíamos estimado.\n",
      "... \n",
      "... Bea: Sí, lo noté también. He estado analizando las predicciones erróneas y creo que el problema puede estar en el preprocesamiento de los datos. Algunos de los ejemplos\n",
      "...  de entrenamiento parecen estar etiquetados incorrectamente.\n",
      "... \n",
      "... Alex: Eso tiene sentido. ¿Crees que deberíamos aplicar alguna técnica de aumento de datos para balancear mejor las clases y mejorar la calidad de las etiquetas?\n",
      "... \n",
      "... Bea: Podría funcionar. De hecho, estaba pensando en utilizar augmentations como rotaciones y traslaciones. También podríamos considerar el uso de GANs para generar más \n",
      "... ejemplos sintéticos, especialmente para las clases minoritarias.\n",
      "... \n",
      "... Alex: Buena idea. Además, podríamos implementar una técnica de oversampling como SMOTE para equilibrar las clases. Pero primero, necesitamos limpiar y verificar nuestra\n",
      "... s etiquetas. ¿Te parece si dividimos las tareas? Yo puedo encargarme del aumento de datos y tú podrías revisar las etiquetas.\n",
      "... \n",
      "... Bea: Perfecto. También me gustaría ajustar un poco los hiperparámetros del modelo. Quizás deberíamos hacer una búsqueda en grid para encontrar los mejores valores para \n",
      "... el learning rate y el batch size.\n",
      "... \n",
      "... Alex: De acuerdo. Después de mejorar las etiquetas y los datos, podemos configurar una búsqueda en grid y usar un cluster de computación para agilizar el proceso. Creo \n",
      "... que podemos obtener resultados mucho más sólidos.\n",
      "... \n",
      "... Bea: Me parece un buen plan. Vamos a dividir las tareas y nos reunimos en unos días para revisar los avances. Con un poco de suerte, podremos mejorar significativamente\n",
      "...  el rendimiento del modelo.\n",
      "... \n",
      "... Alex: Genial, vamos a ello. ¡Gracias por la colaboración, Bea!\n",
      "... \n",
      "... Bea: ¡A ti, Alex! Trabajar en equipo siempre da mejores resultados. ¡Nos vemos pronto!\"\"\"\n",
      "Aquí te presento un resumen de la conversación:\n",
      "\n",
      "La conversación comienza con Alex, quien está revisando el modelo de clasificación de imágenes entrenado la semana pasada y descubre que no está funcionando como \n",
      "esperaban. Bea también había notado el problema y cree que puede estar relacionado con el preprocesamiento de los datos, ya que algunos ejemplos de entrenamiento \n",
      "parecen estar etiquetados incorrectamente.\n",
      "\n",
      "Discuten posibles soluciones para mejorar la calidad del modelo, incluyendo el uso de técnicas de aumento de datos (augmentations) y generación de ejemplos sintéticos\n",
      "con GANs. También mencionan la posibilidad de implementar una técnica de oversampling como SMOTE para equilibrar las clases.\n",
      "\n",
      "Acuerdan dividir las tareas: Alex se encargará del aumento de datos, mientras que Bea revisará las etiquetas. Después de mejorar los datos y las etiquetas, pueden \n",
      "configurar una búsqueda en grid para encontrar los mejores hiperparámetros del modelo.\n",
      "\n",
      "Finalmente, acuerdan reunirse después de unos días para revisar los avances y trabajar juntos en equipo para mejorar el rendimiento del modelo."
     ]
    }
   ],
   "source": [
    "!ollama run llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivos GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos dicho, ollama está basado en `llama.cpp`, por lo que el tipo de modelos que utiliza ollama son los que utiliza `llama.cpp` y son los archivos de tip `GGUF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los archivos `GGUF` se refieren al formato de archivo `Generalized GPU-Friendly Unified Format`, que está diseñado para almacenar y \n",
    "ejecutar modelos de lenguaje grande (LLMs) de manera eficiente en hardware acelerado por GPU.\n",
    "\n",
    "El formato GGUF es optimizado específicamente para almacenar y ejecutar modelos de lenguaje grande, como arquitecturas basadas en transformers. Proporciona una forma compacta y eficiente de representar los pesos del modelo, las activaciones y los grafos de computación, lo que \n",
    "permite tiempos de inferencia más rápidos y un uso reducido de memoria.\n",
    "\n",
    "Los archivos GGUF suelen contener los siguientes componentes:\n",
    "\n",
    " 1. Arquitectura del modelo: El archivo GGUF almacena la arquitectura del modelo, incluyendo el número de capas, tamaños ocultos y mecanismos de atención.\n",
    " 2. Pesos: Los pesos del modelo se almacenan en un formato comprimido para reducir el uso de memoria. Es decir, los modelos están cuantizados\n",
    " 3. Activaciones: El archivo GGUF contiene activaciones pre-computadas para ciertas capas o sub-capas, que pueden ser reutilizadas durante la inferencia.\n",
    " 4. Grafo de computación: El grafo de computación representa la secuencia de operaciones necesarias para realizar la inferencia con el modelo.\n",
    "\n",
    "Al utilizar archivos GGUF, los desarrolladores pueden implementar modelos de lenguaje grande en una variedad de hardware, \n",
    "incluyendo GPUs, TPUs y CPUs, mientras logran tiempos de inferencia más rápidos y un rendimiento mejorado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, el modelo llama3 de 70B de paŕametros en FP32 debería ocupar 280 GB, en FP16 debería ocupar 140 GB y en INT8 debería ocupar 70GB. Sin embargo, cuando nos lo vamos a descargar, en la web pone que ocupa 40 GB, por lo que al ocupar un poco más de la mitad podemos pensar que está cuantizado en INT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tengo 2 GPUs de 24 GB lo podría usar entre las dos GPUs, si lo cargase en paralelo en las dos, por lo que me lo voy a descargar y lo voy a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest \n",
      "pulling 0bd51f8f0c97... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  39 GB\n",
      "pulling 4fa551d4f938... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  12 KB\n",
      "pulling 8ab4849b038c... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  254 B\n",
      "pulling 577073ffcc6c... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  110 B\n",
      "pulling ea8e06d28e47... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  486 B\n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3:70B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo pruebo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hola, me puedes decir cómo pasar un array de numpy a un tensor de pytorch?\n",
      "Sí!\n",
      "\n",
      "En PyTorch, puedes convertir un arreglo de NumPy a un tensor utilizando la función `torch.from_numpy()`.\n",
      "\n",
      "Aquí tienes un ejemplo:\n",
      "```\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Creamos un arreglo de NumPy\n",
      "arr = np.array([1, 2, 3, 4, 5])\n",
      "\n",
      "# Convertimos el arreglo a un tensor de PyTorch\n",
      "tensor = torch.from_numpy(arr)\n",
      "\n",
      "print(tensor)  # Output: tensor([1, 2, 3, 4, 5])\n",
      "```\n",
      "Ten en cuenta que la función `torch.from_numpy()` devuelve un tensor que comparte la memoria con el arreglo de NumPy original. Esto significa que si \n",
      "modificas el tensor, también estarás modificando el arreglo de NumPy original.\n",
      "\n",
      "Si deseas crear un tensor independiente del arreglo de NumPy, puedes utilizar la función `torch.tensor()` y pasarle el arreglo de NumPy como parámetro:\n",
      "```\n",
      "tensor = torch.tensor(arr)\n",
      "```\n",
      "De esta manera, el tensor creado será una copia independiente del arreglo de NumPy original.\n",
      "\n",
      "Espero que esto te ayude!"
     ]
    }
   ],
   "source": [
    "!ollama run llama3:70B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No solo contesta bien, sino que va rapidísimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además se comprueba que ha cargado el modelo en mis dos GPUs en paralelo\n",
    "\n",
    "Por un lado se puede suponer, porque en una sola no entraría como he comentado antes. Pero es que además he creado un [monitor de GPUs](https://github.com/maximofn/gpu_monitor), con el que puedo ver que están las dos GPUs casi llenas enteras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De aquí podemos sacar otra conclusión, y es que mediante los archivos `GGUF` podemos ejecutar los LLMs de la manera más eficiente en nuestros ordenadores, en mis caso ha cargado en paralelo el LLM en mis dos GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos visto lo que son los archivos `GGUF` podemos ver cómo crear nuevos a partir de otros. Al igual que con docker podemor crear imagenes a partir de otras con un dockerfile, ollama nos permite crear modelos a partir de otros con un modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esto vamos a partir del archivo `Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf` que es el archivo `GGUF` del modelo `NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF`. Este modelo es una versión de llama3 de 8B de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de crear modelos y subirlos a internet lo ideal es ponerle nombres descriptivos, por ejemplo, en este caso vemos que tenemos en el nombre\n",
    "\n",
    " * Llama-3-Instruct: Lo que quiere decir que es un modelo que parte de llama3 8B Instruct\n",
    " * Merged: Es un modelo mergeado\n",
    " * DPO: Se ha hecho el alineamiento con DPO\n",
    " * Q4: Está cuantizado a 4 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a crear la carpeta donde vamos a crear el modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ollama_modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch ollama_modelfile/Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora nos descargamos el archivo `GGUF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-22 19:16:24--  https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
      "Resolviendo huggingface.co (huggingface.co)... 54.192.95.21, 54.192.95.79, 54.192.95.70, ...\n",
      "Conectando con huggingface.co (huggingface.co)[54.192.95.21]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 302 Found\n",
      "Ubicación: https://cdn-lfs-us-1.huggingface.co/repos/48/0d/480d1b9ea3df81b6cf974d62b6bc04f402ac5600a1dfdde7bbb9e819a111c572/762b9371a296ab2628592b9462dc676b27d881a3402816492801641a437669b3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf%3B+filename%3D%22Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf%22%3B&Expires=1716657384&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjY1NzM4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzQ4LzBkLzQ4MGQxYjllYTNkZjgxYjZjZjk3NGQ2MmI2YmMwNGY0MDJhYzU2MDBhMWRmZGRlN2JiYjllODE5YTExMWM1NzIvNzYyYjkzNzFhMjk2YWIyNjI4NTkyYjk0NjJkYzY3NmIyN2Q4ODFhMzQwMjgxNjQ5MjgwMTY0MWE0Mzc2NjliMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=GAOROcdWLJSj5Ls9WEhQHxnQKdK1QDmmvdhk4PssdVmLUYXAK-qolhGg-Q0Qz-qr1LCIYt%7EY4tP6SB9iumLiNBlHKOt6xAyii0bVqa15uRDF1iC71-LrqwjmI882AYLQfST2EFlWyHiabIm7dQSgz3czhwYC2PyTXMLHAn8b1mt10nOCWzCpSJBdxBags0ptEDYOYkHWrV0LCO1XS%7E9KLNfP1PODsTHLi1Yb67Vwo1lXCHjC5PdsmnQQO64gB2K79LyfSP46RtL-fQ-P65hg2ByHc%7EBKPXLusxyg8vmOoNzLBM-WGzTHiIELRsW5lOaofgsyMz5Baxes1B674UY6iA__&Key-Pair-Id=KCD77M1F0VK2B [siguiente]\n",
      "--2024-05-22 19:16:24--  https://cdn-lfs-us-1.huggingface.co/repos/48/0d/480d1b9ea3df81b6cf974d62b6bc04f402ac5600a1dfdde7bbb9e819a111c572/762b9371a296ab2628592b9462dc676b27d881a3402816492801641a437669b3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf%3B+filename%3D%22Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf%22%3B&Expires=1716657384&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjY1NzM4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzQ4LzBkLzQ4MGQxYjllYTNkZjgxYjZjZjk3NGQ2MmI2YmMwNGY0MDJhYzU2MDBhMWRmZGRlN2JiYjllODE5YTExMWM1NzIvNzYyYjkzNzFhMjk2YWIyNjI4NTkyYjk0NjJkYzY3NmIyN2Q4ODFhMzQwMjgxNjQ5MjgwMTY0MWE0Mzc2NjliMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=GAOROcdWLJSj5Ls9WEhQHxnQKdK1QDmmvdhk4PssdVmLUYXAK-qolhGg-Q0Qz-qr1LCIYt%7EY4tP6SB9iumLiNBlHKOt6xAyii0bVqa15uRDF1iC71-LrqwjmI882AYLQfST2EFlWyHiabIm7dQSgz3czhwYC2PyTXMLHAn8b1mt10nOCWzCpSJBdxBags0ptEDYOYkHWrV0LCO1XS%7E9KLNfP1PODsTHLi1Yb67Vwo1lXCHjC5PdsmnQQO64gB2K79LyfSP46RtL-fQ-P65hg2ByHc%7EBKPXLusxyg8vmOoNzLBM-WGzTHiIELRsW5lOaofgsyMz5Baxes1B674UY6iA__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolviendo cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.157.98.92, 108.157.98.20, 108.157.98.96, ...\n",
      "Conectando con cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)[108.157.98.92]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 4920733728 (4,6G) [binary/octet-stream]\n",
      "Guardando como: “ollama_modelfile/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf”\n",
      "\n",
      "ollama_modelfile/He 100%[===================>]   4,58G  47,5MB/s    en 91s     \n",
      "\n",
      "2024-05-22 19:17:56 (51,4 MB/s) - “ollama_modelfile/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf” guardado [4920733728/4920733728]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf -O ollama_modelfile/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descargado empezamos a crear el modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FROM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos que especificar desde qué modelo partimos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos partir desde un modelo que esté en el hub de ollama, por ejemplo podríamos hacer\n",
    "\n",
    "``` yaml\n",
    "FROM llama3\n",
    "```\n",
    "\n",
    "Pero como vamos a partir desde el archivo que nos hemos descargado, lo especificamos mediante el path\n",
    "\n",
    "``` yaml\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que establecer los valores de los parámetros del LLM, por ejemplo, si queremos modificar el valor de la temperatura ponemos\n",
    "\n",
    "``` yaml\n",
    "PARAMETER temperature 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros que podemos modificar son\n",
    "\n",
    "| Parameter | Description | Value Type | Example Usage |\n",
    "| --- | --- | --- | --- |\n",
    "| mirostat | Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) | int | mirostat 0 |\n",
    "| mirostat_eta | Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) | float | mirostat_eta 0.1 |\n",
    "| mirostat_tau | Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) | float | mirostat_tau 5.0 |\n",
    "| num_ctx | Sets the size of the context window used to generate the next token. (Default: 2048) | int | num_ctx 4096 |\n",
    "| repeat_last_n | Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx) | int | repeat_last_n 64 |\n",
    "| repeat_penalty | Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1) | float | repeat_penalty 1.1 |\n",
    "| temperature | The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8) | float | temperature 0.7 |\n",
    "| seed | Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0) | int | seed 42 |\n",
    "| stop | Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile. | string | stop \"AI assistant:\" |\n",
    "| tfs_z | Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1) | float | tfs_z 1 |\n",
    "| num_predict | Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context) | int | num_predict 42 |\n",
    "| top_k | Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40) | int | top_k 40 |\n",
    "| top_p | Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9) | float | top_p 0.9 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer al modelo más creativo subiéndole la temperatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "\n",
    "PARAMETER temperature 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos condicionar al modelo con un mensaje de sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "\n",
    "PARAMETER temperature 1\n",
    "\n",
    "SYSTEM \"\"\"Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LICENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sirve para establecer la licencia del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "\n",
    "PARAMETER temperature 1\n",
    "\n",
    "SYSTEM \"\"\"Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales\"\"\"\n",
    "\n",
    "LICENSE \"\"\"Apache-2.0\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MESSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos pasarle un historial de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "\n",
    "PARAMETER temperature 1\n",
    "\n",
    "SYSTEM \"\"\"Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales\"\"\"\n",
    "\n",
    "LICENSE \"\"\"Apache-2.0\"\"\"\n",
    "\n",
    "MESSAGE user Sabes cómo pasar un array de numpy a un tensor de torch?\n",
    "MESSAGE assistant Sí\n",
    "MESSAGE user Y me lo podrías decir?\n",
    "MESSAGE assistant Sí\n",
    "MESSAGE user Dímelo\n",
    "MESSAGE assistant Te lo puedo decir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez escrito el modelfile podemos crear el modelo mediante\n",
    "\n",
    "``` bash\n",
    "ollama create <model-name> -f <path-of-the-modelfile>'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transferring model data\n",
      "using existing layer sha256:762b9371a296ab2628592b9462dc676b27d881a3402816492801641a437669b3\n",
      "creating new layer sha256:ba85fc82591e657e639f779f4fdc5e6e7cb977f9d36487d67cefe589a8af670e\n",
      "creating new layer sha256:2af71558e438db0b73a20beab92dc278a94e1bbe974c00c1a33e3ab62d53a608\n",
      "creating new layer sha256:20dc17cb752058f95ad600722883b22f3328ca8ccccbb667d5e07991ae1985c7\n",
      "using existing layer sha256:d8ba2f9a17b3bbdeb5690efaa409b3fcb0b56296a777c7a69c78aa33bbddf182\n",
      "creating new layer sha256:2156b3b25c67a5712e4c11c889ef91c1625c4ea7b59c5c81ab2a7aa451535acf\n",
      "writing manifest\n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama create assistanmal -f ollama_modelfile/Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos una salida muy similar a la que obtenemos cuando creamos una imagen de docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora listamos los modelos que tenemos disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              \tID          \tSIZE  \tMODIFIED           \n",
      "assistanmal:latest\t84222b0e11b8\t4.9 GB\tAbout a minute ago\t\n",
      "llama3:70b        \t786f3184aec0\t39 GB \t12 hours ago      \t\n",
      "moondream:latest  \t55fc3abd3867\t1.7 GB\t14 hours ago      \t\n",
      "phi3:latest       \ta2c89ceaed85\t2.3 GB\t40 hours ago      \t\n",
      "llama3:latest     \t365c0bd3c000\t4.7 GB\t40 hours ago      \t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que está el asistente que acabamos de crear. Vamos a probarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Sabes cómo pasar un array de numpy a un tensor de torch?\n",
      "Sí\n",
      "\n",
      ">>> Y me lo podrías decir?\n",
      "Sí\n",
      "\n",
      ">>> Dímelo\n",
      "Te lo puedo decir\n",
      "\n",
      ">>> Me puedes responder?\n",
      "¡Claro!\n",
      "\n",
      "Para pasar un arreglo (array) de NumPy a un tensor de PyTorch, simplemente necesitas importar la librería `torch` y usar la función `tensor()` para convertir el \n",
      "arreglo a un tensor. Aquí tienes un ejemplo:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "# Importar arreglo de numpy\n",
      "import numpy as np\n",
      "\n",
      "# Crear arreglo de numpy\n",
      "numpy_array = np.array([1, 2, 3])\n",
      "\n",
      "# Convertir arreglo de numpy a tensor de PyTorch\n",
      "pytorch_tensor = torch.tensor(numpy_array)\n",
      "\n",
      "print(pytorch_tensor)\n",
      "```\n",
      "\n",
      "En este ejemplo, creamos un arreglo de numpy con los valores `[1, 2, 3]`. Luego, lo convertimos a un tensor de PyTorch usando la función `tensor()`. Finalmente, \n",
      "imprimimos el tensor resultante.\n",
      "\n",
      "Ten en cuenta..."
     ]
    }
   ],
   "source": [
    "!_ollama run assistantmal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo he tenido que parar porque no para de responder. No estaba buscando hacer el mejor assistente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
