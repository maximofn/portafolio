{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Multimodal models](https://github.com/ollama/ollama/blob/main/README.md#quickstart)\n",
    "\n",
    "[Multiline input](https://github.com/ollama/ollama/blob/main/README.md#quickstart)\n",
    "\n",
    "[Import model](https://github.com/ollama/ollama/blob/main/docs/import.md)\n",
    "\n",
    "[Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)\n",
    "\n",
    "[API](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "\n",
    "[API Python](https://github.com/ollama/ollama-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama es un software basado en [llama.cpp](https://github.com/ggerganov/llama.cpp) para la inferencia de LLMs.\n",
    "\n",
    "[llama.cpp](https://github.com/ggerganov/llama.cpp) es una librería para la inferencia de LLMs en C/C++, de modo que Ollama se ha construido por encima haciendo que el uso sea más sencillo para cualquier tipo de usuario. Es decir, es un wrapper para poder usar `llama.cpp` con una API de Python, JavaScript y terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar ollama en windows o mac tienes que descargarte un instalador desde su página de [descargas](https://ollama.com/download), sin embargo, si usas linux, como es mi caso, tienes que ejecutar el siguiente comando\n",
    "\n",
    "``` bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para correr un LLM basta con ejecutar `ollama run <LLM>`, por ejemplo puedo usar `llama3` mediante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling 6a0746a1ec1a... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB\n",
      "pulling 4fa551d4f938... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  12 KB\n",
      "pulling 8ab4849b038c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  254 B\n",
      "pulling 577073ffcc6c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  110 B\n",
      "pulling 3f8eb4da87fa... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  485 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success\n",
      ">>> hola\n",
      "Hola! ¿Cómo estás?\n",
      "\n",
      ">>> Send a message (/? for help)\n"
     ]
    }
   ],
   "source": [
    "!ollama run llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver ollama funciona como docker, como no tenía descargado llama3, se lo ha descargado y luego podemos usarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos saber qué modelos hay disponibles podemos ir a su página de [modelos](https://ollama.com/library), donde se pueden ver todos los modelos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargar un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos descargarnos un modelo, lo haremos igual que con docker `ollama pull <LLM>`, por ejemplo, voy a descargarme `phi3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling 4fed7364ee3e... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 2.3 GB\n",
      "pulling c608dc615584... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  149 B\n",
      "pulling fa8235e5b48f... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB\n",
      "pulling d47ab88b61ba... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  140 B\n",
      "pulling f7eda1da5a81... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  485 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama pull phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ayuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos ver todos los comandos que tenemos disponibles podemos usar `ollama -h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language model runner\n",
      "\n",
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "!ollama -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar los modelos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver en la ayuda que si hacemos `ollama list` podremos ver todos los modelos disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         \tID          \tSIZE  \tMODIFIED       \n",
      "phi3:latest  \ta2c89ceaed85\t2.3 GB\t4 minutes ago \t\n",
      "llama3:latest\t365c0bd3c000\t4.7 GB\t10 minutes ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y como vemos solo tenemos los dos que hemos descargado hasta el momento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver los modelos corriendo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hacemos `ollama ps` podemos ver los modelos que tenemos corriendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         \tID          \tSIZE  \tPROCESSOR\tUNTIL              \n",
      "llama3:latest\t365c0bd3c000\t5.4 GB\t100% GPU \t3 minutes from now\t\n"
     ]
    }
   ],
   "source": [
    "!ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos corriendo llama3 porque al principio hicimos `ollama run llama3`, pero si ahora lo intentamos con `phi3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Send a message (/? for help)\n"
     ]
    }
   ],
   "source": [
    "!ollama run phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora volvemos a hacer `ollama ps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       \tID          \tSIZE  \tPROCESSOR\tUNTIL              \n",
      "phi3:latest\ta2c89ceaed85\t3.8 GB\t100% GPU \t3 minutes from now\t\n"
     ]
    }
   ],
   "source": [
    "!ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el modelo que está corriendo es `phi3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos multimodales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lista de modelos de ollama hay modelos multimodales, como [moondream2](https://ollama.com/library/moondream), que siendo un modelo con solo 1.8B de parámetros tiene un desempeño muy bueno, así que vamos a descargarlo para probarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling e554c6b9de01... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 828 MB\n",
      "pulling 4cc1cb3660d8... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 909 MB\n",
      "pulling c71d239df917... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  11 KB\n",
      "pulling 4b021a3b4b4a... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏   77 B\n",
      "pulling 9468773bdc1f... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏   65 B\n",
      "pulling ba5fbb481ada... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  562 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama pull moondream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descargado vamos a preguntarle por esta imagen\n",
    "\n",
    "![this_is_fine](https://maximofn.com/wp-content/uploads/2024/05/this_is_fine-scaled.webp)\n",
    "\n",
    "Para ello tenemos tenemos que pasarle en el prompt la ruta donde tenemos la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: _ollama: command not found\n"
     ]
    }
   ],
   "source": [
    "!_ollama run moondream"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
