{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Import model](https://github.com/ollama/ollama/blob/main/docs/import.md)\n",
    "\n",
    "[API](https://github.com/ollama/ollama/blob/main/docs/api.md)\n",
    "\n",
    "[API Python](https://github.com/ollama/ollama-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama es un software basado en [llama.cpp](https://github.com/ggerganov/llama.cpp) para la inferencia de LLMs.\n",
    "\n",
    "[llama.cpp](https://github.com/ggerganov/llama.cpp) es una librería para la inferencia de LLMs en C/C++, de modo que Ollama se ha construido por encima haciendo que el uso sea más sencillo para cualquier tipo de usuario. Es decir, es un wrapper para poder usar `llama.cpp` con una API de Python, JavaScript y terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar ollama en windows o mac tienes que descargarte un instalador desde su página de [descargas](https://ollama.com/download), sin embargo, si usas linux, como es mi caso, tienes que ejecutar el siguiente comando\n",
    "\n",
    "``` bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para correr un LLM basta con ejecutar `ollama run <LLM>`, por ejemplo puedo usar `llama3` mediante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling 6a0746a1ec1a... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 4.7 GB\n",
      "pulling 4fa551d4f938... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  12 KB\n",
      "pulling 8ab4849b038c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  254 B\n",
      "pulling 577073ffcc6c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  110 B\n",
      "pulling 3f8eb4da87fa... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  485 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success\n",
      ">>> hola\n",
      "Hola! ¿Cómo estás?\n",
      "\n",
      ">>> Send a message (/? for help)\n"
     ]
    }
   ],
   "source": [
    "!ollama run llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver ollama funciona como docker, como no tenía descargado llama3, se lo ha descargado y luego podemos usarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos saber qué modelos hay disponibles podemos ir a su página de [modelos](https://ollama.com/library), donde se pueden ver todos los modelos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargar un modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos descargarnos un modelo, lo haremos igual que con docker `ollama pull <LLM>`, por ejemplo, voy a descargarme `phi3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling 4fed7364ee3e... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 2.3 GB\n",
      "pulling c608dc615584... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  149 B\n",
      "pulling fa8235e5b48f... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 1.1 KB\n",
      "pulling d47ab88b61ba... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  140 B\n",
      "pulling f7eda1da5a81... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  485 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama pull phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ayuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos ver todos los comandos que tenemos disponibles podemos usar `ollama -h`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language model runner\n",
      "\n",
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "!ollama -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listar los modelos disponibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver en la ayuda que si hacemos `ollama list` podremos ver todos los modelos disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         \tID          \tSIZE  \tMODIFIED       \n",
      "phi3:latest  \ta2c89ceaed85\t2.3 GB\t4 minutes ago \t\n",
      "llama3:latest\t365c0bd3c000\t4.7 GB\t10 minutes ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y como vemos solo tenemos los dos que hemos descargado hasta el momento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver los modelos corriendo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hacemos `ollama ps` podemos ver los modelos que tenemos corriendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         \tID          \tSIZE  \tPROCESSOR\tUNTIL              \n",
      "llama3:latest\t365c0bd3c000\t5.4 GB\t100% GPU \t3 minutes from now\t\n"
     ]
    }
   ],
   "source": [
    "!ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos corriendo llama3 porque al principio hicimos `ollama run llama3`, pero si ahora lo intentamos con `phi3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Send a message (/? for help)\n"
     ]
    }
   ],
   "source": [
    "!ollama run phi3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora volvemos a hacer `ollama ps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       \tID          \tSIZE  \tPROCESSOR\tUNTIL              \n",
      "phi3:latest\ta2c89ceaed85\t3.8 GB\t100% GPU \t3 minutes from now\t\n"
     ]
    }
   ],
   "source": [
    "!ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el modelo que está corriendo es `phi3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos multimodales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la lista de modelos de ollama hay modelos multimodales, como [moondream2](https://ollama.com/library/moondream), que siendo un modelo con solo 1.8B de parámetros tiene un desempeño muy bueno, así que vamos a descargarlo para probarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest\n",
      "pulling e554c6b9de01... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 828 MB\n",
      "pulling 4cc1cb3660d8... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏ 909 MB\n",
      "pulling c71d239df917... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  11 KB\n",
      "pulling 4b021a3b4b4a... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏   77 B\n",
      "pulling 9468773bdc1f... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏   65 B\n",
      "pulling ba5fbb481ada... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████▏  562 B\n",
      "verifying sha256 digest\n",
      "writing manifest\n",
      "removing any unused layers\n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama pull moondream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descargado vamos a preguntarle por esta imagen\n",
    "\n",
    "![this_is_fine](https://maximofn.com/wp-content/uploads/2024/05/this_is_fine-scaled.webp)\n",
    "\n",
    "Para ello tenemos tenemos que pasarle en el prompt la ruta donde tenemos la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Puedes explicarme esta imagen? /home/wallabot/Documentos/web/portafolio/images/this_is_fine.png\n",
      "Added image '/home/wallabot/Documentos/web/portafolio/images/this_is_fine.png'\n",
      "\n",
      "The image is a comic strip featuring two dogs sitting at a table. One dog has its eyes closed, while the other one appears to be looking at something or someone in \n",
      "front of it. The scene takes place inside a house with a fireplace and a cup on the table. Above them, there's a speech bubble that says 'This is fine.'"
     ]
    }
   ],
   "source": [
    "!ollama run moondream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts multilinea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando necesitamos escribir más de una línea en nuestro prompt simplemente tenemos que meterlo entre comillas triples `\"\"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> \"\"\"Hola, puedes hacerme un resumen de la siguiente conversación\n",
      "...\n",
      "... Alex: Bea, he estado revisando el modelo de clasificación de imágenes que entrenamos la semana pasada, y me parece que no está funcionando como esperábamos. Las métrica\n",
      "... s de precisión y recall están por debajo de lo que habíamos estimado.\n",
      "... \n",
      "... Bea: Sí, lo noté también. He estado analizando las predicciones erróneas y creo que el problema puede estar en el preprocesamiento de los datos. Algunos de los ejemplos\n",
      "...  de entrenamiento parecen estar etiquetados incorrectamente.\n",
      "... \n",
      "... Alex: Eso tiene sentido. ¿Crees que deberíamos aplicar alguna técnica de aumento de datos para balancear mejor las clases y mejorar la calidad de las etiquetas?\n",
      "... \n",
      "... Bea: Podría funcionar. De hecho, estaba pensando en utilizar augmentations como rotaciones y traslaciones. También podríamos considerar el uso de GANs para generar más \n",
      "... ejemplos sintéticos, especialmente para las clases minoritarias.\n",
      "... \n",
      "... Alex: Buena idea. Además, podríamos implementar una técnica de oversampling como SMOTE para equilibrar las clases. Pero primero, necesitamos limpiar y verificar nuestra\n",
      "... s etiquetas. ¿Te parece si dividimos las tareas? Yo puedo encargarme del aumento de datos y tú podrías revisar las etiquetas.\n",
      "... \n",
      "... Bea: Perfecto. También me gustaría ajustar un poco los hiperparámetros del modelo. Quizás deberíamos hacer una búsqueda en grid para encontrar los mejores valores para \n",
      "... el learning rate y el batch size.\n",
      "... \n",
      "... Alex: De acuerdo. Después de mejorar las etiquetas y los datos, podemos configurar una búsqueda en grid y usar un cluster de computación para agilizar el proceso. Creo \n",
      "... que podemos obtener resultados mucho más sólidos.\n",
      "... \n",
      "... Bea: Me parece un buen plan. Vamos a dividir las tareas y nos reunimos en unos días para revisar los avances. Con un poco de suerte, podremos mejorar significativamente\n",
      "...  el rendimiento del modelo.\n",
      "... \n",
      "... Alex: Genial, vamos a ello. ¡Gracias por la colaboración, Bea!\n",
      "... \n",
      "... Bea: ¡A ti, Alex! Trabajar en equipo siempre da mejores resultados. ¡Nos vemos pronto!\"\"\"\n",
      "Aquí te presento un resumen de la conversación:\n",
      "\n",
      "La conversación comienza con Alex, quien está revisando el modelo de clasificación de imágenes entrenado la semana pasada y descubre que no está funcionando como \n",
      "esperaban. Bea también había notado el problema y cree que puede estar relacionado con el preprocesamiento de los datos, ya que algunos ejemplos de entrenamiento \n",
      "parecen estar etiquetados incorrectamente.\n",
      "\n",
      "Discuten posibles soluciones para mejorar la calidad del modelo, incluyendo el uso de técnicas de aumento de datos (augmentations) y generación de ejemplos sintéticos\n",
      "con GANs. También mencionan la posibilidad de implementar una técnica de oversampling como SMOTE para equilibrar las clases.\n",
      "\n",
      "Acuerdan dividir las tareas: Alex se encargará del aumento de datos, mientras que Bea revisará las etiquetas. Después de mejorar los datos y las etiquetas, pueden \n",
      "configurar una búsqueda en grid para encontrar los mejores hiperparámetros del modelo.\n",
      "\n",
      "Finalmente, acuerdan reunirse después de unos días para revisar los avances y trabajar juntos en equipo para mejorar el rendimiento del modelo."
     ]
    }
   ],
   "source": [
    "!ollama run llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivos GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos dicho, ollama está basado en `llama.cpp`, por lo que el tipo de modelos que utiliza ollama son los que utiliza `llama.cpp` y son los archivos de tip `GGUF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los archivos `GGUF` se refieren al formato de archivo `Generalized GPU-Friendly Unified Format`, que está diseñado para almacenar y \n",
    "ejecutar modelos de lenguaje grande (LLMs) de manera eficiente en hardware acelerado por GPU.\n",
    "\n",
    "El formato GGUF es optimizado específicamente para almacenar y ejecutar modelos de lenguaje grande, como arquitecturas basadas en transformers. Proporciona una forma compacta y eficiente de representar los pesos del modelo, las activaciones y los grafos de computación, lo que \n",
    "permite tiempos de inferencia más rápidos y un uso reducido de memoria.\n",
    "\n",
    "Los archivos GGUF suelen contener los siguientes componentes:\n",
    "\n",
    " 1. Arquitectura del modelo: El archivo GGUF almacena la arquitectura del modelo, incluyendo el número de capas, tamaños ocultos y mecanismos de atención.\n",
    " 2. Pesos: Los pesos del modelo se almacenan en un formato comprimido para reducir el uso de memoria. Es decir, los modelos están cuantizados\n",
    " 3. Activaciones: El archivo GGUF contiene activaciones pre-computadas para ciertas capas o sub-capas, que pueden ser reutilizadas durante la inferencia.\n",
    " 4. Grafo de computación: El grafo de computación representa la secuencia de operaciones necesarias para realizar la inferencia con el modelo.\n",
    "\n",
    "Al utilizar archivos GGUF, los desarrolladores pueden implementar modelos de lenguaje grande en una variedad de hardware, \n",
    "incluyendo GPUs, TPUs y CPUs, mientras logran tiempos de inferencia más rápidos y un rendimiento mejorado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, el modelo llama3 de 70B de paŕametros en FP32 debería ocupar 280 GB, en FP16 debería ocupar 140 GB y en INT8 debería ocupar 70GB. Sin embargo, cuando nos lo vamos a descargar, en la web pone que ocupa 40 GB, por lo que al ocupar un poco más de la mitad podemos pensar que está cuantizado en INT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tengo 2 GPUs de 24 GB lo podría usar entre las dos GPUs, si lo cargase en paralelo en las dos, por lo que me lo voy a descargar y lo voy a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling manifest \n",
      "pulling 0bd51f8f0c97... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  39 GB\n",
      "pulling 4fa551d4f938... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  12 KB\n",
      "pulling 8ab4849b038c... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  254 B\n",
      "pulling 577073ffcc6c... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  110 B\n",
      "pulling ea8e06d28e47... 100% ▕████████████████████████████████████████████████████████████████████████████████████████████████▏  486 B\n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3:70B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo pruebo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Hola, me puedes decir cómo pasar un array de numpy a un tensor de pytorch?\n",
      "Sí!\n",
      "\n",
      "En PyTorch, puedes convertir un arreglo de NumPy a un tensor utilizando la función `torch.from_numpy()`.\n",
      "\n",
      "Aquí tienes un ejemplo:\n",
      "```\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Creamos un arreglo de NumPy\n",
      "arr = np.array([1, 2, 3, 4, 5])\n",
      "\n",
      "# Convertimos el arreglo a un tensor de PyTorch\n",
      "tensor = torch.from_numpy(arr)\n",
      "\n",
      "print(tensor)  # Output: tensor([1, 2, 3, 4, 5])\n",
      "```\n",
      "Ten en cuenta que la función `torch.from_numpy()` devuelve un tensor que comparte la memoria con el arreglo de NumPy original. Esto significa que si \n",
      "modificas el tensor, también estarás modificando el arreglo de NumPy original.\n",
      "\n",
      "Si deseas crear un tensor independiente del arreglo de NumPy, puedes utilizar la función `torch.tensor()` y pasarle el arreglo de NumPy como parámetro:\n",
      "```\n",
      "tensor = torch.tensor(arr)\n",
      "```\n",
      "De esta manera, el tensor creado será una copia independiente del arreglo de NumPy original.\n",
      "\n",
      "Espero que esto te ayude!"
     ]
    }
   ],
   "source": [
    "!ollama run llama3:70B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No solo contesta bien, sino que va rapidísimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además se comprueba que ha cargado el modelo en mis dos GPUs en paralelo\n",
    "\n",
    "Por un lado se puede suponer, porque en una sola no entraría como he comentado antes. Pero es que además he creado un [monitor de GPUs](https://github.com/maximofn/gpu_monitor), con el que puedo ver que están las dos GPUs casi llenas enteras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De aquí podemos sacar otra conclusión, y es que mediante los archivos `GGUF` podemos ejecutar los LLMs de la manera más eficiente en nuestros ordenadores, en mis caso ha cargado en paralelo el LLM en mis dos GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos visto lo que son los archivos `GGUF` podemos ver cómo crear nuevos a partir de otros. Al igual que con docker podemor crear imagenes a partir de otras con un dockerfile, ollama nos permite crear modelos a partir de otros con un modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esto vamos a partir del archivo `Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf` que es el archivo `GGUF` del modelo `NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF`. Este modelo es una versión de llama3 de 8B de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de crear modelos y subirlos a internet lo ideal es ponerle nombres descriptivos, por ejemplo, en este caso vemos que tenemos en el nombre\n",
    "\n",
    " * Llama-3-Instruct: Lo que quiere decir que es un modelo que parte de llama3 8B Instruct\n",
    " * Merged: Es un modelo mergeado\n",
    " * DPO: Se ha hecho el alineamiento con DPO\n",
    " * Q4: Está cuantizado a 4 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a crear la carpeta donde vamos a crear el modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ollama_modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch ollama_modelfile/Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora nos descargamos el archivo `GGUF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-23 06:31:53--  https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 54.192.95.26, 54.192.95.79, 54.192.95.21, ...\n",
      "Connecting to huggingface.co (huggingface.co)|54.192.95.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.huggingface.co/repos/48/0d/480d1b9ea3df81b6cf974d62b6bc04f402ac5600a1dfdde7bbb9e819a111c572/762b9371a296ab2628592b9462dc676b27d881a3402816492801641a437669b3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf%3B+filename%3D%22Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf%22%3B&Expires=1716697914&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjY5NzkxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzQ4LzBkLzQ4MGQxYjllYTNkZjgxYjZjZjk3NGQ2MmI2YmMwNGY0MDJhYzU2MDBhMWRmZGRlN2JiYjllODE5YTExMWM1NzIvNzYyYjkzNzFhMjk2YWIyNjI4NTkyYjk0NjJkYzY3NmIyN2Q4ODFhMzQwMjgxNjQ5MjgwMTY0MWE0Mzc2NjliMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=LQecPNccMxv4R-BowDrcwORGpk%7ETTh-gVmMrAzzmdT8EFDVXNH7KTwmqKF-pEAbbir9iRS1pX7NFCterYF0w5UMe%7EWkvGg46Z73yMn3s96hm7edWypv9OCMFuBvcp8arYonKljUQ4UvdQKFHXAZxKhLrx6OVqx9YoTvHuXmpBoKXAPi1XNLPwY0YTX4wn3DnAFFzkHOsDK7p2TJ3z7RFzmfyZg%7E5daWm008oGBvIV192MKk5uqQGwqv3l-4mi2Nd8eOwMW2Yfzigz7EZJgPM7UDYzin5vABsnyXJorOSRdri13hlC-AMaY9db76vLsT1NBvVU1xVtWxAylDqU6XYlQ__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
      "--2024-05-23 06:31:54--  https://cdn-lfs-us-1.huggingface.co/repos/48/0d/480d1b9ea3df81b6cf974d62b6bc04f402ac5600a1dfdde7bbb9e819a111c572/762b9371a296ab2628592b9462dc676b27d881a3402816492801641a437669b3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf%3B+filename%3D%22Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf%22%3B&Expires=1716697914&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjY5NzkxNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzQ4LzBkLzQ4MGQxYjllYTNkZjgxYjZjZjk3NGQ2MmI2YmMwNGY0MDJhYzU2MDBhMWRmZGRlN2JiYjllODE5YTExMWM1NzIvNzYyYjkzNzFhMjk2YWIyNjI4NTkyYjk0NjJkYzY3NmIyN2Q4ODFhMzQwMjgxNjQ5MjgwMTY0MWE0Mzc2NjliMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=LQecPNccMxv4R-BowDrcwORGpk%7ETTh-gVmMrAzzmdT8EFDVXNH7KTwmqKF-pEAbbir9iRS1pX7NFCterYF0w5UMe%7EWkvGg46Z73yMn3s96hm7edWypv9OCMFuBvcp8arYonKljUQ4UvdQKFHXAZxKhLrx6OVqx9YoTvHuXmpBoKXAPi1XNLPwY0YTX4wn3DnAFFzkHOsDK7p2TJ3z7RFzmfyZg%7E5daWm008oGBvIV192MKk5uqQGwqv3l-4mi2Nd8eOwMW2Yfzigz7EZJgPM7UDYzin5vABsnyXJorOSRdri13hlC-AMaY9db76vLsT1NBvVU1xVtWxAylDqU6XYlQ__&Key-Pair-Id=KCD77M1F0VK2B\n",
      "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.157.98.20, 108.157.98.92, 108.157.98.55, ...\n",
      "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.157.98.20|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4920733728 (4,6G) [binary/octet-stream]\n",
      "Saving to: ‘ollama_modelfile/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf’\n",
      "\n",
      "ollama_modelfile/He 100%[===================>]   4,58G  71,1MB/s    in 65s     \n",
      "\n",
      "2024-05-23 06:32:59 (72,2 MB/s) - ‘ollama_modelfile/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf’ saved [4920733728/4920733728]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf -O ollama_modelfile/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descargado empezamos a crear el modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FROM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero tenemos que especificar desde qué modelo partimos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos partir desde un modelo que esté en el hub de ollama, por ejemplo podríamos hacer\n",
    "\n",
    "``` yaml\n",
    "FROM llama3\n",
    "```\n",
    "\n",
    "Pero como vamos a partir desde el archivo que nos hemos descargado, lo especificamos mediante el path\n",
    "\n",
    "``` yaml\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que establecer los valores de los parámetros del LLM, por ejemplo, si queremos modificar el valor de la temperatura ponemos\n",
    "\n",
    "``` yaml\n",
    "PARAMETER temperature 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los parámetros que podemos modificar son\n",
    "\n",
    "| Parámetro | Descripción | Tipo de Valor | Ejemplo de Uso |\n",
    "| --- | --- | --- | --- |\n",
    "| mirostat | Habilitar el muestreo `Mirostat` para controlar la perplejidad. (por defecto: 0, 0 = deshabilitado, 1 = Mirostat, 2 = Mirostat 2.0) | int | mirostat 0 |\n",
    "| mirostat_eta | Influye en la rapidez con la que el algoritmo responde a la retroalimentación del texto generado. Una tasa de aprendizaje más baja resultará en ajustes más lentos, mientras que una tasa más alta hará que el algoritmo sea más sensible. (Por defecto: 0.1) | float | mirostat_eta 0.1 |\n",
    "| mirostat_tau | Controla el equilibrio entre la coherencia y la diversidad del output. Un valor más bajo resultará en un texto más enfocado y coherente. (Por defecto: 5.0) | float | mirostat_tau 5.0 |\n",
    "| num_ctx | Establece el tamaño de la ventana de contexto utilizada para generar el siguiente token. (Por defecto: 4096) | int | num_ctx 4096 |\n",
    "| repeat_last_n | Establece cuánto tiempo atrás debe mirar el modelo para evitar repeticiones. (Por defecto: 64, 0 = deshabilitado, -1 = num_ctx) | int | repeat_last_n 64 |\n",
    "| repeat_penalty | Establece la intensidad con la que se penalizan las repeticiones. Un valor más alto (por ejemplo, 1.5) penalizará las repeticiones más fuertemente, mientras que un valor más bajo (por ejemplo, 0.9) será más indulgente. (Por defecto: 1.1) | float | repeat_penalty 1.1 |\n",
    "| temperature | La temperatura del modelo. Aumentar la temperatura hará que el modelo responda de manera más creativa. (Por defecto: 0.8) | float | temperature 0.7 |\n",
    "| seed | Establece la semilla de números aleatorios para la generación. Establecer esto a un número específico hará que el modelo genere el mismo texto para el mismo prompt. (Por defecto: 0) | int | seed 42 |\n",
    "| stop | Establece las secuencias de parada a utilizar. Cuando se encuentra este patrón, el LLM dejará de generar texto y retornará. Se pueden establecer múltiples patrones de parada especificando múltiples parámetros stop en un archivo de modelo. | string | stop \"AI assistant:\" |\n",
    "| tfs_z | El muestreo libre de colas se usa para reducir el impacto de los tokens menos probables en el output. Un valor más alto (por ejemplo, 2.0) reducirá más el impacto, mientras que un valor de 1.0 deshabilita esta configuración. (por defecto: 1) | float | tfs_z 1 |\n",
    "| num_predict | Número máximo de tokens a predecir al generar texto. (Por defecto: 128, -1 = generación infinita, -2 = llenar contexto) | int | num_predict 42 |\n",
    "| top_k | Reduce la probabilidad de generar sin sentido. Un valor más alto (por ejemplo, 100) dará respuestas más diversas, mientras que un valor más bajo (por ejemplo, 10) será más conservador. (Por defecto: 40) | int | top_k 40 |\n",
    "| top_p | Funciona junto con top-k. Un valor más alto (por ejemplo, 0.95) conducirá a un texto más diverso, mientras que un valor más bajo (por ejemplo, 0.5) generará un texto más enfocado y conservador. (Por defecto: 0.9) | float | top_p 0.9 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver los parámetros del modelo `llama3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETER num_keep 24\n",
      "PARAMETER stop \"<|start_header_id|>\"\n",
      "PARAMETER stop \"<|end_header_id|>\"\n",
      "PARAMETER stop \"<|eot_id|>\"\n"
     ]
    }
   ],
   "source": [
    "!ollama show llama3 --modelfile | grep PARAMETER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que tiene un parámetro `num_keep = 24` que no viene en la documentación de ollama y además tres tokens de parada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer al modelo más creativo subiéndole la temperatura y vamos a mantener los tokens de parada, porque si no el modelo no parará de generar texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "\n",
    "PARAMETER temperature 1\n",
    "\n",
    "PARAMETER stop \"<|start_header_id|>\"\n",
    "PARAMETER stop \"<|end_header_id|>\"\n",
    "PARAMETER stop \"<|eot_id|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos condicionar al modelo con un mensaje de sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "\n",
    "PARAMETER temperature 1\n",
    "\n",
    "SYSTEM \"\"\"Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la plantilla que le pasaremos al modelo, los tipos de datos que puede incluir son\n",
    "\n",
    "| Variable | Descripción |\n",
    "| --- | --- |\n",
    "| {{ .System }} | El mensaje del sistema utilizado para especificar un comportamiento personalizado. |\n",
    "| {{ .Prompt }} | El mensaje de aviso del usuario. |\n",
    "| {{ .Response }} | La respuesta del modelo. Al generar una respuesta, se omite el texto después de esta variable. |\n",
    "\n",
    "Como visto así no es muy intuitivo vamos a verlo con un ejemplo, imaginemos que tenemos la siguiente plantilla\n",
    "\n",
    "``` yaml\n",
    "TEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n",
    "{{ .System }}<|im_end|>\n",
    "{{ end }}{{ if .Prompt }}<|im_start|>user\n",
    "{{ .Prompt }}<|im_end|>\n",
    "{{ end }}<|im_start|>assistant\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Si le pasamos al modelo el siguiente prompt `Hola, cómo estás?` la plantilla hará tres cosas\n",
    "\n",
    " * `{{ if .System }}<|im_start|>system{{ .System }}<|im_end|>{{ end }}`: \n",
    "    \n",
    "   Si hay mensaje de sistema se le pasa al modelo de la siguiente manera \n",
    "    \n",
    "   `<|im_start|>system mensaje <|im_end|>`. \n",
    "    \n",
    "   Como en nuestro caso sí hay un mensaje de sistema le pasará \n",
    "    \n",
    "   `<|im_start|>Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales<|im_end|>`\n",
    "\n",
    " * `{{ if .Prompt }}<|im_start|>user{{ .Prompt }}<|im_end|>{{ end }}`: \n",
    "   \n",
    "   Si hay mensaje de usuario se le pasa al modelo de la siguiente manera\n",
    "   \n",
    "   `<|im_start|>user mensaje <|im_end|>`.\n",
    "   \n",
    "   Como en nuestro caso sí hay un mensaje de usuario (el prompt) le pasará\n",
    "   \n",
    "   `<|im_start|>user Hola, cómo estás? <|im_end|>`\n",
    "\n",
    " * `<|im_start|>assistant`:\n",
    " \n",
    "   Siempre le pasará al modelo \n",
    "   \n",
    "   `<|im_start|>assistant` \n",
    "   \n",
    "   para que sepa el LLM que ahora le toca responder\n",
    "\n",
    "Después de verlo por partes podemos poner el mensaje total que se le pasará al LLM y es\n",
    "\n",
    "```\n",
    "<|im_start|>Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales<|im_end|>\n",
    "<|im_start|>user Hola, cómo estás? <|im_end|>\n",
    "<|im_start|>assistant\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo es la plantilla de llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPLATE \"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
      "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
      "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
      "{{ .Response }}<|eot_id|>\"\n"
     ]
    }
   ],
   "source": [
    "!ollama show llama3 --modelfile | grep {{"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es muy parecido al que hemos visto, con la plantilla de llama3, en nuestro caso se le pasará al LLM\n",
    "\n",
    "```\n",
    "<|start_header_id|>system<|end_header_id|>Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>Hola, cómo estás?<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>{{ .Response }}<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos a usar un modelo basado en llama3 vamos a usar su misma template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "\n",
    "PARAMETER temperature 1\n",
    "\n",
    "SYSTEM \"\"\"Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales\"\"\"\n",
    "\n",
    "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
    "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
    "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
    "{{ .Response }}<|eot_id|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LICENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sirve para establecer la licencia del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "\n",
    "PARAMETER temperature 1\n",
    "\n",
    "SYSTEM \"\"\"Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales\"\"\"\n",
    "\n",
    "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
    "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
    "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
    "{{ .Response }}<|eot_id|>\"\"\"\n",
    "\n",
    "LICENSE \"\"\"Apache-2.0\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MESSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos pasarle un historial de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ollama_modelfile/Modelfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ollama_modelfile/Modelfile\n",
    "FROM ./Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\n",
    "\n",
    "PARAMETER temperature 1\n",
    "\n",
    "SYSTEM \"\"\"Eres un asistente de IA, que quiere ayudar, pero no tiene muchas habilidades sociales\"\"\"\n",
    "\n",
    "TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n",
    "{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n",
    "{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n",
    "{{ .Response }}<|eot_id|>\"\"\"\n",
    "\n",
    "LICENSE \"\"\"Apache-2.0\"\"\"\n",
    "\n",
    "MESSAGE user Sabes cómo pasar un array de numpy a un tensor de torch?\n",
    "MESSAGE assistant Sí\n",
    "MESSAGE user Y me lo podrías decir?\n",
    "MESSAGE assistant Sí\n",
    "MESSAGE user Dímelo\n",
    "MESSAGE assistant Te lo puedo decir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAPTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un adapter es una matriz que se añade al modelo. Se usa para no tener que hacer fine tunning a los LLMs y hacerselo solo a esa matriz, de manera que al añadírsela al modelo, el LLM se comporta como nosotros queremos.\n",
    "\n",
    "Por lo que para poder hacer esto se le pasa la ruta con el archivo del adapter\n",
    "\n",
    "`ADAPTER ./ollama-lora.bin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez escrito el modelfile podemos crear el modelo mediante\n",
    "\n",
    "``` bash\n",
    "ollama create <model-name> -f <path-of-the-modelfile>'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transferring model data \n",
      "using existing layer sha256:762b9371a296ab2628592b9462dc676b27d881a3402816492801641a437669b3 \n",
      "using existing layer sha256:ba85fc82591e657e639f779f4fdc5e6e7cb977f9d36487d67cefe589a8af670e \n",
      "using existing layer sha256:995b40fa08b7063fd4fe21ef93f0a1c147d82ecd7e2e87216c11ae809693e101 \n",
      "using existing layer sha256:2af71558e438db0b73a20beab92dc278a94e1bbe974c00c1a33e3ab62d53a608 \n",
      "using existing layer sha256:20dc17cb752058f95ad600722883b22f3328ca8ccccbb667d5e07991ae1985c7 \n",
      "using existing layer sha256:d8ba2f9a17b3bbdeb5690efaa409b3fcb0b56296a777c7a69c78aa33bbddf182 \n",
      "using existing layer sha256:c2ca5c1f4ec2cfaa477ed5f5770beff313b28d083ce5ceb90daed9d6ecbb773f \n",
      "writing manifest \n",
      "success"
     ]
    }
   ],
   "source": [
    "!ollama create assistanmal -f ollama_modelfile/Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos una salida muy similar a la que obtenemos cuando creamos una imagen de docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora listamos los modelos que tenemos disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              \tID          \tSIZE  \tMODIFIED       \n",
      "assistanmal:latest\t2be327b28fbf\t4.9 GB\t48 seconds ago\t\n",
      "llama3:70b        \t786f3184aec0\t39 GB \t38 hours ago  \t\n",
      "llava:latest      \t8dd30f6b0cb1\t4.7 GB\t40 hours ago  \t\n",
      "moondream:latest  \t55fc3abd3867\t1.7 GB\t40 hours ago  \t\n",
      "phi3:latest       \ta2c89ceaed85\t2.3 GB\t2 days ago    \t\n",
      "llama3:latest     \t365c0bd3c000\t4.7 GB\t2 days ago    \t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que está el asistente que acabamos de crear. Vamos a probarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Sabes cómo pasar un array de numpy a un tensor de torch?\n",
      "Sí\n",
      "\n",
      ">>> Y me lo podrías decir?\n",
      "Sí\n",
      "\n",
      ">>> Dímelo\n",
      "Te lo puedo decir\n",
      "\n",
      ">>> Dímelo\n",
      "Para convertir un array de NumPy a un tensor de PyTorch, puedes seguir estos pasos:\n",
      "\n",
      "1. Instalar la librería torch: `pip install torch`\n",
      "2. Importar las librerías necesarias: `import numpy as np`, `import torch`\n",
      "3. Convertir el array de NumPy a una variable torch.Tensor: `tensor = torch.from_numpy(array_numpy)`\n",
      "\n",
      "Aquí tienes un ejemplo:\n",
      "```python\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "# Crea un array en NumPy\n",
      "array_numpy = np.array([1, 2, 3])\n",
      "\n",
      "# Convierte el array de NumPy a una variable torch.Tensor\n",
      "tensor = torch.from_numpy(array_numpy)\n",
      "\n",
      "print(tensor)\n",
      "```\n",
      "Este código crea un array en NumPy y luego lo convierte a una variable `torch.Tensor` utilizando la función `from_numpy`. La salida sería:\n",
      "```python\n",
      "tensor([1, 2, 3])"
     ]
    }
   ],
   "source": [
    "!ollama run assistanmal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
