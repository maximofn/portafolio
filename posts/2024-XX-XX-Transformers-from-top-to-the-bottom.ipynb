{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - from top to the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post vamos a ver cómo funcionan los Transformers de arriba a abajo. Pero si te interesa aprendelo de abajo a arriba lo puedes ver en este post [Transformers - from bottom to the top](https://www.maximofn.com/transformers-from-bottom-to-the-top)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer como una caja negra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura transformer se creo para el problema de traducción, por lo que vamos a explicarlo para ese problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginemos el transformer como una caja negra, a la que le entra una frase en un idioma y saca la misma frase traducida en otro idioma.\n",
    "\n",
    "![Transformer - black box](http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero como hemos visto en el post de [tokens](https://maximofn.com/tokens/), los modelos de lenguaje no entienden las palabras como nosotros, sino que necesitan números para poder realizar las operaciones. Por lo que la frase en el idioma original se tiene que convertir a tokens mediante un tokenizador, y a la salida necesitamos un detokenizador para convertir los tokens de salida a palabras\n",
    "\n",
    "![Transformer - black box - tokenizers](http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-tokenizers.png)\n",
    "\n",
    "De modo que el tokenizador crea una secuencia de $n_{input-tokens}$ tokens, y el detokenizador recibe una secuencia de $n_{output-tokens}$ tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el post de [embeddings](https://maximofn.com/embeddings/) vimos que los embeddings son una forma de representar las palabras en un espacio vectorial. Por lo que los tokens de entrada se pasan por una capa de embeddings para convertirlos en vectores.\n",
    "\n",
    "En un resumen rápido, el proceso de embedding consiste en convertir una secuencia de números (tokens) en una secuencia de vectores. De manera que se crea un nuevo espacio vectorial en el que las palabras que tengan similitud semántia estarán cerca.\n",
    "\n",
    "![word_embedding_3_dimmension](http://maximofn.com/wp-content/uploads/2023/12/word_embedding_3_dimmension.webp)\n",
    "\n",
    "Si teníamos $n_{input-tokens}$ tokens, ahora tenemos $n_{input-tokens}$ vectores. Cada uno de esos vectores tiene una longitud de $d_{model}$. Es decir, se cada token se convierte a un vector que representa ese token en un espacio vectorial de $d_{model}$ dimensiones.\n",
    "Por tanto después de pasar por la capa de embeddings, la secuencia de $n_{input-tokens}$ tokens se convierte en una matriz de ($n_{input-tokens}$ x $d_{model}$).\n",
    "\n",
    "![Transformer - black box - input embeddings](http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-input-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto el transformer actuando como una caja negra, pero en realidad el transformer es una arquitectura que se compone de dos partes, un encoder y un decoder.\n",
    "\n",
    "![Transformer - encoder-decoder](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png)\n",
    "\n",
    "El encoder se encarga de comprimir la información de la frase de entrada, crea un espacio latente donde está esa información de la frase de entrada comprimida. A continuación, esa información comprimida entra al decoder, que sabe convertir esa información comprimida en una frase del idioma de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ¿cómo convierte el decoder esa información comprimidaen una frase del idioma de salida? Pues token a token. Para entenderlo mejor vamos a olvidarnos de los tokens de salida por un momento, vamos a imaginar que tenemos esta arquitectura\n",
    "\n",
    "![Transformer - encoder-decoder (no detokenizer)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-no-detokenizer.png)\n",
    "\n",
    "Es decir, la frase del idioma original se convierte a tokens, estos tokens se convierten a embeddings, que entran al encoder, este comprime la información, el decoder la coge y la convierte en palabras del idioma de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo que el decoder va generando una palabra nueva a la salida en cada paso\n",
    "\n",
    "![Transformer - encoder-decoder (no detokenizer)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer).gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero ¿cómo sabe el decoder cúal es la palabra que tiene que generar cada vez? Porque se le está pasando la frase que ya ha traducido, y en cada paso va generando la siguiente palabra. Es decir, en cada paso el decoder recibe la frase que ha traducido hasta el momento, y genera la siguiente palabra.\n",
    "\n",
    "Pero aun así, ¿cómo sabe que tiene que generar la primera palabra? Porque se le pasa una palabra especial que significa \"empezar a traducir\", y a partir de ahí va generando las siguientes palabras.\n",
    "\n",
    "Y por último, ¿cómo sabe el transformer que tiene que dejar de generar palabras? Porque cuando termina de traducir genera una palabra especial que significa \"fin de la traducción\", que cuando vuelve a entrar al transformer hace que no genere más palabras.\n",
    "\n",
    "![Transformer - encoder-decoder (no detokenizer) (input)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer)%20(input).gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que lo hemos entendido con palabras, que es más sencillo, vamos a volver a colocar el detokenizador a la salida\n",
    "\n",
    "![Transformer - encoder-decoder](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png)\n",
    "\n",
    "Por tanto el decoder irá generando tokens. Para saber que tiene que empezar una frase se le mete un token especial comunmente llamado `SOS` (Start Of Sentence), y para saber que tiene que terminar genera otro token especial comunmente llamado `EOS` (End Of Sentence).\n",
    "\n",
    "Y al igual que el encoder, el token de entrada tiene que pasar por una capa de embedding para convertir los tokens en representaciones vectoriales.\n",
    "\n",
    "Suponiendo que cada token equivale a una palabra, el proceso de traducción sería el siguiente\n",
    "\n",
    "![Transformer - encoder-decoder (detokenizer)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(detokenizer).gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De momento tenemos esta arquitectura\n",
    "\n",
    "![Transformer - encoder-decoder (detokenizer)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-detokenizer-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos dicho que al decoder le entra un token que pasa por la capa de embedding `Output embedding`.\n",
    "\n",
    "El `Output decoder` crea un vector por cada token, por lo que a la salida del `Output decoder` tenemos una matriz de ($n_{output-tokens}$ x $d_{model}$).\n",
    "\n",
    "El decoder hace operaciones, pero saca una matriz con la misma dimensión. Así de necesita convertir esa matriz en un token y eso lo hace mediante una capa lineal que a la salida genera un array con la misma dimensión que los posibles tokens que hay en el lenguaje al que se quiere traducir (vocabulario de salida).\n",
    "\n",
    "Ese array corresponde a los logits de cada posible token, por lo que a continuación se pasa por una capa softmax que convierte esos logits en probabilidades. Es decir, tendremos la probabilidad de que cada token sea el siguiente token.\n",
    "\n",
    "![Transformer - projection](http://maximofn.com/wp-content/uploads/2024/02/Transformer-projection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder y decoder x6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el paper original usan 6 capas para el encoder y otras 6 capas para el decoder. No hay ninguna razón para que sean 6, supongo que probaron varios valores y este fue el que mejor les funcionó.\n",
    "\n",
    "A cada uno de los decoder le entra la salida del último encoder\n",
    "\n",
    "![Transformer - encoder-decoder (x6)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-x6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar el diagrama lo representaremos así a partir de ahora\n",
    "\n",
    "![Transformer - encoder-decoder (Nx)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-Nx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention - Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ir empezando a ver qué ha dentro del encoder y el decoder. Básicamente lo que hay es un mecanismo de atención y una capa feed forward.\n",
    "\n",
    "![Transformer - encoder-decoder - attention-ff](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-attention-ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que en los mecanismos de atención entran 3 flechas. Esto ya lo veremos más adelante cuando veamos en profundidad cómo funcionan los mecanismos de atención.\n",
    "\n",
    "Pero de momento podemos decir que son operaciones que se realizan para poder obtener la relación que existe entre los tokens (y por tanto, la relación que existe entre las palabras).\n",
    "\n",
    "Antes de los transformers, para el problema de traducción se usaban las redes neuronales recurrentes, que consistían en redes a las que les entraba un token, lo procesaban y generaban otro token de salida. A continuación le entraba un segundo token, lo procesaban y sacaban otro otken, y así sucesivamente con todos los tokens de la secuencia de entrada. El problema de estas redes es que cuando las frases eran muy largas, cuando se estaba en los últimos tokens, la red se \"olvidaba\" de los primeros. Por ejemplo en frases muy largas, podría pasar que se cambiase el género del sujeto a lo largo de la frase traducida. Y esto es porque después de muchos tokens, la red se había olvidado si el sujeto era masculino o femenino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucionar esto, en el mecanismo de atención de los transformers entra la secuencia entera y de una sola vez se calculan las relaciones (atención) entre todos los tokens. \n",
    "\n",
    "Esto es muy potente, ya que en un solo cálculo se obtiene la relación entre todos los tokens, sea lo larga que sea la secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque esto es una gran ventaja y es lo que ha hecho que los transformers ahora se utilicen en la mayoría de las mejores redes modernas, también es su mayor desventaja, ya que el cálculo de la atención es muy costoso computacionalmente. Ya que requiere unas multiplicaciones matriciales muy grandes.\n",
    "\n",
    "Esas multiplicaciones se realizan entre matrices que corresponden a los embeddings de cada uno de los tokens por ellas mismas. Es decir, la matriz que representa los embeddings de los tokens se multiplica por ella misma. Para poder realizar esta multiplicación hay que rotar una de las matrices (requisitos del algebra para poder multiplicar matrices). Así que se multiplica una matriz por ella misma, si la secuencia de entrada tiene más tokens, las matrices que se multiplican son más grandes, una en alto y otra en ancho, por lo que la memoria necesaria para almacenar esas matrices crece de forma cuadrática.\n",
    "\n",
    "Por lo que a medida que aumenta la longitud de las secuencias, la cantidad de memoria necesaria para almacenar esas matrices crece de forma cuadrática. Y esto es un gran limitante a día de hoy, la cantidad de memoria que tienen las GPUs, que es dónde se suelen realizar esas multiplicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el encoder se usa una sola capa de atención, para sacar las relaciones entre los tokens de entrada\n",
    "\n",
    "En el decoder se utilizan dos capas de atención, una para sacar las relaciones entre los tokens de salida, y otra para sacar las relaciones entre los tokens del encoder y los del decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de la capa de atención, esta capa la veremos más a fondo más adelante, pero de momento podemos decir que tiene dos propósitos\n",
    "\n",
    " * Uno es añadir no linealidades. Como hemos explicado, la atención se consigue mediante multiplicaciones matriciales de los tokens de las secuencias de entrada. Pero si a una red no se le aplican capas no lineales, al final, toda la arquitectura se podría resumir en unos pocos cálculos lineales. Por lo que las redes neuronales no podrían resolveer problemas no lineales. De modo que se añade esta capa para añadir no linealidad\n",
    "\n",
    "  * Otro es extracción de características. Aunque la atención ya extrae características, estas son características de las relaciones entre los tokens. Pero esta capa se encarga de extraer características de los tokens en sí. Es decir, de cada token se extraen características que se consideran importantes para el problema que se está resolviendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positonal encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos explicado que en la capa de atención se obtienen las relaciones entre los tokens, y que esa relación se calcula mediante multiplicaciones matriciales. Además que esas multiplicaciones se realizan entre la matriz de embeddings por ella misma. Por lo que en las frases `El gato come pescado` y `El pescado come gato`, la relación entre `el` y `gato` es la misma en ambas frases, ya que la relación se calcula mediante multiplicaciones matriciales de los embeddings de `el` y `gato`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, en la primera `el` se refiere a el `gato`, mientras que en la segunda `el` se refiere a el `pescado`. Por lo que además de las relaciones entre las palabras necesitamos tener algún mecanismo que nos indique su posición en la frase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el paper proponen meter un mecanismo de atención que se encarga de sumar unos valores a los vectores de embeddings\n",
    "\n",
    "![Transformer - positional encoding](http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding.png)\n",
    "\n",
    "Donde la fórmula para calcular esos valores es\n",
    "\n",
    "![Transformer - positional encoding (formula)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding-formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esto así en frío es un poco difícil de entender, vamos a ver cómo sería una distribución de valores\n",
    "\n",
    "![Transformer - positional encoding (diagram)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding-diagram.png)\n",
    "\n",
    "Esto quiere decir que al primer token se le van a sumar los valores de la primera fila (la de más abajo), al segundo token los de la segunda fila, y así sucesivamente, lo que provocan un cambio en los embeddings como se ve en la figura. Visto en dos dimensiones se aprecian las ondas que se van sumando.\n",
    "\n",
    "Estas ondas hacen que cuando se realizan los cálculos de atención, las palabras más cercanas tengan más relación que las palabras más lejanas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo en esta imagen, se puede ver como la palabra `it` tiene relación con palabras distintas en función de la frase entera\n",
    "\n",
    "![Transformer - attention sentences](http://maximofn.com/wp-content/uploads/2024/02/Transformer-attention-sentences.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero podemos pensar una cosa, si el proceso de embedding consiste en crear un espacio vectorial en el que las palabras con el mismo significado semántico estén cerca, ¿no se estaría rompiendo esa relación si se suman valores a los embeddings?\n",
    "\n",
    "Si nos volvemos a fijar en el ejemplo de espacio vectorial de antes\n",
    "\n",
    "![word_embedding_3_dimmension](http://maximofn.com/wp-content/uploads/2023/12/word_embedding_3_dimmension.webp)\n",
    "\n",
    "Podemos ver que los valores van más o menos de -1000 a 1000 en cada eje, mientras que la gráfica de distribuciones\n",
    "\n",
    "![Transformer - positional encoding (diagram)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding-diagram.png)\n",
    "\n",
    "los valores van de -1 a 1, ya que es el rango de las funciones seno y coseno.\n",
    "\n",
    "Por lo que estamos variando en un rango de entre -1 y 1 los valores de los embeddings que tienen dos o tres órdenes de magnitud más, por lo que la variación va a ser muy pequeña en comparación con el valor de los embeddings.\n",
    "\n",
    "De modo que ya tenemos una manera de saber la relación de la posición de los tokens en la frase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add & Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo nos queda un bloque de alto nivel y son las capas `Add & Norm`\n",
    "\n",
    "![Transformer - Add & norm](http://maximofn.com/wp-content/uploads/2024/02/Transformer-Add-norm.png)\n",
    "\n",
    "Estas son capas que se añaden después de cada capa de atención y de cada capa feed forward. Esta capa suma la salida y la entrada de una capa. A esto se llama conexiones residuales y tiene las siguientes ventajas\n",
    "\n",
    " * Durante el entrenamiento:\n",
    "\n",
    "   * Reducen el problema del desvanecimiento del gradiente: Cuando una red neuronal es muy grande, en el proceso de entrenamiento, los gradientes se van haciendo cada vez más pequeños según se profundizan en las capas. Esto hace que las capas más profundas no puedan actualizar bien sus pesos. Las conexiones residuales permiten el paso de los gradientes directamente a través de las capas, lo que ayuda a mantenerlos lo suficientemente grandes para que el modelo pueda seguir aprendiendo, incluso en las capas más profundas.\n",
    "\n",
    "   * Permiten el entrenamiento de redes más profundas: Al ayudar a mitigar el problema del desvanecimiento del gradiente, las conexiones residuales también facilitan el entrenamiento de redes más profundas, lo cual puede llevar a mejor rendimiento.\n",
    "\n",
    " * Durante la inferencia:\n",
    "\n",
    "   * Permiten la transmisión de información entre diferentes capas: Como las conexiones residuales permiten que la salida de cada capa se convierta en la suma de la entrada y la salida de la capa, la información de la información de las capas más profundas se transmiten a las capas de más alto nivel. Esto puede ser beneficioso en muchas tareas, especialmente en las que la información de bajo y alto nivel puede ser útil.\n",
    "\n",
    "   * Mejoran la robustez del modelo: Dado que las conexiones residuales permiten que las capas aprendan mejor en las capas más profundas, los modelos con conexiones residuales pueden ser más robustos a perturbaciones en los datos de entrada.\n",
    "\n",
    "   * Permiten la recuperación de información perdida: Si alguna información se pierde durante la transformación en alguna capa, las conexiones residuales pueden permitir que esta información sea recuperada en las capas posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La normalización se añade para que al sumar la entrada y la salida no se disparen los valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos visto todas las capas de alto nivel del transformer\n",
    "\n",
    "![transformer](http://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp)\n",
    "\n",
    "por lo que podemos entrar a ver la parte más importante y que le da nombre al paper, los mecanismos de atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mecanismos de atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de ver el verdadero mecanismo de atención tenemos que ver el multi-head attention\n",
    "\n",
    "![Transformer - multi-head attention](http://maximofn.com/wp-content/uploads/2024/02/Transformer-multi-head-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando hemos explicado las capas de alto nivel, hemos visto que en las capas de atención entraban 3 flechas, estas son `Q`, `K` y `V`. Son matrices que corresponden a la información de los tokens, en el caso del mecanismo de atención del encoder, corresponden a los tokens de la frase del idioma original, y en el caso de la capa de atención del decoder, corresponden a los tokens de la frase que se ha traducido hasta el momento y de la salida del encoder.\n",
    "\n",
    "En realidad ahora nos da igual el orogen de los tokens, solo quédate con la idea que corresponden a tokens. Como hemos explicado los tokens se convierten a embeddings, por lo que `Q`, `K` y `V` son matrices de tamaño ($n_{tokens}$ x $d_{model}$). Normalmente la dimensión del embedding ($d_{model}$) suele ser un número grande, como 512, 1024, 2048, etc (no tiene por que ser una potencia de 2, son solo ejemplos).\n",
    "\n",
    "Hemos explicado que los embeddings son representaciones vectoriales de los tokens. Es decir, los tokens se convierten a espacios vectoriales en los que las palabras con significado semántico similar están cerca.\n",
    "\n",
    "Por tanto de todas esa dimensiones, unas pueden estar relacionadas con características morfológicas, otras con características sintácticas, otras con características semánticas, etc. Por lo que tiene sentido que se calculen los mecanismos de atención entre dimensiones de los embeddings de características similares.\n",
    "\n",
    "Recordemos que los mecanismos de atención buscan similitud entre palabras, por lo que tiene sentido que se busque similitud entre características similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es por esto, que antes de calcular los mecanismos de atención se separan las dimensiones de los embeddings en grupos de características similares, y se calculan los mecanismos de atención entre esos grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y cómo se hace esta separación? Habria que buscar las dimensiones similares, pero hacer esto en un espacio de 512, 1024, 2048, etc dimensiones es muy complicado. Además que no se puede saber que características son similares y que en cada caso cambiaran las características que se consideran similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que se utilizan proyecciones lineales para separar las dimensiones en grupos. Es decir, se pasan los embeddings por capas lineales que los separan en grupos de características similares. De esta manera, durante el entrenamiento del transformer irán cambiando los pesos de las capas lineales hasta llegar a un punto en el que la agrupación se haga de una manera óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos tener la duda de en cuántos grupos dividir. En el paper original se dividen en 8 grupos, pero no hay ninguna razón para que sean 8, supongo que probaron varios valores y este fue el que mejor les funcionó."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se han dividido los embeddings en grupos similares y se ha calculado la atención en los distintos grupos se concatenan los resultados. Esto es lógico, supongamos que tenemos un ebedding de 512 dimensiones, y lo dividimos en 8 grupos de 64 dimensiones, si calculamos la atención en cada uno de los grupos, tendremos 8 matrices de atención 64 dimensiones, si las concatenamos tendremos una matriz de atención 512 dimensiones, que es la misma dimensión que teníamos al principio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero la concatenación hace que todas las características estén juntas, debido a la concatenación. Las primeras 64 dimensiones corresponden a una característica, las siguientes 64 a otra, y así sucesivamente. Así que para se vuelve a pasar una capa lineal que mezcla todas las características. Y esa mezcla se va aprendiendo durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llegamos a la parte más importante del transformer, el mecanismo de atención, el scaled dot product attention\n",
    "\n",
    "![Transformer - scaled dot product attention](http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png)\n",
    "![Transformer - scaled dot product attention formula](http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
