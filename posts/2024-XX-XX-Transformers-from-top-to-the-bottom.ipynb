{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - from top to the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post vamos a ver cómo funcionan los Transformers de arriba a abajo. Pero si te interesa aprendelo de abajo a arriba lo puedes ver en este post [Transformers - from bottom to the top](https://www.maximofn.com/transformers-from-bottom-to-the-top)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer como una caja negra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura transformer se creo para el problema de traducción, por lo que vamos a explicarlo para ese problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginemos el transformer como una caja negra, a la que le entra una frase en un idioma y saca la misma frase traducida en otro idioma.\n",
    "\n",
    "![Transformer - black box](http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero como hemos visto en el post de [tokens](https://maximofn.com/tokens/), los modelos de lenguaje no entienden las palabras como nosotros, sino que necesitan números para poder realizar las operaciones. Por lo que la frase en el idioma original se tiene que convertir a tokens mediante un tokenizador, y a la salida necesitamos un detokenizador para convertir los tokens de salida a palabras\n",
    "\n",
    "![Transformer - black box - tokenizers](http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-tokenizers.png)\n",
    "\n",
    "De modo que el tokenizador crea una secuencia de $n_{input-tokens}$ tokens, y el detokenizador recibe una secuencia de $n_{output-tokens}$ tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el post de [embeddings](https://maximofn.com/embeddings/) vimos que los embeddings son una forma de representar las palabras en un espacio vectorial. Por lo que los tokens de entrada se pasan por una capa de embeddings para convertirlos en vectores.\n",
    "\n",
    "En un resumen rápido, el proceso de embedding consiste en convertir una secuencia de números (tokens) en una secuencia de vectores de. Si teníamos $n_{input-tokens}$ tokens, ahora tenemos $n_{input-tokens}$ vectores. Cada uno de esos vectores tiene una longitud de $d_{model}$. Es decir, se cada token se convierte a un vector que representa ese token en un espacio vectorial de $d_{model}$ dimensiones.\n",
    "Por tanto después de pasar por la capa de embeddings, la secuencia de $n_{input-tokens}$ tokens se convierte en una matriz de ($n_{input-tokens}$ x $d_{model}$).\n",
    "\n",
    "![Transformer - black box - input embeddings](http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-input-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto el transformer actuando como una caja negra, pero en realidad el transformer es una arquitectura que se compone de dos partes, un encoder y un decoder.\n",
    "\n",
    "![Transformer - encoder-decoder](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png)\n",
    "\n",
    "El encoder se encarga de comprimir la información de la frase de entrada, crea un espacio latente donde está esa información de la frase de entrada comprimida. A continuación, esa información comprimida entra al decoder, que sabe convertir esa información comprimida en una frase del idioma de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ¿cómo convierte el decoder esa información comprimidaen una frase del idioma de salida? Pues token a token. Para entenderlo mejor vamos a olvidarnos de los tokens de salida por un momento, vamos a imaginar que tenemos esta arquitectura\n",
    "\n",
    "![Transformer - encoder-decoder (no detokenizer)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-no-detokenizer.png)\n",
    "\n",
    "Es decir, la frase del idioma original se convierte a tokens, estos tokens se convierten a embeddings, que entran al encoder, este comprime la información, el decoder la coge y la convierte en palabras del idioma de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo que el decoder va generando una palabra nueva a la salida en cada paso\n",
    "\n",
    "![Transformer - encoder-decoder (no detokenizer)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer).gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero ¿cómo sabe el decoder cúal es la palabra que tiene que generar cada vez? Porque se le está pasando la frase que ya ha traducido, y en cada paso va generando la siguiente palabra. Es decir, en cada paso el decoder recibe la frase que ha traducido hasta el momento, y genera la siguiente palabra.\n",
    "\n",
    "Pero aun así, ¿cómo sabe que tiene que generar la primera palabra? Porque se le pasa una palabra especial que significa \"empezar a traducir\", y a partir de ahí va generando las siguientes palabras.\n",
    "\n",
    "Y por último, ¿cómo sabe el transformer que tiene que dejar de generar palabras? Porque cuando termina de traducir genera una palabra especial que significa \"fin de la traducción\", que cuando vuelve a entrar al transformer hace que no genere más palabras.\n",
    "\n",
    "![Transformer - encoder-decoder (no detokenizer) (input)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer)%20(input).gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que lo hemos entendido con palabras, que es más sencillo, vamos a volver a colocar el detokenizador a la salida\n",
    "\n",
    "![Transformer - encoder-decoder](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png)\n",
    "\n",
    "Por tanto el decoder irá generando tokens. Para saber que tiene que empezar una frase se le mete un token especial comunmente llamado `SOS` (Start Of Sentence), y para saber que tiene que terminar genera otro token especial comunmente llamado `EOS` (End Of Sentence).\n",
    "\n",
    "Y al igual que el encoder, el token de entrada tiene que pasar por una capa de embedding para convertir los tokens en representaciones vectoriales.\n",
    "\n",
    "Suponiendo que cada token equivale a una palabra, el proceso de traducción sería el siguiente\n",
    "\n",
    "![Transformer - encoder-decoder (detokenizer)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(detokenizer).gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De momento tenemos esta arquitectura\n",
    "\n",
    "![Transformer - encoder-decoder (detokenizer)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-detokenizer-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos dicho que al decoder le entra un token que pasa por la capa de embedding `Output embedding`.\n",
    "\n",
    "El `Output decoder` crea un vector por cada token, por lo que a la salida del `Output decoder` tenemos una matriz de ($n_{output-tokens}$ x $d_{model}$).\n",
    "\n",
    "El decoder hace operaciones, pero saca una matriz con la misma dimensión. Así de necesita convertir esa matriz en un token y eso lo hace mediante una capa lineal que a la salida genera un array con la misma dimensión que los posibles tokens que hay en el lenguaje al que se quiere traducir (vocabulario de salida).\n",
    "\n",
    "Ese array corresponde a los logits de cada posible token, por lo que a continuación se pasa por una capa softmax que convierte esos logits en probabilidades. Es decir, tendremos la probabilidad de que cada token sea el siguiente token.\n",
    "\n",
    "![Transformer - projection](http://maximofn.com/wp-content/uploads/2024/02/Transformer-projection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder y decoder x6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el paper original usan 6 capas para el encoder y otras 6 capas para el decoder. No hay ninguna razón para que sean 6, supongo que probaron varios valores y este fue el que mejor les funcionó.\n",
    "\n",
    "A cada uno de los decoder le entra la salida del último encoder\n",
    "\n",
    "![Transformer - encoder-decoder (x6)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-x6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar el dia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
