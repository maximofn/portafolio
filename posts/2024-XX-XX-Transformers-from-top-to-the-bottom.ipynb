{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - from top to the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post vamos a ver cómo funcionan los Transformers de arriba a abajo. Pero si te interesa aprendelo de abajo a arriba lo puedes ver en este post [Transformers - from bottom to the top](https://www.maximofn.com/transformers-from-bottom-to-the-top)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer como una caja negra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitectura transformer se creo para el problema de traducción, por lo que vamos a explicarlo para ese problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginemos el transformer como una caja negra, a la que le entra una frase en un idioma y saca la misma frase traducida en otro idioma.\n",
    "\n",
    "![Transformer - black box](http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero como hemos visto en el post de [tokens](https://maximofn.com/tokens/), los modelos de lenguaje no entienden las palabras como nosotros, sino que necesitan números para poder realizar las operaciones. Por lo que la frase en el idioma original se tiene que convertir a tokens mediante un tokenizador, y a la salida necesitamos un detokenizador para convertir los tokens de salida a palabras\n",
    "\n",
    "![Transformer - black box - tokenizers](http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-tokenizers.png)\n",
    "\n",
    "De modo que el tokenizador crea una secuencia de $n_{input-tokens}$ tokens, y el detokenizador recibe una secuencia de $n_{output-tokens}$ tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el post de [embeddings](https://maximofn.com/embeddings/) vimos que los embeddings son una forma de representar las palabras en un espacio vectorial. Por lo que los tokens de entrada se pasan por una capa de embeddings para convertirlos en vectores.\n",
    "\n",
    "En un resumen rápido, el proceso de embedding consiste en convertir una secuencia de números (tokens) en una secuencia de vectores de. Si teníamos $n_{input-tokens}$ tokens, ahora tenemos $n_{input-tokens}$ vectores. Cada uno de esos vectores tiene una longitud de $d_{model}$. Es decir, se cada token se convierte a un vector que representa ese token en un espacio vectorial de $d_{model}$ dimensiones.\n",
    "Por tanto después de pasar por la capa de embeddings, la secuencia de $n_{input-tokens}$ tokens se convierte en una matriz de ($n_{input-tokens}$ x $d_{model}$).\n",
    "\n",
    "![Transformer - black box - input embeddings](http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-input-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto el transformer actuando como una caja negra, pero en realidad el transformer es una arquitectura que se compone de dos partes, un encoder y un decoder.\n",
    "\n",
    "![Transformer - encoder-decoder](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png)\n",
    "\n",
    "El encoder se encarga de comprimir la información de la frase de entrada, crea un espacio latente donde está esa información de la frase de entrada comprimida. A continuación, esa información comprimida entra al decoder, que sabe convertir esa información comprimida en una frase del idioma de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ¿cómo convierte el decoder esa información comprimidaen una frase del idioma de salida? Pues token a token. Para entenderlo mejor vamos a olvidarnos de los tokens de salida por un momento, vamos a imaginar que tenemos esta arquitectura\n",
    "\n",
    "![Transformer - encoder-decoder (no detokenizer)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-no-detokenizer.png)\n",
    "\n",
    "Es decir, la frase del idioma original se convierte a tokens, estos tokens se convierten a embeddings, que entran al encoder, este comprime la información, el decoder la coge y la convierte en palabras del idioma de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo que el decoder va generando una palabra nueva a la salida en cada paso\n",
    "\n",
    "![Transformer - encoder-decoder (no detokenizer)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer).gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero ¿cómo sabe el decoder cúal es la palabra que tiene que generar cada vez? Porque se le está pasando la frase que ya ha traducido, y en cada paso va generando la siguiente palabra. Es decir, en cada paso el decoder recibe la frase que ha traducido hasta el momento, y genera la siguiente palabra.\n",
    "\n",
    "Pero aun así, ¿cómo sabe que tiene que generar la primera palabra? Porque se le pasa una palabra especial que significa \"empezar a traducir\", y a partir de ahí va generando las siguientes palabras.\n",
    "\n",
    "Y por último, ¿cómo sabe el transformer que tiene que dejar de generar palabras? Porque cuando termina de traducir genera una palabra especial que significa \"fin de la traducción\", que cuando vuelve a entrar al transformer hace que no genere más palabras.\n",
    "\n",
    "![Transformer - encoder-decoder (no detokenizer) (input)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer)%20(input).gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que lo hemos entendido con palabras, que es más sencillo, vamos a volver a colocar el detokenizador a la salida\n",
    "\n",
    "![Transformer - encoder-decoder](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png)\n",
    "\n",
    "Por tanto el decoder irá generando tokens. Para saber que tiene que empezar una frase se le mete un token especial comunmente llamado `SOS` (Start Of Sentence), y para saber que tiene que terminar genera otro token especial comunmente llamado `EOS` (End Of Sentence).\n",
    "\n",
    "Y al igual que el encoder, el token de entrada tiene que pasar por una capa de embedding para convertir los tokens en representaciones vectoriales.\n",
    "\n",
    "Suponiendo que cada token equivale a una palabra, el proceso de traducción sería el siguiente\n",
    "\n",
    "![Transformer - encoder-decoder (detokenizer)](https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(detokenizer).gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De momento tenemos esta arquitectura\n",
    "\n",
    "![Transformer - encoder-decoder (detokenizer)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-detokenizer-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos dicho que al decoder le entra un token que pasa por la capa de embedding `Output embedding`.\n",
    "\n",
    "El `Output decoder` crea un vector por cada token, por lo que a la salida del `Output decoder` tenemos una matriz de ($n_{output-tokens}$ x $d_{model}$).\n",
    "\n",
    "El decoder hace operaciones, pero saca una matriz con la misma dimensión. Así de necesita convertir esa matriz en un token y eso lo hace mediante una capa lineal que a la salida genera un array con la misma dimensión que los posibles tokens que hay en el lenguaje al que se quiere traducir (vocabulario de salida).\n",
    "\n",
    "Ese array corresponde a los logits de cada posible token, por lo que a continuación se pasa por una capa softmax que convierte esos logits en probabilidades. Es decir, tendremos la probabilidad de que cada token sea el siguiente token.\n",
    "\n",
    "![Transformer - projection](http://maximofn.com/wp-content/uploads/2024/02/Transformer-projection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder y decoder x6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el paper original usan 6 capas para el encoder y otras 6 capas para el decoder. No hay ninguna razón para que sean 6, supongo que probaron varios valores y este fue el que mejor les funcionó.\n",
    "\n",
    "A cada uno de los decoder le entra la salida del último encoder\n",
    "\n",
    "![Transformer - encoder-decoder (x6)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-x6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar el diagrama lo representaremos así a partir de ahora\n",
    "\n",
    "![Transformer - encoder-decoder (Nx)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-Nx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention - Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ir empezando a ver qué ha dentro del encoder y el decoder. Básicamente lo que hay es un mecanismo de atención y una capa feed forward.\n",
    "\n",
    "![Transformer - encoder-decoder - attention-ff](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-attention-ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que en los mecanismos de atención entran 3 flechas. Esto ya lo veremos más adelante cuando veamos en profundidad cómo funcionan los mecanismos de atención.\n",
    "\n",
    "Pero de momento podemos decir que son operaciones que se realizan para poder obtener la relación que existe entre los tokens (y por tanto, la relación que existe entre las palabras).\n",
    "\n",
    "Antes de los transformers, para el problema de traducción se usaban las redes neuronales recurrentes, que consistían en redes a las que les entraba un token, lo procesaban y generaban otro token de salida. A continuación le entraba un segundo token, lo procesaban y sacaban otro otken, y así sucesivamente con todos los tokens de la secuencia de entrada. El problema de estas redes es que cuando las frases eran muy largas, cuando se estaba en los últimos tokens, la red se \"olvidaba\" de los primeros. Por ejemplo en frases muy largas, podría pasar que se cambiase el género del sujeto a lo largo de la frase traducida. Y esto es porque después de muchos tokens, la red se había olvidado si el sujeto era masculino o femenino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucionar esto, en el mecanismo de atención de los transformers entra la secuencia entera y de una sola vez se calculan las relaciones (atención) entre todos los tokens. \n",
    "\n",
    "Esto es muy potente, ya que en un solo cálculo se obtiene la relación entre todos los tokens, sea lo larga que sea la secuencia.\n",
    "\n",
    "Por ejemplo en esta imagen, se puede ver como la palabra `it` tiene relación con palabras distintas en función de la frase entera\n",
    "\n",
    "![Transformer - attention sentences](http://maximofn.com/wp-content/uploads/2024/02/Transformer-attention-sentences.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque esto es una gran ventaja y es lo que ha hecho que los transformers ahora se utilicen en la mayoría de las mejores redes modernas, también es su mayor desventaja, ya que el cálculo de la atención es muy costoso computacionalmente. Ya que requiere unas multiplicaciones matriciales muy grandes.\n",
    "\n",
    "Esas multiplicaciones se realizan entre matrices que corresponden a los embeddings de cada uno de los tokens por ellas mismas. Es decir, la matriz que representa los embeddings de los tokens se multiplica por ella misma. Para poder realizar esta multiplicación hay que rotar una de las matrices (requisitos del algebra para poder multiplicar matrices).\n",
    "\n",
    "Por lo que a medida que aumenta la longitud de las secuencias, la cantidad de memoria necesaria para almacenar esas matrices crece de forma cuadrática. Y esto es un gran limitante a día de hoy, la cantidad de memoria que tienen las GPUs, que es dónde se suelen realizar esas multiplicaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el encoder se usa una sola capa de atención, para sacar las relaciones entre los tokens de entrada\n",
    "\n",
    "En el decoder se utilizan dos capas de atención, una para sacar las relaciones entre los tokens de salida, y otra para sacar las relaciones entre los tokens del encoder y los del decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de la capa de atención, esta capa la veremos más a fondo más adelante, pero de momento podemos decir que tiene dos propósitos\n",
    "\n",
    " * Uno es añadir no linealidades. Como hemos explicado, la atención se consigue mediante multiplicaciones matriciales de los tokens de las secuencias de entrada. Pero si a una red no se le aplican capas no lineales, al final, toda la arquitectura se podría resumir en unos pocos cálculos lineales. Por lo que las redes neuronales no podrían resolveer problemas no lineales. De modo que se añade esta capa para añadir no linealidad\n",
    "\n",
    "  * Otro es extracción de características. Aunque la atención ya extrae características, estas son características de las relaciones entre los tokens. Pero esta capa se encarga de extraer características de los tokens en sí. Es decir, de cada token se extraen características que se consideran importantes para el problema que se está resolviendo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
