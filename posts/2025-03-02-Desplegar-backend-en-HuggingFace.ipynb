{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desplegar backend en HuggingFace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post vamos a ver c√≥mo desplegar un backend en HuggingFace. Vamos a ver c√≥mo hacerlo de dos maneras, mediante la forma com√∫n, creando una aplicaci√≥n con Gradio, y mediante un opci√≥n diferente usando FastAPI y Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ambos casos va a ser necesario tener una cuenta en HuggingFace, ya que vamos a desplegar el backend en un space de HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desplegar backend con Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero de todo, creamos un nuevo espacio en HuggingFace.\n",
    "\n",
    " * Ponemos un nombre, una descripci√≥n y elegimos la licencia.\n",
    " * Elegimos Gradio como el tipo de SDK. Al elegir Gradio, nos aparecer√°n plantillas, as√≠ que elegimos la plantilla de chatbot.\n",
    " * Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t√∫ elige lo que mejor consideres.\n",
    " * Y por √∫ltimo hay que elegor si queremos crear el espacio p√∫blico o privado.\n",
    "\n",
    "![backend gradio - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al desplegar el space, podemos clonarlo o podemos ver los archivos en la propia p√°gina de HuggingFace. Podemos ver que se han creado 3 archivos, `app.py`, `requirements.txt` y `README.md`. As√≠ que vamos a ver qu√© poner en cada uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠ tenemos el c√≥digo de la aplicaci√≥n. C√≥mo hemos elegido la plantilla de chatbot, ya tenemos mucho hecho, pero vamos a tener que cambiar 2 cosas, primero el modelo de lenguaje y el system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como modelo de lenguaje ve√≠a ``HuggingFaceH4/zephyr-7b-beta``, pero vamos a usar ``HuggingFaceTB/SmolLM2-1.7B-Instruct``, que es un modelo muy capaz con solo 1.7B de par√°metros.\n",
    "\n",
    "¬øPor qu√© usamos este modelo? Como hemos elegido un space con un HW gratuito, no vamos a tener mucha potencia de c√≥mputo, por lo que elegimos un modelo peque√±o, pero aceptabel. En tu caso, si quieres un modelo m√°s grande y mejor tienes dos opciones, dejar el mismo HW sabiendo que la inferencia va a ser mucho m√°s lenta, o cambiar el HW por uno m√°s potente, pero de pago.\n",
    "\n",
    "As√≠ que busca el texto ``client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")`` y reempl√°zalo por ``client = InferenceClient(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")``, o espera que m√°s adelante pondr√© todo el c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n vamos a cambiar el system prompt, que por defecto es ``You are a friendly Chatbot.``, pero como es un modelo entrenado en su mayor√≠a en ingl√©s, es probable que si le hablas en otro idioma te responda en ingl√©s, as√≠ que vamos a cambiarlo por ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.\n",
    "\n",
    "As√≠ que busca el texto ``gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),`` y reempl√°zalo por ``gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),``, o espera a que ahora voy a poner todo el c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "import gradio as gr\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\"\"\"\n",
    "For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
    "\"\"\"\n",
    "client = InferenceClient(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "\n",
    "\n",
    "def respond(\n",
    "    message,\n",
    "    history: list[tuple[str, str]],\n",
    "    system_message,\n",
    "    max_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    for val in history:\n",
    "        if val[0]:\n",
    "            messages.append({\"role\": \"user\", \"content\": val[0]})\n",
    "        if val[1]:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    for message in client.chat_completion(\n",
    "        messages,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    ):\n",
    "        token = message.choices[0].delta.content\n",
    "\n",
    "        response += token\n",
    "        yield response\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/chatinterface\n",
    "\"\"\"\n",
    "demo = gr.ChatInterface(\n",
    "    respond,\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.95,\n",
    "            step=0.05,\n",
    "            label=\"Top-p (nucleus sampling)\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el archivo en el que estar√°n escritas las dependencias, pero para este caso va a ser muy sencillo:\n",
    "\n",
    "``` txt\n",
    "huggingface_hub==0.25.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el archivo en el que vamos a poner la informaci√≥n del espacio. En los spaces de HuggingFace, al inicio de los readmes, se pone un c√≥digo para que HuggingFace sepa c√≥mo mostrar la miniatura del espacio, qu√© fichero tiene que usar para ejecutar el c√≥digo, version del sdk, etc.\n",
    "\n",
    "``` md\n",
    "---\n",
    "title: SmolLM2\n",
    "emoji: üí¨\n",
    "colorFrom: yellow\n",
    "colorTo: purple\n",
    "sdk: gradio\n",
    "sdk_version: 5.0.1\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: apache-2.0\n",
    "short_description: Gradio SmolLM2 chat\n",
    "---\n",
    "\n",
    "An example chatbot using [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index), and the [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despliegue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.\n",
    "\n",
    "As√≠ que cuando est√©n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.\n",
    "\n",
    "![backend gradio - chatbot](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, hemos hecho un chatbot, pero no era la intenci√≥n, aqu√≠ hab√≠amos venido a hacer un backend! Para, para, f√≠jate lo que pone debajo del chatbot\n",
    "\n",
    "![backend gradio - Use via API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver una texto ``Use via API``, donde si pulsamos se nos abre un men√∫ con una API para poder usar el chatbot.\n",
    "\n",
    "![backend gradio - API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nos da una documentaci√≥n de c√≥mo usar la API, tanto con Python, con JavaScript, como con bash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de la API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el c√≥digo de ejemplo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://maximofn-smollm2.hf.space ‚úî\n",
      "Hola! Estoy bien, gracias por preguntar. ¬øY usted?\n",
      "CPU times: user 77.3 ms, sys: 11.6 ms, total: 88.9 ms\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"Maximofn/SmolLM2\")\n",
    "result = client.predict(\n",
    "\t\tmessage=\"Hola, ¬øc√≥mo est√°s?\",\n",
    "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
    "\t\tmax_tokens=512,\n",
    "\t\ttemperature=0.7,\n",
    "\t\ttop_p=0.95,\n",
    "\t\tapi_name=\"/chat\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He a√±adido un `%%time` para que se vea el tiempo que tarda en responder. Hay que tener en cuenta el tiempo de inferencia y el tiempo de conexi√≥n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
