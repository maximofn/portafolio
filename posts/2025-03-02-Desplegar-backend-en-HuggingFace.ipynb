{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Desplegar backend en HuggingFace\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este post vamos a ver c칩mo desplegar un backend en HuggingFace. Vamos a ver c칩mo hacerlo de dos maneras, mediante la forma com칰n, creando una aplicaci칩n con Gradio, y mediante una opci칩n diferente usando FastAPI, Langchain y Docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para ambos casos va a ser necesario tener una cuenta en HuggingFace, ya que vamos a desplegar el backend en un space de HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend con Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero de todo, creamos un nuevo espacio en Hugging Face.\n",
        "\n",
        " * Ponemos un nombre, una descripci칩n y elegimos la licencia.\n",
        " * Elegimos Gradio como el tipo de SDK. Al elegir Gradio, nos aparecer치n plantillas, as칤 que elegimos la plantilla de chatbot.\n",
        " * Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t칰 elige lo que mejor consideres.\n",
        " * Y por 칰ltimo hay que elegir si queremos crear el espacio p칰blico o privado.\n",
        "\n",
        "![backend gradio - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C칩digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al crear el space, podemos clonarlo o podemos ver los archivos en la propia p치gina de HuggingFace. Podemos ver que se han creado 3 archivos, `app.py`, `requirements.txt` y `README.md`. As칤 que vamos a ver qu칠 poner en cada uno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqu칤 tenemos el c칩digo de la aplicaci칩n. Como hemos elegido la plantilla de chatbot, ya tenemos mucho hecho, pero vamos a tener que cambiar 2 cosas, primero el modelo de lenguaje y el system prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como modelo de lenguaje veo ``HuggingFaceH4/zephyr-7b-beta``, pero vamos a usar ``Qwen/Qwen2.5-72B-Instruct``, que es un modelo muy capaz.\n",
        "\n",
        "As칤 que busca el texto ``client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")`` y reempl치zalo por ``client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")``, o espera que m치s adelante pondr칠 todo el c칩digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tambi칠n vamos a cambiar el system prompt, que por defecto es ``You are a friendly Chatbot.``, pero como es un modelo entrenado en su mayor칤a en ingl칠s, es probable que si le hablas en otro idioma te responda en ingl칠s, as칤 que vamos a cambiarlo por ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.\n",
        "\n",
        "As칤 que busca el texto ``gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),`` y reempl치zalo por ``gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),``, o espera a que ahora voy a poner todo el c칩digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` python\n",
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "\"\"\"\n",
        "For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
        "\"\"\"\n",
        "client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")\n",
        "\n",
        "\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    for val in history:\n",
        "        if val[0]:\n",
        "            messages.append({\"role\": \"user\", \"content\": val[0]})\n",
        "        if val[1]:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    for message in client.chat_completion(\n",
        "        messages,\n",
        "        max_tokens=max_tokens,\n",
        "        stream=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    ):\n",
        "        token = message.choices[0].delta.content\n",
        "\n",
        "        response += token\n",
        "        yield response\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/chatinterface\n",
        "\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),\n",
        "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=1.0,\n",
        "            value=0.95,\n",
        "            step=0.05,\n",
        "            label=\"Top-p (nucleus sampling)\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este es el archivo en el que estar치n escritas las dependencias, pero para este caso va a ser muy sencillo:\n",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este es el archivo en el que vamos a poner la informaci칩n del espacio. En los spaces de HuggingFace, al inicio de los readmes, se pone un c칩digo para que HuggingFace sepa c칩mo mostrar la miniatura del espacio, qu칠 fichero tiene que usar para ejecutar el c칩digo, versi칩n del sdk, etc.\n",
        "\n",
        "``` md\n",
        "---\n",
        "title: SmolLM2\n",
        "emoji: 游눫\n",
        "colorFrom: yellow\n",
        "colorTo: purple\n",
        "sdk: gradio\n",
        "sdk_version: 5.0.1\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "short_description: Gradio SmolLM2 chat\n",
        "---\n",
        "\n",
        "An example chatbot using [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index), and the [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Despliegue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.\n",
        "\n",
        "As칤 que cuando est칠n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.\n",
        "\n",
        "![backend gradio - chatbot](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Muy bien, hemos hecho un chatbot, pero no era la intenci칩n, aqu칤 hab칤amos venido a hacer un backend! Para, para, f칤jate lo que pone debajo del chatbot\n",
        "\n",
        "![backend gradio - Use via API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver un texto ``Use via API``, donde si pulsamos se nos abre un men칰 con una API para poder usar el chatbot.\n",
        "\n",
        "![backend gradio - API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que nos da una documentaci칩n de c칩mo usar la API, tanto con Python, con JavaScript, como con bash."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prueba de la API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos el c칩digo de ejemplo de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space 九\n",
            "춰Hola M치ximo! Mucho gusto, estoy bien, gracias por preguntar. 쮺칩mo est치s t칰? 쮼n qu칠 puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, 쯖칩mo est치s? Me llamo M치ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estamos haciendo llamadas a la API de `InferenceClient` de HuggingFace, as칤 que podr칤amos pensar, 쯇ara qu칠 hemos hecho un backend, si podemos llamar directamente a la API de HuggingFace? Pues lo vas a ver a continuaci칩n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tu nombre es M치ximo. 쮼s correcto?\n"
          ]
        }
      ],
      "source": [
        "result = client.predict(\n",
        "\t\tmessage=\"쮺칩mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La plantilla de chat de Gradio maneja el historial por nosotros, de manera que cada vez que creamos un nuevo `cliente`, se crea un nuevo hilo de conversaci칩n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a probar a crear un nuevo cliente, y ver si se crea un nuevo hilo de conversaci칩n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space 九\n",
            "Hola Luis, estoy muy bien, gracias por preguntar. 쮺칩mo est치s t칰? Es un gusto conocerte. 쮼n qu칠 puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "new_client = Client(\"Maximofn/SmolLM2\")\n",
        "result = new_client.predict(\n",
        "\t\tmessage=\"Hola, 쯖칩mo est치s? Me llamo Luis\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora le volvemos a preguntar c칩mo me llamo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Te llamas Luis. 쮿ay algo m치s en lo que pueda ayudarte?\n"
          ]
        }
      ],
      "source": [
        "result = new_client.predict(\n",
        "\t\tmessage=\"쮺칩mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos, tenemos dos clientes, cada uno con su propio hilo de conversaci칩n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend con FastAPI, Langchain y Docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vamos a hacer lo mismo, crear un backend de un chatbot, con el mismo modelo, pero en este caso usando FastAPI, Langchain y Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos que crear un nuevo espacio, pero en este caso lo haremos de otra manera\n",
        "\n",
        " * Ponemos un nombre, una descripci칩n y elegimos la licencia.\n",
        " * Elegimos Docker como el tipo de SDK. Al elegir Docker, nos aparecer치n plantillas, as칤 que elegimos una plantilla en blanco.\n",
        " * Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t칰 elige lo que mejor consideres.\n",
        " * Y por 칰ltimo hay que elegir si queremos crear el espacio p칰blico o privado.\n",
        "\n",
        "![backend docker - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C칩digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora, al crear el space, vemos que solo tenemos un archivo, el ``README.md``. As칤 que vamos a tener que crear todo el c칩digo nosotros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear el c칩digo de la aplicaci칩n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Empezamos con las librer칤as necesarias\n",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "```\n",
        "\n",
        "Cargamos `fastapi` para poder crear las rutas de la API, `pydantic` para crear la plantilla de las querys, `huggingface_hub` para poder crear un modelo de lenguaje, `langchain` para poder indicarle al modelo si los mensajes son del chatbot o del usuario y `langgraph` para poder crear el chatbot.\n",
        "\n",
        "Adem치s cargamos `os` y `dotenv` para poder cargar las variables de entorno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cargamos el token de HuggingFace\n",
        "\n",
        "``` python\n",
        "# HuggingFace token\n",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos el modelo de lenguaje\n",
        "\n",
        "``` python\n",
        "# Initialize the HuggingFace model\n",
        "model = InferenceClient(\n",
        "    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos ahora una funci칩n para llamar al modelo\n",
        "\n",
        "``` python\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to HuggingFace format\n",
        "    hf_messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Call the API\n",
        "    response = model.chat_completion(\n",
        "        messages=hf_messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=64,\n",
        "        top_p=0.7\n",
        "    )\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response.choices[0].message.content)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "```\n",
        "\n",
        "Convertimos los mensajes de formato LangChain a formato HuggingFace, as칤 podemos usar el modelo de lenguaje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos una plantilla para las queries\n",
        "\n",
        "``` python\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "    thread_id: str = \"default\"\n",
        "```\n",
        "\n",
        "Las queries van a tener un `query`, el mensaje del usuario, y un `thread_id`, que es el identificador del hilo de la conversaci칩n y m치s adelante explicaremos para qu칠 lo usamos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos un grafo de LangGraph\n",
        "\n",
        "``` python\n",
        "# Define the graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "graph_app = workflow.compile(checkpointer=memory)\n",
        "```\n",
        "\n",
        "Con esto lo que hacemos es crear un grafo de LangGraph, que es una estructura de datos que nos permite crear un chatbot y que gestiona por nosotros el estado del chatbot, es decir, entre otras cosas, el historial de mensajes. As칤 no lo tenemos que hacer nosotros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos la aplicaci칩n de FastAPI\n",
        "\n",
        "``` python\n",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API to generate text using LangChain and LangGraph\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos los endpoints de la API\n",
        "\n",
        "``` python\n",
        "# Welcome endpoint\n",
        "@app.get(\"/\")\n",
        "async def api_home():\n",
        "    \"\"\"Welcome endpoint\"\"\"\n",
        "    return {\"detail\": \"Welcome to FastAPI, Langchain, Docker tutorial\"}\n",
        "\n",
        "# Generate endpoint\n",
        "@app.post(\"/generate\")\n",
        "async def generate(request: QueryRequest):\n",
        "    \"\"\"\n",
        "    Endpoint to generate text using the language model\n",
        "    \n",
        "    Args:\n",
        "        request: QueryRequest\n",
        "        query: str\n",
        "        thread_id: str = \"default\"\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Configure the thread ID\n",
        "        config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
        "        \n",
        "        # Create the input message\n",
        "        input_messages = [HumanMessage(content=request.query)]\n",
        "        \n",
        "        # Invoke the graph\n",
        "        output = graph_app.invoke({\"messages\": input_messages}, config)\n",
        "        \n",
        "        # Get the model response\n",
        "        response = output[\"messages\"][-1].content\n",
        "        \n",
        "        return {\n",
        "            \"generated_text\": response,\n",
        "            \"thread_id\": request.thread_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error al generar texto: {str(e)}\")\n",
        "```\n",
        "\n",
        "Hemos creado el endpoint `/` que nos devolver치 un texto cuando accedamos a la API, y el endpoint `/generate` que es el que usaremos para generar el texto.\n",
        "\n",
        "Si nos fijamos en la funci칩n `generate` tenemos la variable `config`, que es un diccionario que contiene el `thread_id`. Este `thread_id` es el que nos permite tener un historial de mensajes de cada usuario, de esta manera, diferentes usuarios pueden usar el mismo endpoint y tener su propio historial de mensajes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por 칰ltimo, tenemos el c칩digo para que se pueda ejecutar la aplicaci칩n\n",
        "\n",
        "``` python\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a escribir todo el c칩digo junto\n",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# HuggingFace token\n",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "model = InferenceClient(\n",
        "    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        ")\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to HuggingFace format\n",
        "    hf_messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Call the API\n",
        "    response = model.chat_completion(\n",
        "        messages=hf_messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=64,\n",
        "        top_p=0.7\n",
        "    )\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response.choices[0].message.content)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "\n",
        "# Define the graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "graph_app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "# Define the data model for the request\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "    thread_id: str = \"default\"\n",
        "\n",
        "# Create the FastAPI application\n",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API to generate text using LangChain and LangGraph\")\n",
        "\n",
        "# Welcome endpoint\n",
        "@app.get(\"/\")\n",
        "async def api_home():\n",
        "    \"\"\"Welcome endpoint\"\"\"\n",
        "    return {\"detail\": \"Welcome to FastAPI, Langchain, Docker tutorial\"}\n",
        "\n",
        "# Generate endpoint\n",
        "@app.post(\"/generate\")\n",
        "async def generate(request: QueryRequest):\n",
        "    \"\"\"\n",
        "    Endpoint to generate text using the language model\n",
        "    \n",
        "    Args:\n",
        "        request: QueryRequest\n",
        "        query: str\n",
        "        thread_id: str = \"default\"\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Configure the thread ID\n",
        "        config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
        "        \n",
        "        # Create the input message\n",
        "        input_messages = [HumanMessage(content=request.query)]\n",
        "        \n",
        "        # Invoke the graph\n",
        "        output = graph_app.invoke({\"messages\": input_messages}, config)\n",
        "        \n",
        "        # Get the model response\n",
        "        response = output[\"messages\"][-1].content\n",
        "        \n",
        "        return {\n",
        "            \"generated_text\": response,\n",
        "            \"thread_id\": request.thread_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error al generar texto: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vemos c칩mo crear el Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero indicamos desde qu칠 imagen vamos a partir\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora creamos el directorio de trabajo\n",
        "\n",
        "``` dockerfile\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copiamos el archivo con las dependencias e instalamos\n",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copiamos el resto del c칩digo\n",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user . /app\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exponemos el puerto 7860\n",
        "\n",
        "``` dockerfile\n",
        "EXPOSE 7860\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos las variables de entorno\n",
        "\n",
        "``` dockerfile\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\\n",
        "    test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Secret exists!\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por 칰ltimo, indicamos el comando para ejecutar la aplicaci칩n\n",
        "\n",
        "``` dockerfile\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora lo ponemos todo junto\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "\n",
        "COPY --chown=user . /app\n",
        "\n",
        "EXPOSE 7860\n",
        "\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\\n",
        "    test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Secret exists!\"\n",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos el archivo con las dependencias\n",
        "\n",
        "``` txt\n",
        "fastapi\n",
        "uvicorn\n",
        "requests\n",
        "pydantic>=2.0.0\n",
        "langchain\n",
        "langchain-huggingface\n",
        "langchain-core\n",
        "langgraph > 0.2.27\n",
        "python-dotenv.2.11\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por 칰ltimo, creamos el archivo README.md con informaci칩n del espacio y con las intrucciones para HugginFace\n",
        "\n",
        "``` md\n",
        "---\n",
        "title: SmolLM2 Backend\n",
        "emoji: 游늵\n",
        "colorFrom: yellow\n",
        "colorTo: red\n",
        "sdk: docker\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "short_description: Backend of SmolLM2 chat\n",
        "app_port: 7860\n",
        "---\n",
        "\n",
        "# SmolLM2 Backend\n",
        "\n",
        "This project implements a FastAPI API that uses LangChain and LangGraph to generate text with the Qwen2.5-72B-Instruct model from HuggingFace.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "### In HuggingFace Spaces\n",
        "\n",
        "This project is designed to run in HuggingFace Spaces. To configure it:\n",
        "\n",
        "1. Create a new Space in HuggingFace with SDK Docker\n",
        "2. Configure the `HUGGINGFACE_TOKEN` or `HF_TOKEN` environment variable in the Space configuration:\n",
        "   - Go to the \"Settings\" tab of your Space\n",
        "   - Scroll down to the \"Repository secrets\" section\n",
        "   - Add a new variable with the name `HUGGINGFACE_TOKEN` and your token as the value\n",
        "   - Save the changes\n",
        "\n",
        "### Local development\n",
        "\n",
        "For local development:\n",
        "\n",
        "1. Clone this repository\n",
        "2. Create a `.env` file in the project root with your HuggingFace token:\n",
        "   ``\n",
        "   HUGGINGFACE_TOKEN=your_token_here\n",
        "   ``\n",
        "3. Install the dependencies:\n",
        "   ``\n",
        "   pip install -r requirements.txt\n",
        "   ``\n",
        "\n",
        "## Local execution\n",
        "\n",
        "``bash\n",
        "uvicorn app:app --reload\n",
        "``\n",
        "\n",
        "The API will be available at `http://localhost:8000`.\n",
        "\n",
        "## Endpoints\n",
        "\n",
        "### GET `/`\n",
        "\n",
        "Welcome endpoint that returns a greeting message.\n",
        "\n",
        "### POST `/generate`\n",
        "\n",
        "Endpoint to generate text using the language model.\n",
        "\n",
        "**Request parameters:**\n",
        "``json\n",
        "{\n",
        "  \"query\": \"Your question here\",\n",
        "  \"thread_id\": \"optional_thread_identifier\"\n",
        "}\n",
        "``\n",
        "\n",
        "**Response:**\n",
        "``json\n",
        "{\n",
        "  \"generated_text\": \"Generated text by the model\",\n",
        "  \"thread_id\": \"thread identifier\"\n",
        "}\n",
        "``\n",
        "\n",
        "## Docker\n",
        "\n",
        "To run the application in a Docker container:\n",
        "\n",
        "``bash\n",
        "# Build the image\n",
        "docker build -t smollm2-backend .\n",
        "\n",
        "# Run the container\n",
        "docker run -p 8000:8000 --env-file .env smollm2-backend\n",
        "``\n",
        "\n",
        "## API documentation\n",
        "\n",
        "The interactive API documentation is available at:\n",
        "- Swagger UI: `http://localhost:8000/docs`\n",
        "- ReDoc: `http://localhost:8000/redoc`\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token de HuggingFace\n",
        "\n",
        "Si te has fijado en el c칩digo y en el Dockerfile hemos usado un token de HuggingFace, as칤 que vamos a tener que crear uno. En nuestra cuenta de HuggingFace creamos un [nuevo token](https://huggingface.co/settings/tokens/new?tokenType=fineGrained), le ponemos un nombre y le damos los siguientes permisos:\n",
        "\n",
        " * Read access to contents of all repos under your personal namespace\n",
        " * Read access to contents of all repos under your personal namespacev\n",
        " * Make calls to inference providers\n",
        " * Make calls to Inference Endpoints\n",
        "\n",
        "![backend docker - token](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-token.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A침adir el token a los secrets del espacio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que ya tenemos el token, necesitamos a침adirlo al espacio. En la parte de arriba de la app, podremos ver un bot칩n llamado `Settings`, lo pulsamos y podremos ver la secci칩n de configuraci칩n del espacio.\n",
        "\n",
        "Si bajamos, podremos ver una secci칩n en la que podemos a침adir `Variables` y `Secrets`. En este caso, como estamos a침adiendo un token, lo vamos a a침adir a los `Secrets`.\n",
        "\n",
        "Le ponemos el nombre `HUGGINGFACE_TOKEN` y el valor del token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Despliegue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.\n",
        "\n",
        "As칤 que cuando est칠n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.\n",
        "\n",
        "En este caso, solo hemos construido un backend, por lo que lo que vamos a ver al entrar al espacio es lo que definimos en el endpoint `/`\n",
        "\n",
        "![backend docker - space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### URL del backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Necesitamos saber la URL del backend para poder hacer llamadas a la API. Para ello, tenemos que pulsar en los tres puntos de la parte superior derecha para ver las opciones\n",
        "\n",
        "![backend docker - options](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-options.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el men칰 que se despliega pulsamos en `Embed this Spade`, se nos abrir치 una ventana en la que indica c칩mo embeber el espacio con un iframe y adem치s nos dar치 la URL del espacio.\n",
        "\n",
        "![backend docker - embed](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-embed.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si ahora nos vamos a esa URL, veremos lo mismo que en el espacio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentaci칩n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FastAPI, a parte de ser una API rapid칤sima, tiene otra gran ventaja, y es que genera documentaci칩n de manera autom치tica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si a침adimos `/docs` a la URL que vimos antes, podremos ver la documentaci칩n de la API con `Swagger UI`.\n",
        "\n",
        "![backend docker - swagger doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-swagger-doc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tambi칠n podemos a침adir `/redoc` a la URL para ver la documentaci칩n con `ReDoc`.\n",
        "\n",
        "![backend docker - redoc doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-redoc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prueba de la API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo bueno de la documentaci칩n `Swagger UI` es que nos permite probar la API directamente desde el navegador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A침adimos `/docs` a la URL que obtuvimos, abrimos el desplegable del endpoint `/generate` y le damos a `Try it out`, modificamos el valor de la `query` y del `thread_id` y pulsamos en `Execute`.\n",
        "\n",
        "En el primer caso voy a poner\n",
        "\n",
        " * **query**: Hola, 쮺칩mo est치s? Soy M치ximo\n",
        " * **thread_id**: user1\n",
        "\n",
        "![backend docker - test API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-test-API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recibimos la siguiente respuesta `춰Hola M치ximo! Estoy muy bien, gracias por preguntar. 쮺칩mo est치s t칰? 쮼n qu칠 puedo ayudarte hoy?`\n",
        "\n",
        "![backend docker -response 1 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user1.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a probar ahora la misma pregunta, pero con un `thread_id` diferente, en este caso `user2`.\n",
        "\n",
        "![backend docker - query 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-query1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y nos responde esto `춰Hola Luis! Estoy muy bien, gracias por preguntar. 쮺칩mo est치s t칰? 쮼n qu칠 puedo ayudarte hoy?`\n",
        "\n",
        "![backend docker - response 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora pedimos nuestro nombre con los dos usuarios y obtenemos esto\n",
        "\n",
        " * Para el usuario **user1**: `Te llamas M치ximo. 쮿ay algo m치s en lo que pueda ayudarte?`\n",
        " * Para el usuario **user2**: `Te llamas Luis. 쮿ay algo m치s en lo que pueda ayudarte hoy, Luis?`\n",
        "\n",
        "![backend docker - response 2 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user1.webp)\n",
        "\n",
        "![backend docker - response 2 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend con Gradio y modelo corriendo en el servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los dos backends que hemos creado en realidad no est치n corriendo un modelo, sino que est치n haciendo llamadas a Inference Endpoints de HuggingFace. Pero puede que queramos que todo corra en el servidor, incluso el modelo. Puede ser que hayas hecho un fine-tuning de un LLM para tu caso de uso, por lo que ya no puedes hacer llamadas a Inference Endpoints.\n",
        "\n",
        "As칤 que vamos a ver c칩mo modificar el c칩digo de los dos backends para correr un modelo en el servidor y no hacer llamadas a Inference Endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripci칩n, seleccionamos Gradio como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW m치s b치sico y gratuito, y seleccionamos si lo hacemos privado o p칰blico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C칩digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos que hacer cambios en `app.py` y en `requirements.txt` para que en lugar de hacer llamadas a Inference Endpoints, se ejecute el modelo localmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los cambios que tenemos que hacer son"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importar `AutoModelForCausalLM` y `AutoTokenizer` de la librer칤a `transformers` e importar `torch`\n",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En lugar de crear un modelo mediante `InferenceClient` lo creamos con `AutoModelForCausalLM` y `AutoTokenizer`\n",
        "\n",
        "``` python\n",
        "# Cargar el modelo y el tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "```\n",
        "\n",
        "Utilizo `HuggingFaceTB/SmolLM2-1.7B-Instruct` porque es un modelo bastante capaz de solo 1.7B de par치metros. Como he elegido el HW m치s b치sico no puedo usar modelos muy grandes. T칰, si quieres usar un modelo m치s grande tienes dos opciones, usar el HW gratuito y aceptar que la inferencia va a ser m치s lenta, o usar un HW m치s potente, pero de pago."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modificar la funci칩n `respond` para que construya el prompt con la estructura necesaria por la librer칤a `transformers`, tokenizar el prompt, hacer la inferencia y destokenizar la respuesta.\n",
        "\n",
        "``` python\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    # Construir el prompt con el formato correcto\n",
        "    prompt = f\"<|system|>\\n{system_message}</s>\\n\"\n",
        "    \n",
        "    for val in history:\n",
        "        if val[0]:\n",
        "            prompt += f\"<|user|>\\n{val[0]}</s>\\n\"\n",
        "        if val[1]:\n",
        "            prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"\n",
        "    \n",
        "    prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"\n",
        "    \n",
        "    # Tokenizar el prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generar la respuesta\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decodificar la respuesta\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extraer solo la parte de la respuesta del asistente\n",
        "    response = response.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "    \n",
        "    yield response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuaci칩n dejo todo el c칩digo\n",
        "\n",
        "``` python\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
        "\"\"\"\n",
        "\n",
        "# Cargar el modelo y el tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    # Construir el prompt con el formato correcto\n",
        "    prompt = f\"<|system|>\\n{system_message}</s>\\n\"\n",
        "    \n",
        "    for val in history:\n",
        "        if val[0]:\n",
        "            prompt += f\"<|user|>\\n{val[0]}</s>\\n\"\n",
        "        if val[1]:\n",
        "            prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"\n",
        "    \n",
        "    prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"\n",
        "    \n",
        "    # Tokenizar el prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generar la respuesta\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decodificar la respuesta\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extraer solo la parte de la respuesta del asistente\n",
        "    response = response.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "    \n",
        "    yield response\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/chatinterface\n",
        "\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(\n",
        "            value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", \n",
        "            label=\"System message\"\n",
        "        ),\n",
        "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=1.0,\n",
        "            value=0.95,\n",
        "            step=0.05,\n",
        "            label=\"Top-p (nucleus sampling)\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este archivo hay que a침adir las nuevas librer칤as que vamos a usar, en este caso `transformers`, `accelerate` y `torch`. El archivo entero quedar칤a:\n",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2\n",
        "gradio>=4.0.0\n",
        "transformers>=4.36.0\n",
        "torch>=2.0.0\n",
        "accelerate>=0.25.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prueba de la API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Desplegamos el space y probamos directamente la API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2-localmodel.hf.space 九\n",
            "Hola M치ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en d칤a. 쮺칩mo puedo servirte?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2_localModel\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, 쯖칩mo est치s? Me llamo M치ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Me sorprende lo r치pido que responde el modelo estando en un servidor sin GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora hacemos lo mismo que antes, pero con FastAPI, LangChain y Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripci칩n, seleccionamos Docker como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW m치s b치sico y gratuito, y seleccionamos si lo hacemos privado o p칰blico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C칩digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya no importamos `InferenceClient` y ahora importamos `AutoModelForCausalLM` y `AutoTokenizer` de la librer칤a `transformers` e importamos `torch`.\n",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instanciamos el modelo y el tokenizer con `AutoModelForCausalLM` y `AutoTokenizer`.\n",
        "\n",
        "``` python\n",
        "# Initialize the model and tokenizer\n",
        "print(\"Cargando modelo y tokenizer...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "\n",
        "try:\n",
        "    # Load the model in BF16 format for better performance and lower memory usage\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        print(\"Usando GPU para el modelo...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "    else:\n",
        "        print(\"Usando CPU para el modelo...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map={\"\": device},\n",
        "            torch_dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    print(f\"Modelo cargado exitosamente en: {device}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo: {str(e)}\")\n",
        "    raise\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redefinimos la funci칩n `call_model` para que haga la inferencia con el modelo local.\n",
        "\n",
        "``` python\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to chat format\n",
        "    messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Prepare the input using the chat template\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=512,  # Increase the number of tokens for longer responses\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode and clean the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant's response (after the last user message)\n",
        "    response = response.split(\"Assistant:\")[-1].strip()\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos que quitar `langchain-huggingface` y a침adir `transformers`, `accelerate` y `torch` en el archivo `requirements.txt`. El archivo quedar칤a:\n",
        "\n",
        "``` txt\n",
        "fastapi\n",
        "uvicorn\n",
        "requests\n",
        "pydantic>=2.0.0\n",
        "langchain>=0.1.0\n",
        "langchain-core>=0.1.10\n",
        "langgraph>=0.2.27\n",
        "python-dotenv>=1.0.0\n",
        "transformers>=4.36.0\n",
        "torch>=2.0.0\n",
        "accelerate>=0.26.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya no necesitamos tener `RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true` porque como el modelo va a estar en el servidor y no vamos a hacer llamadas a Inference Endpoints, no necesitamos el token. El archivo quedar칤a:\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "\n",
        "COPY --chown=user . /app\n",
        "\n",
        "EXPOSE 7860\n",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prueba de la API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Desplegamos el space y probamos la API. En este caso lo voy a probar directamente desde python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta: system\n",
            "You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\n",
            "user\n",
            "Hola, 쯖칩mo est치s?\n",
            "assistant\n",
            "Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.\n",
            "Thread ID: user1\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://maximofn-smollm2-backend-localmodel.hf.space/generate\"\n",
        "data = {\n",
        "    \"query\": \"Hola, 쯖칩mo est치s?\",\n",
        "    \"thread_id\": \"user1\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"Respuesta:\", result[\"generated_text\"])\n",
        "    print(\"Thread ID:\", result[\"thread_id\"])\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este tarda un poco m치s que el anterior. En realidad tarda lo normal para un modelo ejecut치ndose en un servidor sin GPU. Lo raro es cuando lo desplegamos en Gradio. No s칠 qu칠 har치 HuggingFace por detr치s, o tal vez ha sido coincidencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos visto c칩mo crear una backend con un LLM, tanto haciendo llamadas al Inference Endpoint de HuggingFace, como haciendo llamadas a un modelo corriendo localmente. Hemos visto c칩mo hacerlo con Gradio o con FastAPI, Langchain y Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A partir de aqu칤 tienes el conocimiento para poder desplegar tus propios modelos, incluso aunque no sean LLMs, podr칤an ser modelos multimodales. A partir de aqu칤 puedes hacer lo que quieras."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "maximofn": {
      "date": "2025-03-02",
      "description_en": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "description_es": "쯈uieres desplegar un backend con tu propio LLM? En este post te explico c칩mo hacerlo con HuggingFace Spaces, FastAPI, Langchain y Docker.",
      "description_pt": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "end_url": "deploy-backend-with-llm-in-huggingface",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp",
      "keywords_en": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_es": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_pt": "hugging face, fastapi, langchain, docker, backend, llm",
      "title_en": "Deploy backend with LLM in HuggingFace",
      "title_es": "Desplegar backend con LLM en HuggingFace",
      "title_pt": "Desplegar backend com LLM no HuggingFace"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
