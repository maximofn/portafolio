{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Desplegar backend en HuggingFace\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este post vamos a ver c√≥mo desplegar un backend en HuggingFace. Vamos a ver c√≥mo hacerlo de dos maneras, mediante la forma com√∫n, creando una aplicaci√≥n con Gradio, y mediante una opci√≥n diferente usando FastAPI, Langchain y Docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para ambos casos va a ser necesario tener una cuenta en HuggingFace, ya que vamos a desplegar el backend en un space de HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend con Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero de todo, creamos un nuevo espacio en Hugging Face.\n",
        "\n",
        " * Ponemos un nombre, una descripci√≥n y elegimos la licencia.\n",
        " * Elegimos Gradio como el tipo de SDK. Al elegir Gradio, nos aparecer√°n plantillas, as√≠ que elegimos la plantilla de chatbot.\n",
        " * Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t√∫ elige lo que mejor consideres.\n",
        " * Y por √∫ltimo hay que elegir si queremos crear el espacio p√∫blico o privado.\n",
        "\n",
        "![backend gradio - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√≥digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al crear el space, podemos clonarlo o podemos ver los archivos en la propia p√°gina de HuggingFace. Podemos ver que se han creado 3 archivos, `app.py`, `requirements.txt` y `README.md`. As√≠ que vamos a ver qu√© poner en cada uno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqu√≠ tenemos el c√≥digo de la aplicaci√≥n. Como hemos elegido la plantilla de chatbot, ya tenemos mucho hecho, pero vamos a tener que cambiar 2 cosas, primero el modelo de lenguaje y el system prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como modelo de lenguaje veo ``HuggingFaceH4/zephyr-7b-beta``, pero vamos a usar ``Qwen/Qwen2.5-72B-Instruct``, que es un modelo muy capaz.\n",
        "\n",
        "As√≠ que busca el texto ``client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")`` y reempl√°zalo por ``client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")``, o espera que m√°s adelante pondr√© todo el c√≥digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tambi√©n vamos a cambiar el system prompt, que por defecto es ``You are a friendly Chatbot.``, pero como es un modelo entrenado en su mayor√≠a en ingl√©s, es probable que si le hablas en otro idioma te responda en ingl√©s, as√≠ que vamos a cambiarlo por ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.\n",
        "\n",
        "As√≠ que busca el texto ``gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),`` y reempl√°zalo por ``gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),``, o espera a que ahora voy a poner todo el c√≥digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``` python\n",
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "\"\"\"\n",
        "For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
        "\"\"\"\n",
        "client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")\n",
        "\n",
        "\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    for val in history:\n",
        "        if val[0]:\n",
        "            messages.append({\"role\": \"user\", \"content\": val[0]})\n",
        "        if val[1]:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    response = \"\"\n",
        "\n",
        "    for message in client.chat_completion(\n",
        "        messages,\n",
        "        max_tokens=max_tokens,\n",
        "        stream=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    ):\n",
        "        token = message.choices[0].delta.content\n",
        "\n",
        "        response += token\n",
        "        yield response\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/chatinterface\n",
        "\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),\n",
        "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=1.0,\n",
        "            value=0.95,\n",
        "            step=0.05,\n",
        "            label=\"Top-p (nucleus sampling)\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este es el archivo en el que estar√°n escritas las dependencias, pero para este caso va a ser muy sencillo:\n",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este es el archivo en el que vamos a poner la informaci√≥n del espacio. En los spaces de HuggingFace, al inicio de los readmes, se pone un c√≥digo para que HuggingFace sepa c√≥mo mostrar la miniatura del espacio, qu√© fichero tiene que usar para ejecutar el c√≥digo, versi√≥n del sdk, etc.\n",
        "\n",
        "``` md\n",
        "---\n",
        "title: SmolLM2\n",
        "emoji: üí¨\n",
        "colorFrom: yellow\n",
        "colorTo: purple\n",
        "sdk: gradio\n",
        "sdk_version: 5.0.1\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "short_description: Gradio SmolLM2 chat\n",
        "---\n",
        "\n",
        "An example chatbot using [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index), and the [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Despliegue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.\n",
        "\n",
        "As√≠ que cuando est√©n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.\n",
        "\n",
        "![backend gradio - chatbot](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Muy bien, hemos hecho un chatbot, pero no era la intenci√≥n, aqu√≠ hab√≠amos venido a hacer un backend! Para, para, f√≠jate lo que pone debajo del chatbot\n",
        "\n",
        "![backend gradio - Use via API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver un texto ``Use via API``, donde si pulsamos se nos abre un men√∫ con una API para poder usar el chatbot.\n",
        "\n",
        "![backend gradio - API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que nos da una documentaci√≥n de c√≥mo usar la API, tanto con Python, con JavaScript, como con bash."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prueba de la API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos el c√≥digo de ejemplo de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space ‚úî\n",
            "¬°Hola M√°ximo! Mucho gusto, estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estamos haciendo llamadas a la API de `InferenceClient` de HuggingFace, as√≠ que podr√≠amos pensar, ¬øPara qu√© hemos hecho un backend, si podemos llamar directamente a la API de HuggingFace? Pues lo vas a ver a continuaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tu nombre es M√°ximo. ¬øEs correcto?\n"
          ]
        }
      ],
      "source": [
        "result = client.predict(\n",
        "\t\tmessage=\"¬øC√≥mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La plantilla de chat de Gradio maneja el historial por nosotros, de manera que cada vez que creamos un nuevo `cliente`, se crea un nuevo hilo de conversaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a probar a crear un nuevo cliente, y ver si se crea un nuevo hilo de conversaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2.hf.space ‚úî\n",
            "Hola Luis, estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? Es un gusto conocerte. ¬øEn qu√© puedo ayudarte hoy?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "new_client = Client(\"Maximofn/SmolLM2\")\n",
        "result = new_client.predict(\n",
        "\t\tmessage=\"Hola, ¬øc√≥mo est√°s? Me llamo Luis\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora le volvemos a preguntar c√≥mo me llamo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte?\n"
          ]
        }
      ],
      "source": [
        "result = new_client.predict(\n",
        "\t\tmessage=\"¬øC√≥mo me llamo?\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como vemos, tenemos dos clientes, cada uno con su propio hilo de conversaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend con FastAPI, Langchain y Docker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vamos a hacer lo mismo, crear un backend de un chatbot, con el mismo modelo, pero en este caso usando FastAPI, Langchain y Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos que crear un nuevo espacio, pero en este caso lo haremos de otra manera\n",
        "\n",
        " * Ponemos un nombre, una descripci√≥n y elegimos la licencia.\n",
        " * Elegimos Docker como el tipo de SDK. Al elegir Docker, nos aparecer√°n plantillas, as√≠ que elegimos una plantilla en blanco.\n",
        " * Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t√∫ elige lo que mejor consideres.\n",
        " * Y por √∫ltimo hay que elegir si queremos crear el espacio p√∫blico o privado.\n",
        "\n",
        "![backend docker - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√≥digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora, al crear el space, vemos que solo tenemos un archivo, el ``README.md``. As√≠ que vamos a tener que crear todo el c√≥digo nosotros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a crear el c√≥digo de la aplicaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Empezamos con las librer√≠as necesarias\n",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "```\n",
        "\n",
        "Cargamos `fastapi` para poder crear las rutas de la API, `pydantic` para crear la plantilla de las querys, `huggingface_hub` para poder crear un modelo de lenguaje, `langchain` para poder indicarle al modelo si los mensajes son del chatbot o del usuario y `langgraph` para poder crear el chatbot.\n",
        "\n",
        "Adem√°s cargamos `os` y `dotenv` para poder cargar las variables de entorno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cargamos el token de HuggingFace\n",
        "\n",
        "``` python\n",
        "# HuggingFace token\n",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos el modelo de lenguaje\n",
        "\n",
        "``` python\n",
        "# Initialize the HuggingFace model\n",
        "model = InferenceClient(\n",
        "    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos ahora una funci√≥n para llamar al modelo\n",
        "\n",
        "``` python\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to HuggingFace format\n",
        "    hf_messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Call the API\n",
        "    response = model.chat_completion(\n",
        "        messages=hf_messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=64,\n",
        "        top_p=0.7\n",
        "    )\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response.choices[0].message.content)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "```\n",
        "\n",
        "Convertimos los mensajes de formato LangChain a formato HuggingFace, as√≠ podemos usar el modelo de lenguaje."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos una plantilla para las queries\n",
        "\n",
        "``` python\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "    thread_id: str = \"default\"\n",
        "```\n",
        "\n",
        "Las queries van a tener un `query`, el mensaje del usuario, y un `thread_id`, que es el identificador del hilo de la conversaci√≥n y m√°s adelante explicaremos para qu√© lo usamos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos un grafo de LangGraph\n",
        "\n",
        "``` python\n",
        "# Define the graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "graph_app = workflow.compile(checkpointer=memory)\n",
        "```\n",
        "\n",
        "Con esto lo que hacemos es crear un grafo de LangGraph, que es una estructura de datos que nos permite crear un chatbot y que gestiona por nosotros el estado del chatbot, es decir, entre otras cosas, el historial de mensajes. As√≠ no lo tenemos que hacer nosotros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos la aplicaci√≥n de FastAPI\n",
        "\n",
        "``` python\n",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API to generate text using LangChain and LangGraph\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos los endpoints de la API\n",
        "\n",
        "``` python\n",
        "# Welcome endpoint\n",
        "@app.get(\"/\")\n",
        "async def api_home():\n",
        "    \"\"\"Welcome endpoint\"\"\"\n",
        "    return {\"detail\": \"Welcome to FastAPI, Langchain, Docker tutorial\"}\n",
        "\n",
        "# Generate endpoint\n",
        "@app.post(\"/generate\")\n",
        "async def generate(request: QueryRequest):\n",
        "    \"\"\"\n",
        "    Endpoint to generate text using the language model\n",
        "    \n",
        "    Args:\n",
        "        request: QueryRequest\n",
        "        query: str\n",
        "        thread_id: str = \"default\"\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Configure the thread ID\n",
        "        config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
        "        \n",
        "        # Create the input message\n",
        "        input_messages = [HumanMessage(content=request.query)]\n",
        "        \n",
        "        # Invoke the graph\n",
        "        output = graph_app.invoke({\"messages\": input_messages}, config)\n",
        "        \n",
        "        # Get the model response\n",
        "        response = output[\"messages\"][-1].content\n",
        "        \n",
        "        return {\n",
        "            \"generated_text\": response,\n",
        "            \"thread_id\": request.thread_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error al generar texto: {str(e)}\")\n",
        "```\n",
        "\n",
        "Hemos creado el endpoint `/` que nos devolver√° un texto cuando accedamos a la API, y el endpoint `/generate` que es el que usaremos para generar el texto.\n",
        "\n",
        "Si nos fijamos en la funci√≥n `generate` tenemos la variable `config`, que es un diccionario que contiene el `thread_id`. Este `thread_id` es el que nos permite tener un historial de mensajes de cada usuario, de esta manera, diferentes usuarios pueden usar el mismo endpoint y tener su propio historial de mensajes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por √∫ltimo, tenemos el c√≥digo para que se pueda ejecutar la aplicaci√≥n\n",
        "\n",
        "``` python\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a escribir todo el c√≥digo junto\n",
        "\n",
        "``` python\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# HuggingFace token\n",
        "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
        "\n",
        "# Initialize the HuggingFace model\n",
        "model = InferenceClient(\n",
        "    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        ")\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to HuggingFace format\n",
        "    hf_messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Call the API\n",
        "    response = model.chat_completion(\n",
        "        messages=hf_messages,\n",
        "        temperature=0.5,\n",
        "        max_tokens=64,\n",
        "        top_p=0.7\n",
        "    )\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response.choices[0].message.content)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "\n",
        "# Define the graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "# Define the node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "graph_app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "# Define the data model for the request\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "    thread_id: str = \"default\"\n",
        "\n",
        "# Create the FastAPI application\n",
        "app = FastAPI(title=\"LangChain FastAPI\", description=\"API to generate text using LangChain and LangGraph\")\n",
        "\n",
        "# Welcome endpoint\n",
        "@app.get(\"/\")\n",
        "async def api_home():\n",
        "    \"\"\"Welcome endpoint\"\"\"\n",
        "    return {\"detail\": \"Welcome to FastAPI, Langchain, Docker tutorial\"}\n",
        "\n",
        "# Generate endpoint\n",
        "@app.post(\"/generate\")\n",
        "async def generate(request: QueryRequest):\n",
        "    \"\"\"\n",
        "    Endpoint to generate text using the language model\n",
        "    \n",
        "    Args:\n",
        "        request: QueryRequest\n",
        "        query: str\n",
        "        thread_id: str = \"default\"\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Configure the thread ID\n",
        "        config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
        "        \n",
        "        # Create the input message\n",
        "        input_messages = [HumanMessage(content=request.query)]\n",
        "        \n",
        "        # Invoke the graph\n",
        "        output = graph_app.invoke({\"messages\": input_messages}, config)\n",
        "        \n",
        "        # Get the model response\n",
        "        response = output[\"messages\"][-1].content\n",
        "        \n",
        "        return {\n",
        "            \"generated_text\": response,\n",
        "            \"thread_id\": request.thread_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error al generar texto: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=7860)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vemos c√≥mo crear el Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero indicamos desde qu√© imagen vamos a partir\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora creamos el directorio de trabajo\n",
        "\n",
        "``` dockerfile\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copiamos el archivo con las dependencias e instalamos\n",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copiamos el resto del c√≥digo\n",
        "\n",
        "``` dockerfile\n",
        "COPY --chown=user . /app\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exponemos el puerto 7860\n",
        "\n",
        "``` dockerfile\n",
        "EXPOSE 7860\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos las variables de entorno\n",
        "\n",
        "``` dockerfile\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\\n",
        "    test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Secret exists!\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por √∫ltimo, indicamos el comando para ejecutar la aplicaci√≥n\n",
        "\n",
        "``` dockerfile\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora lo ponemos todo junto\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "\n",
        "COPY --chown=user . /app\n",
        "\n",
        "EXPOSE 7860\n",
        "\n",
        "RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true \\\n",
        "    test -f /run/secrets/HUGGINGFACE_TOKEN && echo \"Secret exists!\"\n",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creamos el archivo con las dependencias\n",
        "\n",
        "``` txt\n",
        "fastapi\n",
        "uvicorn\n",
        "requests\n",
        "pydantic>=2.0.0\n",
        "langchain\n",
        "langchain-huggingface\n",
        "langchain-core\n",
        "langgraph > 0.2.27\n",
        "python-dotenv.2.11\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por √∫ltimo, creamos el archivo README.md con informaci√≥n del espacio y con las intrucciones para HugginFace\n",
        "\n",
        "``` md\n",
        "---\n",
        "title: SmolLM2 Backend\n",
        "emoji: üìä\n",
        "colorFrom: yellow\n",
        "colorTo: red\n",
        "sdk: docker\n",
        "pinned: false\n",
        "license: apache-2.0\n",
        "short_description: Backend of SmolLM2 chat\n",
        "app_port: 7860\n",
        "---\n",
        "\n",
        "# SmolLM2 Backend\n",
        "\n",
        "This project implements a FastAPI API that uses LangChain and LangGraph to generate text with the Qwen2.5-72B-Instruct model from HuggingFace.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "### In HuggingFace Spaces\n",
        "\n",
        "This project is designed to run in HuggingFace Spaces. To configure it:\n",
        "\n",
        "1. Create a new Space in HuggingFace with SDK Docker\n",
        "2. Configure the `HUGGINGFACE_TOKEN` or `HF_TOKEN` environment variable in the Space configuration:\n",
        "   - Go to the \"Settings\" tab of your Space\n",
        "   - Scroll down to the \"Repository secrets\" section\n",
        "   - Add a new variable with the name `HUGGINGFACE_TOKEN` and your token as the value\n",
        "   - Save the changes\n",
        "\n",
        "### Local development\n",
        "\n",
        "For local development:\n",
        "\n",
        "1. Clone this repository\n",
        "2. Create a `.env` file in the project root with your HuggingFace token:\n",
        "   ``\n",
        "   HUGGINGFACE_TOKEN=your_token_here\n",
        "   ``\n",
        "3. Install the dependencies:\n",
        "   ``\n",
        "   pip install -r requirements.txt\n",
        "   ``\n",
        "\n",
        "## Local execution\n",
        "\n",
        "``bash\n",
        "uvicorn app:app --reload\n",
        "``\n",
        "\n",
        "The API will be available at `http://localhost:8000`.\n",
        "\n",
        "## Endpoints\n",
        "\n",
        "### GET `/`\n",
        "\n",
        "Welcome endpoint that returns a greeting message.\n",
        "\n",
        "### POST `/generate`\n",
        "\n",
        "Endpoint to generate text using the language model.\n",
        "\n",
        "**Request parameters:**\n",
        "``json\n",
        "{\n",
        "  \"query\": \"Your question here\",\n",
        "  \"thread_id\": \"optional_thread_identifier\"\n",
        "}\n",
        "``\n",
        "\n",
        "**Response:**\n",
        "``json\n",
        "{\n",
        "  \"generated_text\": \"Generated text by the model\",\n",
        "  \"thread_id\": \"thread identifier\"\n",
        "}\n",
        "``\n",
        "\n",
        "## Docker\n",
        "\n",
        "To run the application in a Docker container:\n",
        "\n",
        "``bash\n",
        "# Build the image\n",
        "docker build -t smollm2-backend .\n",
        "\n",
        "# Run the container\n",
        "docker run -p 8000:8000 --env-file .env smollm2-backend\n",
        "``\n",
        "\n",
        "## API documentation\n",
        "\n",
        "The interactive API documentation is available at:\n",
        "- Swagger UI: `http://localhost:8000/docs`\n",
        "- ReDoc: `http://localhost:8000/redoc`\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token de HuggingFace\n",
        "\n",
        "Si te has fijado en el c√≥digo y en el Dockerfile hemos usado un token de HuggingFace, as√≠ que vamos a tener que crear uno. En nuestra cuenta de HuggingFace creamos un [nuevo token](https://huggingface.co/settings/tokens/new?tokenType=fineGrained), le ponemos un nombre y le damos los siguientes permisos:\n",
        "\n",
        " * Read access to contents of all repos under your personal namespace\n",
        " * Read access to contents of all repos under your personal namespacev\n",
        " * Make calls to inference providers\n",
        " * Make calls to Inference Endpoints\n",
        "\n",
        "![backend docker - token](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-token.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A√±adir el token a los secrets del espacio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que ya tenemos el token, necesitamos a√±adirlo al espacio. En la parte de arriba de la app, podremos ver un bot√≥n llamado `Settings`, lo pulsamos y podremos ver la secci√≥n de configuraci√≥n del espacio.\n",
        "\n",
        "Si bajamos, podremos ver una secci√≥n en la que podemos a√±adir `Variables` y `Secrets`. En este caso, como estamos a√±adiendo un token, lo vamos a a√±adir a los `Secrets`.\n",
        "\n",
        "Le ponemos el nombre `HUGGINGFACE_TOKEN` y el valor del token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Despliegue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.\n",
        "\n",
        "As√≠ que cuando est√©n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.\n",
        "\n",
        "En este caso, solo hemos construido un backend, por lo que lo que vamos a ver al entrar al espacio es lo que definimos en el endpoint `/`\n",
        "\n",
        "![backend docker - space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-space.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### URL del backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Necesitamos saber la URL del backend para poder hacer llamadas a la API. Para ello, tenemos que pulsar en los tres puntos de la parte superior derecha para ver las opciones\n",
        "\n",
        "![backend docker - options](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-options.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el men√∫ que se despliega pulsamos en `Embed this Spade`, se nos abrir√° una ventana en la que indica c√≥mo embeber el espacio con un iframe y adem√°s nos dar√° la URL del espacio.\n",
        "\n",
        "![backend docker - embed](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-embed.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si ahora nos vamos a esa URL, veremos lo mismo que en el espacio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentaci√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FastAPI, a parte de ser una API rapid√≠sima, tiene otra gran ventaja, y es que genera documentaci√≥n de manera autom√°tica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si a√±adimos `/docs` a la URL que vimos antes, podremos ver la documentaci√≥n de la API con `Swagger UI`.\n",
        "\n",
        "![backend docker - swagger doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-swagger-doc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tambi√©n podemos a√±adir `/redoc` a la URL para ver la documentaci√≥n con `ReDoc`.\n",
        "\n",
        "![backend docker - redoc doc](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-redoc.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prueba de la API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo bueno de la documentaci√≥n `Swagger UI` es que nos permite probar la API directamente desde el navegador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A√±adimos `/docs` a la URL que obtuvimos, abrimos el desplegable del endpoint `/generate` y le damos a `Try it out`, modificamos el valor de la `query` y del `thread_id` y pulsamos en `Execute`.\n",
        "\n",
        "En el primer caso voy a poner\n",
        "\n",
        " * **query**: Hola, ¬øC√≥mo est√°s? Soy M√°ximo\n",
        " * **thread_id**: user1\n",
        "\n",
        "![backend docker - test API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-test-API.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recibimos la siguiente respuesta `¬°Hola M√°ximo! Estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?`\n",
        "\n",
        "![backend docker -response 1 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user1.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a probar ahora la misma pregunta, pero con un `thread_id` diferente, en este caso `user2`.\n",
        "\n",
        "![backend docker - query 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-query1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y nos responde esto `¬°Hola Luis! Estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?`\n",
        "\n",
        "![backend docker - response 1 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response1-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora pedimos nuestro nombre con los dos usuarios y obtenemos esto\n",
        "\n",
        " * Para el usuario **user1**: `Te llamas M√°ximo. ¬øHay algo m√°s en lo que pueda ayudarte?`\n",
        " * Para el usuario **user2**: `Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte hoy, Luis?`\n",
        "\n",
        "![backend docker - response 2 - user1](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user1.webp)\n",
        "\n",
        "![backend docker - response 2 - user2](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-response2-user2.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend con Gradio y modelo corriendo en el servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los dos backends que hemos creado en realidad no est√°n corriendo un modelo, sino que est√°n haciendo llamadas a Inference Endpoints de HuggingFace. Pero puede que queramos que todo corra en el servidor, incluso el modelo. Puede ser que hayas hecho un fine-tuning de un LLM para tu caso de uso, por lo que ya no puedes hacer llamadas a Inference Endpoints.\n",
        "\n",
        "As√≠ que vamos a ver c√≥mo modificar el c√≥digo de los dos backends para correr un modelo en el servidor y no hacer llamadas a Inference Endpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripci√≥n, seleccionamos Gradio como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW m√°s b√°sico y gratuito, y seleccionamos si lo hacemos privado o p√∫blico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√≥digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos que hacer cambios en `app.py` y en `requirements.txt` para que en lugar de hacer llamadas a Inference Endpoints, se ejecute el modelo localmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los cambios que tenemos que hacer son"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importar `AutoModelForCausalLM` y `AutoTokenizer` de la librer√≠a `transformers` e importar `torch`\n",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En lugar de crear un modelo mediante `InferenceClient` lo creamos con `AutoModelForCausalLM` y `AutoTokenizer`\n",
        "\n",
        "``` python\n",
        "# Cargar el modelo y el tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "```\n",
        "\n",
        "Utilizo `HuggingFaceTB/SmolLM2-1.7B-Instruct` porque es un modelo bastante capaz de solo 1.7B de par√°metros. Como he elegido el HW m√°s b√°sico no puedo usar modelos muy grandes. T√∫, si quieres usar un modelo m√°s grande tienes dos opciones, usar el HW gratuito y aceptar que la inferencia va a ser m√°s lenta, o usar un HW m√°s potente, pero de pago."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modificar la funci√≥n `respond` para que construya el prompt con la estructura necesaria por la librer√≠a `transformers`, tokenizar el prompt, hacer la inferencia y destokenizar la respuesta.\n",
        "\n",
        "``` python\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    # Construir el prompt con el formato correcto\n",
        "    prompt = f\"<|system|>\\n{system_message}</s>\\n\"\n",
        "    \n",
        "    for val in history:\n",
        "        if val[0]:\n",
        "            prompt += f\"<|user|>\\n{val[0]}</s>\\n\"\n",
        "        if val[1]:\n",
        "            prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"\n",
        "    \n",
        "    prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"\n",
        "    \n",
        "    # Tokenizar el prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generar la respuesta\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decodificar la respuesta\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extraer solo la parte de la respuesta del asistente\n",
        "    response = response.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "    \n",
        "    yield response\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuaci√≥n dejo todo el c√≥digo\n",
        "\n",
        "``` python\n",
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
        "\"\"\"\n",
        "\n",
        "# Cargar el modelo y el tokenizer\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "def respond(\n",
        "    message,\n",
        "    history: list[tuple[str, str]],\n",
        "    system_message,\n",
        "    max_tokens,\n",
        "    temperature,\n",
        "    top_p,\n",
        "):\n",
        "    # Construir el prompt con el formato correcto\n",
        "    prompt = f\"<|system|>\\n{system_message}</s>\\n\"\n",
        "    \n",
        "    for val in history:\n",
        "        if val[0]:\n",
        "            prompt += f\"<|user|>\\n{val[0]}</s>\\n\"\n",
        "        if val[1]:\n",
        "            prompt += f\"<|assistant|>\\n{val[1]}</s>\\n\"\n",
        "    \n",
        "    prompt += f\"<|user|>\\n{message}</s>\\n<|assistant|>\\n\"\n",
        "    \n",
        "    # Tokenizar el prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generar la respuesta\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decodificar la respuesta\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extraer solo la parte de la respuesta del asistente\n",
        "    response = response.split(\"<|assistant|>\\n\")[-1].strip()\n",
        "    \n",
        "    yield response\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/chatinterface\n",
        "\"\"\"\n",
        "demo = gr.ChatInterface(\n",
        "    respond,\n",
        "    additional_inputs=[\n",
        "        gr.Textbox(\n",
        "            value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", \n",
        "            label=\"System message\"\n",
        "        ),\n",
        "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(\n",
        "            minimum=0.1,\n",
        "            maximum=1.0,\n",
        "            value=0.95,\n",
        "            step=0.05,\n",
        "            label=\"Top-p (nucleus sampling)\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este archivo hay que a√±adir las nuevas librer√≠as que vamos a usar, en este caso `transformers`, `accelerate` y `torch`. El archivo entero quedar√≠a:\n",
        "\n",
        "``` txt\n",
        "huggingface_hub==0.25.2\n",
        "gradio>=4.0.0\n",
        "transformers>=4.36.0\n",
        "torch>=2.0.0\n",
        "accelerate>=0.25.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prueba de la API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Desplegamos el space y probamos directamente la API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded as API: https://maximofn-smollm2-localmodel.hf.space ‚úî\n",
            "Hola M√°ximo, soy su Chatbot amable y estoy funcionando bien. Gracias por tu mensaje, me complace ayudarte hoy en d√≠a. ¬øC√≥mo puedo servirte?\n"
          ]
        }
      ],
      "source": [
        "from gradio_client import Client\n",
        "\n",
        "client = Client(\"Maximofn/SmolLM2_localModel\")\n",
        "result = client.predict(\n",
        "\t\tmessage=\"Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo\",\n",
        "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
        "\t\tmax_tokens=512,\n",
        "\t\ttemperature=0.7,\n",
        "\t\ttop_p=0.95,\n",
        "\t\tapi_name=\"/chat\"\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Me sorprende lo r√°pido que responde el modelo estando en un servidor sin GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desplegar backend con FastAPI, Langchain y Docker y modelo corriendo en el servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora hacemos lo mismo que antes, pero con FastAPI, LangChain y Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Crear Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A la hora de crear el space en HuggingFace hacemos lo mismo que antes, creamos un nuevo space, ponemos un nombre y una descripci√≥n, seleccionamos Docker como SDK, seleccionamos el HW en el que lo vamos a desplegar, en mi caso selecciono el HW m√°s b√°sico y gratuito, y seleccionamos si lo hacemos privado o p√∫blico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C√≥digo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya no importamos `InferenceClient` y ahora importamos `AutoModelForCausalLM` y `AutoTokenizer` de la librer√≠a `transformers` e importamos `torch`.\n",
        "\n",
        "``` python\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instanciamos el modelo y el tokenizer con `AutoModelForCausalLM` y `AutoTokenizer`.\n",
        "\n",
        "``` python\n",
        "# Initialize the model and tokenizer\n",
        "print(\"Cargando modelo y tokenizer...\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
        "\n",
        "try:\n",
        "    # Load the model in BF16 format for better performance and lower memory usage\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    if device == \"cuda\":\n",
        "        print(\"Usando GPU para el modelo...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "    else:\n",
        "        print(\"Usando CPU para el modelo...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map={\"\": device},\n",
        "            torch_dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    print(f\"Modelo cargado exitosamente en: {device}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo: {str(e)}\")\n",
        "    raise\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redefinimos la funci√≥n `call_model` para que haga la inferencia con el modelo local.\n",
        "\n",
        "``` python\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Call the model with the given messages\n",
        "\n",
        "    Args:\n",
        "        state: MessagesState\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the generated text and the thread ID\n",
        "    \"\"\"\n",
        "    # Convert LangChain messages to chat format\n",
        "    messages = []\n",
        "    for msg in state[\"messages\"]:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "    \n",
        "    # Prepare the input using the chat template\n",
        "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Generate response\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=512,  # Increase the number of tokens for longer responses\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode and clean the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant's response (after the last user message)\n",
        "    response = response.split(\"Assistant:\")[-1].strip()\n",
        "    \n",
        "    # Convert the response to LangChain format\n",
        "    ai_message = AIMessage(content=response)\n",
        "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tenemos que quitar `langchain-huggingface` y a√±adir `transformers`, `accelerate` y `torch` en el archivo `requirements.txt`. El archivo quedar√≠a:\n",
        "\n",
        "``` txt\n",
        "fastapi\n",
        "uvicorn\n",
        "requests\n",
        "pydantic>=2.0.0\n",
        "langchain>=0.1.0\n",
        "langchain-core>=0.1.10\n",
        "langgraph>=0.2.27\n",
        "python-dotenv>=1.0.0\n",
        "transformers>=4.36.0\n",
        "torch>=2.0.0\n",
        "accelerate>=0.26.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dockerfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya no necesitamos tener `RUN --mount=type=secret,id=HUGGINGFACE_TOKEN,mode=0444,required=true` porque como el modelo va a estar en el servidor y no vamos a hacer llamadas a Inference Endpoints, no necesitamos el token. El archivo quedar√≠a:\n",
        "\n",
        "``` dockerfile\n",
        "FROM python:3.13-slim\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "WORKDIR /app\n",
        "\n",
        "COPY --chown=user ./requirements.txt requirements.txt\n",
        "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
        "\n",
        "COPY --chown=user . /app\n",
        "\n",
        "EXPOSE 7860\n",
        "\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prueba de la API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Desplegamos el space y probamos la API. En este caso lo voy a probar directamente desde python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta: system\n",
            "You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\n",
            "user\n",
            "Hola, ¬øc√≥mo est√°s?\n",
            "assistant\n",
            "Estoy bien, gracias por preguntar. Estoy muy emocionado de la semana que viene.\n",
            "Thread ID: user1\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://maximofn-smollm2-backend-localmodel.hf.space/generate\"\n",
        "data = {\n",
        "    \"query\": \"Hola, ¬øc√≥mo est√°s?\",\n",
        "    \"thread_id\": \"user1\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"Respuesta:\", result[\"generated_text\"])\n",
        "    print(\"Thread ID:\", result[\"thread_id\"])\n",
        "else:\n",
        "    print(\"Error:\", response.status_code, response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este tarda un poco m√°s que el anterior. En realidad tarda lo normal para un modelo ejecut√°ndose en un servidor sin GPU. Lo raro es cuando lo desplegamos en Gradio. No s√© qu√© har√° HuggingFace por detr√°s, o tal vez ha sido coincidencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hemos visto c√≥mo crear una backend con un LLM, tanto haciendo llamadas al Inference Endpoint de HuggingFace, como haciendo llamadas a un modelo corriendo localmente. Hemos visto c√≥mo hacerlo con Gradio o con FastAPI, Langchain y Docker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A partir de aqu√≠ tienes el conocimiento para poder desplegar tus propios modelos, incluso aunque no sean LLMs, podr√≠an ser modelos multimodales. A partir de aqu√≠ puedes hacer lo que quieras."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "maximofn": {
      "date": "2025-03-02",
      "description_en": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "description_es": "¬øQuieres desplegar un backend con tu propio LLM? En este post te explico c√≥mo hacerlo con HuggingFace Spaces, FastAPI, Langchain y Docker.",
      "description_pt": "Do you want to deploy a backend with your own LLM? In this post I explain how to do it with HuggingFace Spaces, FastAPI, Langchain and Docker.",
      "end_url": "deploy-backend-with-llm-in-huggingface",
      "image": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp",
      "image_hover_path": "https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend_llm_thumbnail.webp",
      "keywords_en": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_es": "hugging face, fastapi, langchain, docker, backend, llm",
      "keywords_pt": "hugging face, fastapi, langchain, docker, backend, llm",
      "title_en": "Deploy backend with LLM in HuggingFace",
      "title_es": "Desplegar backend con LLM en HuggingFace",
      "title_pt": "Desplegar backend com LLM no HuggingFace"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
