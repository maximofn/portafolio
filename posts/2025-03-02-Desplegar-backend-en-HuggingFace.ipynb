{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desplegar backend en HuggingFace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post vamos a ver c칩mo desplegar un backend en HuggingFace. Vamos a ver c칩mo hacerlo de dos maneras, mediante la forma com칰n, creando una aplicaci칩n con Gradio, y mediante un opci칩n diferente usando FastAPI y Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ambos casos va a ser necesario tener una cuenta en HuggingFace, ya que vamos a desplegar el backend en un space de HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desplegar backend con Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero de todo, creamos un nuevo espacio en HuggingFace.\n",
    "\n",
    " * Ponemos un nombre, una descripci칩n y elegimos la licencia.\n",
    " * Elegimos Gradio como el tipo de SDK. Al elegir Gradio, nos aparecer치n plantillas, as칤 que elegimos la plantilla de chatbot.\n",
    " * Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t칰 elige lo que mejor consideres.\n",
    " * Y por 칰ltimo hay que elegor si queremos crear el espacio p칰blico o privado.\n",
    "\n",
    "![backend gradio - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C칩digo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al desplegar el space, podemos clonarlo o podemos ver los archivos en la propia p치gina de HuggingFace. Podemos ver que se han creado 3 archivos, `app.py`, `requirements.txt` y `README.md`. As칤 que vamos a ver qu칠 poner en cada uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu칤 tenemos el c칩digo de la aplicaci칩n. C칩mo hemos elegido la plantilla de chatbot, ya tenemos mucho hecho, pero vamos a tener que cambiar 2 cosas, primero el modelo de lenguaje y el system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como modelo de lenguaje ve칤a ``HuggingFaceH4/zephyr-7b-beta``, pero vamos a usar ``HuggingFaceTB/SmolLM2-1.7B-Instruct``, que es un modelo muy capaz con solo 1.7B de par치metros.\n",
    "\n",
    "쯇or qu칠 usamos este modelo? Como hemos elegido un space con un HW gratuito, no vamos a tener mucha potencia de c칩mputo, por lo que elegimos un modelo peque침o, pero aceptabel. En tu caso, si quieres un modelo m치s grande y mejor tienes dos opciones, dejar el mismo HW sabiendo que la inferencia va a ser mucho m치s lenta, o cambiar el HW por uno m치s potente, pero de pago.\n",
    "\n",
    "As칤 que busca el texto ``client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")`` y reempl치zalo por ``client = InferenceClient(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")``, o espera que m치s adelante pondr칠 todo el c칩digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi칠n vamos a cambiar el system prompt, que por defecto es ``You are a friendly Chatbot.``, pero como es un modelo entrenado en su mayor칤a en ingl칠s, es probable que si le hablas en otro idioma te responda en ingl칠s, as칤 que vamos a cambiarlo por ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.\n",
    "\n",
    "As칤 que busca el texto ``gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),`` y reempl치zalo por ``gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),``, o espera a que ahora voy a poner todo el c칩digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "import gradio as gr\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\"\"\"\n",
    "For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
    "\"\"\"\n",
    "client = InferenceClient(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "\n",
    "\n",
    "def respond(\n",
    "    message,\n",
    "    history: list[tuple[str, str]],\n",
    "    system_message,\n",
    "    max_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    for val in history:\n",
    "        if val[0]:\n",
    "            messages.append({\"role\": \"user\", \"content\": val[0]})\n",
    "        if val[1]:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    for message in client.chat_completion(\n",
    "        messages,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    ):\n",
    "        token = message.choices[0].delta.content\n",
    "\n",
    "        response += token\n",
    "        yield response\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/chatinterface\n",
    "\"\"\"\n",
    "demo = gr.ChatInterface(\n",
    "    respond,\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.95,\n",
    "            step=0.05,\n",
    "            label=\"Top-p (nucleus sampling)\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el archivo en el que estar치n escritas las dependencias, pero para este caso va a ser muy sencillo:\n",
    "\n",
    "``` txt\n",
    "huggingface_hub==0.25.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el archivo en el que vamos a poner la informaci칩n del espacio. En los spaces de HuggingFace, al inicio de los readmes, se pone un c칩digo para que HuggingFace sepa c칩mo mostrar la miniatura del espacio, qu칠 fichero tiene que usar para ejecutar el c칩digo, version del sdk, etc.\n",
    "\n",
    "``` md\n",
    "---\n",
    "title: SmolLM2\n",
    "emoji: 游눫\n",
    "colorFrom: yellow\n",
    "colorTo: purple\n",
    "sdk: gradio\n",
    "sdk_version: 5.0.1\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: apache-2.0\n",
    "short_description: Gradio SmolLM2 chat\n",
    "---\n",
    "\n",
    "An example chatbot using [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index), and the [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despliegue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.\n",
    "\n",
    "As칤 que cuando est칠n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.\n",
    "\n",
    "![backend gradio - chatbot](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, hemos hecho un chatbot, pero no era la intenci칩n, aqu칤 hab칤amos venido a hacer un backend! Para, para, f칤jate lo que pone debajo del chatbot\n",
    "\n",
    "![backend gradio - Use via API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver una texto ``Use via API``, donde si pulsamos se nos abre un men칰 con una API para poder usar el chatbot.\n",
    "\n",
    "![backend gradio - API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nos da una documentaci칩n de c칩mo usar la API, tanto con Python, con JavaScript, como con bash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de la API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el c칩digo de ejemplo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://maximofn-smollm2.hf.space 九\n",
      "Hola! Estoy bien, gracias por preguntar. 쯏 usted?\n",
      "CPU times: user 77.3 ms, sys: 11.6 ms, total: 88.9 ms\n",
      "Wall time: 5.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"Maximofn/SmolLM2\")\n",
    "result = client.predict(\n",
    "\t\tmessage=\"Hola, 쯖칩mo est치s?\",\n",
    "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
    "\t\tmax_tokens=512,\n",
    "\t\ttemperature=0.7,\n",
    "\t\ttop_p=0.95,\n",
    "\t\tapi_name=\"/chat\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He a침adido un `%%time` para que se vea el tiempo que tarda en responder. Hay que tener en cuenta el tiempo de inferencia y el tiempo de conexi칩n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
