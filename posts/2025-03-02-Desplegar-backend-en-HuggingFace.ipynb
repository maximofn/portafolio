{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desplegar backend en HuggingFace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post vamos a ver c√≥mo desplegar un backend en HuggingFace. Vamos a ver c√≥mo hacerlo de dos maneras, mediante la forma com√∫n, creando una aplicaci√≥n con Gradio, y mediante un opci√≥n diferente usando FastAPI, Langchain y Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ambos casos va a ser necesario tener una cuenta en HuggingFace, ya que vamos a desplegar el backend en un space de HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desplegar backend con Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero de todo, creamos un nuevo espacio en HuggingFace.\n",
    "\n",
    " * Ponemos un nombre, una descripci√≥n y elegimos la licencia.\n",
    " * Elegimos Gradio como el tipo de SDK. Al elegir Gradio, nos aparecer√°n plantillas, as√≠ que elegimos la plantilla de chatbot.\n",
    " * Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t√∫ elige lo que mejor consideres.\n",
    " * Y por √∫ltimo hay que elegor si queremos crear el espacio p√∫blico o privado.\n",
    "\n",
    "![backend gradio - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-create-space.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al crear el space, podemos clonarlo o podemos ver los archivos en la propia p√°gina de HuggingFace. Podemos ver que se han creado 3 archivos, `app.py`, `requirements.txt` y `README.md`. As√≠ que vamos a ver qu√© poner en cada uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠ tenemos el c√≥digo de la aplicaci√≥n. C√≥mo hemos elegido la plantilla de chatbot, ya tenemos mucho hecho, pero vamos a tener que cambiar 2 cosas, primero el modelo de lenguaje y el system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como modelo de lenguaje ve√≠a ``HuggingFaceH4/zephyr-7b-beta``, pero vamos a usar ``Qwen/Qwen2.5-72B-Instruct``, que es un modelo muy capaz.\n",
    "\n",
    "As√≠ que busca el texto ``client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")`` y reempl√°zalo por ``client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")``, o espera que m√°s adelante pondr√© todo el c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n vamos a cambiar el system prompt, que por defecto es ``You are a friendly Chatbot.``, pero como es un modelo entrenado en su mayor√≠a en ingl√©s, es probable que si le hablas en otro idioma te responda en ingl√©s, as√≠ que vamos a cambiarlo por ``You are a friendly Chatbot. Always reply in the language in which the user is writing to you.``.\n",
    "\n",
    "As√≠ que busca el texto ``gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),`` y reempl√°zalo por ``gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),``, o espera a que ahora voy a poner todo el c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "import gradio as gr\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\"\"\"\n",
    "For more information on `huggingface_hub` Inference API support, please check the docs: https://huggingface.co/docs/huggingface_hub/v0.22.2/en/guides/inference\n",
    "\"\"\"\n",
    "client = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")\n",
    "\n",
    "\n",
    "def respond(\n",
    "    message,\n",
    "    history: list[tuple[str, str]],\n",
    "    system_message,\n",
    "    max_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    for val in history:\n",
    "        if val[0]:\n",
    "            messages.append({\"role\": \"user\", \"content\": val[0]})\n",
    "        if val[1]:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = \"\"\n",
    "\n",
    "    for message in client.chat_completion(\n",
    "        messages,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    ):\n",
    "        token = message.choices[0].delta.content\n",
    "\n",
    "        response += token\n",
    "        yield response\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/chatinterface\n",
    "\"\"\"\n",
    "demo = gr.ChatInterface(\n",
    "    respond,\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\", label=\"System message\"),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.95,\n",
    "            step=0.05,\n",
    "            label=\"Top-p (nucleus sampling)\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el archivo en el que estar√°n escritas las dependencias, pero para este caso va a ser muy sencillo:\n",
    "\n",
    "``` txt\n",
    "huggingface_hub==0.25.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el archivo en el que vamos a poner la informaci√≥n del espacio. En los spaces de HuggingFace, al inicio de los readmes, se pone un c√≥digo para que HuggingFace sepa c√≥mo mostrar la miniatura del espacio, qu√© fichero tiene que usar para ejecutar el c√≥digo, version del sdk, etc.\n",
    "\n",
    "``` md\n",
    "---\n",
    "title: SmolLM2\n",
    "emoji: üí¨\n",
    "colorFrom: yellow\n",
    "colorTo: purple\n",
    "sdk: gradio\n",
    "sdk_version: 5.0.1\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "license: apache-2.0\n",
    "short_description: Gradio SmolLM2 chat\n",
    "---\n",
    "\n",
    "An example chatbot using [Gradio](https://gradio.app), [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/v0.22.2/en/index), and the [Hugging Face Inference API](https://huggingface.co/docs/api-inference/index).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despliegue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hemos clonado el espacio, tenemos que hacer un commit y un push. Si hemos modificado los archivos en HuggingFace, con guardarlos es suficiente.\n",
    "\n",
    "As√≠ que cuando est√©n los cambios en HuggingFace, tendremos que esperar unos segundos para que se construya el espacio y podremos usarlo.\n",
    "\n",
    "![backend gradio - chatbot](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, hemos hecho un chatbot, pero no era la intenci√≥n, aqu√≠ hab√≠amos venido a hacer un backend! Para, para, f√≠jate lo que pone debajo del chatbot\n",
    "\n",
    "![backend gradio - Use via API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-gradio-chatbot-edited.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver una texto ``Use via API``, donde si pulsamos se nos abre un men√∫ con una API para poder usar el chatbot.\n",
    "\n",
    "![backend gradio - API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend%20gradio%20-%20API.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nos da una documentaci√≥n de c√≥mo usar la API, tanto con Python, con JavaScript, como con bash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de la API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el c√≥digo de ejemplo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://maximofn-smollm2.hf.space ‚úî\n",
      "¬°Hola M√°ximo! Mucho gusto, estoy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? ¬øEn qu√© puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "client = Client(\"Maximofn/SmolLM2\")\n",
    "result = client.predict(\n",
    "\t\tmessage=\"Hola, ¬øc√≥mo est√°s? Me llamo M√°ximo\",\n",
    "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
    "\t\tmax_tokens=512,\n",
    "\t\ttemperature=0.7,\n",
    "\t\ttop_p=0.95,\n",
    "\t\tapi_name=\"/chat\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos haciendo llamadas a la API de `InferenceClient` de HuggingFace, as√≠ que podr√≠amos pensar, ¬øPara qu√© hemos hecho un backend, si podemos llamar directamente a la API de HuggingFace? Pues lo vas a ver a continuaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tu nombre es M√°ximo. ¬øEs correcto?\n"
     ]
    }
   ],
   "source": [
    "result = client.predict(\n",
    "\t\tmessage=\"¬øC√≥mo me llamo?\",\n",
    "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
    "\t\tmax_tokens=512,\n",
    "\t\ttemperature=0.7,\n",
    "\t\ttop_p=0.95,\n",
    "\t\tapi_name=\"/chat\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plantilla de chat de Gradio maneja el historial por nosotros, de manera que cada vez que creamos un nuevo `cliente`, se crea un nuevo hilo de conversaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar a crear un nuevo cliente, y ver si se crea un nuevo hilo de conversaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://maximofn-smollm2.hf.space ‚úî\n",
      "Hola Luis, estoy muy bien, gracias por preguntar. ¬øC√≥mo est√°s t√∫? Es un gusto conocerte. ¬øEn qu√© puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "from gradio_client import Client\n",
    "\n",
    "new_client = Client(\"Maximofn/SmolLM2\")\n",
    "result = new_client.predict(\n",
    "\t\tmessage=\"Hola, ¬øc√≥mo est√°s? Me llamo Luis\",\n",
    "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
    "\t\tmax_tokens=512,\n",
    "\t\ttemperature=0.7,\n",
    "\t\ttop_p=0.95,\n",
    "\t\tapi_name=\"/chat\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora le volvemos a preguntar c√≥mo me llamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Te llamas Luis. ¬øHay algo m√°s en lo que pueda ayudarte?\n"
     ]
    }
   ],
   "source": [
    "result = new_client.predict(\n",
    "\t\tmessage=\"¬øC√≥mo me llamo?\",\n",
    "\t\tsystem_message=\"You are a friendly Chatbot. Always reply in the language in which the user is writing to you.\",\n",
    "\t\tmax_tokens=512,\n",
    "\t\ttemperature=0.7,\n",
    "\t\ttop_p=0.95,\n",
    "\t\tapi_name=\"/chat\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, tenemos dos clientes, cada uno con su propio hilo de conversaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desplegar backend con FastAPI, Langchain y Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer lo mismo, crear un backend de un chatbot, con el mismo modelo, pero en este caso usando FastAPI, Langchain y Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos que crear un nuevo espacio, pero en este caso lo vamos a hacer de otra manera\n",
    "\n",
    " * Ponemos un nombre, una descripci√≥n y elegimos la licencia.\n",
    " * Elegimos Docker como el tipo de SDK. Al elegir Docker, nos aparecer√°n plantillas, as√≠ que elegimos una plantilla en blanco.\n",
    " * Seleccionamos el HW en el que vamos a desplegar el backend, yo voy a elegir la CPU gratuita, pero t√∫ elige lo que mejor consideres.\n",
    " * Y por √∫ltimo hay que elegor si queremos crear el espacio p√∫blico o privado.\n",
    "\n",
    "![backend docker - create space](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/backend-docker-create-space.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, al crear el space, vemos que solo tenemos un archivo, el ``README.md``. As√≠ que vamos a tener que crear todo el c√≥digo nosotros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear el c√≥digo de la aplicaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos con las librer√≠as necesarias\n",
    "\n",
    "``` python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "Cargamos `fastapi` para poder crear las rutas de la API, `pydantic` para crear la plantilla de las querys, `huggingface_hub` para poder crear un modelo de lengiaje, `langchain` para poder indicarle al modelo si los mensajes son del chatbot o del usuario y `langgraph` para poder crear el chatbot.\n",
    "\n",
    "Adem√°s cargamos `os` y `dotenv` para poder cargar las variables de entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el token de HuggingFace\n",
    "\n",
    "``` python\n",
    "# HuggingFace token\n",
    "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo de lenguaje\n",
    "\n",
    "``` python\n",
    "model = InferenceClient(\n",
    "    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos ahora un funci√≥n para llamar al modelo\n",
    "\n",
    "``` python\n",
    "def call_model(state: MessagesState):\n",
    "    \"\"\"\n",
    "    Call the model with the given messages\n",
    "\n",
    "    Args:\n",
    "        state: MessagesState\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated text and the thread ID\n",
    "    \"\"\"\n",
    "    # Convert LangChain messages to HuggingFace format\n",
    "    hf_messages = []\n",
    "    for msg in state[\"messages\"]:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "    \n",
    "    # Call the API\n",
    "    response = model.chat_completion(\n",
    "        messages=hf_messages,\n",
    "        temperature=0.5,\n",
    "        max_tokens=64,\n",
    "        top_p=0.7\n",
    "    )\n",
    "    \n",
    "    # Convert the response to LangChain format\n",
    "    ai_message = AIMessage(content=response.choices[0].message.content)\n",
    "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
    "```\n",
    "\n",
    "Convertimos los mensajes de formato LangChain a formato HuggingFace, as√≠ podemos usar el modelo de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos un plantilla para las querys\n",
    "\n",
    "``` python\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    thread_id: str = \"default\"\n",
    "```\n",
    "\n",
    "Las querys van a tener un `query`, el mensaje del usuario, y un `thread_id`, que es el identificador del hilo de la conversaci√≥n y m√°s adelante explicaremos para qu√© lo usamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un grafo de LangGraph\n",
    "\n",
    "``` python\n",
    "# Define the graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "graph_app = workflow.compile(checkpointer=memory)\n",
    "```\n",
    "\n",
    "Con esto lo que hacemos es crear un grafo de LangGraph, que es una estructura de datos que nos permite crear un chatbot y que gestiona por nosotros el estado del chatbot, es decir, entre otras cosas, el historial de mensajes. As√≠ no lo tenemos que hacer nosotros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la aplicaci√≥n de FastAPI\n",
    "\n",
    "``` python\n",
    "app = FastAPI(title=\"LangChain FastAPI\", description=\"API to generate text using LangChain and LangGraph\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los endpoints de la API\n",
    "\n",
    "``` python\n",
    "# Welcome endpoint\n",
    "@app.get(\"/\")\n",
    "async def api_home():\n",
    "    \"\"\"Welcome endpoint\"\"\"\n",
    "    return {\"detail\": \"Welcome to FastAPI, Langchain, Docker tutorial\"}\n",
    "\n",
    "# Generate endpoint\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: QueryRequest):\n",
    "    \"\"\"\n",
    "    Endpoint to generate text using the language model\n",
    "    \n",
    "    Args:\n",
    "        request: QueryRequest\n",
    "        query: str\n",
    "        thread_id: str = \"default\"\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated text and the thread ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure the thread ID\n",
    "        config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
    "        \n",
    "        # Create the input message\n",
    "        input_messages = [HumanMessage(content=request.query)]\n",
    "        \n",
    "        # Invoke the graph\n",
    "        output = graph_app.invoke({\"messages\": input_messages}, config)\n",
    "        \n",
    "        # Get the model response\n",
    "        response = output[\"messages\"][-1].content\n",
    "        \n",
    "        return {\n",
    "            \"generated_text\": response,\n",
    "            \"thread_id\": request.thread_id\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error al generar texto: {str(e)}\")\n",
    "```\n",
    "\n",
    "Hemos creado el endpoint `/` que nos devolver√° un texto cuando accedamos a la API, y el endpoint `/generate` que es el que usaremos para generar el texto.\n",
    "\n",
    "Si nos fijamos en la funci√≥n `generate` tenemos la variable `config`, que es un diccionario que contiene el `thread_id`. Este `thread_id` es el que nos permite tener un historial de mensajes de cada usuario, de esta manera, diferentes usuarios pueden usar el mismo endpoint y tener su propio historial de mensajes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por √∫ltimo, tenemos el c√≥digo para que se pueda ejecutar la aplicaci√≥n\n",
    "\n",
    "``` python\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a escribir todo el c√≥digo junto\n",
    "\n",
    "``` python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\", os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "# Initialize the HuggingFace model\n",
    "model = InferenceClient(\n",
    "    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    api_key=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    ")\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    \"\"\"\n",
    "    Call the model with the given messages\n",
    "\n",
    "    Args:\n",
    "        state: MessagesState\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated text and the thread ID\n",
    "    \"\"\"\n",
    "    # Convert LangChain messages to HuggingFace format\n",
    "    hf_messages = []\n",
    "    for msg in state[\"messages\"]:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            hf_messages.append({\"role\": \"user\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            hf_messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "    \n",
    "    # Call the API\n",
    "    response = model.chat_completion(\n",
    "        messages=hf_messages,\n",
    "        temperature=0.5,\n",
    "        max_tokens=64,\n",
    "        top_p=0.7\n",
    "    )\n",
    "    \n",
    "    # Convert the response to LangChain format\n",
    "    ai_message = AIMessage(content=response.choices[0].message.content)\n",
    "    return {\"messages\": state[\"messages\"] + [ai_message]}\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "graph_app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# Define the data model for the request\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    thread_id: str = \"default\"\n",
    "\n",
    "# Create the FastAPI application\n",
    "app = FastAPI(title=\"LangChain FastAPI\", description=\"API to generate text using LangChain and LangGraph\")\n",
    "\n",
    "# Welcome endpoint\n",
    "@app.get(\"/\")\n",
    "async def api_home():\n",
    "    \"\"\"Welcome endpoint\"\"\"\n",
    "    return {\"detail\": \"Welcome to FastAPI, Langchain, Docker tutorial\"}\n",
    "\n",
    "# Generate endpoint\n",
    "@app.post(\"/generate\")\n",
    "async def generate(request: QueryRequest):\n",
    "    \"\"\"\n",
    "    Endpoint to generate text using the language model\n",
    "    \n",
    "    Args:\n",
    "        request: QueryRequest\n",
    "        query: str\n",
    "        thread_id: str = \"default\"\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated text and the thread ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Configure the thread ID\n",
    "        config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
    "        \n",
    "        # Create the input message\n",
    "        input_messages = [HumanMessage(content=request.query)]\n",
    "        \n",
    "        # Invoke the graph\n",
    "        output = graph_app.invoke({\"messages\": input_messages}, config)\n",
    "        \n",
    "        # Get the model response\n",
    "        response = output[\"messages\"][-1].content\n",
    "        \n",
    "        return {\n",
    "            \"generated_text\": response,\n",
    "            \"thread_id\": request.thread_id\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error al generar texto: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
