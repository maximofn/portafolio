{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4756f7d0",
   "metadata": {},
   "source": [
    "# MCP durability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff865e",
   "metadata": {},
   "source": [
    "En este post vamos a ver cómo crear un servidor y un cliente MCP con la capacidad de que el cliente le pida una tarea de larga duración al servidor, parar el cliente, la tarea se sigue ejecutando en el servidor, volver a conectar el cliente y ver el estado de la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcaed61",
   "metadata": {},
   "source": [
    "## Servidor MCP con durabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4d83d",
   "metadata": {},
   "source": [
    "El servidor va a ser parecido al que hicimos en los posts [streamable MCP](https://www.maximofn.com/streamable-mcp) y [resumable MCP](https://www.maximofn.com/resumable-mcp), con la diferencia de que ahora vamos a añadir la capacidad de que el cliente le pida una tarea de larga duración al servidor, parar el cliente, la tarea se sigue ejecutando en el servidor, volver a conectar el cliente y ver el estado de la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45bf2b",
   "metadata": {},
   "source": [
    "Además, en este caso, vamos a guardar las tareas en una base de datos SQLite para hacerlo más profesional, ya que en los anteriores post guardábamos la información en archivos json."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e918478",
   "metadata": {},
   "source": [
    "### Implementación del servidor MCP con durabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9063d038",
   "metadata": {},
   "source": [
    "El servidor va a tener una clase `DurableTaskManager` que va a gestionar la creación de las tareas, las va a guardar en la base de datos, va a reportar el estado y las va a poder cancelar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1a22b",
   "metadata": {},
   "source": [
    "Va a tener tres resources, para obtener el estado de una tarea, para listar todas las tareas y para listar las tareas por estado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44788c58",
   "metadata": {},
   "source": [
    "Y va a tener cinco tools, tres van a ser tareas en concreto (migración de datos, procesado de datos y entrenamiento de modelo), una para cancelar una tarea y la última para obtener las estadísticas del servidor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207aee5",
   "metadata": {},
   "source": [
    "#### Crear entorno virtual del servidor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be353742",
   "metadata": {},
   "source": [
    "Primero creamos la carpeta donde lo vamos a desarrollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de1c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir MCP_durability_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eaec53",
   "metadata": {},
   "source": [
    "Creamos el entorno con `uv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46904e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized project `\u001b[36mmcp-durability-server\u001b[39m` at `\u001b[36m/Users/macm1/Documents/web/portafolio/posts/MCP_durability_server\u001b[39m`\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_server && uv init ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b19a22",
   "metadata": {},
   "source": [
    "Lo iniciamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd46c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython \u001b[36m3.12.8\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_server && uv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c935f",
   "metadata": {},
   "source": [
    "Instalamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60feb5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m64 packages\u001b[0m \u001b[2min 344ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m61 packages\u001b[0m \u001b[2min 136ms\u001b[0m\u001b[0m                              \u001b[0m     \u001b[0m░░░░░░░░░░░░░░░░░░░░ [0/0] \u001b[2mInstalling wheels...                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mauthlib\u001b[0m\u001b[2m==1.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.8.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcffi\u001b[0m\u001b[2m==1.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcryptography\u001b[0m\u001b[2m==45.0.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcyclopts\u001b[0m\u001b[2m==3.23.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocutils\u001b[0m\u001b[2m==0.22\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1memail-validator\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastmcp\u001b[0m\u001b[2m==2.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1misodate\u001b[0m\u001b[2m==0.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.25.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema-path\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema-specifications\u001b[0m\u001b[2m==2025.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlazy-object-proxy\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmcp\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmore-itertools\u001b[0m\u001b[2m==10.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-core\u001b[0m\u001b[2m==0.19.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-pydantic\u001b[0m\u001b[2m==0.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-schema-validator\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-spec-validator\u001b[0m\u001b[2m==0.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mparse\u001b[0m\u001b[2m==1.20.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpathable\u001b[0m\u001b[2m==0.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycparser\u001b[0m\u001b[2m==2.22\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.10.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyperclip\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mreferencing\u001b[0m\u001b[2m==0.36.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrfc3339-validator\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-rst\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrpds-py\u001b[0m\u001b[2m==0.27.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msse-starlette\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.47.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwerkzeug\u001b[0m\u001b[2m==3.1.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_server && uv add fastmcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad539f5f",
   "metadata": {},
   "source": [
    "#### Código del servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467dc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MCP_durability_server/server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MCP_durability_server/server.py\n",
    "\n",
    "\"\"\"\n",
    "MCP Durability Server\n",
    "\n",
    "MPC server that implements durability for long-running agents using Resource links.\n",
    "Allows long-running operations to survive server restarts and provides state tracking\n",
    "outside of band.\n",
    "\n",
    "Implemented pattern:\n",
    "1. Tools return resource links immediately\n",
    "2. Background processing continues asynchronously\n",
    "3. Clients can poll or subscribe to resources for state updates\n",
    "\n",
    "Usage:\n",
    "    python server.py\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import sqlite3\n",
    "import time\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from fastmcp import FastMCP\n",
    "from fastmcp.server.context import Context\n",
    "\n",
    "\n",
    "class TaskStatus(Enum):\n",
    "    \"\"\"Long running task status.\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\" \n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "    CANCELLED = \"cancelled\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TaskResult:\n",
    "    \"\"\"Result data for a completed task.\"\"\"\n",
    "    task_id: str\n",
    "    status: TaskStatus\n",
    "    progress: float = 0.0\n",
    "    total: Optional[float] = None\n",
    "    message: Optional[str] = None\n",
    "    result_data: Optional[Dict[str, Any]] = None\n",
    "    error: Optional[str] = None\n",
    "    created_at: float = 0.0\n",
    "    updated_at: float = 0.0\n",
    "    completed_at: Optional[float] = None\n",
    "\n",
    "\n",
    "class DurableTaskManager:\n",
    "    \"\"\"Manages persistent task state and background execution.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \"mcp_tasks.db\"):\n",
    "        self.db_path = db_path\n",
    "        self._background_tasks: Dict[str, asyncio.Task] = {}\n",
    "        self._setup_database()\n",
    "    \n",
    "    def _setup_database(self) -> None:\n",
    "        \"\"\"Initializes SQLite database for task persistence.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS tasks (\n",
    "                task_id TEXT PRIMARY KEY,\n",
    "                status TEXT NOT NULL,\n",
    "                progress REAL DEFAULT 0.0,\n",
    "                total REAL,\n",
    "                message TEXT,\n",
    "                result_data TEXT,\n",
    "                error TEXT,\n",
    "                created_at REAL NOT NULL,\n",
    "                updated_at REAL NOT NULL,\n",
    "                completed_at REAL\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    async def create_task(self, task_id: Optional[str] = None) -> str:\n",
    "        \"\"\"Creates a new task and returns its ID.\"\"\"\n",
    "        if not task_id:\n",
    "            task_id = str(uuid.uuid4())\n",
    "        \n",
    "        current_time = time.time()\n",
    "        task_result = TaskResult(\n",
    "            task_id=task_id,\n",
    "            status=TaskStatus.PENDING,\n",
    "            created_at=current_time,\n",
    "            updated_at=current_time\n",
    "        )\n",
    "        \n",
    "        await self._save_task(task_result)\n",
    "        return task_id\n",
    "    \n",
    "    async def start_background_task(\n",
    "        self, \n",
    "        task_id: str, \n",
    "        task_function, \n",
    "        context: Optional[Context] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Starts a background task and tracks its execution.\"\"\"\n",
    "        async def wrapper():\n",
    "            try:\n",
    "                await self._update_task_status(task_id, TaskStatus.RUNNING)\n",
    "                \n",
    "                # Execute the real task\n",
    "                if context:\n",
    "                    result = await task_function(task_id, context, self)\n",
    "                else:\n",
    "                    result = await task_function(task_id, self)\n",
    "                \n",
    "                # Mark as completed with results\n",
    "                await self._update_task_completion(task_id, result)\n",
    "                \n",
    "            except asyncio.CancelledError:\n",
    "                await self._update_task_status(task_id, TaskStatus.CANCELLED)\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                await self._update_task_error(task_id, str(e))\n",
    "            finally:\n",
    "                # Clean up background task reference\n",
    "                self._background_tasks.pop(task_id, None)\n",
    "        \n",
    "        # Start the task in the background\n",
    "        task = asyncio.create_task(wrapper())\n",
    "        self._background_tasks[task_id] = task\n",
    "    \n",
    "    async def _save_task(self, task_result: TaskResult) -> None:\n",
    "        \"\"\"Saves the task result to the database.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        conn.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO tasks \n",
    "            (task_id, status, progress, total, message, result_data, error, \n",
    "             created_at, updated_at, completed_at)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            task_result.task_id,\n",
    "            task_result.status.value,\n",
    "            task_result.progress,\n",
    "            task_result.total,\n",
    "            task_result.message,\n",
    "            json.dumps(task_result.result_data) if task_result.result_data else None,\n",
    "            task_result.error,\n",
    "            task_result.created_at,\n",
    "            task_result.updated_at,\n",
    "            task_result.completed_at\n",
    "        ))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    async def get_task(self, task_id: str) -> Optional[TaskResult]:\n",
    "        \"\"\"Retrieves the task result from the database.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.execute(\"\"\"\n",
    "            SELECT task_id, status, progress, total, message, result_data, error,\n",
    "                   created_at, updated_at, completed_at\n",
    "            FROM tasks WHERE task_id = ?\n",
    "        \"\"\", (task_id,))\n",
    "        \n",
    "        row = cursor.fetchone()\n",
    "        conn.close()\n",
    "        \n",
    "        if not row:\n",
    "            return None\n",
    "        \n",
    "        result_data = None\n",
    "        if row[5]:  # result_data\n",
    "            try:\n",
    "                result_data = json.loads(row[5])\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        return TaskResult(\n",
    "            task_id=row[0],\n",
    "            status=TaskStatus(row[1]),\n",
    "            progress=row[2],\n",
    "            total=row[3],\n",
    "            message=row[4],\n",
    "            result_data=result_data,\n",
    "            error=row[6],\n",
    "            created_at=row[7],\n",
    "            updated_at=row[8],\n",
    "            completed_at=row[9]\n",
    "        )\n",
    "    \n",
    "    async def update_task_progress(\n",
    "        self, \n",
    "        task_id: str, \n",
    "        progress: float, \n",
    "        total: Optional[float] = None,\n",
    "        message: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Updates the task progress.\"\"\"\n",
    "        task_result = await self.get_task(task_id)\n",
    "        if not task_result:\n",
    "            return\n",
    "        \n",
    "        task_result.progress = progress\n",
    "        if total is not None:\n",
    "            task_result.total = total\n",
    "        if message is not None:\n",
    "            task_result.message = message\n",
    "        task_result.updated_at = time.time()\n",
    "        \n",
    "        await self._save_task(task_result)\n",
    "    \n",
    "    async def _update_task_status(self, task_id: str, status: TaskStatus) -> None:\n",
    "        \"\"\"Updates the task status.\"\"\"\n",
    "        task_result = await self.get_task(task_id)\n",
    "        if not task_result:\n",
    "            return\n",
    "        \n",
    "        task_result.status = status\n",
    "        task_result.updated_at = time.time()\n",
    "        await self._save_task(task_result)\n",
    "    \n",
    "    async def _update_task_completion(self, task_id: str, result_data: Any) -> None:\n",
    "        \"\"\"Marks the task as completed with results.\"\"\"\n",
    "        task_result = await self.get_task(task_id)\n",
    "        if not task_result:\n",
    "            return\n",
    "        \n",
    "        task_result.status = TaskStatus.COMPLETED\n",
    "        task_result.progress = task_result.total or 100.0\n",
    "        task_result.result_data = result_data if isinstance(result_data, dict) else {\"result\": result_data}\n",
    "        task_result.completed_at = time.time()\n",
    "        task_result.updated_at = time.time()\n",
    "        \n",
    "        await self._save_task(task_result)\n",
    "    \n",
    "    async def _update_task_error(self, task_id: str, error: str) -> None:\n",
    "        \"\"\"Marks the task as failed with error.\"\"\"\n",
    "        task_result = await self.get_task(task_id)\n",
    "        if not task_result:\n",
    "            return\n",
    "        \n",
    "        task_result.status = TaskStatus.FAILED\n",
    "        task_result.error = error\n",
    "        task_result.updated_at = time.time()\n",
    "        \n",
    "        await self._save_task(task_result)\n",
    "    \n",
    "    async def cancel_task(self, task_id: str) -> bool:\n",
    "        \"\"\"Cancels a background task in execution.\"\"\"\n",
    "        if task_id in self._background_tasks:\n",
    "            task = self._background_tasks[task_id]\n",
    "            task.cancel()\n",
    "            await self._update_task_status(task_id, TaskStatus.CANCELLED)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    async def list_tasks(self, status_filter: Optional[TaskStatus] = None) -> List[TaskResult]:\n",
    "        \"\"\"Lists all tasks, optionally filtered by status.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        if status_filter:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT task_id, status, progress, total, message, result_data, error,\n",
    "                       created_at, updated_at, completed_at\n",
    "                FROM tasks WHERE status = ? ORDER BY updated_at DESC\n",
    "            \"\"\", (status_filter.value,))\n",
    "        else:\n",
    "            cursor = conn.execute(\"\"\"\n",
    "                SELECT task_id, status, progress, total, message, result_data, error,\n",
    "                       created_at, updated_at, completed_at\n",
    "                FROM tasks ORDER BY updated_at DESC\n",
    "            \"\"\")\n",
    "        \n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        tasks = []\n",
    "        for row in rows:\n",
    "            result_data = None\n",
    "            if row[5]:\n",
    "                try:\n",
    "                    result_data = json.loads(row[5])\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "            \n",
    "            tasks.append(TaskResult(\n",
    "                task_id=row[0],\n",
    "                status=TaskStatus(row[1]),\n",
    "                progress=row[2],\n",
    "                total=row[3],\n",
    "                message=row[4],\n",
    "                result_data=result_data,\n",
    "                error=row[6],\n",
    "                created_at=row[7],\n",
    "                updated_at=row[8],\n",
    "                completed_at=row[9]\n",
    "            ))\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "\n",
    "# Create the MCP server with durability capabilities\n",
    "server = FastMCP(\n",
    "    name=\"MCP Durability Server\",\n",
    "    instructions=\"An MCP server that demonstrates durability for long-running agents\"\n",
    ")\n",
    "\n",
    "# Initialize the task manager\n",
    "task_manager = DurableTaskManager()\n",
    "\n",
    "\n",
    "# Resources to track the state of tasks\n",
    "@server.resource(\"task://status/{task_id}\")\n",
    "async def get_task_status(task_id: str) -> str:\n",
    "    \"\"\"Gets the current status of a task by ID.\"\"\"\n",
    "    task_result = await task_manager.get_task(task_id)\n",
    "    if not task_result:\n",
    "        return json.dumps({\"error\": \"Task not found\", \"task_id\": task_id})\n",
    "    \n",
    "    data = {\n",
    "        \"task_id\": task_result.task_id,\n",
    "        \"status\": task_result.status.value,\n",
    "        \"progress\": task_result.progress,\n",
    "        \"total\": task_result.total,\n",
    "        \"message\": task_result.message,\n",
    "        \"result_data\": task_result.result_data,\n",
    "        \"error\": task_result.error,\n",
    "        \"created_at\": task_result.created_at,\n",
    "        \"updated_at\": task_result.updated_at,\n",
    "        \"completed_at\": task_result.completed_at,\n",
    "    }\n",
    "    \n",
    "    return json.dumps(data)\n",
    "\n",
    "\n",
    "@server.resource(\"task://list\")\n",
    "async def list_all_tasks() -> str:\n",
    "    \"\"\"Lists all tasks with their current status.\"\"\"\n",
    "    tasks = await task_manager.list_tasks()\n",
    "    data = {\n",
    "        \"tasks\": [\n",
    "            {\n",
    "                \"task_id\": task.task_id,\n",
    "                \"status\": task.status.value,\n",
    "                \"progress\": task.progress,\n",
    "                \"total\": task.total,\n",
    "                \"message\": task.message,\n",
    "                \"created_at\": task.created_at,\n",
    "                \"updated_at\": task.updated_at,\n",
    "                \"completed_at\": task.completed_at,\n",
    "                \"has_result\": task.result_data is not None,\n",
    "                \"has_error\": task.error is not None\n",
    "            } for task in tasks\n",
    "        ],\n",
    "        \"total\": len(tasks)\n",
    "    }\n",
    "    return json.dumps(data)\n",
    "\n",
    "\n",
    "@server.resource(\"task://list/{status}\")\n",
    "async def list_tasks_by_status(status: str) -> str:\n",
    "    \"\"\"Lists tasks filtered by status.\"\"\"\n",
    "    try:\n",
    "        status_enum = TaskStatus(status.lower())\n",
    "        tasks = await task_manager.list_tasks(status_enum)\n",
    "        data = {\n",
    "            \"tasks\": [\n",
    "                {\n",
    "                    \"task_id\": task.task_id,\n",
    "                    \"status\": task.status.value,\n",
    "                    \"progress\": task.progress,\n",
    "                    \"total\": task.total,\n",
    "                    \"message\": task.message,\n",
    "                    \"created_at\": task.created_at,\n",
    "                    \"updated_at\": task.updated_at,\n",
    "                    \"completed_at\": task.completed_at\n",
    "                } for task in tasks\n",
    "            ],\n",
    "            \"status_filter\": status,\n",
    "            \"total\": len(tasks)\n",
    "        }\n",
    "        return json.dumps(data)\n",
    "    except ValueError:\n",
    "        data = {\n",
    "            \"error\": f\"Invalid status: {status}\",\n",
    "            \"valid_statuses\": [s.value for s in TaskStatus]\n",
    "        }\n",
    "        return json.dumps(data)\n",
    "\n",
    "\n",
    "# Tools to demonstrate durability functionality\n",
    "@server.tool\n",
    "async def start_data_migration(\n",
    "    source_path: str,\n",
    "    destination_path: str,\n",
    "    ctx: Context,\n",
    "    record_count: int = 1000\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Starts a long-running data migration task.\n",
    "    \n",
    "    Args:\n",
    "        source_path: Source path of the data\n",
    "        destination_path: Destination path of the data  \n",
    "        record_count: Number of records to migrate\n",
    "        ctx: Contexto MCP\n",
    "    \n",
    "    Returns:\n",
    "        Resource URI to track progress\n",
    "    \"\"\"\n",
    "    await ctx.info(f\"Starting migration of {record_count} records from {source_path} to {destination_path}\")\n",
    "    print(f\"Starting migration of {record_count} records from {source_path} to {destination_path}\")\n",
    "    \n",
    "    async def migration_task(task_id: str, context: Context, manager: DurableTaskManager):\n",
    "        \"\"\"Simulated data migration task.\"\"\"\n",
    "        batch_size = 100\n",
    "        migrated = 0\n",
    "        \n",
    "        for batch_start in range(0, record_count, batch_size):\n",
    "            # Simulate batch processing\n",
    "            await asyncio.sleep(1)  # Simulate work\n",
    "            \n",
    "            batch_end = min(batch_start + batch_size, record_count)\n",
    "            migrated = batch_end\n",
    "            \n",
    "            # Update progress\n",
    "            progress_pct = (migrated / record_count) * 100\n",
    "            message = f\"Migrated {migrated}/{record_count} records\"\n",
    "            \n",
    "            await manager.update_task_progress(task_id, progress_pct, 100.0, message)\n",
    "            try:\n",
    "                await context.report_progress(migrated, record_count, message)\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        try:\n",
    "            await context.info(f\"Data migration completed: {migrated} records\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            \"migrated_records\": migrated,\n",
    "            \"total_records\": record_count,\n",
    "            \"source_path\": source_path,\n",
    "            \"destination_path\": destination_path,\n",
    "            \"success\": True,\n",
    "            \"completion_time\": time.time()\n",
    "        }\n",
    "    \n",
    "    # Create the task and start background processing\n",
    "    task_id = await task_manager.create_task()\n",
    "    await task_manager.start_background_task(task_id, migration_task, ctx)\n",
    "    \n",
    "    resource_link = f\"task://status/{task_id}\"\n",
    "    await ctx.info(f\"Data migration started. Track progress at: {resource_link}\")\n",
    "    \n",
    "    return resource_link\n",
    "\n",
    "\n",
    "@server.tool  \n",
    "async def start_batch_processing(\n",
    "    batch_size: int,\n",
    "    total_items: int,\n",
    "    ctx: Context,\n",
    "    processing_delay: float = 10\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Starts a batch processing task.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Size of each batch\n",
    "        total_items: Total number of items to process\n",
    "        processing_delay: Delay per batch in seconds\n",
    "        ctx: MCP context\n",
    "    \n",
    "    Returns:\n",
    "        Resource URI to track progress\n",
    "    \"\"\"\n",
    "    await ctx.info(f\"Starting batch processing: {total_items} items in batches of {batch_size}\")\n",
    "    \n",
    "    async def batch_task(task_id: str, context: Context, manager: DurableTaskManager):\n",
    "        \"\"\"Batch processing task.\"\"\"\n",
    "        processed = 0\n",
    "        \n",
    "        for i in range(0, total_items, batch_size):\n",
    "            await asyncio.sleep(processing_delay)   # Simulate work\n",
    "            batch_end = min(i + batch_size, total_items)\n",
    "            processed = batch_end\n",
    "            \n",
    "            progress = (processed / total_items) * 100\n",
    "            message = f\"Processed {processed}/{total_items} items\"\n",
    "            \n",
    "            await manager.update_task_progress(task_id, progress, 100.0, message)\n",
    "            try:\n",
    "                await context.report_progress(processed, total_items, message)\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        try:\n",
    "            await context.info(f\"Batch processing completed: {processed} items\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            \"processed\": processed, \n",
    "            \"total\": total_items,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"processing_delay\": processing_delay,\n",
    "            \"success\": True\n",
    "        }\n",
    "    \n",
    "    task_id = await task_manager.create_task()\n",
    "    await task_manager.start_background_task(task_id, batch_task, ctx)\n",
    "    \n",
    "    resource_link = f\"task://status/{task_id}\"\n",
    "    await ctx.info(f\"Batch processing started. Track progress at: {resource_link}\")\n",
    "    \n",
    "    return resource_link\n",
    "\n",
    "\n",
    "@server.tool\n",
    "async def start_ml_training(\n",
    "    model_name: str,\n",
    "    ctx: Context,\n",
    "    dataset_size: int = 10000,\n",
    "    epochs: int = 100\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Simulates machine learning model training.\n",
    "        \n",
    "    Args:\n",
    "        model_name: Name of the model to train\n",
    "        dataset_size: Size of the dataset\n",
    "        epochs: Number of training epochs\n",
    "        ctx: MCP context\n",
    "    \n",
    "    Returns:\n",
    "        Resource URI to track progress\n",
    "    \"\"\"\n",
    "    await ctx.info(f\"Starting training of model '{model_name}' with {dataset_size} samples by {epochs} epochs\")\n",
    "    \n",
    "    async def training_task(task_id: str, context: Context, manager: DurableTaskManager):\n",
    "        \"\"\"Simulated ML training task.\"\"\"\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Simulate training of an epoch\n",
    "            await asyncio.sleep(10)\n",
    "            \n",
    "            # Simulate training metrics\n",
    "            loss = 1.0 - (epoch / epochs) * 0.8 + (epoch % 10) * 0.01\n",
    "            accuracy = (epoch / epochs) * 0.95 + (epoch % 5) * 0.002\n",
    "            \n",
    "            progress = (epoch / epochs) * 100\n",
    "            message = f\"Epoch {epoch}/{epochs} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\"\n",
    "            \n",
    "            await manager.update_task_progress(task_id, progress, 100.0, message)\n",
    "            try:\n",
    "                await context.report_progress(epoch, epochs, message)\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        try:\n",
    "            await context.info(f\"Model '{model_name}' training completed\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"dataset_size\": dataset_size,\n",
    "            \"epochs_completed\": epochs,\n",
    "            \"final_loss\": loss,\n",
    "            \"final_accuracy\": accuracy,\n",
    "            \"success\": True,\n",
    "            \"training_time_seconds\": epochs * 0.5\n",
    "        }\n",
    "    \n",
    "    task_id = await task_manager.create_task()\n",
    "    await task_manager.start_background_task(task_id, training_task, ctx)\n",
    "    \n",
    "    resource_link = f\"task://status/{task_id}\"\n",
    "    await ctx.info(f\"Training started. Track progress at: {resource_link}\")\n",
    "    \n",
    "    return resource_link\n",
    "\n",
    "\n",
    "@server.tool\n",
    "async def cancel_task(task_id: str, ctx: Context) -> str:\n",
    "    \"\"\"\n",
    "    Cancels an in-progress task.\n",
    "    \n",
    "    Args:\n",
    "        task_id: ID of the task to cancel\n",
    "        ctx: MCP context\n",
    "    \n",
    "    Returns:\n",
    "        Status message of the cancellation\n",
    "    \"\"\"\n",
    "    success = await task_manager.cancel_task(task_id)\n",
    "    if success:\n",
    "        await ctx.info(f\"Task {task_id} cancelled successfully\")\n",
    "        return f\"Task {task_id} cancelled. Status available at: task://status/{task_id}\"\n",
    "    else:\n",
    "        await ctx.warning(f\"Task {task_id} not found or cannot be cancelled\")\n",
    "        return f\"Task {task_id} not found or cannot be cancelled\"\n",
    "\n",
    "\n",
    "@server.tool\n",
    "async def get_server_stats(ctx: Context) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Gets durability server statistics.\n",
    "    \n",
    "    Args:\n",
    "        ctx: MCP context\n",
    "    \n",
    "    Returns:\n",
    "        Server statistics\n",
    "    \"\"\"\n",
    "    all_tasks = await task_manager.list_tasks()\n",
    "    \n",
    "    stats = {\n",
    "        \"total_tasks\": len(all_tasks),\n",
    "        \"running_tasks\": len([t for t in all_tasks if t.status == TaskStatus.RUNNING]),\n",
    "        \"completed_tasks\": len([t for t in all_tasks if t.status == TaskStatus.COMPLETED]),\n",
    "        \"failed_tasks\": len([t for t in all_tasks if t.status == TaskStatus.FAILED]),\n",
    "        \"cancelled_tasks\": len([t for t in all_tasks if t.status == TaskStatus.CANCELLED]),\n",
    "        \"pending_tasks\": len([t for t in all_tasks if t.status == TaskStatus.PENDING]),\n",
    "        \"active_background_tasks\": len(task_manager._background_tasks),\n",
    "    }\n",
    "    \n",
    "    await ctx.info(f\"Server statistics: {stats['total_tasks']} total tasks, {stats['running_tasks']} running\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Durability MCP server started\")\n",
    "    print(\"Available tools:\")\n",
    "    print(\"  - start_data_migration: Starts data migration\")\n",
    "    print(\"  - start_batch_processing: Starts batch processing\")\n",
    "    print(\"  - start_ml_training: Simulates ML training\")\n",
    "    print(\"  - cancel_task: Cancels a task\")\n",
    "    print(\"  - get_server_stats: Gets server statistics\")\n",
    "    print(\"\\nResources available:\")\n",
    "    print(\"  - task://status/{task_id}: Specific task status\")\n",
    "    print(\"  - task://list: Lists all tasks\")\n",
    "    print(\"  - task://list/{status}: Lists tasks by status\")\n",
    "    \n",
    "    server.run(transport=\"http\", host=\"127.0.0.1\", port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff16e2",
   "metadata": {},
   "source": [
    "## Cliente MCP con durabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f9134",
   "metadata": {},
   "source": [
    "El cliente va a ser muy parecido al que hicimos en los posts [streamable MCP](https://www.maximofn.com/streamable-mcp) y [resumable MCP](https://www.maximofn.com/resumable-mcp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc3f8da",
   "metadata": {},
   "source": [
    "### Implementación del cliente MCP con durabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604d9ff",
   "metadata": {},
   "source": [
    "Va a haber una clase `DurabilityClient` que va a tener métodos para conectarse al servidor, iniciar las tres posibles tareas (migración de datos, procesado de datos y entrenamiento de un modelo), obtener el estado de una tarea, obtener una lista de todas las tareas o cancelar una tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1724693",
   "metadata": {},
   "source": [
    "#### Crear entorno virtual del cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1579be",
   "metadata": {},
   "source": [
    "Primero creamos la carpeta donde lo vamos a desarrollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5718c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir MCP_durability_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8a234",
   "metadata": {},
   "source": [
    "Creamos el entorno con `uv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1a33b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized project `\u001b[36mmcp-durability-client\u001b[39m` at `\u001b[36m/Users/macm1/Documents/web/portafolio/posts/MCP_durability_client\u001b[39m`\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && uv init ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a572b",
   "metadata": {},
   "source": [
    "Lo iniciamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0151a6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython \u001b[36m3.12.8\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && uv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c132d1",
   "metadata": {},
   "source": [
    "Instalamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ce2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m64 packages\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m61 packages\u001b[0m \u001b[2min 80ms\u001b[0m\u001b[0m2                              \u001b[0m     \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mauthlib\u001b[0m\u001b[2m==1.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.8.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcffi\u001b[0m\u001b[2m==1.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcryptography\u001b[0m\u001b[2m==45.0.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcyclopts\u001b[0m\u001b[2m==3.23.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocutils\u001b[0m\u001b[2m==0.22\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1memail-validator\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastmcp\u001b[0m\u001b[2m==2.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1misodate\u001b[0m\u001b[2m==0.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.25.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema-path\u001b[0m\u001b[2m==0.3.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema-specifications\u001b[0m\u001b[2m==2025.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlazy-object-proxy\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmcp\u001b[0m\u001b[2m==1.13.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmore-itertools\u001b[0m\u001b[2m==10.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-core\u001b[0m\u001b[2m==0.19.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-pydantic\u001b[0m\u001b[2m==0.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-schema-validator\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-spec-validator\u001b[0m\u001b[2m==0.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mparse\u001b[0m\u001b[2m==1.20.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpathable\u001b[0m\u001b[2m==0.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycparser\u001b[0m\u001b[2m==2.22\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.10.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyperclip\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mreferencing\u001b[0m\u001b[2m==0.36.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrfc3339-validator\u001b[0m\u001b[2m==0.1.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-rst\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrpds-py\u001b[0m\u001b[2m==0.27.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msse-starlette\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.47.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.35.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwerkzeug\u001b[0m\u001b[2m==3.1.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && uv add fastmcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c9168c",
   "metadata": {},
   "source": [
    "#### Código del cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d801b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing MCP_durability_client/client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MCP_durability_client/client.py\n",
    "\n",
    "\"\"\"\n",
    "Durability MCP client\n",
    "\n",
    "Client that demonstrates how to interact with an MCP server that implements durability.\n",
    "\n",
    "Shows how to:\n",
    "1. Start long-running tasks\n",
    "2. Monitor progress using resource polling\n",
    "3. Subscribe to status updates (simulated)\n",
    "4. Handle persistent tasks that survive server restarts\n",
    "\n",
    "\n",
    "Usage:\n",
    "    python client.py\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from fastmcp import Client\n",
    "from fastmcp.client.transports import StreamableHttpTransport\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "class DurabilityClient:\n",
    "    \"\"\"Client that demonstrates durability patterns with MCP.\"\"\"\n",
    "    \n",
    "    def __init__(self, server_command: List[str]):\n",
    "        \"\"\"\n",
    "        Initializes the durability client.\n",
    "        \n",
    "        Args:\n",
    "            server_command: Command to execute the MCP server\n",
    "        \"\"\"\n",
    "        self.server_command = server_command\n",
    "        self._polling_tasks: Dict[str, asyncio.Task] = {}\n",
    "    \n",
    "    async def connect(self) -> Client:\n",
    "        \"\"\"Creates and connects the client to the server.\"\"\"\n",
    "        transport = StreamableHttpTransport(\n",
    "            url=\"http://127.0.0.1:8080/mcp\"\n",
    "        )\n",
    "        client = Client(transport)\n",
    "        return client\n",
    "    \n",
    "    async def start_data_migration(\n",
    "        self, \n",
    "        client: Client,\n",
    "        source_path: str = \"/data/source\",\n",
    "        destination_path: str = \"/data/destination\", \n",
    "        record_count: int = 1000\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Starts a data migration task and returns the resource link.\n",
    "        \n",
    "        Args:\n",
    "            client: MCP connected client\n",
    "            source_path: Source path\n",
    "            destination_path: Destination path\n",
    "            record_count: Number of records to migrate\n",
    "            \n",
    "        Returns:\n",
    "            Resource URI to track progress\n",
    "        \"\"\"\n",
    "        print(f\"🚀 Starting data migration: {record_count} records\")\n",
    "        print(f\"   Source: {source_path}\")\n",
    "        print(f\"   Destination: {destination_path}\")\n",
    "        \n",
    "        result = await client.call_tool(\n",
    "            \"start_data_migration\",\n",
    "            {\n",
    "                \"source_path\": source_path,\n",
    "                \"destination_path\": destination_path,\n",
    "                \"record_count\": record_count\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        resource_uri = result.content[0].text\n",
    "        print(f\"✅ Task started. Resource URI: {resource_uri}\")\n",
    "        return resource_uri\n",
    "    \n",
    "    async def start_batch_processing(\n",
    "        self,\n",
    "        client: Client,\n",
    "        batch_size: int = 50,\n",
    "        total_items: int = 500,\n",
    "        processing_delay: float = 0.3\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Starts batch processing.\n",
    "        \n",
    "        Args:\n",
    "            client: MCP connected client\n",
    "            batch_size: Batch size\n",
    "            total_items: Total items\n",
    "            processing_delay: Processing delay\n",
    "            \n",
    "        Returns:\n",
    "            Resource URI to track progress\n",
    "        \"\"\"\n",
    "        print(f\"🔄 Starting batch processing:\")\n",
    "        print(f\"   Total items: {total_items}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Processing delay: {processing_delay}s\")\n",
    "        \n",
    "        result = await client.call_tool(\n",
    "            \"start_batch_processing\",\n",
    "            {\n",
    "                \"batch_size\": batch_size,\n",
    "                \"total_items\": total_items,\n",
    "                \"processing_delay\": processing_delay\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        resource_uri = result.content[0].text\n",
    "        print(f\"✅ Batch processing started. Resource URI: {resource_uri}\")\n",
    "        return resource_uri\n",
    "    \n",
    "    async def start_ml_training(\n",
    "        self,\n",
    "        client: Client,\n",
    "        model_name: str = \"clasificador-texto\",\n",
    "        dataset_size: int = 5000,\n",
    "        epochs: int = 50\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Starts ML model training.\n",
    "        \n",
    "        Args:\n",
    "            client: MCP connected client\n",
    "            model_name: Model name\n",
    "            dataset_size: Dataset size\n",
    "            epochs: Number of epochs\n",
    "            \n",
    "        Returns:\n",
    "            Resource URI to track progress\n",
    "        \"\"\"\n",
    "        print(f\"🤖 Starting ML training:\")\n",
    "        print(f\"   Model: {model_name}\")\n",
    "        print(f\"   Dataset size: {dataset_size} samples\")\n",
    "        print(f\"   Epochs: {epochs}\")\n",
    "        \n",
    "        result = await client.call_tool(\n",
    "            \"start_ml_training\",\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"dataset_size\": dataset_size,\n",
    "                \"epochs\": epochs\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        resource_uri = result.content[0].text\n",
    "        print(f\"✅ ML training started. Resource URI: {resource_uri}\")\n",
    "        return resource_uri\n",
    "    \n",
    "    async def get_task_status(self, client: Client, resource_uri: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Gets the current status of a task.\n",
    "        \n",
    "        Args:\n",
    "            client: MCP connected client\n",
    "            resource_uri: Resource URI of the task\n",
    "            \n",
    "        Returns:\n",
    "            Current status of the task\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = await client.read_resource(resource_uri)\n",
    "            \n",
    "            # result should be a list of ReadResourceContents  \n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                content_block = result[0]\n",
    "                \n",
    "                # The content is in the 'text' attribute\n",
    "                if hasattr(content_block, 'text') and content_block.text:\n",
    "                    return json.loads(content_block.text)\n",
    "                else:\n",
    "                    return {\"error\": f\"No text found in the resource. Type: {type(content_block)}, text={getattr(content_block, 'text', None)}\"}\n",
    "            else:\n",
    "                return {\"error\": \"Empty resource\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error reading resource: {str(e)}\"}\n",
    "    \n",
    "    async def poll_task_until_complete(\n",
    "        self, \n",
    "        client: Client, \n",
    "        resource_uri: str,\n",
    "        poll_interval: float = 1.0,\n",
    "        max_duration: float = 10.0\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Polls a task until it completes or fails.\n",
    "        \n",
    "        Args:\n",
    "            client: MCP connected client\n",
    "            resource_uri: Resource URI of the task\n",
    "            poll_interval: Interval between polls in seconds\n",
    "            max_duration: Maximum duration of polling\n",
    "            \n",
    "        Returns:\n",
    "            Final status of the task\n",
    "        \"\"\"\n",
    "        task_id = resource_uri.split(\"/\")[-1]\n",
    "        print(f\"📊 Monitoring task {task_id}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        last_progress = -1\n",
    "        \n",
    "        while time.time() - start_time < max_duration:\n",
    "            status = await self.get_task_status(client, resource_uri)\n",
    "            \n",
    "            if \"error\" in status and status[\"error\"] is not None:\n",
    "                print(f\"Error getting status: {status['error']}\")\n",
    "                return status\n",
    "            \n",
    "            current_status = status.get(\"status\", \"unknown\")\n",
    "            progress = status.get(\"progress\", 0)\n",
    "            message = status.get(\"message\", \"\")\n",
    "            \n",
    "            # Show updates only if the progress changed\n",
    "            if progress != last_progress:\n",
    "                progress_bar = self._create_progress_bar(progress)\n",
    "                print(f\"   {progress_bar} {progress:5.1f}% | {current_status.upper()} | {message}\")\n",
    "                last_progress = progress\n",
    "            \n",
    "            # Check if the task is complete\n",
    "            if current_status in [\"completed\", \"failed\", \"cancelled\"]:\n",
    "                print(f\"Task {current_status}: {task_id}\")\n",
    "                \n",
    "                if current_status == \"completed\" and status.get(\"result_data\"):\n",
    "                    print(\"Results:\")\n",
    "                    self._print_results(status[\"result_data\"])\n",
    "                elif current_status == \"failed\" and status.get(\"error\"):\n",
    "                    print(f\"Error: {status['error']}\")\n",
    "                \n",
    "                return status\n",
    "            \n",
    "            await asyncio.sleep(poll_interval)\n",
    "        \n",
    "        print(f\"Timeout for task {task_id}\")\n",
    "        return await self.get_task_status(client, resource_uri)\n",
    "    \n",
    "    def _create_progress_bar(self, progress: float, width: int = 20) -> str:\n",
    "        \"\"\"Creates a visual progress bar.\"\"\"\n",
    "        filled = int(width * progress / 100)\n",
    "        bar = \"█\" * filled + \"░\" * (width - filled)\n",
    "        return f\"[{bar}]\"\n",
    "    \n",
    "    def _print_results(self, result_data: Dict[str, Any], indent: int = 4) -> None:\n",
    "        \"\"\"Prints the results in a formatted way.\"\"\"\n",
    "        spaces = \" \" * indent\n",
    "        for key, value in result_data.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"{spaces}{key}:\")\n",
    "                self._print_results(value, indent + 2)\n",
    "            elif isinstance(value, list):\n",
    "                print(f\"{spaces}{key}: {len(value)} items\")\n",
    "            else:\n",
    "                print(f\"{spaces}{key}: {value}\")\n",
    "    \n",
    "    async def list_all_tasks(self, client: Client) -> Dict[str, Any]:\n",
    "        \"\"\"Lists all tasks on the server.\"\"\"\n",
    "        try:\n",
    "            result = await client.read_resource(\"task://list\")\n",
    "            \n",
    "            # result should be a list of ReadResourceContents  \n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                content_block = result[0]\n",
    "                # The content is in the 'text' attribute\n",
    "                if hasattr(content_block, 'text') and content_block.text:\n",
    "                    return json.loads(content_block.text)\n",
    "                else:\n",
    "                    return {\"error\": f\"No text found in the resource. Type: {type(content_block)}\"}\n",
    "            else:\n",
    "                return {\"error\": \"Empty resource\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error reading task list: {str(e)}\"}\n",
    "    \n",
    "    async def list_tasks_by_status(self, client: Client, status: str) -> Dict[str, Any]:\n",
    "        \"\"\"Lists tasks filtered by status.\"\"\"\n",
    "        try:\n",
    "            result = await client.read_resource(f\"task://list/{status}\")\n",
    "            \n",
    "            # result should be a list of ReadResourceContents  \n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                content_block = result[0]\n",
    "                # The content is in the 'text' attribute\n",
    "                if hasattr(content_block, 'text') and content_block.text:\n",
    "                    return json.loads(content_block.text)\n",
    "                else:\n",
    "                    return {\"error\": f\"No text found in the resource. Type: {type(content_block)}\"}\n",
    "            else:\n",
    "                return {\"error\": \"Empty resource\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error reading task list: {str(e)}\"}\n",
    "    \n",
    "    async def print_task_list(self, client: Client) -> None:\n",
    "        \"\"\"Prints a formatted list of all tasks.\"\"\"\n",
    "        print(\"Task list:\")\n",
    "        \n",
    "        task_list = await self.list_all_tasks(client)\n",
    "        \n",
    "        if \"error\" in task_list:\n",
    "            print(f\"   {task_list['error']}\")\n",
    "            return\n",
    "        \n",
    "        tasks = task_list.get(\"tasks\", [])\n",
    "        if not tasks:\n",
    "            print(\"   No tasks\")\n",
    "            return\n",
    "        \n",
    "        for task in tasks:\n",
    "            task_id = task[\"task_id\"]\n",
    "            status = task[\"status\"].upper()\n",
    "            progress = task.get(\"progress\", 0)\n",
    "            message = task.get(\"message\", \"\")\n",
    "            \n",
    "            print(f\"   ID: {task_id} | {status:9} | {progress:5.1f}% | {message}\")\n",
    "    \n",
    "    async def select_task(self, client: Client) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Shows a numbered list of tasks and allows the user to select one.\n",
    "        \n",
    "        Args:\n",
    "            client: MCP connected client\n",
    "            \n",
    "        Returns:\n",
    "            The ID of the selected task, or None if no task was selected\n",
    "        \"\"\"\n",
    "        print(\"📋 Available tasks:\")\n",
    "        \n",
    "        task_list = await self.list_all_tasks(client)\n",
    "        \n",
    "        if \"error\" in task_list:\n",
    "            print(f\"   ❌ {task_list['error']}\")\n",
    "            return None\n",
    "        \n",
    "        tasks = task_list.get(\"tasks\", [])\n",
    "        if not tasks:\n",
    "            print(\"   No tasks available\")\n",
    "            return None\n",
    "        \n",
    "        # Show tasks with numbers\n",
    "        for i, task in enumerate(tasks, 1):\n",
    "            task_id = task[\"task_id\"]\n",
    "            status = task[\"status\"].upper()\n",
    "            progress = task.get(\"progress\", 0)\n",
    "            message = task.get(\"message\", \"\")\n",
    "            \n",
    "            print(f\"   {i}. ID: {task_id} | {status:9} | {progress:5.1f}% | {message}\")\n",
    "        \n",
    "        # Ask user to select\n",
    "        try:\n",
    "            choice = input(f\"\\nSelect a task (1-{len(tasks)}) or press Enter to cancel: \").strip()\n",
    "            \n",
    "            if not choice:\n",
    "                print(\"Selection cancelled\")\n",
    "                return None\n",
    "                \n",
    "            choice_num = int(choice)\n",
    "            \n",
    "            if 1 <= choice_num <= len(tasks):\n",
    "                selected_task = tasks[choice_num - 1]\n",
    "                selected_id = selected_task[\"task_id\"]\n",
    "                print(f\"✅ Selected task: {selected_id}\")\n",
    "                return selected_id\n",
    "            else:\n",
    "                print(f\"❌ Invalid selection. Please choose between 1 and {len(tasks)}\")\n",
    "                return None\n",
    "                \n",
    "        except ValueError:\n",
    "            print(\"❌ Invalid input. Please enter a number\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error selecting task: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    async def cancel_task(self, client: Client, task_id: str) -> str:\n",
    "        \"\"\"Cancels a specific task.\"\"\"\n",
    "        try:\n",
    "            result = await client.call_tool(\"cancel_task\", {\"task_id\": task_id})\n",
    "            response = result.content[0].text\n",
    "            print(f\"🚫 {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error canceling task: {str(e)}\"\n",
    "            print(f\"❌ {error_msg}\")\n",
    "            return error_msg\n",
    "    \n",
    "    async def get_server_stats(self, client: Client) -> Dict[str, Any]:\n",
    "        \"\"\"Gets server statistics.\"\"\"\n",
    "        try:\n",
    "            result = await client.call_tool(\"get_server_stats\", {})\n",
    "            \n",
    "            # result.content is a list of ContentBlock\n",
    "            if hasattr(result, 'content') and len(result.content) > 0:\n",
    "                content = result.content[0]\n",
    "                if hasattr(content, 'text'):\n",
    "                    return json.loads(content.text)\n",
    "                else:\n",
    "                    # If not text, it may be the object directly\n",
    "                    return content if isinstance(content, dict) else {\"error\": f\"Unexpected format: {type(content)}\"}\n",
    "            else:\n",
    "                return {\"error\": \"Empty response from get_server_stats\"}\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Error getting server statistics: {str(e)}\"}\n",
    "    \n",
    "    async def print_server_stats(self, client: Client) -> None:\n",
    "        \"\"\"Prints server statistics.\"\"\"\n",
    "        print(\"📊 Server statistics:\")\n",
    "        \n",
    "        stats = await self.get_server_stats(client)\n",
    "        \n",
    "        if \"error\" in stats:\n",
    "            print(f\"   ❌ {stats['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"   Total tasks: {stats.get('total_tasks', 0)}\")\n",
    "        print(f\"   Running: {stats.get('running_tasks', 0)}\")\n",
    "        print(f\"   Completed: {stats.get('completed_tasks', 0)}\")\n",
    "        print(f\"   Failed: {stats.get('failed_tasks', 0)}\")\n",
    "        print(f\"   Cancelled: {stats.get('cancelled_tasks', 0)}\")\n",
    "        print(f\"   Pending: {stats.get('pending_tasks', 0)}\")\n",
    "        print(f\"   Active background tasks: {stats.get('active_background_tasks', 0)}\")\n",
    "\n",
    "\n",
    "async def interactive_demo(server_path: Optional[str] = None):\n",
    "    \"\"\"Interactive demo to explore the system.\"\"\"\n",
    "    print(\"🚀 Interactive Demo of the MCP Durability System\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    client_manager = DurabilityClient([])\n",
    "    \n",
    "    async with await client_manager.connect() as client:\n",
    "        print(\"🔗 Connected to the durability server\")\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\n\" + \"=\" * 40)\n",
    "            print(\"Available options:\")\n",
    "            print(\"1. Start data migration\")\n",
    "            print(\"2. Start batch processing\")\n",
    "            print(\"3. Start ML training\")\n",
    "            print(\"4. View server statistics\")\n",
    "            print(\"5. View task list\")\n",
    "            print(\"6. Monitor specific task\")\n",
    "            print(\"7. Cancel task\")\n",
    "            print(\"8. Exit\")\n",
    "            \n",
    "            try:\n",
    "                choice = input(\"\\nSelect an option (1-8): \").strip()\n",
    "                \n",
    "                if choice == \"1\":\n",
    "                    records = int(input(\"Number of records to migrate (default 500): \") or \"500\")\n",
    "                    uri = await client_manager.start_data_migration(client, record_count=records)\n",
    "                    print(f\"Task URI: {uri}\")\n",
    "                    \n",
    "                elif choice == \"2\":\n",
    "                    total = int(input(\"Total elements (default 300): \") or \"300\")\n",
    "                    batch_size = int(input(\"Batch size (default 30): \") or \"30\")\n",
    "                    uri = await client_manager.start_batch_processing(client, total_items=total, batch_size=batch_size)\n",
    "                    print(f\"Task URI: {uri}\")\n",
    "                    \n",
    "                elif choice == \"3\":\n",
    "                    model_name = input(\"Model name (default 'demo-model'): \") or \"demo-model\"\n",
    "                    epochs = int(input(\"Number of epochs (default 25): \") or \"25\")\n",
    "                    uri = await client_manager.start_ml_training(client, model_name=model_name, epochs=epochs)\n",
    "                    print(f\"Task URI: {uri}\")\n",
    "                    \n",
    "                elif choice == \"4\":\n",
    "                    await client_manager.print_server_stats(client)\n",
    "                    \n",
    "                elif choice == \"5\":\n",
    "                    await client_manager.print_task_list(client)\n",
    "                    \n",
    "                elif choice == \"6\":\n",
    "                    task_id = await client_manager.select_task(client)\n",
    "                    if task_id:\n",
    "                        uri = f\"task://status/{task_id}\"\n",
    "                        await client_manager.poll_task_until_complete(client, uri)\n",
    "                    \n",
    "                elif choice == \"7\":\n",
    "                    task_id = await client_manager.select_task(client)\n",
    "                    if task_id:\n",
    "                        await client_manager.cancel_task(client, task_id)\n",
    "                    \n",
    "                elif choice == \"8\":\n",
    "                    print(\"👋 Bye!\")\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    print(\"❌ Invalid option. Please select 1-8.\")\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n👋 Demo interrupted. Bye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(interactive_demo())\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n👋 Bye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09714f",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c6861",
   "metadata": {},
   "source": [
    "Primero levantamos el servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba2baa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durability MCP server started\n",
      "Available tools:\n",
      "  - start_data_migration: Starts data migration\n",
      "  - start_batch_processing: Starts batch processing\n",
      "  - start_ml_training: Simulates ML training\n",
      "  - cancel_task: Cancels a task\n",
      "  - get_server_stats: Gets server statistics\n",
      "\n",
      "Resources available:\n",
      "  - task://status/{task_id}: Specific task status\n",
      "  - task://list: Lists all tasks\n",
      "  - task://list/{status}: Lists tasks by status\n",
      "\n",
      "\n",
      "\u001b[2m╭─\u001b[0m\u001b[2m FastMCP 2.0 \u001b[0m\u001b[2m─────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;32m    _ __ ___ ______           __  __  _____________    ____    ____ \u001b[0m    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;32m   _ __ ___ / ____/___ ______/ /_/  |/  / ____/ __ \\  |___ \\  / __ \\\u001b[0m    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;32m  _ __ ___ / /_  / __ `/ ___/ __/ /|_/ / /   / /_/ /  ___/ / / / / /\u001b[0m    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;32m _ __ ___ / __/ / /_/ (__  ) /_/ /  / / /___/ ____/  /  __/_/ /_/ / \u001b[0m    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1;32m_ __ ___ /_/    \\__,_/____/\\__/_/  /_/\\____/_/      /_____(_)____/  \u001b[0m    \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1m🖥️ \u001b[0m\u001b[1m \u001b[0m\u001b[1;36mServer name:    \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mMCP Durability Server    \u001b[0m                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1m📦\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mTransport:      \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mStreamable-HTTP          \u001b[0m                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1m🔗\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mServer URL:     \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mhttp://127.0.0.1:8080/mcp\u001b[0m                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1m  \u001b[0m\u001b[1m \u001b[0m\u001b[1;36m                \u001b[0m\u001b[1;36m \u001b[0m\u001b[37m                         \u001b[0m                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1m📚\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mDocs:           \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mhttps://gofastmcp.com    \u001b[0m                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1m🚀\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mDeploy:         \u001b[0m\u001b[1;36m \u001b[0m\u001b[37mhttps://fastmcp.cloud    \u001b[0m                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1m  \u001b[0m\u001b[1m \u001b[0m\u001b[1;36m                \u001b[0m\u001b[1;36m \u001b[0m\u001b[37m                         \u001b[0m                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1m🏎️ \u001b[0m\u001b[1m \u001b[0m\u001b[1;36mFastMCP version:\u001b[0m\u001b[1;36m \u001b[0m\u001b[2;37m2.11.3                   \u001b[0m                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m    \u001b[1m🤝\u001b[0m\u001b[1m \u001b[0m\u001b[1;36mMCP version:    \u001b[0m\u001b[1;36m \u001b[0m\u001b[2;37m1.13.1                   \u001b[0m                           \u001b[2m│\u001b[0m\n",
      "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
      "\u001b[2m╰────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[2;36m[08/28/25 11:46:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting MCP server \u001b[32m'MCP Durability \u001b[0m \u001b]8;id=396160;file:///Users/macm1/Documents/web/portafolio/posts/MCP_durability_server/.venv/lib/python3.12/site-packages/fastmcp/server/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=646262;file:///Users/macm1/Documents/web/portafolio/posts/MCP_durability_server/.venv/lib/python3.12/site-packages/fastmcp/server/server.py#1522\u001b\\\u001b[2m1522\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[32mServer'\u001b[0m with transport \u001b[32m'http'\u001b[0m on     \u001b[2m              \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttp://127.0.0.1:8080/mcp\u001b[0m            \u001b[2m              \u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m55234\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8080\u001b[0m (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_server && source .venv/bin/activate && uv run server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476354ac",
   "metadata": {},
   "source": [
    "Ahora ejecutamos el cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec2dc6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Interactive Demo of the MCP Durability System\n",
      "=======================================================\n",
      "🔗 Connected to the durability server\n",
      "\n",
      "========================================\n",
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): \n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052a585",
   "metadata": {},
   "source": [
    "Primero seleccionamos la opción `5` para ver que no hay ninguna tarea corriendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fcf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Interactive Demo of the MCP Durability System\n",
      "=======================================================\n",
      "🔗 Connected to the durability server\n",
      "\n",
      "========================================\n",
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): 5\n",
      "Task list:\n",
      "No tasks\n",
      "\n",
      "========================================\n",
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): \n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae3b4f",
   "metadata": {},
   "source": [
    "Vemos que sale \n",
    "\n",
    "``` json\n",
    "Task list:\n",
    "No tasks\n",
    "```\n",
    "\n",
    "No hay ninguna tarea corriendo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d9f9eb",
   "metadata": {},
   "source": [
    "Ahora seleccionamos la opción `1` para empezar la tarea de migración de datos y seleccionamos `100.000` muestras para asegurar que la tarea dura lo suficiente para poder hacer todo lo que necesitamos en el post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34f6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): 1\n",
      "Number of records to migrate (default 500): 100000\n",
      "🚀 Starting data migration: 100000 records\n",
      "   Source: /data/source\n",
      "   Destination: /data/destination\n",
      "[08/28/25 12:05:22] INFO     Server log: Starting migration of     logging.py:40\n",
      "                             100000 records from /data/source to                \n",
      "                             /data/destination                                  \n",
      "                    INFO     Server log: Data migration started.   logging.py:40\n",
      "                             Track progress at:                                 \n",
      "                             task://status/821a210a-8672-4eba-b27c              \n",
      "                             -500bf63e58c1                                      \n",
      "✅ Task started. Resource URI: task://status/821a210a-8672-4eba-b27c-500bf63e58c1\n",
      "Task URI: task://status/821a210a-8672-4eba-b27c-500bf63e58c1\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884fec98",
   "metadata": {},
   "source": [
    "Seleccionamos la opción `5` para ver la lista de tareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): 5\n",
      "Task list:\n",
      "   ID: 821a210a-8672-4eba-b27c-500bf63e58c1 | RUNNING   |   1.0% | Migrated 1000/100000 records\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b3241",
   "metadata": {},
   "source": [
    "Ahora aparece la tarea que acabamos de pedir al servidor MCP, podemos ver su ID y su estado, lleva 1000 datos de 100.000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6594ada",
   "metadata": {},
   "source": [
    "Seleccionamos la opción `8` para terminar el cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25a3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): 8\n",
      "👋 Bye!\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa549c",
   "metadata": {},
   "source": [
    "Volvemos a ejecutar el cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841527a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Interactive Demo of the MCP Durability System\n",
      "=======================================================\n",
      "🔗 Connected to the durability server\n",
      "\n",
      "========================================\n",
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): \n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389ab52",
   "metadata": {},
   "source": [
    "Volvemos a seleccionar la opción `5` para ver la lista de tareas ejecutándose en el servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33b3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Interactive Demo of the MCP Durability System\n",
      "=======================================================\n",
      "🔗 Connected to the durability server\n",
      "\n",
      "========================================\n",
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): 5\n",
      "Task list:\n",
      "   ID: 821a210a-8672-4eba-b27c-500bf63e58c1 | RUNNING   |   3.4% | Migrated 3400/100000 records\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceee846",
   "metadata": {},
   "source": [
    "Vemos que la tarea se ha seguido ejecutando y que lleva más datos procecesados. Antes llevaba 1000 y ahora lleva 3400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875e4e3",
   "metadata": {},
   "source": [
    "Ahora seleccionamos la opción `6` para monitorizar la tarea que hemos pedido al servidor. Esto nos va a permitir monitorizarla durante 10 segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): 6\n",
      "📋 Available tasks:\n",
      "   1. ID: 821a210a-8672-4eba-b27c-500bf63e58c1 | RUNNING   |   4.2% | Migrated 4200/100000 records\n",
      "\n",
      "Select a task (1-1) or press Enter to cancel: 1\n",
      "✅ Selected task: 821a210a-8672-4eba-b27c-500bf63e58c1\n",
      "📊 Monitoring task 821a210a-8672-4eba-b27c-500bf63e58c1...\n",
      "   [░░░░░░░░░░░░░░░░░░░░]   4.9% | RUNNING | Migrated 4900/100000 records\n",
      "   [█░░░░░░░░░░░░░░░░░░░]   5.0% | RUNNING | Migrated 5000/100000 records\n",
      "   [█░░░░░░░░░░░░░░░░░░░]   5.1% | RUNNING | Migrated 5100/100000 records\n",
      "   [█░░░░░░░░░░░░░░░░░░░]   5.2% | RUNNING | Migrated 5200/100000 records\n",
      "   [█░░░░░░░░░░░░░░░░░░░]   5.3% | RUNNING | Migrated 5300/100000 records\n",
      "   [█░░░░░░░░░░░░░░░░░░░]   5.4% | RUNNING | Migrated 5400/100000 records\n",
      "   [█░░░░░░░░░░░░░░░░░░░]   5.5% | RUNNING | Migrated 5500/100000 records\n",
      "   [█░░░░░░░░░░░░░░░░░░░]   5.6% | RUNNING | Migrated 5600/100000 records\n",
      "   [█░░░░░░░░░░░░░░░░░░░]   5.7% | RUNNING | Migrated 5700/100000 records\n",
      "   [█░░░░░░░░░░░░░░░░░░░]   5.8% | RUNNING | Migrated 5800/100000 records\n",
      "Timeout for task 821a210a-8672-4eba-b27c-500bf63e58c1\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a57aff",
   "metadata": {},
   "source": [
    "Por último, cancelamos la tarea mediante la opción `7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bfd491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): 7\n",
      "📋 Available tasks:\n",
      "   1. ID: 821a210a-8672-4eba-b27c-500bf63e58c1 | RUNNING   |   6.1% | Migrated 6100/100000 records\n",
      "\n",
      "Select a task (1-1) or press Enter to cancel: 1\n",
      "✅ Selected task: 821a210a-8672-4eba-b27c-500bf63e58c1\n",
      "[08/28/25 12:06:25] INFO     Server log: Task                      logging.py:40\n",
      "                             821a210a-8672-4eba-b27c-500bf63e58c1               \n",
      "                             cancelled successfully                             \n",
      "🚫 Task 821a210a-8672-4eba-b27c-500bf63e58c1 cancelled. Status available at: task://status/821a210a-8672-4eba-b27c-500bf63e58c1\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add6c3f",
   "metadata": {},
   "source": [
    "Ahora volvemos a elegir la opción `5` para ver la lista de tareas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f49c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available options:\n",
      "1. Start data migration\n",
      "2. Start batch processing\n",
      "3. Start ML training\n",
      "4. View server statistics\n",
      "5. View task list\n",
      "6. Monitor specific task\n",
      "7. Cancel task\n",
      "8. Exit\n",
      "\n",
      "Select an option (1-8): 5\n",
      "Task list:\n",
      "   ID: 821a210a-8672-4eba-b27c-500bf63e58c1 | CANCELLED |   6.3% | Migrated 6300/100000 records\n"
     ]
    }
   ],
   "source": [
    "!cd MCP_durability_client && source .venv/bin/activate && uv run client.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f4664",
   "metadata": {},
   "source": [
    "Vemos que la tarea que le pedimos al servidor aparece como cancelada (`CANCELLED`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
