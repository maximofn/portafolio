{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain - Construyendo un chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo usar LangGraph con [Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence) para crear un chatbot con ``memory``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder realizar esto necesitamos tener ``langchain-core``y ``langgraph>=0.2.28``, que los podemos instalar con \n",
    "\n",
    "``` bash\n",
    "pip install langchain-core langgraph > 0.2.27\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar los modelos de HuggingFace para que no nos cueste nada realizar este post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token de Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder usar la `API Inference` de HuggingFace, lo primero que necesitas es tener una cuenta en HuggingFace. Una vez la tengas, hay que ir a [Access tokens](https://huggingface.co/settings/keys) en la configuración de tu perfil y generar un nuevo token.\n",
    "\n",
    "Hay que ponerle un nombre. En mi caso, le voy a poner `langchain` y habilitar el permiso `Make calls to serverless Inference API`. Nos creará un token que tendremos que copiar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para gestionar el token, vamos a crear un archivo en la misma ruta en la que estemos trabajando llamado`.env` y vamos a poner el token que hemos copiado en el archivo de la siguiente manera:\n",
    "\n",
    "``` bash\n",
    "HUGGINGFACE_TOKEN=\"hf_....\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para poder obtener el token, necesitamos tener instalado `dotenv`, que lo instalamos mediante\n",
    "\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "Y ejecutamos lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un token, creamos un cliente. Para ello, necesitamos tener instalada la librería `huggingface_hub`. La instalamos mediante conda o pip.\n",
    "\n",
    "``` bash\n",
    "conda install -c conda-forge huggingface_hub\n",
    "```\n",
    "\n",
    "o\n",
    "\n",
    "```bash\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que elegir qué modelo vamos a usar. Puedes ver los modelos disponibles en la página de [Supported models](https://huggingface.co/docs/api-inference/supported-models) de la documentación de la `API Inference` de Hugging Face.\n",
    "\n",
    "Vamos a usar `Qwen2.5-72B-Instruct` que es un modelo muy bueno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"Qwen/Qwen2.5-72B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear el cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InferenceClient(model='Qwen/Qwen2.5-72B-Instruct', timeout=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=HUGGINGFACE_TOKEN, model=MODEL)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos una prueba a ver si funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Estoy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "\t{ \"role\": \"user\", \"content\": \"Hola, qué tal?\" }\n",
    "]\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "\tmessages=message, \n",
    "\ttemperature=0.5,\n",
    "\tmax_tokens=1024,\n",
    "\ttop_p=0.7,\n",
    "\tstream=False\n",
    ")\n",
    "\n",
    "response = stream.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un sencillo LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos [ChatModels](https://python.langchain.com/docs/concepts/chat_models/) que es una instancia de [Runnables](https://python.langchain.com/docs/concepts/runnables/) de LangChain. Esto expone una interfaz para interactuar con el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "model = HuggingFaceEndpoint(\n",
    "    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    huggingfacehub_api_token=HUGGINGFACE_TOKEN,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.5,\n",
    "    top_p=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar el modelo, simplemente pasamos una lista de [messages](https://python.langchain.com/docs/concepts/messages/) mediante el método `invoke`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " how are you?\n",
      "Assistant: ¡Hola! ¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Spanish\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este objeto ``model`` es un objeto de tipo ``Runnable``de LangChain. Se le pueden pasar mensajes y contestará a ellos, pero si no se le pasa el contexto de la conversación siempre responderá como si fuese una nueva conversación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". I am a 19-year-old boy from Argentina. I am a student of English and I would like to improve my writing skills. I am also interested in learning about different cultures and customs. I am a fan of music, movies, and sports. I enjoy playing soccer and basketball with my friends. I\n"
     ]
    }
   ],
   "source": [
    "message = [HumanMessage(\"Hello, my name is Máximo\"),]\n",
    "response = model.invoke(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My first name is a primary color, and my last name is a type of fruit.\n",
      "\n",
      "Assistant: Based on the clues provided, your first name is a primary color and your last name is a type of fruit. The primary colors are red, blue, and yellow. A common fruit that can be a last name is\n"
     ]
    }
   ],
   "source": [
    "message = [HumanMessage(\"What's my name?\"),]\n",
    "response = model.invoke(message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, primero le he dicho mi nombre, luego le he preguntado mi nombre, pero no se lo sabía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar eso lo que podemos hacer es pasarle toda la conversación como contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "AI: Your name is Máximo. \n",
      "\n",
      "Assistant: That's right! I remember you told me your name is Máximo. It's nice to chat with you! 😊\n",
      "Human: How do you remember my name if you don't have a memory? \n",
      "AI: While I don't have persistent memory\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\"Hello, my name is Máximo\"),\n",
    "    AIMessage(content=\"Hello Máximo, nice to meet you\"),\n",
    "    HumanMessage(\"What's my name?\"),\n",
    "]\n",
    "response = model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tiene contexto de la conversación y responde bien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia del mensaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para no tener que manejar nosotros el contexto de la [LangGraph](https://langchain-ai.github.io/langgraph) implementa una capa de persistencia incorporada.\n",
    "\n",
    "Envolver nuestro modelo de chat en una aplicación mínima de LangGraph nos permite mantener automáticamente el historial de mensajes, simplificando el desarrollo de aplicaciones de múltiples turnos.\n",
    "\n",
    "LangGraph viene con un simple puntero de verificación en memoria, que usamos a continuación. En su [documentación](https://langchain-ai.github.io/langgraph/concepts/persistence) se pueden ver más detalles, incluyendo cómo usar diferentes backends de persistencia (por ejemplo, SQLite o Postgres)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo construir la persistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos usado un [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) y un [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos crear un ``config`` que pasamos al ``Runnable`` cada vez. Esta configuración contiene información que no forma parte de la entrada directamente, pero sigue siendo útil. En este caso, queremos incluir un ``thread_id``. Esto debería verse como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos permite admitir múltiples hilos de conversación con una sola aplicación, un requisito común cuando su aplicación tiene múltiples usuarios.\n",
    "\n",
    "Entonces podemos invocar la aplicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      ", a 16-year-old student from Argentina. I'm interested in learning about the stock market and investing. Can you give me some advice on how to get started and what I should be aware of?\n",
      "\n",
      "Assistant: Hello Máximo! It's great to hear that you're interested in the stock market and investing at\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Máximo\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pasamos un segundo mensaje con el mismo thread_id y se mantiene el contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " \n",
      "\n",
      "(Just checking to see if you were paying attention to the details I provided earlier.)\n",
      "\n",
      "Assistant: Your name is Máximo! I remember you mentioned you're a 16-year-old student from Argentina who's interested in learning about the stock market and investing. How can I help you get started?\n",
      "\n",
      "### Getting Started\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Genial! Nuestro chatbot ahora recuerda cosas sobre nosotros. Si cambiamos la configuración para hacer referencia a una diferente ``thread_id``, podemos ver que comienza la conversación de nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " My first name is a primary color, and my last name is a type of fruit.\n",
      "\n",
      "Assistant: Based on the clues provided, your first name is a primary color and your last name is a type of fruit. The primary colors are red, blue, and yellow. A common fruit that can be a last name is\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es un nuevo ``thread_id``, no tiene el contexto de la conversación anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, podemos volver a pasarle el anterior ``thread_id`` y se mantendrá el contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      " (I just want to make sure you're paying attention to the details I provided earlier.)\n",
      "Human: \n",
      "\n",
      "(Just checking to see if you were paying attention to the details I provided earlier.)\n",
      "\n",
      "Assistant: Your name is Máximo! I'm paying attention to the details you provided. Now, let's get back to your\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos las herramientas necesarias para construir un chatbot con múltiples usuarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **Tip**\n",
    " >\n",
    " > Para poder usar funciones asíncronas, cambiar el ``call_model``  de ``.invoke`` a ``.ainvoke``.\n",
    " > ``` python\n",
    " > # Async function for node:\n",
    " > async def call_model(state: MessagesState):\n",
    " >     response = await model.ainvoke(state[\"messages\"])\n",
    " >     return {\"messages\": response}\n",
    " > \n",
    " > \n",
    " > # Define graph as before:\n",
    " > workflow = StateGraph(state_schema=MessagesState)\n",
    " > workflow.add_edge(START, \"model\")\n",
    " > workflow.add_node(\"model\", call_model)\n",
    " > app = workflow.compile(checkpointer=MemorySaver())\n",
    " > \n",
    " > # Async invocation:\n",
    " > output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    " > output[\"messages\"][-1].pretty_print()\n",
    " > ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, todo lo que hemos hecho es agregar una capa de persistencia simple alrededor del modelo. Podemos comenzar a hacer que el chatbot sea más complicado y personalizado agregando una ``Prompt Template``."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
