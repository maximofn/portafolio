{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es MCP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCP (Model Context Protocol) es un estandar open source desarrollado por Anthropic para permitir a los modelos de IA interactuar con herramientas externas mediante un estandar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta el desarrollo del protocolo MCP, cuando queríamos que un LLM interactuara con herramientas, teníamos que crear código para poder interactuar con la herramienta, y mediante `function calling` enviarle la información al LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MCP vs API](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/MCP_vs_APIs.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que mediante MCP, un LLM puede interactuar con herramientas gracias a un estandar. De esta manera si una persona crea un servidor MCP, dicho servidor puede ser reutilizado por otros con un único cliente. Si en tu aplicación desarrollas un cliente, puedes descargarte un servidor MCP desarrollado por otro, y usarlo sin problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comunmente MCP se asemeja al estandar USB. Antes del USB, cada periférico tenía un tipo de conexión diferente, unos tenían puertos serie, otros paralelo. Diferentes formatos de conectores, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![USB MCP](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mcp-usb.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la llegada del USB, todos los periféricos se adaptaron a este estandar, por lo que con un solo conector USB en tu ordenador, puedes conectar casi cualquier periférico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCP tiene 7 componentes principales:\n",
    " * **Host**: Aplicación LLM que tiene acceso a herramientas MCP.\n",
    " * **Servidor MCP**: Servidor que realiza la comunicación con la API o herramienta a la que queremos exponer al LLM\n",
    " * **Cliente MCP**: Cliente que se conecta al servidor MCP y realiza las peticiones\n",
    " * **Tool**: Función que se ejecuta en el servidor MCP y que puede ser invocada por el LLM\n",
    " * **Resource**: Recurso que se puede usar en el servidor MCP. Suelen dar al LLM acceso a recursos estáticos como archivos, bases de datos, etc.\n",
    " * **Resource template**: Template para crear recursos dinámicos. Mediante estas plantillas, el LLM puede crear dinámicamente el recurso al que quiere acceder\n",
    " * **Prompt**: Prompt que se usa para generar un prompt que será usado por el LLM para interactuar con el servidor MCP.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un único host (aplicación) puede tener varios clientes. Cada cliente se conectará a un servidor MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mcp architecture](https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/mcp-system-architecture.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastMCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque en la documentación de MCP recomiendan instalar `mcp[\"cli\"]`, hay una librería creada por encima llamada `fastmcp`, que ayuda mucho a la hora de crear servidores MCP, así que vamos a usarla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear entorno virtual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear un servidor y un cliente MCP, vamos a crear entornos virtuales con `uv` con las dependencias que vamos a necesitar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Servidor MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero creamos una carpeta para el servidor de MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir gitHub_MCP_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos el entorno `uv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized project `github-mcp-server` at `/Users/macm1/Documents/web/portafolio/posts/gitHub_MCP_server`\n"
     ]
    }
   ],
   "source": [
    "!cd gitHub_MCP_server && uv init ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo activamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython 3.11.11\n",
      "Creating virtual environment at: .venv\n",
      "Activate with: source .venv/bin/activate\n"
     ]
    }
   ],
   "source": [
    "!cd gitHub_MCP_server && uv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E instalamos las linrerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved 42 packages in 34ms\n",
      "Installed 40 packages in 71ms\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mauthlib\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.6.15\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcffi\u001b[0m\u001b[2m==1.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcryptography\u001b[0m\u001b[2m==45.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastmcp\u001b[0m\u001b[2m==2.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmcp\u001b[0m\u001b[2m==1.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-pydantic\u001b[0m\u001b[2m==0.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycparser\u001b[0m\u001b[2m==2.22\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msse-starlette\u001b[0m\u001b[2m==2.3.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.47.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd gitHub_MCP_server && uv add anthropic fastmcp python-dotenv requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliente MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos una carpeta donde programaremos el cliente MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir client_MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos el entorno uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized project `client-mcp` at `/Users/macm1/Documents/web/portafolio/posts/client_MCP`\n"
     ]
    }
   ],
   "source": [
    "!cd client_MCP && uv init ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo activamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython 3.11.11\n",
      "Creating virtual environment at: .venv\n",
      "Activate with: source .venv/bin/activate\n"
     ]
    }
   ],
   "source": [
    "!cd client_MCP && uv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último, instalamos las librerías necesarias para el cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved 42 packages in 307ms\n",
      "Prepared 5 packages in 115ms\n",
      "Installed 40 packages in 117ms\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manthropic\u001b[0m\u001b[2m==0.55.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mauthlib\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.6.15\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcffi\u001b[0m\u001b[2m==1.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcryptography\u001b[0m\u001b[2m==45.0.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastmcp\u001b[0m\u001b[2m==2.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown-it-py\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmcp\u001b[0m\u001b[2m==1.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmdurl\u001b[0m\u001b[2m==0.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenapi-pydantic\u001b[0m\u001b[2m==0.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycparser\u001b[0m\u001b[2m==2.22\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich\u001b[0m\u001b[2m==14.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msse-starlette\u001b[0m\u001b[2m==2.3.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.47.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd client_MCP && uv add anthropic fastmcp python-dotenv requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar Sonnet 3.5 como modelo LLM, así que creamos un archivo `.env` en la carpeta del cliente con la API KEY de Claude que se puede obtener en la página [keys](https://console.anthropic.com/settings/keys) de la API de Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing client_MCP/.env\n"
     ]
    }
   ],
   "source": [
    "%%writefile client_MCP/.env\n",
    "\n",
    "ANTHROPIC_API_KEY=\"ANTHROPIC_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP básico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribímos el mínimo código que necesitamos para tener un servidor MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gitHub_MCP_server/github_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gitHub_MCP_server/github_server.py\n",
    "\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Create an MCP server\n",
    "mcp = FastMCP(\"GitHubMCP\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run the server\n",
    "    mcp.run(transport='stdio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, tenemos que crear un objeto `FastMCP` y luego ejecutar el servidor con `mcp.run`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librería con funciones para leer de GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos a crear un servidor MCP para poder usar utilidades de GitHub, vamos a crear un archivo con las funciones necesarias para construir los headers necesarios para poder usar la API de GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gitHub_MCP_server/github.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gitHub_MCP_server/github.py\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the GitHub token from the .env file\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.environ.get(\"GITHUB_TOKEN\")\n",
    "\n",
    "# Check if the GitHub token is configured\n",
    "if not GITHUB_TOKEN:\n",
    "    print(\"WARNING: The GITHUB_TOKEN environment variable is not configured.\")\n",
    "    print(\"Requests to the GitHub API may fail due to rate limits.\")\n",
    "    print(\"Create a .env file in this directory with GITHUB_TOKEN='your_token_here'\")\n",
    "    raise ValueError(\"GITHUB_TOKEN is not configured\")\n",
    "\n",
    "# Helper function to create headers for GitHub API requests\n",
    "def create_github_headers():\n",
    "    headers = {}\n",
    "    if GITHUB_TOKEN:\n",
    "        headers[\"Authorization\"] = f\"Bearer {GITHUB_TOKEN}\"\n",
    "    # GitHub recommends including a User-Agent\n",
    "    headers[\"User-Agent\"] = \"MCP_GitHub_Server_Example\"\n",
    "    headers[\"Accept\"] = \"application/vnd.github.v3+json\" # Good practice\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder construir los headers, necesitamos una token de GitHub. Para ello, vamos a [personal-access-tokens](https://github.com/settings/personal-access-tokens) y creamos una nueva token. Lo copiamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, creamos un `.env`, dónde vamos a almacenar el token de GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gitHub_MCP_server/.env\n"
     ]
    }
   ],
   "source": [
    "%%writefile gitHub_MCP_server/.env\n",
    "\n",
    "GITHUB_TOKEN = \"GITHUB_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear `tool` de MCP para obtener una lista de issues de un repositorio de GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Servidor MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos una función para poder listar los issues de un repositorio de GitHub. Para convertir dicha función en una `tool` de MCP, usamos el decorador `@mcp.tool()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gitHub_MCP_server/github_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gitHub_MCP_server/github_server.py\n",
    "\n",
    "import httpx\n",
    "from fastmcp import FastMCP\n",
    "from github import GITHUB_TOKEN, create_github_headers\n",
    "\n",
    "# Create a FastMCP server\n",
    "mcp = FastMCP(\"GitHubMCP\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def list_repository_issues(owner: str, repo_name: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Lists open issues for a given GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        owner: The owner of the repository (e.g., 'modelcontextprotocol')\n",
    "        repo_name: The name of the repository (e.g., 'python-sdk')\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing information about an issue\n",
    "    \"\"\"\n",
    "    # Limit to the first 10 issues to avoid long responses\n",
    "    api_url = f\"https://api.github.com/repos/{owner}/{repo_name}/issues?state=open&per_page=10\"\n",
    "    print(f\"Fetching issues from {api_url}...\")\n",
    "    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.get(api_url, headers=create_github_headers())\n",
    "            response.raise_for_status()\n",
    "            issues_data = response.json()\n",
    "            \n",
    "            if not issues_data:\n",
    "                print(\"No open issues found for this repository.\")\n",
    "                return [{\"message\": \"No open issues found for this repository.\"}]\n",
    "\n",
    "            issues_summary = []\n",
    "            for issue in issues_data:\n",
    "                # Create a more concise summary\n",
    "                summary = f\"#{issue.get('number', 'N/A')}: {issue.get('title', 'Sin título')}\"\n",
    "                if issue.get('comments', 0) > 0:\n",
    "                    summary += f\" ({issue.get('comments')} comentarios)\"\n",
    "                \n",
    "                issues_summary.append({\n",
    "                    \"number\": issue.get(\"number\"),\n",
    "                    \"title\": issue.get(\"title\"),\n",
    "                    \"user\": issue.get(\"user\", {}).get(\"login\"),\n",
    "                    \"url\": issue.get(\"html_url\"),\n",
    "                    \"comments\": issue.get(\"comments\"),\n",
    "                    \"summary\": summary\n",
    "                })\n",
    "            \n",
    "            print(f\"Found {len(issues_summary)} open issues.\")\n",
    "            \n",
    "            # Add context information\n",
    "            result = {\n",
    "                \"total_found\": len(issues_summary),\n",
    "                \"repository\": f\"{owner}/{repo_name}\",\n",
    "                \"note\": \"Mostrando los primeros 10 issues abiertos\" if len(issues_summary) == 10 else f\"Mostrando todos los {len(issues_summary)} issues abiertos\",\n",
    "                \"issues\": issues_summary\n",
    "            }\n",
    "            \n",
    "            return [result]\n",
    "            \n",
    "        except httpx.HTTPStatusError as e:\n",
    "            error_message = e.response.json().get(\"message\", \"No additional message from API.\")\n",
    "            if e.response.status_code == 403 and GITHUB_TOKEN:\n",
    "                error_message += \" (Rate limit with token or token lacks permissions?)\"\n",
    "            elif e.response.status_code == 403 and not GITHUB_TOKEN:\n",
    "                error_message += \" (Rate limit without token. Consider creating a .env file with GITHUB_TOKEN.)\"\n",
    "            \n",
    "            print(f\"GitHub API error: {e.response.status_code}. {error_message}\")\n",
    "            return [{\n",
    "                \"error\": f\"GitHub API error: {e.response.status_code}\",\n",
    "                \"message\": error_message\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {str(e)}\")\n",
    "            return [{\"error\": f\"An unexpected error occurred: {str(e)}\"}]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"DEBUG: Starting GitHub FastMCP server...\")\n",
    "    print(f\"DEBUG: Server name: {mcp.name}\")\n",
    "    print(\"DEBUG: Available tools: list_repository_issues\")\n",
    "    \n",
    "    # Initialize and run the server\n",
    "    mcp.run() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliente MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un cliente MCP para poder usar la `tool` que hemos creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting client_MCP/client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile client_MCP/client.py\n",
    "\n",
    "import sys\n",
    "import asyncio\n",
    "from contextlib import AsyncExitStack\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "from fastmcp import Client\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class FastMCPClient:\n",
    "    \"\"\"\n",
    "    FastMCP client that integrates with Claude to process user queries\n",
    "    and use tools exposed by a FastMCP server.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the FastMCP client with Anthropic and resource management.\"\"\"\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.anthropic = Anthropic()\n",
    "        self.client = None\n",
    "        \n",
    "    async def connect_to_server(self, server_script_path: str):\n",
    "        \"\"\"\n",
    "        Connect to the specified FastMCP server.\n",
    "        \n",
    "        Args:\n",
    "            server_script_path: Path to the server script (Python)\n",
    "        \"\"\"\n",
    "        print(f\"🔗 Connecting to FastMCP server: {server_script_path}\")\n",
    "        \n",
    "        # Determine the server type based on the extension\n",
    "        if not server_script_path.endswith('.py'):\n",
    "            raise ValueError(f\"Unsupported server type. Use .py files. Received: {server_script_path}\")\n",
    "        \n",
    "        # Create FastMCP client \n",
    "        self.client = Client(server_script_path)\n",
    "        # Note: FastMCP Client automatically infers transport from .py files\n",
    "        \n",
    "        print(\"✅ Client created successfully\")\n",
    "        \n",
    "    async def list_available_tools(self):\n",
    "        \"\"\"List available tools in the FastMCP server.\"\"\"\n",
    "        try:\n",
    "            # Get list of tools from the server using FastMCP context\n",
    "            async with self.client as client:\n",
    "                tools = await client.list_tools()\n",
    "                \n",
    "                if tools:\n",
    "                    print(f\"\\n🛠️  Available tools ({len(tools)}):\")\n",
    "                    print(\"=\" * 50)\n",
    "                    \n",
    "                    for tool in tools:\n",
    "                        print(f\"📋 {tool.name}\")\n",
    "                        if tool.description:\n",
    "                            print(f\"   Description: {tool.description}\")\n",
    "                        \n",
    "                        # Show parameters if available\n",
    "                        if hasattr(tool, 'inputSchema') and tool.inputSchema:\n",
    "                            if 'properties' in tool.inputSchema:\n",
    "                                params = list(tool.inputSchema['properties'].keys())\n",
    "                                print(f\"   Parameters: {', '.join(params)}\")\n",
    "                        print()\n",
    "                else:\n",
    "                    print(\"⚠️  No tools found in the server\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error listing tools: {str(e)}\")\n",
    "\n",
    "    async def process_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Process a user query, interacting with Claude and FastMCP tools.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            \n",
    "        Returns:\n",
    "            str: Final processed response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use FastMCP context for all operations\n",
    "            async with self.client as client:\n",
    "                # Get available tools\n",
    "                tools_list = await client.list_tools()\n",
    "                \n",
    "                # Prepare tools for Claude in correct format\n",
    "                claude_tools = []\n",
    "                for tool in tools_list:\n",
    "                    claude_tool = {\n",
    "                        \"name\": tool.name,\n",
    "                        \"description\": tool.description or f\"Tool {tool.name}\",\n",
    "                        \"input_schema\": tool.inputSchema or {\"type\": \"object\", \"properties\": {}}\n",
    "                    }\n",
    "                    claude_tools.append(claude_tool)\n",
    "                \n",
    "                # Create initial message for Claude\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "                # First call to Claude\n",
    "                response = self.anthropic.messages.create(\n",
    "                    model=\"claude-3-5-sonnet-20241022\",\n",
    "                    max_tokens=6000,\n",
    "                    messages=messages,\n",
    "                    tools=claude_tools if claude_tools else None\n",
    "                )\n",
    "                \n",
    "                # Process Claude's response\n",
    "                response_text = \"\"\n",
    "                \n",
    "                for content_block in response.content:\n",
    "                    if content_block.type == \"text\":\n",
    "                        response_text += content_block.text\n",
    "                        \n",
    "                    elif content_block.type == \"tool_use\":\n",
    "                        # Claude wants to use a tool\n",
    "                        tool_name = content_block.name\n",
    "                        tool_args = content_block.input\n",
    "                        tool_call_id = content_block.id\n",
    "                        \n",
    "                        print(f\"🔧 Claude wants to use: {tool_name}\")\n",
    "                        print(f\"📝 Arguments: {tool_args}\")\n",
    "                        \n",
    "                        try:\n",
    "                            # Execute tool on the FastMCP server\n",
    "                            tool_result = await client.call_tool(tool_name, tool_args)\n",
    "                            \n",
    "                            print(f\"✅ Tool executed successfully\")\n",
    "                            \n",
    "                            # Add tool result to the conversation\n",
    "                            messages.append({\n",
    "                                \"role\": \"assistant\", \n",
    "                                \"content\": response.content\n",
    "                            })\n",
    "                            \n",
    "                            # Format result for Claude\n",
    "                            if tool_result:\n",
    "                                # Convert result to string format for Claude\n",
    "                                result_content = str(tool_result)\n",
    "                                \n",
    "                                messages.append({\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": [{\n",
    "                                        \"type\": \"tool_result\",\n",
    "                                        \"tool_use_id\": tool_call_id,\n",
    "                                        \"content\": f\"Tool result: {result_content}\"\n",
    "                                    }]\n",
    "                                })\n",
    "                            else:\n",
    "                                messages.append({\n",
    "                                    \"role\": \"user\", \n",
    "                                    \"content\": [{\n",
    "                                        \"type\": \"tool_result\",\n",
    "                                        \"tool_use_id\": tool_call_id, \n",
    "                                        \"content\": \"Tool executed without response content\"\n",
    "                                    }]\n",
    "                                })\n",
    "                            \n",
    "                            # Second call to Claude with the tool result\n",
    "                            final_response = self.anthropic.messages.create(\n",
    "                                model=\"claude-3-5-sonnet-20241022\",\n",
    "                                max_tokens=6000,\n",
    "                                messages=messages,\n",
    "                                tools=claude_tools if claude_tools else None\n",
    "                            )\n",
    "                            \n",
    "                            # Extract text from the final response\n",
    "                            for final_content in final_response.content:\n",
    "                                if final_content.type == \"text\":\n",
    "                                    response_text += final_content.text\n",
    "                                    \n",
    "                        except Exception as e:\n",
    "                            error_msg = f\"❌ Error executing {tool_name}: {str(e)}\"\n",
    "                            print(error_msg)\n",
    "                            response_text += f\"\\n\\n{error_msg}\"\n",
    "                \n",
    "                return response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Error processing query: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"\n",
    "        Main chat loop with user interaction.\n",
    "        \"\"\"\n",
    "        print(\"\\n🤖 FastMCP client started. Write 'quit', 'q', 'exit', 'salir' to exit.\")\n",
    "        print(\"💬 You can ask questions about GitHub repositories!\")\n",
    "        print(\"📚 The client can use tools from the FastMCP server\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Request user input\n",
    "                user_input = input(\"\\n👤 You: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'q', 'exit', 'salir']:\n",
    "                    print(\"👋 Bye!\")\n",
    "                    break\n",
    "                    \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\n🤔 Claude is thinking...\")\n",
    "                \n",
    "                # Process query\n",
    "                response = await self.process_query(user_input)\n",
    "                \n",
    "                # Show response\n",
    "                print(f\"\\n🤖 Claude: {response}\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\n👋 Disconnecting...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ Error in chat: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up resources and close connections.\"\"\"\n",
    "        print(\"🧹 Cleaning up resources...\")\n",
    "        # FastMCP Client cleanup is handled automatically by context manager\n",
    "        await self.exit_stack.aclose()\n",
    "        print(\"✅ Resources released\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function that initializes and runs the FastMCP client.\n",
    "    \"\"\"\n",
    "    # Verify command line arguments\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"❌ Usage: python client.py <path_to_fastmcp_server>\")\n",
    "        print(\"📝 Example: python client.py ../MCP_github/github_server.py\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    server_script_path = sys.argv[1]\n",
    "    \n",
    "    # Create and run client\n",
    "    client = FastMCPClient()\n",
    "    \n",
    "    try:\n",
    "        # Connect to the server\n",
    "        await client.connect_to_server(server_script_path)\n",
    "        \n",
    "        # List available tools after connection\n",
    "        await client.list_available_tools()\n",
    "        \n",
    "        # Start chat loop\n",
    "        await client.chat_loop()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fatal error: {str(e)}\")\n",
    "    finally:\n",
    "        # Ensure resources are cleaned up\n",
    "        await client.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Entry point of the script\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación del cliente MCP\n",
    "\n",
    " * En `main` se comprueba que se ha pasado un argumento con el path del servidor MCP.\n",
    " * Se crea un objeto de la clase `FastMCPClient` con el path del servidor MCP. Al crear el objeto se ejecuta el método `__init__` que crea la conexión con el LLM de Anthropic, que va a ser el LLM que va a poner el \"cerebro\"\n",
    " * Se intenta conectar con el servidor MCP llamando al método `connect_to_server` abrir una sesión con el servidor MCP.\n",
    " * Se listan las `tool`s disponibles con el método `list_available_tools`\n",
    " * Si se ha podido conectar, se llama al método `chat_loop` que es un bucle infinito para chatear con el LLM que se acaba de crear en el cliente. Solo se para la ejecución cuando se introduce `quit`, `q`, `exit` o `salir` en el chat.\n",
    " * Se procesa la entrada del usuario con el método `process_query` que obtiene la lista de `tool`s disponibles y hace una petición al LLM con el mensaje del usuario y la lista de `tool`s\n",
    "   * Si el LLM responde con texto, se devuelve el texto, que será impreso\n",
    "   * Si el LLM responde con `tool_use`, se obtiene el nombre de la `tool`, los argumentos y se crea una ID de ejecución. Se ejecuta la tool. Con el resultado de la tool, se crea un nuevo mensaje que se le manda al LLM para que lo procese y genere una respuesta, que será devuelta e impresa.\n",
    " * Cuando se termine la conversación, se llamará al método `cleanup`, que cerrará lo que sea necesario cerrar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de la `tool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos vamos a la ruta del cliente y lo ejecutamos, dándole la ruta del servidor MCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Connecting to FastMCP server: ../gitHub_MCP_server/github_server.py\n",
      "✅ Client created successfully\n",
      "[06/28/25 09:22:09] INFO     Starting MCP server 'GitHubMCP' with transport 'stdio'                          server.py:1246\n",
      "\n",
      "🛠️  Available tools (1):\n",
      "==================================================\n",
      "📋 list_repository_issues\n",
      "   Description: Lists open issues for a given GitHub repository.\n",
      "\n",
      "Args:\n",
      "    owner: The owner of the repository (e.g., 'modelcontextprotocol')\n",
      "    repo_name: The name of the repository (e.g., 'python-sdk')\n",
      "\n",
      "Returns:\n",
      "    list[dict]: A list of dictionaries, each containing information about an issue\n",
      "   Parameters: owner, repo_name\n",
      "\n",
      "🤖 FastMCP client started. Write 'quit', 'q', 'exit', 'salir' to exit.\n",
      "💬 You can ask questions about GitHub repositories!\n",
      "📚 The client can use tools from the FastMCP server\n",
      "------------------------------------------------------------\n",
      "\n",
      "👤 You: Tell me de issues of repository transformers of huggingface\n",
      "\n",
      "🤔 Claude is thinking...\n",
      "🔧 Claude wants to use: list_repository_issues\n",
      "📝 Arguments: {'owner': 'huggingface', 'repo_name': 'transformers'}\n",
      "✅ Tool executed successfully\n",
      "\n",
      "🤖 Claude: I'll help you list the issues from the Hugging Face transformers repository. Let me use the `list_repository_issues` function with the appropriate parameters.I'll summarize the current open issues from the Hugging Face transformers repository. Here are the 10 most recent open issues:\n",
      "\n",
      "1. [#39097] Core issue about saving models with multiple shared tensor groups when dispatched\n",
      "2. [#39096] Pull request to fix position index in v4.52.4\n",
      "3. [#39095] Issue with Qwen2_5_VLVisionAttention flash attention missing 'is_causal' attribute\n",
      "4. [#39094] Documentation improvement for PyTorch examples\n",
      "5. [#39093] Style change PR for lru_cache decorator\n",
      "6. [#39091] Compatibility issue with sentencepiece on Windows in Python 3.13\n",
      "7. [#39090] Pull request for fixing bugs in finetune and batch inference\n",
      "8. [#39089] Bug report for LlavaOnevisonConfig initialization in version 4.52.4\n",
      "9. [#39087] Documentation PR for Gemma 3n audio encoder\n",
      "10. [#39084] Pull request for refactoring gemma3n\n",
      "\n",
      "Note that this is showing the 10 most recent open issues, and there might be more issues in the repository. Each issue has a link where you can find more details about the specific problem or proposed changes.\n",
      "\n",
      "Would you like more specific information about any of these issues?\n",
      "\n",
      "👤 You: q\n",
      "👋 Bye!\n",
      "🧹 Cleaning up resources...\n",
      "✅ Resources released\n"
     ]
    }
   ],
   "source": [
    "!cd client_MCP && source .venv/bin/activate && python client.py ../gitHub_MCP_server/github_server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutarlo vemos\n",
    "\n",
    "```\n",
    "🛠️  Available tools (1):\n",
    "==================================================\n",
    "📋 list_repository_issues\n",
    "   Description: Lists open issues for a given GitHub repository.\n",
    "\n",
    "Args:\n",
    "    owner: The owner of the repository (e.g., 'modelcontextprotocol')\n",
    "    repo_name: The name of the repository (e.g., 'python-sdk')\n",
    "\n",
    "Returns:\n",
    "    list[dict]: A list of dictionaries, each containing information about an issue\n",
    "   Parameters: owner, repo_name\n",
    "```\n",
    "\n",
    "Lo que indica que el cliente MCP puede ver la `tool` que hemos creado en el servidor MCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depués podemos ver\n",
    "\n",
    "```\n",
    "👤 You: Tell me de issues of repository transformers of huggingface\n",
    "\n",
    "🤔 Claude is thinking...\n",
    "🔧 Calling tool: list_repository_issues\n",
    "📝 Arguments: {'owner': 'huggingface', 'repo_name': 'transformers'}\n",
    "✅ Tool executed successfully\n",
    "```\n",
    "\n",
    "Le pedimos los issues del repositorio `transformers` de `huggingface`. Tras pensar un rato nos dice que va a usar la `tool` `list_repository_issues` con los argumentos `{'owner': 'huggingface', 'repo_name': 'transformers'}`.\n",
    "\n",
    "Por último, nos dice que la `tool` se ha ejecutado correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, con el resultado de ejecutar la `tool`, Claude lo procesa y nos crea una respuesta con la lista de issues.\n",
    "\n",
    "```\n",
    "🤖 Claude: I'll help you list the issues from the Hugging Face transformers repository. Let me use the `list_repository_issues` function with the appropriate parameters.I'll summarize the current open issues from the Hugging Face transformers repository. Here are the 10 most recent open issues:\n",
    "\n",
    "1. [#39097] Core issue about saving models with multiple shared tensor groups when dispatched\n",
    "2. [#39096] Pull request to fix position index in v4.52.4\n",
    "3. [#39095] Issue with Qwen2_5_VLVisionAttention flash attention missing 'is_causal' attribute\n",
    "4. [#39094] Documentation improvement for PyTorch examples\n",
    "5. [#39093] Style change PR for lru_cache decorator\n",
    "6. [#39091] Compatibility issue with sentencepiece on Windows in Python 3.13\n",
    "7. [#39090] Pull request for fixing bugs in finetune and batch inference\n",
    "8. [#39089] Bug report for LlavaOnevisonConfig initialization in version 4.52.4\n",
    "9. [#39087] Documentation PR for Gemma 3n audio encoder\n",
    "10. [#39084] Pull request for refactoring gemma3n\n",
    "\n",
    "Note that this is showing the 10 most recent open issues, and there might be more issues in the repository. Each issue has a link where you can find more details about the specific problem or proposed changes.\n",
    "\n",
    "Would you like more specific information about any of these issues?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Añadir contexto a la `tool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Servidor MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos una nueva función al servidor MCP que nos permite obtener la información de un repositorio de GitHub. Gracias al decorador `@mcp.resource(\"github://repo/{owner}/{repo_name}\")` convertimos la función a un `resource` MCP. Este `resource` es un endpoint al que se le puede pasar el repositorio y el dueño del repositorio, haciendo que nos devuelva la información del repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gitHub_MCP_server/github_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gitHub_MCP_server/github_server.py\n",
    "\n",
    "import httpx\n",
    "from fastmcp import FastMCP\n",
    "from github import GITHUB_TOKEN, create_github_headers\n",
    "\n",
    "# Create a FastMCP server\n",
    "mcp = FastMCP(\"GitHubMCP\")\n",
    "\n",
    "@mcp.resource(\"github://repo/{owner}/{repo_name}\")\n",
    "async def get_repository_info(owner: str, repo_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Gets detailed information for a given GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        owner: The owner of the repository (e.g., 'modelcontextprotocol')\n",
    "        repo_name: The name of the repository (e.g., 'python-sdk')\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing repository details.\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.github.com/repos/{owner}/{repo_name}\"\n",
    "    print(f\"Fetching repository info from {api_url}...\")\n",
    "    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.get(api_url, headers=create_github_headers())\n",
    "            response.raise_for_status()\n",
    "            repo_data = response.json()\n",
    "            \n",
    "            # Return the data as a dictionary with repository information\n",
    "            return {\n",
    "                \"full_name\": repo_data.get(\"full_name\"),\n",
    "                \"description\": repo_data.get(\"description\"),\n",
    "                \"stars\": repo_data.get(\"stargazers_count\"),\n",
    "                \"forks\": repo_data.get(\"forks_count\"),\n",
    "                \"open_issues\": repo_data.get(\"open_issues_count\"),\n",
    "                \"language\": repo_data.get(\"language\"),\n",
    "                \"url\": repo_data.get(\"html_url\"),\n",
    "                \"created_at\": repo_data.get(\"created_at\"),\n",
    "                \"updated_at\": repo_data.get(\"updated_at\"),\n",
    "                \"size\": repo_data.get(\"size\"),\n",
    "                \"watchers\": repo_data.get(\"watchers_count\"),\n",
    "                \"default_branch\": repo_data.get(\"default_branch\"),\n",
    "                \"license\": repo_data.get(\"license\", {}).get(\"name\") if repo_data.get(\"license\") else None,\n",
    "                \"owner\": repo_data.get(\"owner\", {}).get(\"login\"),\n",
    "                \"owner_type\": repo_data.get(\"owner\", {}).get(\"type\"),\n",
    "                \"homepage\": repo_data.get(\"homepage\"),\n",
    "                \"topics\": repo_data.get(\"topics\", [])\n",
    "            }\n",
    "\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            error_message = e.response.json().get(\"message\", \"No additional message from API.\")\n",
    "            if e.response.status_code == 403 and GITHUB_TOKEN:\n",
    "                error_message += \" (Rate limit with token or token lacks permissions?)\"\n",
    "            elif e.response.status_code == 403 and not GITHUB_TOKEN:\n",
    "                error_message += \" (Rate limit without token. Consider creating a .env file with GITHUB_TOKEN.)\"\n",
    "            \n",
    "            print(f\"GitHub API error: {e.response.status_code}. {error_message}\")\n",
    "            return {\n",
    "                \"error\": f\"GitHub API error: {e.response.status_code}\",\n",
    "                \"message\": error_message\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {str(e)}\")\n",
    "            return {\"error\": f\"An unexpected error occurred: {str(e)}\"}\n",
    "\n",
    "@mcp.tool()\n",
    "async def list_repository_issues(owner: str, repo_name: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Lists open issues for a given GitHub repository.\n",
    "\n",
    "    Args:\n",
    "        owner: The owner of the repository (e.g., 'modelcontextprotocol')\n",
    "        repo_name: The name of the repository (e.g., 'python-sdk')\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing information about an issue\n",
    "    \"\"\"\n",
    "    # Limitar a los primeros 10 issues para evitar respuestas muy largas\n",
    "    api_url = f\"https://api.github.com/repos/{owner}/{repo_name}/issues?state=open&per_page=10\"\n",
    "    print(f\"Fetching issues from {api_url}...\")\n",
    "    \n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.get(api_url, headers=create_github_headers())\n",
    "            response.raise_for_status()\n",
    "            issues_data = response.json()\n",
    "            \n",
    "            if not issues_data:\n",
    "                print(\"No open issues found for this repository.\")\n",
    "                return [{\"message\": \"No open issues found for this repository.\"}]\n",
    "\n",
    "            issues_summary = []\n",
    "            for issue in issues_data:\n",
    "                # Create a more concise summary\n",
    "                summary = f\"#{issue.get('number', 'N/A')}: {issue.get('title', 'Sin título')}\"\n",
    "                if issue.get('comments', 0) > 0:\n",
    "                    summary += f\" ({issue.get('comments')} comentarios)\"\n",
    "                \n",
    "                issues_summary.append({\n",
    "                    \"number\": issue.get(\"number\"),\n",
    "                    \"title\": issue.get(\"title\"),\n",
    "                    \"user\": issue.get(\"user\", {}).get(\"login\"),\n",
    "                    \"url\": issue.get(\"html_url\"),\n",
    "                    \"comments\": issue.get(\"comments\"),\n",
    "                    \"summary\": summary\n",
    "                })\n",
    "            \n",
    "            print(f\"Found {len(issues_summary)} open issues.\")\n",
    "            \n",
    "            # Add context information\n",
    "            result = {\n",
    "                \"total_found\": len(issues_summary),\n",
    "                \"repository\": f\"{owner}/{repo_name}\",\n",
    "                \"note\": \"Mostrando los primeros 10 issues abiertos\" if len(issues_summary) == 10 else f\"Mostrando todos los {len(issues_summary)} issues abiertos\",\n",
    "                \"issues\": issues_summary\n",
    "            }\n",
    "            \n",
    "            return [result]\n",
    "            \n",
    "        except httpx.HTTPStatusError as e:\n",
    "            error_message = e.response.json().get(\"message\", \"No additional message from API.\")\n",
    "            if e.response.status_code == 403 and GITHUB_TOKEN:\n",
    "                error_message += \" (Rate limit with token or token lacks permissions?)\"\n",
    "            elif e.response.status_code == 403 and not GITHUB_TOKEN:\n",
    "                error_message += \" (Rate limit without token. Consider creating a .env file with GITHUB_TOKEN.)\"\n",
    "            \n",
    "            print(f\"GitHub API error: {e.response.status_code}. {error_message}\")\n",
    "            return [{\n",
    "                \"error\": f\"GitHub API error: {e.response.status_code}\",\n",
    "                \"message\": error_message\n",
    "            }]\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {str(e)}\")\n",
    "            return [{\"error\": f\"An unexpected error occurred: {str(e)}\"}]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"DEBUG: Starting GitHub FastMCP server...\")\n",
    "    print(f\"DEBUG: Server name: {mcp.name}\")\n",
    "    print(\"DEBUG: Available tools: list_repository_issues\")\n",
    "    print(\"DEBUG: Available resources: github://repo/{owner}/{repo_name}\")\n",
    "    \n",
    "    # Initialize and run the server\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliente MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completamos el cliente para poder usar el nuevo `resource` creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting client_MCP/client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile client_MCP/client.py\n",
    "\n",
    "import sys\n",
    "import asyncio\n",
    "from contextlib import AsyncExitStack\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import mcp.types as types\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class FastMCPClient:\n",
    "    \"\"\"\n",
    "    FastMCP client that integrates with Claude to process user queries\n",
    "    and use tools exposed by a FastMCP server via STDIO.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, server_script_path: str):\n",
    "        \"\"\"Initialize the FastMCP client with Anthropic and resource management.\"\"\"\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.anthropic = Anthropic()\n",
    "        self.session = None\n",
    "        self.server_params = None\n",
    "        self.server_script_path = server_script_path\n",
    "        \n",
    "    async def connect_to_server(self):\n",
    "        \"\"\"\n",
    "        Connect to the FastMCP server via STDIO.\n",
    "        \"\"\"\n",
    "        print(f\"🔗 Connecting to FastMCP server: {self.server_script_path}\")\n",
    "        \n",
    "        # Determine the server type based on the extension\n",
    "        if self.server_script_path.endswith('.py'):\n",
    "            # Python server\n",
    "            self.server_params = StdioServerParameters(\n",
    "                command=\"python\",\n",
    "                args=[self.server_script_path],\n",
    "                env=None\n",
    "            )\n",
    "        elif self.server_script_path.endswith('.js'):\n",
    "            # JavaScript/Node.js server\n",
    "            self.server_params = StdioServerParameters(\n",
    "                command=\"node\", \n",
    "                args=[self.server_script_path],\n",
    "                env=None\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported server type. Use .py or .js files. Got: {self.server_script_path}\")\n",
    "        \n",
    "        # Set up connection to the server\n",
    "        stdio_transport = await self.exit_stack.enter_async_context(\n",
    "            stdio_client(self.server_params)\n",
    "        )\n",
    "        \n",
    "        # Create MCP session\n",
    "        self.session = await self.exit_stack.enter_async_context(\n",
    "            ClientSession(stdio_transport[0], stdio_transport[1])\n",
    "        )\n",
    "        \n",
    "        # Initialize the session\n",
    "        await self.session.initialize()\n",
    "        \n",
    "        print(\"✅ Connection established successfully\")\n",
    "        \n",
    "        # List available tools and resources\n",
    "        await self.list_available_tools()\n",
    "        await self.list_available_resources()\n",
    "        \n",
    "    async def list_available_tools(self):\n",
    "        \"\"\"List available tools in the FastMCP server.\"\"\"\n",
    "        try:\n",
    "            # Get list of tools from the server\n",
    "            tools_result = await self.session.list_tools()\n",
    "            \n",
    "            if tools_result.tools:\n",
    "                print(f\"\\n🛠️  Available tools ({len(tools_result.tools)}):\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                for tool in tools_result.tools:\n",
    "                    print(f\"📋 {tool.name}\")\n",
    "                    if tool.description:\n",
    "                        print(f\"   Description: {tool.description}\")\n",
    "                    \n",
    "                    # Show parameters if available\n",
    "                    if hasattr(tool, 'inputSchema') and tool.inputSchema:\n",
    "                        if 'properties' in tool.inputSchema:\n",
    "                            params = list(tool.inputSchema['properties'].keys())\n",
    "                            print(f\"   Parameters: {', '.join(params)}\")\n",
    "                    print()\n",
    "            else:\n",
    "                print(\"⚠️  No tools found in the server\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error listing tools: {str(e)}\")\n",
    "\n",
    "    async def list_available_resources(self):\n",
    "        \"\"\"List available resources and resource templates in the FastMCP server.\"\"\"\n",
    "        try:\n",
    "            # Get list of static resources from the server\n",
    "            resources_result = await self.session.list_resources()\n",
    "            \n",
    "            # Get list of resource templates from the server  \n",
    "            templates_result = await self.session.list_resource_templates()\n",
    "            \n",
    "            total_count = len(resources_result.resources) + len(templates_result.resourceTemplates)\n",
    "            \n",
    "            if total_count > 0:\n",
    "                print(f\"\\n📂 Available resources & templates ({total_count}):\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                # Show static resources\n",
    "                for resource in resources_result.resources:\n",
    "                    print(f\"📄 {resource.uri}\")\n",
    "                    if resource.name:\n",
    "                        print(f\"   Name: {resource.name}\")\n",
    "                    if resource.description:\n",
    "                        print(f\"   Description: {resource.description}\")\n",
    "                    if resource.mimeType:\n",
    "                        print(f\"   Type: {resource.mimeType}\")\n",
    "                    print()\n",
    "                \n",
    "                # Show resource templates\n",
    "                for template in templates_result.resourceTemplates:\n",
    "                    print(f\"📋 {template.uriTemplate} (template)\")\n",
    "                    if template.name:\n",
    "                        print(f\"   Name: {template.name}\")\n",
    "                    if template.description:\n",
    "                        print(f\"   Description: {template.description}\")\n",
    "                    if template.mimeType:\n",
    "                        print(f\"   Type: {template.mimeType}\")\n",
    "                    print()\n",
    "            else:\n",
    "                print(\"⚠️  No resources or templates found in the server\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error listing resources: {str(e)}\")\n",
    "\n",
    "    async def read_resource(self, uri: str) -> str:\n",
    "        \"\"\"\n",
    "        Read a resource from the FastMCP server.\n",
    "        \n",
    "        Args:\n",
    "            uri: URI of the resource to read\n",
    "            \n",
    "        Returns:\n",
    "            str: Content of the resource\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"📖 Reading resource: {uri}\")\n",
    "            \n",
    "            # Try to read resource from the server\n",
    "            resource_result = await self.session.read_resource(uri)\n",
    "            \n",
    "            if resource_result.contents:\n",
    "                # Combine all content into a single string\n",
    "                combined_content = \"\"\n",
    "                for content in resource_result.contents:\n",
    "                    # Check if it's TextResourceContents (has text attribute)\n",
    "                    if hasattr(content, 'text'):\n",
    "                        combined_content += content.text + \"\\n\"\n",
    "                    # Check if it's BlobResourceContents (has data attribute)\n",
    "                    elif hasattr(content, 'data'):\n",
    "                        combined_content += f\"[Binary content from resource {uri}]\\n\"\n",
    "                    else:\n",
    "                        combined_content += f\"[Unknown content type from resource {uri}]\\n\"\n",
    "                \n",
    "                print(f\"✅ Resource read successfully\")\n",
    "                return combined_content.strip()\n",
    "            else:\n",
    "                return f\"❌ Resource {uri} has no content\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Error reading resource {uri}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "\n",
    "    async def call_tool(self, tool_name: str, arguments: dict) -> str:\n",
    "        \"\"\"\n",
    "        Call a tool on the FastMCP server via STDIO.\n",
    "        \n",
    "        Args:\n",
    "            tool_name: Name of the tool to call\n",
    "            arguments: Arguments for the tool\n",
    "            \n",
    "        Returns:\n",
    "            str: Result from the tool\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"🔧 Calling tool: {tool_name}\")\n",
    "            print(f\"📝 Arguments: {arguments}\")\n",
    "            \n",
    "            # Execute tool on the MCP server\n",
    "            tool_result = await self.session.call_tool(tool_name, arguments)\n",
    "            \n",
    "            print(f\"✅ Tool executed successfully\")\n",
    "            \n",
    "            # Format result for Claude\n",
    "            if tool_result.content:\n",
    "                # Combine all content into a single result\n",
    "                combined_content = \"\"\n",
    "                for content in tool_result.content:\n",
    "                    # Check if it's TextContent (has text attribute)\n",
    "                    if hasattr(content, 'text'):\n",
    "                        combined_content += content.text + \"\\n\"\n",
    "                    # Check if it's ImageContent (has data attribute)  \n",
    "                    elif hasattr(content, 'data'):\n",
    "                        combined_content += f\"[Binary content returned by tool {tool_name}]\\n\"\n",
    "                    else:\n",
    "                        combined_content += f\"[Unknown content type returned by tool {tool_name}]\\n\"\n",
    "                \n",
    "                return combined_content.strip()\n",
    "            else:\n",
    "                return \"Tool executed without response content\"\n",
    "                    \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Error calling tool {tool_name}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "    \n",
    "    async def process_query(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Process a user query, interacting with Claude and FastMCP tools.\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            \n",
    "        Returns:\n",
    "            str: Final processed response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get available tools, resources and templates from the server\n",
    "            tools_result = await self.session.list_tools()\n",
    "            resources_result = await self.session.list_resources()\n",
    "            templates_result = await self.session.list_resource_templates()\n",
    "            \n",
    "            # Prepare tools for Claude in correct format\n",
    "            claude_tools = []\n",
    "            for tool in tools_result.tools:\n",
    "                claude_tool = {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description or f\"Tool {tool.name}\",\n",
    "                    \"input_schema\": tool.inputSchema or {\"type\": \"object\", \"properties\": {}}\n",
    "                }\n",
    "                claude_tools.append(claude_tool)\n",
    "            \n",
    "            # Add a special tool for reading resources and templates\n",
    "            available_uris = []\n",
    "            available_uris.extend([str(r.uri) for r in resources_result.resources])\n",
    "            available_uris.extend([str(t.uriTemplate) for t in templates_result.resourceTemplates])\n",
    "            \n",
    "            if available_uris:\n",
    "                resource_tool = {\n",
    "                    \"name\": \"read_mcp_resource\",\n",
    "                    \"description\": \"Read a resource from the FastMCP server. Available resources and templates: \" + \n",
    "                                 \", \".join(available_uris),\n",
    "                    \"input_schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"uri\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"URI of the resource to read (can be static resource or template with parameters filled)\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"uri\"]\n",
    "                    }\n",
    "                }\n",
    "                claude_tools.append(resource_tool)\n",
    "            \n",
    "            # Create initial message for Claude\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": query\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # First call to Claude\n",
    "            response = self.anthropic.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=6000,\n",
    "                messages=messages,\n",
    "                tools=claude_tools if claude_tools else None\n",
    "            )\n",
    "            \n",
    "            # Process Claude's response\n",
    "            response_text = \"\"\n",
    "            \n",
    "            for content_block in response.content:\n",
    "                if content_block.type == \"text\":\n",
    "                    response_text += content_block.text\n",
    "                    \n",
    "                elif content_block.type == \"tool_use\":\n",
    "                    # Claude wants to use a tool\n",
    "                    tool_name = content_block.name\n",
    "                    tool_args = content_block.input\n",
    "                    tool_call_id = content_block.id\n",
    "                    \n",
    "                    # Check if it's the special resource reading tool\n",
    "                    if tool_name == \"read_mcp_resource\":\n",
    "                        # Read resource directly\n",
    "                        tool_result = await self.read_resource(tool_args[\"uri\"])\n",
    "                    else:\n",
    "                        # Execute regular tool on the FastMCP server\n",
    "                        tool_result = await self.call_tool(tool_name, tool_args)\n",
    "                    \n",
    "                    # Add tool result to the conversation\n",
    "                    messages.append({\n",
    "                        \"role\": \"assistant\", \n",
    "                        \"content\": response.content\n",
    "                    })\n",
    "                    \n",
    "                    messages.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\n",
    "                            \"type\": \"tool_result\",\n",
    "                            \"tool_use_id\": tool_call_id,\n",
    "                            \"content\": f\"Tool result: {tool_result}\"\n",
    "                        }]\n",
    "                    })\n",
    "                    \n",
    "                    # Second call to Claude with the tool result\n",
    "                    final_response = self.anthropic.messages.create(\n",
    "                        model=\"claude-3-5-sonnet-20241022\",\n",
    "                        max_tokens=6000,\n",
    "                        messages=messages,\n",
    "                        tools=claude_tools if claude_tools else None\n",
    "                    )\n",
    "                    \n",
    "                    # Extract text from the final response\n",
    "                    for final_content in final_response.content:\n",
    "                        if final_content.type == \"text\":\n",
    "                            response_text += final_content.text\n",
    "            \n",
    "            return response_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Error processing query: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return error_msg\n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        \"\"\"\n",
    "        Main chat loop with user interaction.\n",
    "        \"\"\"\n",
    "        print(\"\\n🤖 FastMCP client started. Write 'quit', 'q', 'exit', 'salir' to exit.\")\n",
    "        print(\"💬 You can ask questions about GitHub repositories!\")\n",
    "        print(\"📚 The client can use both tools and resources from the FastMCP server\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Get user input\n",
    "                user_input = input(\"\\n👤 You: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'q', 'exit', 'salir']:\n",
    "                    print(\"👋 Bye!\")\n",
    "                    break\n",
    "                    \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\n🤔 Claude is thinking...\")\n",
    "                \n",
    "                # Process query\n",
    "                response = await self.process_query(user_input)\n",
    "                \n",
    "                # Show response\n",
    "                print(f\"\\n🤖 Claude: {response}\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\n👋 Disconnecting...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ Error in chat: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up resources and close connections.\"\"\"\n",
    "        print(\"🧹 Cleaning up resources...\")\n",
    "        await self.exit_stack.aclose()\n",
    "        print(\"✅ Resources released\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main function that initializes and runs the FastMCP client.\n",
    "    \"\"\"\n",
    "    # Verify command line arguments\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"❌ Usage: python client.py <path_to_fastmcp_server>\")\n",
    "        print(\"📝 Example: python client.py ../MCP_github/github_server.py\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    server_script_path = sys.argv[1]\n",
    "    \n",
    "    print(f\"🚀 Starting FastMCP client...\")\n",
    "    print(f\"📄 Server script: {server_script_path}\")\n",
    "    \n",
    "    # Create and run client\n",
    "    client = FastMCPClient(server_script_path)\n",
    "    \n",
    "    try:\n",
    "        # Connect to the server\n",
    "        await client.connect_to_server()\n",
    "        \n",
    "        # Start chat loop\n",
    "        await client.chat_loop()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fatal error: {str(e)}\")\n",
    "        print(\"💡 Make sure:\")\n",
    "        print(\"   1. The server script path is correct\")\n",
    "        print(\"   2. You have ANTHROPIC_API_KEY in your .env file\")\n",
    "        print(\"   3. The server script is executable\")\n",
    "    finally:\n",
    "        # Ensure resources are cleaned up\n",
    "        await client.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Entry point of the script\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos creado los métodos `list_available_resources` y `read_resource`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba del `resource`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el `resource` creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting FastMCP client...\n",
      "📄 Server script: ../gitHub_MCP_server/github_server.py\n",
      "🔗 Connecting to FastMCP server: ../gitHub_MCP_server/github_server.py\n",
      "\u001b[2;36m[06/28/25 09:11:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting MCP server \u001b[32m'GitHubMCP'\u001b[0m with \u001b]8;id=206543;file:///Users/macm1/Documents/web/portafolio/posts/client_MCP/.venv/lib/python3.11/site-packages/fastmcp/server/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=700093;file:///Users/macm1/Documents/web/portafolio/posts/client_MCP/.venv/lib/python3.11/site-packages/fastmcp/server/server.py#1246\u001b\\\u001b[2m1246\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         transport \u001b[32m'stdio'\u001b[0m                    \u001b[2m              \u001b[0m\n",
      "✅ Connection established successfully\n",
      "\n",
      "🛠️  Available tools (1):\n",
      "==================================================\n",
      "📋 list_repository_issues\n",
      "   Description: Lists open issues for a given GitHub repository.\n",
      "\n",
      "Args:\n",
      "    owner: The owner of the repository (e.g., 'modelcontextprotocol')\n",
      "    repo_name: The name of the repository (e.g., 'python-sdk')\n",
      "\n",
      "Returns:\n",
      "    list[dict]: A list of dictionaries, each containing information about an issue\n",
      "   Parameters: owner, repo_name\n",
      "\n",
      "\n",
      "📂 Available resources & templates (1):\n",
      "==================================================\n",
      "📋 github://repo/{owner}/{repo_name} (template)\n",
      "   Name: get_repository_info\n",
      "   Description: Gets detailed information for a given GitHub repository.\n",
      "\n",
      "Args:\n",
      "    owner: The owner of the repository (e.g., 'modelcontextprotocol')\n",
      "    repo_name: The name of the repository (e.g., 'python-sdk')\n",
      "\n",
      "Returns:\n",
      "    dict: A dictionary containing repository details.\n",
      "   Type: text/plain\n",
      "\n",
      "\n",
      "🤖 FastMCP client started. Write 'quit', 'q', 'exit', 'salir' to exit.\n",
      "💬 You can ask questions about GitHub repositories!\n",
      "📚 The client can use both tools and resources from the FastMCP server\n",
      "------------------------------------------------------------\n",
      "\n",
      "👤 You: ^C\n",
      "\n",
      "👤 You: "
     ]
    }
   ],
   "source": [
    "!cd client_MCP && source .venv/bin/activate && python client.py ../gitHub_MCP_server/github_server.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
