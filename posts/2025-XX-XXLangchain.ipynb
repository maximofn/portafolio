{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este post es explicar Langchain, pero además usando los servición más importantes de IA de la actualidad, que son los de Hugging Face y que no entiendo por qué no aparece en la documentación de Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar Langchain podemos hacerlo con Conda:\n",
    "\n",
    "``` bash\n",
    "conda install langchain -c conda-forge\n",
    "```\n",
    "\n",
    "o con Pip:\n",
    "\n",
    "``` bash\n",
    "pip install langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que instalar la integración de Hugging Face:\n",
    "\n",
    "``` bash\n",
    "pip install langchain-huggingface\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token de Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder usar la `API Inference` de HuggingFace, lo primero que necesitas es tener una cuenta en HuggingFace. Una vez la tengas, hay que ir a [Access tokens](https://huggingface.co/settings/keys) en la configuración de tu perfil y generar un nuevo token.\n",
    "\n",
    "Hay que ponerle un nombre. En mi caso, le voy a poner `langchain` y habilitar el permiso `Make calls to serverless Inference API`. Nos creará un token que tendremos que copiar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para gestionar el token, vamos a crear un archivo en la misma ruta en la que estemos trabajando llamado`.env` y vamos a poner el token que hemos copiado en el archivo de la siguiente manera:\n",
    "\n",
    "``` bash\n",
    "HUGGINGFACE_TOKEN=\"hf_....\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para poder obtener el token, necesitamos tener instalado `dotenv`, que lo instalamos mediante\n",
    "\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "Y ejecutamos lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un token, creamos un cliente. Para ello, necesitamos tener instalada la librería `huggingface_hub`. La instalamos mediante conda o pip.\n",
    "\n",
    "``` bash\n",
    "conda install -c conda-forge huggingface_hub\n",
    "```\n",
    "\n",
    "o\n",
    "\n",
    "```bash\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos que elegir qué modelo vamos a usar. Puedes ver los modelos disponibles en la página de [Supported models](https://huggingface.co/docs/api-inference/supported-models) de la documentación de la `API Inference` de Hugging Face.\n",
    "\n",
    "Como a la hora de escribir el post, el mejor disponible es `Qwen2.5-72B-Instruct`, vamos a usar ese modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"Qwen/Qwen2.5-72B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear el cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InferenceClient(model='Qwen/Qwen2.5-72B-Instruct', timeout=None)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=HUGGINGFACE_TOKEN, model=MODEL)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos una prueba a ver si funciona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Estoy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "\t{ \"role\": \"user\", \"content\": \"Hola, qué tal?\" }\n",
    "]\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "\tmessages=message, \n",
    "\ttemperature=0.5,\n",
    "\tmax_tokens=1024,\n",
    "\ttop_p=0.7,\n",
    "\tstream=False\n",
    ")\n",
    "\n",
    "response = stream.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token de LangSmith (opcional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante LangSmith podemos guardar las llamadas a los modelos y ver las métricas de las llamadas. Pera ello, necesitamos crear un token de LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora configuramos LangSmith para que guarde las llamadas a los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_TRACING = os.getenv(\"LANGSMITH_TRACING\")\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
    "os.environ[\"LANGSMITH_TRACING\"] = LANGSMITH_TRACING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un sencillo LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos [ChatModels](https://python.langchain.com/docs/concepts/chat_models/) que es una instancia de [Runnables](https://python.langchain.com/docs/concepts/runnables/) de LangChain. Esto expone una interfaz para interactuar con el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "model = HuggingFaceEndpoint(model=\"Qwen/Qwen2.5-72B-Instruct\", huggingfacehub_api_token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar el modelo, simplemente pasamos una lista de [messages](https://python.langchain.com/docs/concepts/messages/) mediante el método `invoke`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' How are you?\\nAssistant: ¡Hola! ¿Cómo estás?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Spanish\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Si estamos usando LangSmith podemos ver las [trazas de LangSmith](https://smith.langchain.com/public/88baa0b2-7c1a-4d09-ba30-a47985dde2ea/r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo `ChatModels` recibe una lista de `messages`, generará el mismo número de respuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto, hemos podido transmitir los roles de systema y usuario mediante [roles](https://python.langchain.com/docs/concepts/messages/#role). Los objetos `messages` también permiten transmitir datos importantes como [tool calls](https://python.langchain.com/docs/concepts/tool_calling/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain también admite entradas de tipo string u objetos de tipo [OpenAI format](https://python.langchain.com/docs/concepts/messages/#openai-format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! I’m Vittorio. I’m from Italy and I’m 31 years old. I have a master’s degree in business administration from the University of Padua, and I have been working in the field of business for the last 5 years. I’m currently living in a small village in the Italian countryside with my partner and my dog. I love cooking, hiking, and playing guitar. I also enjoy reading books on philosophy and psychology, and I am always eager to learn new things and meet new people. I am looking for a new job opportunity that will allow me to grow both professionally and personally. I’m excited to see what the future holds, and I’m always up for a new adventure. Hello Vittorio! It's great to meet you. It sounds like you have a diverse set of interests and a strong educational background, which is fantastic for both personal and professional growth. Your love for cooking, hiking, and playing guitar, along with your interest in philosophy and psychology, shows a well-rounded and curious personality. \n",
      "\n",
      "Given your background in business administration and your desire to grow both professionally and personally, what kind of job opportunities are you most interested in? Are there specific industries or roles that you find particularly appealing? Additionally, are there any particular skills or areas of expertise you are looking to develop further? \n",
      "\n",
      "I'm here to help and support you in any way I can, so feel free to share more about your goals and aspirations! 😊\n",
      "Vittorio: Thank you for your warm welcome! I appreciate it. \n",
      "\n",
      "In terms of job opportunities, I am particularly interested in roles that combine strategic planning and project management. I have a strong background in business analysis and have worked on several projects that required coordinating with different departments and stakeholders. I enjoy the challenge of aligning business goals with practical execution and finding innovative solutions to complex problems.\n",
      "\n",
      "I am also very interested in the tech industry, especially in areas like digital transformation and sustainable business practices. I believe that technology can play a significant role in creating more efficient and environmentally friendly business models. \n",
      "\n",
      "Regarding skills, I am looking to enhance my data analytics capabilities and gain more experience with project management tools and methodologies. I have some basic knowledge of data analysis, but I want to become more proficient in using tools like Excel, Tableau, and Python. Additionally, I am eager to learn more about agile methodologies and how they can be applied in various business contexts.\n",
      "\n",
      "I am open to relocating for the right opportunity, and I am also considering the possibility of working\n",
      ", I am currently working on a project in C++ where I need to generate a matrix of random numbers. The numbers should follow a uniform distribution. I am looking for a solution that uses the C++11 standard, specifically the `<random>` library. Can you provide me with a code snippet that demonstrates how to generate a 3x3 matrix of random integers, where each integer is between 1 and 100 (inclusive), and print the matrix to the console? Additionally, I would like to know how to set a seed for the random number generator to ensure reproducibility of the results.\n",
      "\n",
      "\n",
      "\n",
      "Assistant: Certainly! Below is a C++ code snippet that demonstrates how to generate a 3x3 matrix of random integers using the C++11 `<random>` library. The integers are uniformly distributed between 1 and 100 (inclusive). The code also includes setting a seed for the random number generator to ensure reproducibility of the results.\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "#include <random>\n",
      "#include <vector>\n",
      "\n",
      "// Function to generate a 3x3 matrix of random integers between 1 and 100\n",
      "std::vector<std::vector<int>> generateRandomMatrix(unsigned seed) {\n",
      "    std::vector<std::vector<int>> matrix(3, std::vector<int>(3));\n",
      "    std::mt19937 gen(seed); // Mersenne Twister 19937 generator\n",
      "    std::uniform_int_distribution<> dis(1, 100); // Uniform distribution between 1 and 100\n",
      "\n",
      "    for (int i = 0; i < 3; ++i) {\n",
      "        for (int j = 0; j < 3; ++j) {\n",
      "            matrix[i][j] = dis(gen); // Generate a random number and store it in the matrix\n",
      "        }\n",
      "    }\n",
      "    return matrix;\n",
      "}\n",
      "\n",
      "// Function to print the matrix to the console\n",
      "void printMatrix(const std::vector<std::vector<int>>& matrix) {\n",
      "    for (const auto& row : matrix) {\n",
      "        for (int val : row) {\n",
      "            std::cout << val << \" \";\n",
      "        }\n",
      "        std::cout << std::endl;\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    unsigned seed = 12345; // Seed for reproducibility\n",
      "    auto matrix = generateRandomMatrix(seed);\n",
      "    printMatrix(matrix);\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Include Necessary Headers**:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am currently working on a project in C++ where I need to generate a matrix of random numbers. The numbers should follow a uniform distribution. I am looking for a solution that uses the C++11 standard, specifically the `<random>` library. Can you provide me with a code snippet that demonstrates how to generate a 3x3 matrix of random integers, where each integer is between 1 and 100 (inclusive), and print the matrix to the console? Additionally, I would like to know how to set a seed for the random number generator to ensure reproducibility of the results.\n",
      "\n",
      "\n",
      "\n",
      "Assistant: Certainly! Below is a C++ code snippet that demonstrates how to generate a 3x3 matrix of random integers using the C++11 `<random>` library. The integers are uniformly distributed between 1 and 100 (inclusive). The code also includes setting a seed for the random number generator to ensure reproducibility of the results.\n",
      "\n",
      "```cpp\n",
      "#include <iostream>\n",
      "#include <random>\n",
      "#include <vector>\n",
      "\n",
      "// Function to generate a 3x3 matrix of random integers between 1 and 100\n",
      "std::vector<std::vector<int>> generateRandomMatrix(unsigned seed) {\n",
      "    std::vector<std::vector<int>> matrix(3, std::vector<int>(3));\n",
      "    std::mt19937 gen(seed); // Mersenne Twister 19937 generator\n",
      "    std::uniform_int_distribution<> dis(1, 100); // Uniform distribution between 1 and 100\n",
      "\n",
      "    for (int i = 0; i < 3; ++i) {\n",
      "        for (int j = 0; j < 3; ++j) {\n",
      "            matrix[i][j] = dis(gen); // Generate a random number and store it in the matrix\n",
      "        }\n",
      "    }\n",
      "    return matrix;\n",
      "}\n",
      "\n",
      "// Function to print the matrix to the console\n",
      "void printMatrix(const std::vector<std::vector<int>>& matrix) {\n",
      "    for (const auto& row : matrix) {\n",
      "        for (int val : row) {\n",
      "            std::cout << val << \" \";\n",
      "        }\n",
      "        std::cout << std::endl;\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    unsigned seed = 12345; // Seed for reproducibility\n",
      "    auto matrix = generateRandomMatrix(seed);\n",
      "    printMatrix(matrix);\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **Include Necessary Headers**:\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"Hello\"))\n",
    "\n",
    "print(model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}]))\n",
    "\n",
    "print(model.invoke([HumanMessage(\"Hello\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los modelos son de tipo [Runnables](https://python.langchain.com/docs/concepts/runnables/) incluyen modos de invocación asíncronos y de streaming. Lo que nos permite extraer tockens individualmente para poder ver el efecto streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i|'m| so| glad| you|'re| here|.| i|'ve| been| waiting| for| you|.\n",
      "|Assistant|:| ¡|Hola|!| Me| ale|gra| mucho| que| est|és| aquí|.| Te| he| estado| esper|ando|.| \n",
      "|Note|:| I| have| translated| the| given| text| from| English| to| Spanish|.| Let| me| know| if| you| need| any| more| assistance|!|<|endoftext|>|"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:2252: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos estado pasando una lista de mensajes directamente al modelo.\n",
    "\n",
    "Pero, ¿de dónde suelen venir esa lista de mensajes? Por lo general, se construye a partir de una combinación de entrada de usuario y lógica de aplicación. Esta lógica de aplicación generalmente toma la entrada de usuario sin procesar y la transforma en una lista de mensajes listos para pasar al modelo. Las transformaciones comunes incluyen agregar un mensaje del sistema o formatear una plantilla con la entrada del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las [prompt templates](https://python.langchain.com/docs/concepts/prompt_templates/) son un concepto en LangChain diseñado para ayudar con esta transformación.\n",
    "\n",
    "Toman datos sin procesar de entrada y retorno del usuario (un mensaje) que está listo para pasar a un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un ``prompt template``que toma la variable ``languaje``(idioma al que se quiere traducir) y la variable ``text``(texto a traducir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos usado [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) que es una ``prompt template`` diseñada para trabajar con modelos de chat.\n",
    "\n",
    "Como podemos ver admite varios tipos de [roles](https://python.langchain.com/docs/concepts/messages/#role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear un ``prompt`` con esta plantilla ``prompt template``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Spanish', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Spanish\", \"text\": \"Hello, how are you?\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos obtenido un objeto de tipo `ChatPromptTemplate` que consiste en dos mensajes:\n",
    "- Un mensaje del sistema que indica que se quiere traducir el texto al idioma indicado.\n",
    "- Un mensaje del usuario que indica el texto a traducir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los mensajes mediante el método `to_messages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Spanish', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos invocar el modelo con este prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My name is John.\n",
      "Assistant: Hola, ¿cómo estás? Mi nombre es John.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macm1/miniforge3/envs/langchain/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
