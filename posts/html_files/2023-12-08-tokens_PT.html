<section class="section-block-markdown-cell">
<h1 id="Tokens">Tokens<a class="anchor-link" href="#Tokens">¬∂</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>Agora que os <code>LLLMs</code> est√£o na moda, continuamos ouvindo sobre o n√∫mero de <code>tokens</code> suportados por cada modelo, mas o que s√£o <code>tokens</code>? S√£o as unidades m√≠nimas de representa√ß√£o de palavras.</p>
</section>
<section class="section-block-markdown-cell">
<p>Este caderno foi traduzido automaticamente para torn√°-lo acess√≠vel a mais pessoas, por favor me avise se voc√™ vir algum erro de digita√ß√£o..</p>
<p>Para explicar o que s√£o <code>tokens</code>, vamos primeiro dar uma olhada em um exemplo pr√°tico: vamos usar o tokenizador da OpenAI, chamado [tiktoken] (<a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>).</p>
<p>Ent√£o, primeiro instalamos o pacote:</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>tiktoken
<span class="sb">```</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Depois de instalado, criamos um tokenizador usando o modelo <code>cl100k_base</code>, que o notebook de exemplo [How to count tokens with tiktoken] (<a href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb">https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb</a>) explica que √© usado pelos modelos <code>gpt-4</code>, <code>gpt-3.5-turbo</code> e <code>text-embedding-ada-002</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tiktoken</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">"cl100k_base"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, criamos uma palavra de amostra tara e a tokenizamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">example_word</span> <span class="o">=</span> <span class="s2">"breakdown"</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E n√≥s o simbolizamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">example_word</span><span class="p">)</span>
<span class="n">tokens</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[9137, 2996]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A palavra foi dividida em 2 tokens, o <code>9137</code> e o <code>2996</code>. Vamos ver a quais palavras elas correspondem</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">word1</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">word2</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">word1</span><span class="p">,</span> <span class="n">word2</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[21]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>('break', 'down')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O tokenizador <code>OpenAI</code> dividiu a palavra <code>breakdown</code> nas palavras <code>break</code> e <code>down</code>. Ou seja, ele dividiu a palavra em duas palavras mais simples.</p>
<p>Isso √© importante porque, quando se diz que um <code>LLM</code> suporta x <code>token</code>s`, n√£o significa que ele suporta x palavras, mas que suporta x unidades m√≠nimas de representa√ß√£o de palavras.</p>
</section>
<section class="section-block-markdown-cell">
<p>Se voc√™ tiver um texto e quiser ver o n√∫mero de <code>token</code>s que ele tem para o tokenizador <code>OpenAI</code>, poder√° visualiz√°-lo na p√°gina <a href="https://platform.openai.com/tokenizer">Tokenizer</a>, que mostra cada <code>token</code> em uma cor diferente.</p>
<p><img alt="tokenizer" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/tokenizer.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>Vimos o tokenizador <code>OpenAI</code>, mas cada <code>LLM</code> poder√° usar outro.</p>
</section>
<section class="section-block-markdown-cell">
<p>Como dissemos, os <code>tokens</code> s√£o as unidades m√≠nimas de representa√ß√£o de palavras, portanto, vamos ver quantos tokens diferentes o <code>tiktoken</code> tem.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">n_vocab</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">n_vocab</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Vocab size: </span><span class="si">{</span><span class="n">n_vocab</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Vocab size: 100277
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver como ele tokeniza outros tipos de palavras.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="n">decode_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="n">decode_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">token</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">word</span> <span class="o">=</span> <span class="s2">"dog"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"tomorrow..."</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"artificial intelligence"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"Python"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"12/25/2023"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"üòä"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Word: dog ==&gt; tokens: [18964], decode_tokens: ['dog']
Word: tomorrow... ==&gt; tokens: [38501, 7924, 1131], decode_tokens: ['tom', 'orrow', '...']
Word: artificial intelligence ==&gt; tokens: [472, 16895, 11478], decode_tokens: ['art', 'ificial', ' intelligence']
Word: Python ==&gt; tokens: [31380], decode_tokens: ['Python']
Word: 12/25/2023 ==&gt; tokens: [717, 14, 914, 14, 2366, 18], decode_tokens: ['12', '/', '25', '/', '202', '3']
Word: üòä ==&gt; tokens: [76460, 232], decode_tokens: ['ÔøΩ', 'ÔøΩ']
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Por fim, vamos dar uma olhada em palavras em outro idioma</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">word</span> <span class="o">=</span> <span class="s2">"perro"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"perra"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"ma√±ana..."</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"inteligencia artificial"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"Python"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"12/25/2023"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">word</span> <span class="o">=</span> <span class="s2">"üòä"</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">decode_tokens</span> <span class="o">=</span> <span class="n">encode_decode</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word: </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> ==&gt; tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, decode_tokens: </span><span class="si">{</span><span class="n">decode_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Word: perro ==&gt; tokens: [716, 299], decode_tokens: ['per', 'ro']
Word: perra ==&gt; tokens: [79, 14210], decode_tokens: ['p', 'erra']
Word: ma√±ana... ==&gt; tokens: [1764, 88184, 1131], decode_tokens: ['ma', '√±ana', '...']
Word: inteligencia artificial ==&gt; tokens: [396, 39567, 8968, 21075], decode_tokens: ['int', 'elig', 'encia', ' artificial']
Word: Python ==&gt; tokens: [31380], decode_tokens: ['Python']
Word: 12/25/2023 ==&gt; tokens: [717, 14, 914, 14, 2366, 18], decode_tokens: ['12', '/', '25', '/', '202', '3']
Word: üòä ==&gt; tokens: [76460, 232], decode_tokens: ['ÔøΩ', 'ÔøΩ']
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver que, para palavras semelhantes, mais tokens s√£o gerados em espanhol do que em ingl√™s, portanto, para o mesmo texto, com um n√∫mero semelhante de palavras, o n√∫mero de tokens ser√° maior em espanhol do que em ingl√™s.</p>
</section>
