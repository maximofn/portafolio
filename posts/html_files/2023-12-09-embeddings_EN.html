<section class="section-block-markdown-cell">
<h1 id="Embeddings">Embeddings<a class="anchor-link" href="#Embeddings">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>In a previous post about <a href="https://maximofn.com/tokens/">tokens</a>, we already saw the minimal representation of each word, which corresponds to assigning a number to the smallest division of each word.</p>
<p>However, transformers and therefore LLMs do not represent word information in this way, but rather through <code>embeddings</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Let's first look at two ways to represent words: <code>ordinal encoding</code> and <code>one hot encoding</code>. By examining the problems with these two types of representations, we can move on to <code>word embeddings</code> and <code>sentence embeddings</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>We will also see an example of how to train a <code>word embeddings</code> model with the <code>gensim</code> library.</p>
</section>
<section class="section-block-markdown-cell">
<p>And finally, we will see how to use pretrained <code>embedding</code> models with the <code>transformers</code> library from HuggingFace.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Ordinal encoding">Ordinal encoding<a class="anchor-link" href="#Ordinal encoding">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>This is the most basic way to represent words within transformers. It consists of assigning a number to each word, or keeping the numbers that are already assigned to the tokens.</p>
<p>However, this type of representation has two problems</p>
<ul>
  <li>Let's imagine that table corresponds to token 3, cat to token 1, and dog to token 2. One might assume that <code>table = cat + dog</code>, but this is not the case. There is no such relationship between these words. We might even think that by assigning the correct tokens, this type of relationship could be established. However, this thought falls apart with words that have more than one meaning, such as the word <code>bank</code>.</li>
</ul>
<ul>
  <li>The second problem is that neural networks internally perform many numerical calculations, so it could happen that if table has the token 3, it might internally have more importance than the word cat, which has the token 1.</li>
</ul>
<p>So this kind of word representation can be discarded very quickly</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="One hot encoding">One hot encoding<a class="anchor-link" href="#One hot encoding">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Here we use vectors of <code>N</code> dimensions. For example, we saw that OpenAI has a vocabulary of <code>100277</code> distinct tokens. Therefore, if we use <code>one hot encoding</code>, each word would be represented by a vector of <code>100277</code> dimensions.</p>
<p>However, one hot encoding has two other major problems</p>
<ul>
  <li>It does not take into account the relationship between words. Therefore, if we have two words that are synonyms, such as <code>gato</code> and <code>felino</code>, we would have two different vectors to represent them.</li>
</ul>
<p>In language, the relationship between words is very important, and not taking this relationship into account is a big problem.</p>
<ul>
  <li>The second problem is that the vectors are very large. If we have a vocabulary of <code>100277</code> tokens, each word would be represented by a vector of <code>100277</code> dimensions. This makes the vectors very large and the calculations very expensive. Additionally, these vectors will be all zeros except for the position corresponding to the token of the word. Therefore, most of the calculations will be multiplications by zero, which are calculations that do not contribute anything. So we will have a lot of memory assigned to vectors in which only a <code>1</code> is present at a specific position.</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h2 id="Word embeddings">Word embeddings<a class="anchor-link" href="#Word embeddings">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>With word embeddings, the aim is to solve the problems of the two previous types of representations. To do this, vectors of <code>N</code> dimensions are used, but in this case, vectors with 100277 dimensions are not used; instead, vectors with many fewer dimensions are used. For example, we will see that OpenAI uses <code>1536</code> dimensions.</p>
<p>Each of the dimensions of these vectors represents a feature of the word. For example, one of the dimensions could represent whether the word is a verb or a noun. Another dimension could represent whether the word is an animal or not. Another dimension could represent whether the word is a proper name or not. And so on.</p>
<p>However, these features are not defined by hand, but are learned automatically. During the training of transformers, the values of each dimension of the vectors are adjusted so that the features of each word are learned.</p>
</section>
<section class="section-block-markdown-cell">
<p>By making each dimension of the word vectors represent a feature of the word, it is achieved that words with similar features have similar vectors. For example, the words <code>gato</code> and <code>felino</code> will have very similar vectors, since both are animals. And the words <code>mesa</code> and <code>silla</code> will have similar vectors, as both are furniture.</p>
<p>In the following image we can see a 3-dimensional representation of words, and we can see that all words related to <code>school</code> are close together, all words related to <code>food</code> are close together, and all words related to <code>ball</code> are close together.</p>
<img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/word_embedding_3_dimmension.webp" alt="word_embedding_3_dimmension">
</section>
<section class="section-block-markdown-cell">
<p>Having each dimension of the vectors represent a feature of the word allows us to perform operations with words. For example, if we subtract the word <code>man</code> from the word <code>king</code> and add the word <code>woman</code>, we get a word very similar to the word <code>queen</code>. We will verify this with an example later.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Word Similarity">Word Similarity<a class="anchor-link" href="#Word Similarity">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>As each word is represented by an <code>N</code>-dimensional vector, we can calculate the similarity between two words. For this purpose, the cosine similarity function or <code>cosine similarity</code> is used.</p>
<p>If two words are close in the vector space, it means that the angle between their vectors is small, so their cosine is close to 1. If there is a 90-degree angle between the vectors, the cosine is 0, meaning there is no similarity between the words. And if there is a 180-degree angle between the vectors, the cosine is -1, meaning the words are opposites.</p>
<img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/cosine_similarity.webp" alt="cosine similarity">
</section>
<section class="section-block-markdown-cell">
<h3 id="Example with OpenAI Embeddings">Example with OpenAI Embeddings<a class="anchor-link" href="#Example with OpenAI Embeddings">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now that we know what <code>embeddings</code> are, let's look at some examples with the <code>embeddings</code> provided by the <code>OpenAI</code> API.</p>
<p>To do this, we first need to have the <code>OpenAI</code> package installed.</p>
<div class='highlight'><pre><code class="language-bash">pip install openai
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>We import the necessary libraries</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We use an <code>API key</code> from OpenAI. To do this, we go to the <a href="https://openai.com/">OpenAI</a> page and register. Once registered, we navigate to the <a href="https://platform.openai.com/api-keys">API Keys</a> section and create a new <code>API Key</code>.</p>
<img src="https://raw.githubusercontent.com/maximofn/alfred/main/gifs/openaix2.gif" alt="open ai api key">
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">api_key</span> <span class="o">=</span> <span class="s2">&quot;Pon aquí tu API key&quot;</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We select the embedding model we want to use. In this case, we will use <code>text-embedding-ada-002</code>, which is the one recommended by <code>OpenAI</code> in their <a href="https://platform.openai.com/docs/guides/embeddings/">embeddings</a> documentation.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_openai</span> <span class="o">=</span> <span class="s2">&quot;text-embedding-ada-002&quot;</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We create an <code>API</code> client</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">client_openai</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span> <span class="n">organization</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see what the <code>embeddings</code> of the word <code>King</code> look like.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">word</span> <span class="o">=</span> <span class="s2">&quot;Rey&quot;</span>
<span class="n">embedding_openai</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">embedding_openai</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_openai</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(torch.Size([1536]),
 tensor([-0.0103, -0.0005, -0.0189,  ..., -0.0009, -0.0226,  0.0045]))
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, we obtain a vector of <code>1536</code> dimensions</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Example with HuggingFace embeddings">Example with HuggingFace embeddings<a class="anchor-link" href="#Example with HuggingFace embeddings">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Since OpenAI's embedding generation is paid, we are going to see how to use HuggingFace embeddings, which are free. To do this, first we need to make sure the <code>sentence-transformers</code> library is installed.</p>
<div class='highlight'><pre><code class="language-bash">pip install -U sentence-transformers
</code></pre></div>
<p>And now we start generating the word embeddings.</p>
</section>
<section class="section-block-markdown-cell">
<p>First we import the library</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformer</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we create an <code>embeddings</code> model from <code>HuggingFace</code>. We use <code>paraphrase-MiniLM-L6-v2</code> because it is a small and fast model that gives good results, and for our example, it suffices.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;paraphrase-MiniLM-L6-v2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>And now we can generate the <code>embeddings</code> of the words</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Rey&#39;</span><span class="p">]</span>
<span class="n">embedding_huggingface</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">embedding_huggingface</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_huggingface</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>((1, 384),
 array([ 4.99837071e-01, -7.60397986e-02,  5.47384083e-01,  1.89465046e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.21713984e-01, -1.01025246e-01,  6.44087136e-01,  4.91398573e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.73571329e-02, -2.77234882e-01,  4.34713453e-01, -1.06284058e+00,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.44114518e-01,  8.98794234e-01,  4.74923879e-01, -7.48904228e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.84665376e-01, -1.75070837e-01,  5.92192829e-01, -1.02512836e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;9.45721626e-01,  2.43777707e-01,  3.91995460e-01,  3.35530996e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.58333105e-01,  1.18869759e-01,  5.31717360e-01, -1.21750660e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.45580745e-01, -7.63889611e-01, -3.19075316e-01,  2.55386919e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.06407446e-01, -8.99556637e-01,  6.34190366e-02, -2.96231866e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.22994244e-01,  7.44934231e-02, -4.49327320e-01, -2.71379113e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.88012260e-01, -2.82730222e-01,  2.50365853e-01,  3.06314558e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5.01561277e-02, -5.73592126e-01, -4.93096076e-02, -2.54629493e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.45663840e-01, -1.54654181e-03,  1.85357735e-01,  2.49421135e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.80077875e-01, -2.99735814e-01,  7.34686375e-01,  9.35385004e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-8.64403173e-02,  5.90056717e-01,  9.62065995e-01, -3.89911681e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.52635378e-01,  1.10802782e+00, -4.28262979e-01,  8.98583114e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.79768258e-01, -7.25559890e-01,  4.38431054e-01,  6.08255446e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.06222546e+00,  1.86217821e-03,  5.23232877e-01, -5.59782684e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.08870542e+00, -1.29855171e-01, -1.34669527e-01,  4.24595959e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.99118191e-01, -2.53481418e-01, -1.82368979e-01,  9.74772453e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-7.66527832e-01,  2.02146843e-01, -9.27186012e-01, -3.72025579e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.51360565e-01,  3.66043419e-01,  3.58169287e-01, -5.50914466e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.87659878e-01,  2.67650932e-01, -1.30100116e-01, -9.08647776e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.58671075e-01, -4.44935560e-01, -1.43231079e-01, -2.83272982e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.21463636e-02,  1.98998764e-01, -9.47986841e-02,  1.74529219e+00,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.71559617e-01,  5.96294463e-01,  1.38505893e-02,  3.90956283e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.46427560e-01,  2.63105750e-01,  2.64972121e-01, -2.67196923e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.54366294e-02,  9.39224422e-01,  3.35206270e-01, -1.99105024e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.06340271e-01,  3.83643419e-01,  4.37904626e-01,  8.92579079e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.86432815e-01, -2.59302586e-01, -6.39415443e-01,  1.21703267e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;6.44594133e-01,  2.56335083e-02,  5.53315282e-02,  5.85618019e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.03075497e-01, -4.17360187e-01,  5.00189543e-01,  4.23062295e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-7.62073815e-01, -4.36184794e-01, -4.13090199e-01, -2.14746520e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.76077414e-01, -1.51846036e-02, -6.51694953e-01,  2.05930993e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.73996288e-01,  1.14034235e-01, -7.40544260e-01,  1.98710993e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-6.66027904e-01,  3.00016254e-01, -4.03109461e-01,  1.85078502e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.27183425e-01,  4.19003010e-01,  1.16863050e-01, -4.33366179e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.62291127e-01,  6.25310719e-01, -3.34749371e-01,  3.18448655e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-9.09660235e-02,  3.58690947e-01,  1.23402506e-01, -5.08333087e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.18513209e-01,  5.83032072e-01, -8.37822199e-01, -1.52947128e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5.07765234e-01, -2.90990144e-01, -2.56464798e-02,  5.69117546e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.43118417e-01, -3.27799052e-01, -1.70862004e-01,  4.14014012e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.74694878e-01,  5.15708327e-01,  3.21234539e-02,  1.55380607e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.21141332e-01, -1.72114551e-01,  6.43211603e-01, -3.89207341e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.29103401e-01,  4.13877398e-01, -9.22305062e-02, -4.54976231e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.50242126e+00, -2.81573564e-01,  1.70057654e-01,  4.53076512e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.25060362e-01, -1.33391351e-01,  5.40394569e-03,  3.71117502e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.29107875e-01,  1.35897202e-02,  2.44936779e-01,  1.04574718e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.65612388e-01,  4.33572650e-01, -4.09719855e-01, -2.95067448e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.26362443e-02, -7.43583977e-01, -7.35885441e-01, -1.35508239e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.12558493e-01, -5.46157181e-01,  7.55161867e-02, -3.57991695e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.20607555e-01,  5.53125329e-02, -3.23110700e-01,  4.88573104e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.07487953e+00,  1.72190830e-01,  8.48749802e-02,  5.73584400e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.06147277e-01,  3.26699704e-01,  5.09487510e-01, -2.60940105e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.85459042e-01,  3.15197736e-01, -8.84049162e-02, -2.14854136e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.04228538e-01, -3.53874594e-01,  3.30587216e-02, -2.04278827e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.45132256e-01, -4.05272096e-01,  9.07981098e-01, -1.70708492e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.62848401e-01, -3.17223936e-01,  1.53909430e-01,  7.24429131e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.27339968e-01, -1.16330147e+00, -9.58504915e-01,  4.87008452e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.30886355e-01, -1.40117988e-01,  7.84571916e-02, -2.93157458e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.00778294e+00,  1.34625390e-01, -4.66320179e-02,  6.51122704e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.50451362e-02, -2.15500608e-01, -2.42915586e-01, -3.21900517e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.94186682e-01,  4.71027017e-01,  1.56058431e-01,  1.30854800e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.84257025e-01, -1.44421116e-01, -7.09840000e-01, -1.80235609e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-8.30230191e-02,  9.08326149e-01, -8.22497830e-02,  1.46948382e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.41326815e-01,  3.81170362e-01, -6.37023628e-01,  1.70148894e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.00046806e-01,  5.70729785e-02, -1.09820545e+00, -1.03613675e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-6.21219516e-01,  4.55532551e-01,  1.86942443e-01, -2.04409719e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;7.81394243e-01, -7.88963258e-01,  2.19068691e-01, -3.62780124e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.41522694e-01, -1.73794985e-01, -4.00943428e-01,  5.01900315e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.53949839e-01,  1.03774257e-01, -1.66873619e-01, -4.63893116e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.78147718e-01,  4.85655308e-01, -3.02978605e-02, -5.67060888e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.68107373e-01, -6.57559693e-01, -5.02855539e-01, -1.94635347e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-9.58659649e-01, -4.97986436e-01,  1.33874401e-01,  3.09395105e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-4.52993363e-01,  7.43827343e-01, -1.87271550e-01, -6.11483693e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.08927953e+00, -2.30332208e-03,  2.11169615e-01, -3.46892715e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-3.32458824e-01,  2.07640216e-01, -4.10387546e-01,  3.12181324e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.69687408e-01,  8.62928331e-01,  2.40735337e-01, -3.65841389e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;6.84210837e-01,  3.45884450e-02,  5.63964128e-01,  2.39361122e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.10872793e-01, -6.34638309e-01, -9.07931089e-01, -6.35836497e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.20288679e-01,  2.59186536e-01, -4.45540816e-01,  6.33085072e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.97424471e-01,  7.51152515e-01, -2.68558711e-01, -4.39288855e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;4.13556695e-01, -1.89288303e-01,  5.81856608e-01,  4.75860722e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.60344616e-01, -2.96180040e-01,  2.91323394e-01,  1.34404674e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-1.22037649e-01,  4.19363379e-02, -3.87936801e-01, -9.25336123e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-5.28307915e-01, -1.74257740e-01, -1.52818128e-01,  4.31716293e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.12064430e-01,  2.98252910e-01,  9.86064151e-02,  3.84781063e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;6.68018535e-02, -2.29525566e-01, -8.20755959e-03,  5.17108142e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-6.66776478e-01, -1.38897672e-01,  4.68370765e-01, -2.14766636e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;2.43549764e-01,  2.25854263e-01, -1.92763060e-02,  2.78505355e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;3.39088053e-01, -9.69757214e-02, -2.71263003e-01,  1.05703615e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.14365645e-01,  4.16649908e-01,  4.18699026e-01, -1.76222697e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;-2.08620593e-01, -5.79392374e-01, -1.68948188e-01, -1.77841976e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;5.69338985e-02,  2.12916449e-01,  4.24367547e-01, -7.13860095e-02,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;8.28932896e-02, -2.40542665e-01, -5.94049037e-01,  4.09415931e-01,
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;1.01326215e+00, -5.71239054e-01,  4.35258061e-01, -3.64619821e-01],
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;dtype=float32))
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, we obtain a vector of <code>384</code> dimensions. In this case, a vector of this dimension is obtained because the model <code>paraphrase-MiniLM-L6-v2</code> has been used. If we use another model, we will obtain vectors of different dimensions.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Operations with words">Operations with words<a class="anchor-link" href="#Operations with words">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Let's get the embeddings of the words <code>king</code>, <code>man</code>, <code>woman</code>, and <code>queen</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding_openai_rey</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;rey&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
<span class="n">embedding_openai_hombre</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;hombre&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
<span class="n">embedding_openai_mujer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;mujer&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
<span class="n">embedding_openai_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;reina&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding_openai_reina</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_openai_reina</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(torch.Size([1536]),
 tensor([-0.0110, -0.0084, -0.0115,  ...,  0.0082, -0.0096, -0.0024]))
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's obtain the resulting embedding by subtracting the <code>man</code> embedding from the <code>king</code> embedding and adding the <code>woman</code> embedding.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding_openai</span> <span class="o">=</span> <span class="n">embedding_openai_rey</span> <span class="o">-</span> <span class="n">embedding_openai_hombre</span> <span class="o">+</span> <span class="n">embedding_openai_mujer</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding_openai</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">embedding_openai</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(torch.Size([1536]),
 tensor([-0.0226, -0.0323,  0.0017,  ...,  0.0014, -0.0290, -0.0188]))
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Finally, we compare the obtained result with the embedding of <code>queen</code>. For this, we use the <code>cosine_similarity</code> function provided by the <code>pytorch</code> library.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">similarity_openai</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding_openai</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">embedding_openai_reina</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="w"> </span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;similarity_openai: </span><span class="si">{</span><span class="n">similarity_openai</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>similarity_openai: 0.7564167976379395
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, it is a value very close to 1, so we can say that the result obtained is very similar to the embedding of <code>queen</code></p>
</section>
<section class="section-block-markdown-cell">
<p>If we use English words, we get a result closer to 1</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding_openai_rey</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
<span class="n">embedding_openai_hombre</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
<span class="n">embedding_openai_mujer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
<span class="n">embedding_openai_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">client_openai</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="s2">&quot;queen&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_openai</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">embedding</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding_openai</span> <span class="o">=</span> <span class="n">embedding_openai_rey</span> <span class="o">-</span> <span class="n">embedding_openai_hombre</span> <span class="o">+</span> <span class="n">embedding_openai_mujer</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">similarity_openai</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding_openai</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">embedding_openai_reina</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;similarity_openai: </span><span class="si">{</span><span class="n">similarity_openai</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>similarity_openai: tensor([0.8849])
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>This is normal, since the OpenAI model has been trained with more English texts than Spanish.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Types of Word Embeddings">Types of Word Embeddings<a class="anchor-link" href="#Types of Word Embeddings">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>There are several types of word embeddings, and each of them has its advantages and disadvantages. Let's look at the most important ones.</p>
<ul>
  <li>Word2Vec</li>
  <li>GloVe</li>
  <li>FastText</li>
  <li>BERT</li>
  <li>GPT-2</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h4 id="Word2Vec">Word2Vec<a class="anchor-link" href="#Word2Vec">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Word2Vec is an algorithm used to create word embeddings. This algorithm was created by Google in 2013, and it is one of the most widely used algorithms for creating word embeddings.</p>
<p>It has two variants, <code>CBOW</code> and <code>Skip-gram</code>. <code>CBOW</code> is faster to train, while <code>Skip-gram</code> is more accurate. Let's see how each of them works.</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="CBOW">CBOW<a class="anchor-link" href="#CBOW">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p><code>CBOW</code> or <code>Continuous Bag of Words</code> is an algorithm used to predict a word based on the words surrounding it. For example, if we have the sentence <code>The cat is an animal</code>, the algorithm will try to predict the word <code>cat</code> based on the surrounding words, in this case <code>The</code>, <code>is</code>, <code>an</code>, and <code>animal</code>.</p>
<img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/cbow.webp" alt="CBOW">
<p>In this architecture, the model predicts which word is most likely in the given context. Therefore, words that have the same probability of appearing are considered similar and thus move closer together in the dimensional space.</p>
<p>Let's assume that in a sentence we replace <code>barco</code> with <code>bote</code>, then the model predicts the probability for both and if it turns out to be similar, we can consider the words to be similar.</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Skip-gram">Skip-gram<a class="anchor-link" href="#Skip-gram">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p><code>Skip-gram</code> or <code>Skip-gram with Negative Sampling</code> is an algorithm used to predict the words surrounding a given word. For example, if we have the sentence <code>The cat is an animal</code>, the algorithm will try to predict the words <code>The</code>, <code>is</code>, <code>an</code>, and <code>animal</code> based on the word <code>cat</code>.</p>
<img src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Skip-gram.webp" alt="Skip-gram">
<p>This architecture is similar to that of CBOW, but instead the model works in reverse. The model predicts the context using the given word. Therefore, words that have the same context are considered similar and thus move closer together in the dimensional space.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="GloVe">GloVe<a class="anchor-link" href="#GloVe">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p><code>GloVe</code> or <code>Global Vectors for Word Representation</code> is an algorithm used to create word embeddings. This algorithm was developed by Stanford University in 2014.</p>
<p>Word2Vec ignores the fact that some context words occur more frequently than others and also only takes into account the local context, and therefore, does not capture the global context.</p>
<p>This algorithm uses a co-occurrence matrix to create the word embeddings. This co-occurrence matrix is a matrix that contains the number of times each word appears alongside each of the other words in the vocabulary.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="FastText">FastText<a class="anchor-link" href="#FastText">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p><code>FastText</code> is an algorithm used to create word embeddings. This algorithm was created by Facebook in 2016.</p>
<p>One of the main disadvantages of <code>Word2Vec</code> and <code>GloVe</code> is that they cannot encode unknown words or out-of-vocabulary words.</p>
<p>So, to deal with this problem, Facebook proposed a model <code>FastText</code>. It is an extension of <code>Word2Vec</code> and follows the same <code>Skip-gram</code> and <code>CBOW</code> models. However, unlike <code>Word2Vec</code> which feeds whole words into the neural network, <code>FastText</code> first splits the words into several subwords (or <code>n-grams</code>) and then feeds them into the neural network.</p>
<p>For example, if the value of <code>n</code> is 3 and the word is <code>manzana</code> then its tri-gram will be [<code>&#x3C;ma</code>, <code>man</code>, <code>anz</code>, <code>nza</code>, <code>zan</code>, <code>ana</code>, <code>na&#x3E;</code>] and its word embedding will be the sum of the vector representation of these tri-grams. Here, the hyperparameters <code>min_n</code> and <code>max_n</code> are considered as 3 and the characters <code>&#x3C;</code> and <code>&#x3E;</code> represent the beginning and end of the word.</p>
<p>Therefore, using this methodology, unknown words can be represented in vector form, as they have a high probability that their <code>n-grams</code> are also present in other words.</p>
<p>This algorithm is an improvement over <code>Word2Vec</code>, as it not only takes into account the words surrounding a word, but also considers the <code>n-grams</code> of the word. For example, if we have the word <code>gato</code>, it also takes into account the <code>n-grams</code> of the word, in this case <code>ga</code>, <code>at</code>, and <code>to</code>, for <code>n = 2</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Limitations of Word Embeddings">Limitations of Word Embeddings<a class="anchor-link" href="#Limitations of Word Embeddings">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Word embedding techniques have yielded decent results, but the problem is that the approach is not accurate enough. They do not take into account the order in which words appear, leading to a loss of syntactic and semantic understanding of the sentence.</p>
<p>For example, <code>You go there to teach, not to play</code> and <code>You go there to play, not to teach</code> Both sentences will have the same representation in the vector space, but they do not mean the same thing.</p>
<p>Moreover, the word embedding model cannot provide satisfactory results on a large amount of text data, as the same word can have a different meaning in a different sentence depending on the context of the sentence.</p>
<p>For example, <code>I&#x27;m going to sit on the bench</code> and <code>I&#x27;m going to do some errands at the bank</code>. In both sentences, the word <code>bank</code> has different meanings.</p>
<p>Therefore, we require a type of representation that can retain the contextual meaning of the word present in a sentence.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Sentence embeddings">Sentence embeddings<a class="anchor-link" href="#Sentence embeddings">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>The sentence embedding is similar to the word embedding, but instead of words, it encodes the entire sentence into the vector representation.</p>
<p>A simple way to obtain sentence embeddings is by averaging the word embeddings of all the words present in the sentence. But they are not accurate enough.</p>
<p>Some of the most advanced models for sentence embedding are <code>ELMo</code>, <code>InferSent</code>, and <code>Sentence-BERT</code></p>
</section>
<section class="section-block-markdown-cell">
<h3 id="ELMo">ELMo<a class="anchor-link" href="#ELMo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p><code>ELMo</code> or <code>Embeddings from Language Models</code> is a sentence embedding model created by the Allen University in 2018. It uses a deep bidirectional LSTM network to produce vector representations. <code>ELMo</code> can represent unknown words or out-of-vocabulary words in vector form since it is character-based.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="InferSent">InferSent<a class="anchor-link" href="#InferSent">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p><code>InferSent</code> is a sentence embedding model created by Facebook in 2017. It uses a deep bidirectional LSTM network to produce a vector representation. <code>InferSent</code> can represent unknown or out-of-vocabulary words in vector form, as it is character-based. Sentences are encoded into a 4096-dimensional vector representation.</p>
<p>The model training is performed on the Stanford Natural Language Inference (<code>SNLI</code>) dataset. This dataset is labeled and written by humans for around 500K sentence pairs.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Sentence-BERT">Sentence-BERT<a class="anchor-link" href="#Sentence-BERT">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p><code>Sentence-BERT</code> is a sentence embedding model created by the University of London in 2019. It uses a deep bidirectional LSTM network to produce vector representations. <code>Sentence-BERT</code> can represent unknown or out-of-vocabulary words in vector form since it is character-based. Sentences are encoded into a 768-dimensional vector representation.</p>
<p>The state-of-the-art NLP model <code>BERT</code> excels in Semantic Textual Similarity tasks, but the issue is that it would take a long time for a huge corpus (65 hours for 10,000 sentences), as it requires both sentences to be fed into the network, which increases the computation by an enormous factor.</p>
<p>Therefore, <code>Sentence-BERT</code> is a modification of the <code>BERT</code> model.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Training a word2vec model with gensim">Training a word2vec model with gensim<a class="anchor-link" href="#Training a word2vec model with gensim">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>To download the dataset we are going to use, you need to install the <code>dataset</code> library from huggingface:</p>
<p>``` bash</p>
<p>pip install datasets```</p>
</section>
<section class="section-block-markdown-cell">
<p>To train the embeddings model, we will use the <code>gensim</code> library. To install it with Conda, we use</p>
<div class='highlight'><pre><code class="language-bash">conda install -c conda-forge gensim
</code></pre></div>
<p>And to install it with pip we use</p>
<div class='highlight'><pre><code class="language-bash">pip install gensim
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>To clean the dataset we have downloaded, we are going to use regular expressions, which is usually already installed in Python, and <code>nltk</code>, which is a natural language processing library. To install it with Conda, we use</p>
<div class='highlight'><pre><code class="language-bash">conda install -c anaconda nltk
</code></pre></div>
<p>And to install it with pip we use</p>
<div class='highlight'><pre><code class="language-bash">pip install nltk
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have everything installed, we can import the libraries we are going to use:</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">gensim.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Word2Vec</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">gensim.parsing.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">strip_punctuation</span><span class="p">,</span> <span class="n">strip_numeric</span><span class="p">,</span> <span class="n">strip_short</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.corpus</span><span class="w"> </span><span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">word_tokenize</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Download the dataset">Download the dataset<a class="anchor-link" href="#Download the dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We are going to download a dataset of texts from the Spanish Wikipedia. To do this, we run the following:</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="w"> </span>
<span class="n">dataset_corpus</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;large_spanish_corpus&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;all_wikis&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see how it is</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset_corpus</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>DatasetDict(&#x7B;
&#x20;&#x20;&#x20;&#x20;train: Dataset(&#x7B;
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;features: [&#x27;text&#x27;],
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;num_rows: 28109484
&#x20;&#x20;&#x20;&#x20;&#x7D;)
&#x7D;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, the dataset has more than 28 million texts. Let's take a look at some of them:</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset_corpus</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x27;¡Bienvenidos!&#x27;,
 &#x27;Ir a los contenidos»&#x27;,
 &#x27;= Contenidos =&#x27;,
 &#x27;&#x27;,
 &#x27;Portada&#x27;,
 &#x27;Tercera Lengua más hablada en el mundo.&#x27;,
 &#x27;La segunda en número de habitantes en el mundo occidental.&#x27;,
 &#x27;La de mayor proyección y crecimiento día a día.&#x27;,
 &#x27;El español es, hoy en día, nombrado en cada vez más contextos, tomando realce internacional como lengua de cultura y civilización siempre de mayor envergadura.&#x27;,
 &#x27;Ejemplo de ello es que la comunidad minoritaria más hablada en los Estados Unidos es precisamente la que habla idioma español.&#x27;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As there are many examples, we are going to create a subset of 10 million examples to be able to work faster:</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">subset</span> <span class="o">=</span> <span class="n">dataset_corpus</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10000000</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Cleaning the dataset">Cleaning the dataset<a class="anchor-link" href="#Cleaning the dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now we download the <code>stopwords</code> from <code>nltk</code>, which are words that do not provide information and that we are going to remove from the texts.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[nltk_data] Downloading package stopwords to
[nltk_data]     /home/wallabot/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>True
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we are going to download the <code>punkt</code> from <code>nltk</code>, which is a <code>tokenizer</code> that will allow us to split the texts into sentences.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[nltk_data] Downloading package punkt to /home/wallabot/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>True
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We create a function to clean the data. This function will:</p>
<ul>
  <li>convert the text to lowercase</li>
  <li>Remove the URLs</li>
  <li>Remove mentions of social media such as <code>@twitter</code> and <code>#hashtag</code></li>
  <li>Remove punctuation marks</li>
  <li>Remove the numbers</li>
  <li>Remove short words</li>
  <li>Remove the stop words</li>
</ul>
<p>Since we are using a huggingface dataset, the texts are in <code>dict</code> format, so we return a dictionary.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">clean_text</span><span class="p">(</span><span class="n">sentence_batch</span><span class="p">):</span>
<span class="w">    </span><span class="c1"># extrae el texto de la entrada</span>
<span class="w">    </span><span class="n">text_list</span> <span class="o">=</span> <span class="n">sentence_batch</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">    </span><span class="n">cleaned_text_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="w">    </span><span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">text_list</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># Convierte el texto a minúsculas</span>
<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Elimina URLs</span>
<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;http\S+|www\S+|https\S+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">MULTILINE</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Elimina las menciones @ y &#39;#&#39; de las redes sociales</span>
<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\@\w+|\#\w+&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Elimina los caracteres de puntuación</span>
<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">strip_punctuation</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Elimina los números</span>
<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">strip_numeric</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Elimina las palabras cortas</span>
<span class="w">        </span><span class="n">text</span> <span class="o">=</span> <span class="n">strip_short</span><span class="p">(</span><span class="n">text</span><span class="p">,</span><span class="n">minsize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">        </span><span class="c1"># Elimina las palabras comunes (stop words)</span>
<span class="w">        </span><span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;spanish&#39;</span><span class="p">))</span>
<span class="w">        </span><span class="n">word_tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="w">        </span><span class="n">filtered_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_tokens</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
<span class="w"> </span>
<span class="w">        </span><span class="n">cleaned_text_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filtered_text</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="c1"># Devuelve el texto limpio</span>
<span class="w">    </span><span class="k">return</span> <span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">cleaned_text_list</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We apply the function to the data</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentences_corpus</span> <span class="o">=</span> <span class="n">subset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">clean_text</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Map:   0%|          | 0/10000000 [00:00&amp;lt;?, ? examples/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's save the filtered dataset to a file so we don't have to run the cleaning process again.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentences_corpus</span><span class="o">.</span><span class="n">save_to_disk</span><span class="p">(</span><span class="s2">&quot;sentences_corpus&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Saving the dataset (0/4 shards):   0%|          | 0/15000000 [00:00&amp;lt;?, ? examples/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>To load it we can do</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_from_disk</span>
<span class="n">sentences_corpus</span> <span class="o">=</span> <span class="n">load_from_disk</span><span class="p">(</span><span class="s1">&#39;sentences_corpus&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we will have a list of lists, where each list is a tokenized sentence and without stopwords. That is, we have a list of sentences, and each sentence is a list of words. Let's see how it looks:</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;La frase &quot;</span><span class="si">{</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&quot; se convierte en la lista de palabras &quot;</span><span class="si">{</span><span class="n">sentences_corpus</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>La frase &quot;¡Bienvenidos!&quot; se convierte en la lista de palabras &quot;[&#x27;¡bienvenidos&#x27;]&quot;
La frase &quot;Ir a los contenidos»&quot; se convierte en la lista de palabras &quot;[&#x27;ir&#x27;, &#x27;contenidos&#x27;, &#x27;»&#x27;]&quot;
La frase &quot;= Contenidos =&quot; se convierte en la lista de palabras &quot;[&#x27;contenidos&#x27;]&quot;
La frase &quot;&quot; se convierte en la lista de palabras &quot;[]&quot;
La frase &quot;Portada&quot; se convierte en la lista de palabras &quot;[&#x27;portada&#x27;]&quot;
La frase &quot;Tercera Lengua más hablada en el mundo.&quot; se convierte en la lista de palabras &quot;[&#x27;tercera&#x27;, &#x27;lengua&#x27;, &#x27;hablada&#x27;, &#x27;mundo&#x27;]&quot;
La frase &quot;La segunda en número de habitantes en el mundo occidental.&quot; se convierte en la lista de palabras &quot;[&#x27;segunda&#x27;, &#x27;número&#x27;, &#x27;habitantes&#x27;, &#x27;mundo&#x27;, &#x27;occidental&#x27;]&quot;
La frase &quot;La de mayor proyección y crecimiento día a día.&quot; se convierte en la lista de palabras &quot;[&#x27;mayor&#x27;, &#x27;proyección&#x27;, &#x27;crecimiento&#x27;, &#x27;día&#x27;, &#x27;día&#x27;]&quot;
La frase &quot;El español es, hoy en día, nombrado en cada vez más contextos, tomando realce internacional como lengua de cultura y civilización siempre de mayor envergadura.&quot; se convierte en la lista de palabras &quot;[&#x27;español&#x27;, &#x27;hoy&#x27;, &#x27;día&#x27;, &#x27;nombrado&#x27;, &#x27;cada&#x27;, &#x27;vez&#x27;, &#x27;contextos&#x27;, &#x27;tomando&#x27;, &#x27;realce&#x27;, &#x27;internacional&#x27;, &#x27;lengua&#x27;, &#x27;cultura&#x27;, &#x27;civilización&#x27;, &#x27;siempre&#x27;, &#x27;mayor&#x27;, &#x27;envergadura&#x27;]&quot;
La frase &quot;Ejemplo de ello es que la comunidad minoritaria más hablada en los Estados Unidos es precisamente la que habla idioma español.&quot; se convierte en la lista de palabras &quot;[&#x27;ejemplo&#x27;, &#x27;ello&#x27;, &#x27;comunidad&#x27;, &#x27;minoritaria&#x27;, &#x27;hablada&#x27;, &#x27;unidos&#x27;, &#x27;precisamente&#x27;, &#x27;habla&#x27;, &#x27;idioma&#x27;, &#x27;español&#x27;]&quot;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Training the word2vec model">Training the word2vec model<a class="anchor-link" href="#Training the word2vec model">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We are going to train an embedding model that will convert words into vectors. For this, we will use the <code>gensim</code> library and its <code>Word2Vec</code> model.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">sentences_corpus</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="n">dim_embedding</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">window_size</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># 5 palabras a la izquierda y 5 palabras a la derecha</span>
<span class="n">min_count</span> <span class="o">=</span> <span class="mi">5</span>   <span class="c1"># Ignora las palabras con frecuencia menor a 5</span>
<span class="n">workers</span> <span class="o">=</span> <span class="mi">4</span>    <span class="c1"># Número de hilos de ejecución</span>
<span class="n">sg</span> <span class="o">=</span> <span class="mi">1</span>        <span class="c1"># 0 para CBOW, 1 para Skip-gram</span>
<span class="w"> </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="n">dim_embedding</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_count</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="n">sg</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>This model has been trained on the CPU, since <code>gensim</code> does not have an option to perform training on the GPU and even so, it took X minutes to train the model on my computer. Although the embedding dimension we chose is only 100 (as opposed to the size of OpenAI's embeddings which is 1536), this is not too long a time, given that the dataset has 10 million sentences.</p>
<p>Large language models are trained with datasets consisting of billions of sentences, so it's normal for training an embeddings model with a dataset of 10 million sentences to take several minutes.</p>
</section>
<section class="section-block-markdown-cell">
<p>Once the model is trained, we save it to a file so that we can use it in the future.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;word2vec.model&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>If we wanted to load it in the future, we could do so with</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;word2vec.model&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Evaluation of the word2vec model">Evaluation of the word2vec model<a class="anchor-link" href="#Evaluation of the word2vec model">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Let's look at the most similar words for some words</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;perro&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[(&#x27;gato&#x27;, 0.7948548197746277),
 (&#x27;perros&#x27;, 0.77247554063797),
 (&#x27;cachorro&#x27;, 0.7638891339302063),
 (&#x27;hámster&#x27;, 0.7540281414985657),
 (&#x27;caniche&#x27;, 0.7514827251434326),
 (&#x27;bobtail&#x27;, 0.7492328882217407),
 (&#x27;mastín&#x27;, 0.7491254210472107),
 (&#x27;lobo&#x27;, 0.7312178611755371),
 (&#x27;semental&#x27;, 0.7292628288269043),
 (&#x27;sabueso&#x27;, 0.7290207147598267)]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;gato&#39;</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[(&#x27;conejo&#x27;, 0.8148329854011536),
 (&#x27;zorro&#x27;, 0.8109457492828369),
 (&#x27;perro&#x27;, 0.7948548793792725),
 (&#x27;lobo&#x27;, 0.7878773808479309),
 (&#x27;ardilla&#x27;, 0.7860757112503052),
 (&#x27;mapache&#x27;, 0.7817519307136536),
 (&#x27;huiña&#x27;, 0.766639232635498),
 (&#x27;oso&#x27;, 0.7656188011169434),
 (&#x27;mono&#x27;, 0.7633568644523621),
 (&#x27;camaleón&#x27;, 0.7623056769371033)]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now let's look at the example where we check the similarity of the word <code>queen</code> with the result of subtracting the word <code>man</code> from the word <code>king</code> and adding the word <code>woman</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding_hombre</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;hombre&#39;</span><span class="p">]</span>
<span class="n">embedding_mujer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;mujer&#39;</span><span class="p">]</span>
<span class="n">embedding_rey</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;rey&#39;</span><span class="p">]</span>
<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;reina&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_rey</span> <span class="o">-</span> <span class="n">embedding_hombre</span> <span class="o">+</span> <span class="n">embedding_mujer</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="w"> </span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding_reina</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">embedding_reina</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">similarity</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>tensor([0.8156])
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, there is quite a bit of similarity</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Visualization of the embeddings">Visualization of the embeddings<a class="anchor-link" href="#Visualization of the embeddings">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Let's visualize the embeddings. First, we'll obtain the vectors and words from the model.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vectors</span>
<span class="n">words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Since the dimension of the embeddings is 100, to be able to visualize them in 2 or 3 dimensions we have to reduce the dimension. For this, we will use <code>PCA</code> (faster) or <code>TSNE</code> (more accurate) from <code>sklearn</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>
<span class="w"> </span>
<span class="n">dimmesions</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">dimmesions</span><span class="p">)</span>
<span class="n">reduced_embeddings_PCA</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">TSNE</span>
<span class="w"> </span>
<span class="n">dimmesions</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">dimmesions</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">reduced_embeddings_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[t-SNE] Computing 121 nearest neighbors...
[t-SNE] Indexed 493923 samples in 0.013s...
[t-SNE] Computed neighbors for 493923 samples in 377.143s...
[t-SNE] Computed conditional probabilities for sample 1000 / 493923
[t-SNE] Computed conditional probabilities for sample 2000 / 493923
[t-SNE] Computed conditional probabilities for sample 3000 / 493923
[t-SNE] Computed conditional probabilities for sample 4000 / 493923
[t-SNE] Computed conditional probabilities for sample 5000 / 493923
[t-SNE] Computed conditional probabilities for sample 6000 / 493923
[t-SNE] Computed conditional probabilities for sample 7000 / 493923
[t-SNE] Computed conditional probabilities for sample 8000 / 493923
[t-SNE] Computed conditional probabilities for sample 9000 / 493923
[t-SNE] Computed conditional probabilities for sample 10000 / 493923
[t-SNE] Computed conditional probabilities for sample 11000 / 493923
[t-SNE] Computed conditional probabilities for sample 12000 / 493923
[t-SNE] Computed conditional probabilities for sample 13000 / 493923
[t-SNE] Computed conditional probabilities for sample 14000 / 493923
[t-SNE] Computed conditional probabilities for sample 15000 / 493923
[t-SNE] Computed conditional probabilities for sample 16000 / 493923
[t-SNE] Computed conditional probabilities for sample 17000 / 493923
[t-SNE] Computed conditional probabilities for sample 18000 / 493923
[t-SNE] Computed conditional probabilities for sample 19000 / 493923
[t-SNE] Computed conditional probabilities for sample 20000 / 493923
[t-SNE] Computed conditional probabilities for sample 21000 / 493923
[t-SNE] Computed conditional probabilities for sample 22000 / 493923
...
[t-SNE] Computed conditional probabilities for sample 493923 / 493923
[t-SNE] Mean sigma: 0.275311
[t-SNE] KL divergence after 250 iterations with early exaggeration: 117.413788
[t-SNE] KL divergence after 300 iterations: 5.774648
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we visualize them in 2 dimensions with <code>matplotlib</code>. We are going to visualize the dimensionality reduction we have done with <code>PCA</code> and <code>TSNE</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="w"> </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">200</span><span class="p">]):</span> <span class="c1"># Limitar a las primeras 200 palabras</span>
<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_PCA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Embeddings (PCA)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;Figure size 1000x1000 with 1 Axes&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">[:</span><span class="mi">200</span><span class="p">]):</span> <span class="c1"># Limitar a las primeras 200 palabras</span>
<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="w">    </span><span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">reduced_embeddings_tsne</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<span class="w">                 </span><span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;bottom&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;Figure size 1000x1000 with 1 Axes&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Using Pretrained Models with HuggingFace">Using Pretrained Models with HuggingFace<a class="anchor-link" href="#Using Pretrained Models with HuggingFace">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>To use pre-trained <code>embedding</code> models, we will use the <code>transformers</code> library from <code>huggingface</code>. To install it with Conda, we use</p>
<div class='highlight'><pre><code class="language-bash">conda install -c conda-forge transformers
</code></pre></div>
<p>And to install it with pip we use</p>
<div class='highlight'><pre><code class="language-bash">pip install transformers
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>With the <code>feature-extraction</code> task from <code>huggingface</code>, we can use pretrained models to obtain word embeddings. To do this, we first import the necessary library.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We are going to obtain the <code>embeddings</code> from <code>BERT</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;feature-extraction&quot;</span><span class="p">,</span><span class="n">framework</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span><span class="n">model</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's look at the <code>embeddings</code> of the word <code>king</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;rey&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">embedding</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>torch.Size([3, 768])
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, we obtain a vector of <code>768</code> dimensions, that is, the <code>embeddings</code> of <code>BERT</code> have <code>768</code> dimensions. On the other hand, we see that it has 3 <code>embedding</code> vectors, this is because <code>BERT</code> adds a token at the beginning and another at the end of the sentence, so we are only interested in the middle vector.</p>
</section>
<section class="section-block-markdown-cell">
<p>Let's redo the example where we check the similarity of the word <code>queen</code> with the result of subtracting the word <code>man</code> from the word <code>king</code> and adding the word <code>woman</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding_hombre</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">embedding_mujer</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">embedding_rey</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="s2">&quot;queen&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_rey</span> <span class="o">-</span> <span class="n">embedding_hombre</span> <span class="o">+</span> <span class="n">embedding_mujer</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see the similarity</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="w"> </span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">embedding_reina</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embedding_reina</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">embedding_reina</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">similarity</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>/tmp/ipykernel_33343/4248442045.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
&#x20;&#x20;embedding = torch.tensor(embedding).unsqueeze(0)
/tmp/ipykernel_33343/4248442045.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
&#x20;&#x20;embedding_reina = torch.tensor(embedding_reina).unsqueeze(0)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>0.742547333240509
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Using the <code>embeddings</code> of <code>BERT</code> also yields a result very close to 1</p>
</section>