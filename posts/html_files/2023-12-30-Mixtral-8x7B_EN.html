<section class="section-block-markdown-cell">
<h1 id="Mixtral-8x7B MoE">Mixtral-8x7B MoE<a class="anchor-link" href="#Mixtral-8x7B MoE">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>For me, the best description of <code>mixtral-8x7b</code> is the following image</p>
<img src="https://images.maximofn.com/mixtral-gemini.webp" alt="mixtral-gemini">
<p>There was only a very short gap between the release of <code>gemini</code> and the release of <code>mixtral-8x7b</code>. For the first two days after <code>gemini</code> launched, there was quite a bit of talk about that model, but as soon as <code>mixtral-8x7b</code> came out, <code>gemini</code> was completely forgotten, and the entire community couldn't stop talking about <code>mixtral-8x7b</code>.</p>
<p>And it's no wonder, looking at its benchmarks, we can see that it is on par with models like <code>llama2-70B</code> and <code>GPT3.5</code>, but with the difference being that while <code>mixtral-8x7b</code> has only 46.7B parameters, <code>llama2-70B</code> has 70B and <code>GPT3.5</code> has 175B.</p>
<img src="https://images.maximofn.com/mixtral_8x7b_benchmark.webp" alt="mixtral benchmarks">
</section>
<section class="section-block-markdown-cell">
<h2 id="Number of parameters">Number of parameters<a class="anchor-link" href="#Number of parameters">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>As the name suggests, <code>mixtral-8x7b</code> is a set of 8 models with 7B parameters each, so we might think it has 56B parameters (7Bx8), but that's not the case. As <a href="https://twitter.com/karpathy/status/1734251375163511203">Andrej Karpathy</a> explains, only the <code>Feed forward</code> blocks of the transformers are multiplied by 8, while the rest of the parameters are shared among the 8 models. Therefore, the final model has 46.7B parameters.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Mixture of Experts (MoE)">Mixture of Experts (MoE)<a class="anchor-link" href="#Mixture of Experts (MoE)">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>As we have said, the model is a set of 8 models with 7B parameters, hence <code>MoE</code>, which stands for <code>Mixture of Experts</code>. Each of the 8 models is trained independently, but during inference, a router decides which model's output will be used.</p>
<p>In the following image we can see what the architecture of a <code>Transformer</code> looks like.</p>
<img src="https://images.maximofn.com/transformer-scaled.webp" alt="transformer">
<p>If you don't know it, that's fine. The important thing is that this architecture consists of an encoder and a decoder.</p>
<img src="https://images.maximofn.com/transformer-encoder-decoder.webp" alt="transformer-encoder-decoder">
<p>LLMs are models that only have the decoder, so they do not have an encoder. You can see that in the architecture there are three attention modules, one of which actually connects the encoder with the decoder. But since LLMs do not have an encoder, the attention module that connects the decoder and the decoder is not necessary.</p>
<img src="https://images.maximofn.com/transformer-decoder.webp" alt="transformer-decoder">
<p>Now that we know how the architecture of an LLM works, we can look at the architecture of <code>mixtral-8x7b</code>. In the following image, we can see the architecture of the model.</p>
<img src="https://images.maximofn.com/MoE_architecture.webp" alt="MoE architecture">
<p>As can be seen, the architecture consists of the decoder of a 7B parameter <code>Transformer</code>, except that the <code>Feed forward</code> layer consists of 8 <code>Feed forward</code> layers with a router that chooses which of the 8 <code>Feed forward</code> layers will be used. In the previous image, only four <code>Feed forward</code> layers are shown, I suppose this is to simplify the diagram, but in reality there are 8 <code>Feed forward</code> layers. Two paths for two different words are also visible, the word <code>More</code> and the word <code>Parameters</code>, and how the router chooses which <code>Feed forward</code> layer will be used for each word.</p>
</section>
<section class="section-block-markdown-cell">
<p>Looking at the architecture, we can understand why the model has 46.7B parameters and not 56B. As we have said, only the <code>Feed forward</code> blocks are multiplied by 8, the rest of the parameters are shared among the 8 models.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Using Mixtral-8x7b in the Cloud">Using Mixtral-8x7b in the Cloud<a class="anchor-link" href="#Using Mixtral-8x7b in the Cloud">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Unfortunately, using <code>mixtral-8x7b</code> locally is complicated due to the following hardware requirements:</p>
<ul>
  <li>float32: VRAM > 180 GB, that is, since each parameter occupies 4 bytes, we need 46.7B * 4 = 186.8 GB of VRAM just to store the model, in addition to this, we must add the VRAM required to store the input and output data</li>
  <li>float16: VRAM > 90 GB, in this case each parameter takes up 2 bytes, so we need 46.7B * 2 = 93.4 GB of VRAM just to store the model, and on top of that, we need to add the VRAM required to store the input and output data</li>
  <li>8-bit: VRAM > 45 GB, here each parameter takes up 1 byte, so we need 46.7B * 1 = 46.7 GB of VRAM just to store the model, in addition to that, we need to add the VRAM required to store the input and output data* 4-bit: VRAM > 23 GB, here each parameter takes up 0.5 bytes, so we need 46.7B * 0.5 = 23.35 GB of VRAM just to store the model, in addition to that, we have to add the VRAM needed to store the input and output data</li>
</ul>
<p>We need some very powerful GPUs to be able to run it, even when we use the model quantized to 4 bits.</p>
</section>
<section class="section-block-markdown-cell">
<p>Therefore, the simplest way to use <code>Mixtral-8x7B</code> is by using it already deployed in the cloud. I have found several sites where you can use it</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Using Mixtral-8x7b in Hugging Face Chat">Using Mixtral-8x7b in Hugging Face Chat<a class="anchor-link" href="#Using Mixtral-8x7b in Hugging Face Chat">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>The first one is on <a href="https://huggingface.co/chat">huggingface chat</a>. To use it, you need to click on the gear icon inside the <code>Current Model</code> box and select <code>Mistral AI - Mixtral-8x7B</code>. Once selected, you can start chatting with the model.</p>
<img src="https://images.maximofn.com/huggingface_chat_01.webp" alt="huggingface_chat_01">
<p>Once inside, select <code>mistralai/Mixtral-8x7B-Instruct-v0.1</code> and finally click the <code>Activate</code> button. Now we can test the model.</p>
<img src="https://images.maximofn.com/huggingface_chat_02.webp" alt="huggingface_chat_02">
<p>As you can see, I asked him in Spanish what <code>MoE</code> is and he explained it to me.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Use of Mixtral-8x7b in Perplexity Labs">Use of Mixtral-8x7b in Perplexity Labs<a class="anchor-link" href="#Use of Mixtral-8x7b in Perplexity Labs">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Another option is to use <a href="https://labs.perplexity.ai/">Perplexity Labs</a>. Once inside, you need to select <code>mixtral-8x7b-instruct</code> from a dropdown menu in the bottom right corner.</p>
<img src="https://images.maximofn.com/perplexity_labs.webp" alt="perplexity_labs">
<p>As you can see, I also asked him in Spanish what <code>MoE</code> is and he explained it to me.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Using Mixtral-8x7b Locally via the Hugging Face API">Using Mixtral-8x7b Locally via the Hugging Face API<a class="anchor-link" href="#Using Mixtral-8x7b Locally via the Hugging Face API">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>A way to use it locally, regardless of the hardware resources you have, is through the huggingface API. For this, you need to install the <code>huggingface-hub</code> library from huggingface.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="n">huggingface</span><span class="o">-</span><span class="n">hub</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Here is an implementation with <code>gradio</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceClient</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">gradio</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">gr</span>
<span class="w"> </span>
<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="s2">&quot;mistralai/Mixtral-8x7B-Instruct-v0.1&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">format_prompt</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">history</span><span class="p">):</span>
<span class="w">  </span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&amp;lt;s&amp;gt;&quot;</span>
<span class="w">  </span><span class="k">for</span> <span class="n">user_prompt</span><span class="p">,</span> <span class="n">bot_response</span> <span class="ow">in</span> <span class="n">history</span><span class="p">:</span>
<span class="w">    </span><span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;[INST] </span><span class="si">{</span><span class="n">user_prompt</span><span class="si">}</span><span class="s2"> [/INST]&quot;</span>
<span class="w">    </span><span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">bot_response</span><span class="si">}</span><span class="s2">&amp;lt;/s&amp;gt; &quot;</span>
<span class="w">  </span><span class="n">prompt</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;[INST] </span><span class="si">{</span><span class="n">message</span><span class="si">}</span><span class="s2"> [/INST]&quot;</span>
<span class="w">  </span><span class="k">return</span> <span class="n">prompt</span>
<span class="w"> </span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">history</span><span class="p">,</span> <span class="n">system_prompt</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,):</span>
<span class="w">    </span><span class="n">temperature</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">temperature</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span> <span class="n">temperature</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="mf">1e-2</span><span class="p">:</span>
<span class="w">        </span><span class="n">temperature</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="w">    </span><span class="n">top_p</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">top_p</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">    </span><span class="n">generate_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span> <span class="n">repetition_penalty</span><span class="o">=</span><span class="n">repetition_penalty</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,)</span>
<span class="w"> </span>
<span class="w">    </span><span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">format_prompt</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">system_prompt</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">history</span><span class="p">)</span>
<span class="w">    </span><span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">text_generation</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">generate_kwargs</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">details</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_full_text</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="w">    </span><span class="n">output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
<span class="w">        </span><span class="n">output</span> <span class="o">+=</span> <span class="n">response</span><span class="o">.</span><span class="n">token</span><span class="o">.</span><span class="n">text</span>
<span class="w">        </span><span class="k">yield</span> <span class="n">output</span>
<span class="w">    </span><span class="k">return</span> <span class="n">output</span>
<span class="w"> </span>
<span class="n">additional_inputs</span><span class="o">=</span><span class="p">[</span>
<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;System Prompt&quot;</span><span class="p">,</span> <span class="n">max_lines</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,),</span>
<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Temperature&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">&quot;Higher values produce more diverse outputs&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Max new tokens&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">1048</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">&quot;The maximum numbers of new tokens&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Top-p (nucleus sampling)&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.90</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">&quot;Higher values sample more low-probability tokens&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Repetition penalty&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">interactive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="s2">&quot;Penalize repeated tokens&quot;</span><span class="p">)</span>
<span class="p">]</span>
<span class="w"> </span>
<span class="n">gr</span><span class="o">.</span><span class="n">ChatInterface</span><span class="p">(</span>
<span class="w">    </span><span class="n">fn</span><span class="o">=</span><span class="n">generate</span><span class="p">,</span>
<span class="w">    </span><span class="n">chatbot</span><span class="o">=</span><span class="n">gr</span><span class="o">.</span><span class="n">Chatbot</span><span class="p">(</span><span class="n">show_label</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_share_button</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show_copy_button</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">likeable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="s2">&quot;panel&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">additional_inputs</span><span class="o">=</span><span class="n">additional_inputs</span><span class="p">,</span>
<span class="w">    </span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Mixtral 46.7B&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="n">concurrency_limit</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">show_api</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<img src="https://images.maximofn.com/Mixtral-8x7B_huggingface_API.webp" alt="Mixtral-8x7B huggingface API">
</section>