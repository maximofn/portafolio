<p>
	<!-- Custom stylesheet, it must be in the same directory as the html file -->
	<!-- Loading mathjax macro --><!-- Load mathjax --><!-- MathJax configuration -->
<p>
<style>
	/* Media query para pantallas grandes */
	@media screen and (min-width: 1000px) {
		.container {
			flex-wrap: nowrap;
			display: flex;
			justify-content: space-between;
			height: 100vh; /* altura del viewport */
		}
		
		.indice {
			width: 25%; /* Ancho del índice en pantallas grandes */
			border-right: 1px solid #ccc;
			max-height: calc(100vh - 40px); /* Ajustar según tus necesidades. Aquí estamos restando el doble del padding para mantener todo visible */
			padding: 20px;
			box-sizing: border-box;
			overflow-y: auto; /* desplazamiento vertical cuando sea necesario */
		}
		
		.contenido {
			width: 70%; /* Ancho del contenido en pantallas grandes */
			max-height: calc(100vh - 40px); /* Ajustar según tus necesidades. Aquí estamos restando el doble del padding para mantener todo visible */
			padding: 20px;
			box-sizing: border-box;
			overflow-y: auto; /* desplazamiento vertical cuando sea necesario */
		}
	}
</style>
<!-- Open div notebook -->
<div id="notebook" class="border-box-sizing" tabindex="-1">
	<!-- Open div notebook container -->
	<div id="notebook-container" class="container">
		<!-- Open div indice -->
		<div class="indice">
			<!-- Open div index header -->
			<div class="cell border-box-sizing text_cell rendered">
				<h2 class="prompt input_prompt">
					<span style="
					color: var(--darkreader-text--heading-color, var(--darkreader-text--heading-2-color,
					var(--darkreader-text--headings-color)));
					font-family: var(--fontFamily); font-size: var(--fontSize);
					font-style: var(--fontStyle, inherit);
					font-weight: var(--fontWeight);
					letter-spacing: var(--letterSpacing);
					text-transform: var(--textTransform);
					background-color: transparent;
					--darkreader-inline-color: var(--darkreader-text--darkreader-text--heading-color,
					var(--darkreader-text--darkreader-text--heading-2-color, var(--darkreader-text--darkreader-text--headings-color)));
					--darkreader-inline-bgcolor: transparent;"
					data-darkreader-inline-color="" data-darkreader-inline-bgcolor="">
						Índice
					</span>
					<a class="anchor-link" style="
					font-family: var(--fontFamily);
					font-size: var(--fontSize);
					font-style: var(--fontStyle, inherit);
					font-weight: var(--fontWeight);
					letter-spacing: var(--letterSpacing);
					text-transform: var(--textTransform);
					background-color: transparent;
					--darkreader-inline-bgcolor: transparent;"
					href="#Índice" data-darkreader-inline-bgcolor="">
					</a>
				</h2>
			<!-- Close div index header -->
			</div>
			<!-- Index body -->
			<div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Tokenization-index">
									<a class="anchor-link" href="#Tokenization">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Tokenization</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Input-embeddings-index">
									<a class="anchor-link" href="#Input-embeddings">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Input embeddings</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Encoder---decoder-index">
									<a class="anchor-link" href="#Encoder---decoder">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Encoder - decoder</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Projection-index">
									<a class="anchor-link" href="#Projection">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Projection</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Encoder-and-decoder-x6-index">
									<a class="anchor-link" href="#Encoder-and-decoder-x6">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Encoder and decoder x6</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Attention---Feed-forward-index">
									<a class="anchor-link" href="#Attention---Feed-forward">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Attention - Feed forward</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Attention-index">
									<a class="anchor-link" href="#Attention">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Attention</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Feed-forward-index">
									<a class="anchor-link" href="#Feed-forward">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Feed forward</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Positonal-encoding-index">
									<a class="anchor-link" href="#Positonal-encoding">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Positonal encoding</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Add-&-Norm-index">
									<a class="anchor-link" href="#Add-&-Norm">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Add & Norm</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Mechanisms-of-attention-index">
									<a class="anchor-link" href="#Mechanisms-of-attention">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Mechanisms of attention</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Multi-head-attention-index">
									<a class="anchor-link" href="#Multi-head-attention">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Multi-head attention</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Scale-dot-product-attention-index">
									<a class="anchor-link" href="#Scale-dot-product-attention">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Scale dot product attention</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h4 id="Endocer-scale-dot-product-attention-index">
									<a class="anchor-link" href="#Endocer-scale-dot-product-attention">
										<p style="margin-left: 80px; font-size: 12px; line-height: 1.5;">Endocer scale dot product attention</p>
									</a>
								</h4>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h5 id="Matmul-index">
									<a class="anchor-link" href="#Matmul">
										<p style="margin-left: 120px; font-size: 12px; line-height: 1.5;">Matmul</p>
									</a>
								</h5>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h5 id="Scale-index">
									<a class="anchor-link" href="#Scale">
										<p style="margin-left: 120px; font-size: 12px; line-height: 1.5;">Scale</p>
									</a>
								</h5>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h5 id="Mask-(opt)-index">
									<a class="anchor-link" href="#Mask-(opt)">
										<p style="margin-left: 120px; font-size: 12px; line-height: 1.5;">Mask (opt)</p>
									</a>
								</h5>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h5 id="Softmax-index">
									<a class="anchor-link" href="#Softmax">
										<p style="margin-left: 120px; font-size: 12px; line-height: 1.5;">Softmax</p>
									</a>
								</h5>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h5 id="Matmul-index">
									<a class="anchor-link" href="#Matmul">
										<p style="margin-left: 120px; font-size: 12px; line-height: 1.5;">Matmul</p>
									</a>
								</h5>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h5 id="Summary-index">
									<a class="anchor-link" href="#Summary">
										<p style="margin-left: 120px; font-size: 12px; line-height: 1.5;">Summary</p>
									</a>
								</h5>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h4 id="Decoder-masked-scale-dot-product-attention-index">
									<a class="anchor-link" href="#Decoder-masked-scale-dot-product-attention">
										<p style="margin-left: 80px; font-size: 12px; line-height: 1.5;">Decoder masked scale dot product attention</p>
									</a>
								</h4>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h5 id="Why-mask-index">
									<a class="anchor-link" href="#Why-mask">
										<p style="margin-left: 120px; font-size: 12px; line-height: 1.5;">Why mask</p>
									</a>
								</h5>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h5 id="Mask-index">
									<a class="anchor-link" href="#Mask">
										<p style="margin-left: 120px; font-size: 12px; line-height: 1.5;">Mask</p>
									</a>
								</h5>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h4 id="Encoder-decoder-scale-dot-product-attention-index">
									<a class="anchor-link" href="#Encoder-decoder-scale-dot-product-attention">
										<p style="margin-left: 80px; font-size: 12px; line-height: 1.5;">Encoder-decoder scale dot product attention</p>
									</a>
								</h4>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Summary-index">
									<a class="anchor-link" href="#Summary">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Summary</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
			</div>
		<!-- Close div indice -->
		</div>
 
		<!-- Content -->
		<div class="contenido">
		<div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><a href="https://colab.research.google.com/github/maximofn/portafolio/blob/main/posts/notebooks_translated/2024-02-23-Transformers-from-top-to-the-bottom_EN.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h1 id="Transformers---from-top-to-the-bottom">
								<a class="anchor-link" href="#Transformers---from-top-to-the-bottom">
									<p style="margin-left: 0px">Transformers - from top to the bottom</p>
								</a>
							</h1>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In this post we are going to look at how Transformers work from top to bottom.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">## Transformer as a black box</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The transformer architecture was created for the translation problem, so let's explain it for that problem.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Imagine the transformer as a black box, which takes a sentence in one language and outputs the same sentence translated into another language.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box.png" alt="Transformer - black box"></p></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Tokenization">
								<a class="anchor-link" href="#Tokenization">
									<p style="margin-left: 10px">Tokenization</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">But as we have seen in the <a href="https://maximofn.com/tokens/" target="_blank">tokens</a> post, language models do not understand words as we do, they need numbers to be able to perform operations. So the original language sentence has to be converted to tokens by a tokenizer, and at the output we need a detokenizer to convert the output tokens to words</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-tokenizers.png" alt="Transformer - black box - tokenizers"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">So the tokenizer creates a sequence of $n_{input-tokens}$ tokens, and the detokenizer receives a sequence of $n_{output-tokens}$ tokens.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Input-embeddings">
								<a class="anchor-link" href="#Input-embeddings">
									<p style="margin-left: 10px">Input embeddings</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In the <a href="https://maximofn.com/embeddings/" target="_blank">embeddings</a> post we saw that embeddings are a way of representing words in a vector space. So the input tokens are passed through an embeddings layer to convert them into vectors.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In a quick summary, the embedding process consists of converting a sequence of numbers (tokens) into a sequence of vectors. So a new vector space is created in which words that have semantic similarity will be close to each other.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2023/12/word_embedding_3_dimmension.webp" alt="word_embedding_3_dimmension"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">If we had $n_{input-tokens}$ tokens, we now have $n_{input-tokens}$ vectors. Each of those vectors has a length of $d_{model}$. That is, each token is converted to a vector representing that token in a $d_{model}$$-dimensional vector space.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Therefore after passing through the embeddings layer, the sequence of $n_{input-tokens}$ tokens becomes an array of ($n_{input-tokens}$$ x $d_{model}$).</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-black-box-input-embeddings.png" alt="Transformer - black box - input embeddings"></p></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Encoder---decoder">
								<a class="anchor-link" href="#Encoder---decoder">
									<p style="margin-left: 10px">Encoder - decoder</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We have seen the transformer acting as a black box, but in reality the transformer is an architecture that is composed of two parts, an encoder and a decoder.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png" alt="Transformer - encoder-decoder"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The encoder is in charge of compressing the information of the input sentence, it creates a latent space where the information of the input sentence is compressed. Then, that compressed information enters the decoder, which knows how to convert that compressed information into a phrase of the output language.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">And how does the decoder convert this compressed information into a sentence of the output language? Well, token by token. To understand it better, let's forget about the output tokens for a moment, let's imagine that we have this architecture</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Transformer - encoder-decoder (no detokenizer)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-no-detokenizer.png)</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">That is, the original language sentence is converted to tokens, these tokens are converted to embeddings, which enter the encoder, which compresses the information, the decoder takes it and converts it into words of the output language.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">So the decoder generates a new word at the output at each step.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer" alt="Transformer - encoder-decoder (no detokenizer)"></p>.gif)</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">But how does the decoder know which word to generate each time? Because it is being passed the phrase it has already translated, and at each step it generates the next word. That is, at each step the decoder receives the phrase it has translated so far, and generates the next word.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">But still, how does it know to generate the first word? Because it is passed a special word that means "start translating", and from there it generates the following words.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">And finally, how does the transformer know that it has to stop generating words? Because when it finishes translating it generates a special word meaning "end of translation", which when it re-enters the transformer makes it not generate any more words.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(no%20detokenizer" alt="Transformer - encoder-decoder (not detokenizer) (input)"></p>%20(input).gif)</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now that we have understood it in words, which is simpler, let's put the detokenizer back to the output</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder.png" alt="Transformer - encoder-decoder"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Therefore the decoder will be generating tokens. To know that it has to start a sentence a special token commonly called <code><b>SOS</b></code> (Start Of Sentence) is inserted, and to know that it has to end it generates another special token commonly called <code><b>EOS</b></code> (End Of Sentence).</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">And just like the encoder, the input token has to go through an embedding layer to convert the tokens into vector representations.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Assuming that each token is equivalent to one word, the translation process would be as follows</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="https://raw.githubusercontent.com/maximofn/portafolio/main/images/Transformer%20-%20encoder-decoder%20(detokenizer" alt="Transformer - encoder-decoder (detokenizer)"></p>.gif)</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">At the moment we have this architecture</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-detokenizer-2.png" alt="Transformer - encoder-decoder (detokenizer)"></p></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Projection">
								<a class="anchor-link" href="#Projection">
									<p style="margin-left: 10px">Projection</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We have said that the decoder receives a token that passes through the embedding layer <code><b>Output embedding</b></code>.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The <code><b>Output decoder</b></code> creates a vector for each token, so at the output of the <code><b>Output decoder</b></code> we have an array of ($n_{output-tokens}$ x $d_{model}$).</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The decoder performs operations, but outputs an array with the same dimension. So it needs to convert that matrix into a token and it does it by means of a linear layer that generates an array with the same dimension as the possible tokens in the language to be translated (output vocabulary).</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">That array corresponds to the logits of each possible token, so it is then passed through a softmax layer that converts those logits into probabilities. That is, we will have the probability that each token is the next token.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-projection.png" alt="Transformer - projection"></p></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Encoder-and-decoder-x6">
								<a class="anchor-link" href="#Encoder-and-decoder-x6">
									<p style="margin-left: 10px">Encoder and decoder x6</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In the original paper they use 6 layers for the encoder and another 6 layers for the decoder. There is no reason why there should be 6, I guess they tried several values and this was the one that worked best for them.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The output of the last encoder is fed to each decoder.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-x6.png" alt="Transformer - encoder-decoder (x6)"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">To simplify the diagram we will represent it as follows from now on</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-Nx.png" alt="Transformer - encoder-decoder (Nx)"></p></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Attention---Feed-forward">
								<a class="anchor-link" href="#Attention---Feed-forward">
									<p style="margin-left: 10px">Attention - Feed forward</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's start to see what's inside the encoder and decoder. Basically there is an attention mechanism and a feed forward layer.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-encoder-decoder-attention-ff.png" alt="Transformer - encoder-decoder - attention-ff"></p></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Attention">
								<a class="anchor-link" href="#Attention">
									<p style="margin-left: 20px">Attention</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We can see that 3 arrows enter in the attention mechanisms. We will see this later when we will see in depth how the attention mechanisms work.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">But for the moment we can say that they are operations that are performed in order to obtain the relationship that exists between tokens (and therefore, the relationship that exists between words).</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Before transformers, recurrent neural networks were used for the translation problem, which consisted of networks that received an input token, processed it and generated another output token. Then a second token was input, processed and another token was output, and so on with all the tokens in the input sequence. The problem with these networks is that when the sentences were very long, when the last tokens were in, the network "forgot" the first tokens. For example in very long sentences, it could happen that the gender of the subject would change throughout the translated sentence. And this is because after many tokens, the network had forgotten whether the subject was masculine or feminine.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">To solve this, the entire sequence is entered into the attention mechanism of the transformers and the relationships (attention) between all the tokens are calculated at once.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">This is very powerful, since in a single calculation the relationship between all the tokens is obtained, no matter how long the sequence is.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">While this is a great advantage and is what has led to transformers now being used in most of the best modern networks, it is also their biggest disadvantage, as the computation of the attention is very computationally expensive. It requires very large matrix multiplications.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">These multiplications are performed between matrices that correspond to the embeddings of each of the tokens by themselves. That is, the matrix representing the embeddings of the tokens is multiplied by itself. In order to perform this multiplication, one of the matrices must be rotated (algebra requirements to be able to multiply matrices). So a matrix is multiplied by itself, if the input sequence has more tokens, the matrices that are multiplied are larger, one in height and one in width, so the memory needed to store these matrices grows quadratically.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">So as the length of the sequences increases, the amount of memory needed to store those matrices grows quadratically. And this is a major limitation today, the amount of memory that GPUs have, which is where these multiplications are usually performed.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In the encoder, a single layer of attention is used to extract the relationships between the input tokens.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Two layers of attention are used in the decoder, one to extract the relationships between the output tokens, and the other to extract the relationships between the encoder tokens and the decoder tokens.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Feed-forward">
								<a class="anchor-link" href="#Feed-forward">
									<p style="margin-left: 20px">Feed forward</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">After the attention layer, the sequence enters a <code><b>Feed forward</b></code> layer, which has two purposes</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">
								<ul>
									<li>One is to add nonlinearities. As we have explained, attention is achieved by matrix multiplications of the tokens of the input sequences. But if no nonlinear layers are applied to a network, in the end, the whole architecture could be summarized in a few linear computations. So neural networks would not be able to solve nonlinear problems. So this layer is added to add nonlinearity.</li>
									<ul><li>Another is feature extraction. Although the attention already extracts features, these are features of the relationships between the tokens. But this <code><b>Feed forward</b></code> layer is responsible for extracting features from the tokens themselves. That is, features are extracted from each token that are considered important for the problem being solved.</li></ul>
								</ul>
							</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Positonal-encoding">
								<a class="anchor-link" href="#Positonal-encoding">
									<p style="margin-left: 10px">Positonal encoding</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We have explained that in the attention layer the relations between the tokens are obtained, that this relation is calculated by matrix multiplications and that these multiplications are performed between the embedding matrix by itself. So in the sentences <code><b>The cat eats fish</b></code> and <code><b>The fish eats cat</b></code>, the relationship between <code><b>the</b></code> and <code><b>cat</b></code> is the same in both sentences, since the relationship is computed by matrix multiplications of the embeddings of <code><b>the</b></code> and <code><b>cat</b></code>.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">However, in the first <code><b>the</b></code> refers to the <code><b>cat</b></code>, while in the second <code><b>the</b></code> refers to the <code><b>fish</b></code>. So in addition to the relationships between the words we need to have some mechanism that tells us their position in the sentence.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In the paper they propose to introduce an attention mechanism that is in charge of adding values to the embedding vectors</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding.png" alt="Transformer - positional encoding"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Where the formula for calculating these values is</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Transformer - positional encoding (formula)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding-formula.png)</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As this is a bit difficult to understand, let's see what a distribution of values of the <code><b>positional encoding</b></code> would look like.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Transformer - positional encoding (diagram)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding-diagram.png)</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The first token will have the values of the first row (the bottom one) added to it, the second token the values of the second row, and so on, which causes a change in the embeddings as shown in the figure. Seen in two dimensions, the waves that are being added can be seen.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">These waves cause that when attention calculations are performed, closer words have more relationship than more distant words.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">But we can think one thing, if the embedding process consists of creating a vector space in which words with the same semantic meaning are close to each other, wouldn't this relationship be broken if values are added to the embeddings?</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">If we look again at the vector space example from before</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2023/12/word_embedding_3_dimmension.webp" alt="word_embedding_3_dimmension"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We can see that the values go more or less from -1000 to 1000 on each axis, while the distribution graph</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Transformer - positional encoding (diagram)](http://maximofn.com/wp-content/uploads/2024/02/Transformer-positional-encoding-diagram.png)</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">values range from -1 to 1, since this is the range of the sine and cosine functions.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">So we are varying in a range between -1 and 1 the values of the embeddings which are two or three orders of magnitude more, so the variation is going to be very small compared to the value of the embeddings.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">So we already have a way to know the relation of the position of the tokens in the phrase</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Add-&-Norm">
								<a class="anchor-link" href="#Add-&-Norm">
									<p style="margin-left: 10px">Add & Norm</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Only one high-level block remains, and that is the <code><b>Add & Norm</b></code> layers.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-Add-norm.png" alt="Transformer - Add & norm"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">These are layers that are added after each attention layer and each feed forward layer. This layer aggregates the output and input of a layer. This is called residual connections and has the following advantages</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">
								<ul>
									<li>During training:</li>
									<ul><li>They reduce the gradient fading problem: When a neural network is very large, in the training process, the gradients become smaller and smaller as you go deeper into the layers. This results in the deeper layers not being able to update their weights well. Residual connections allow gradients to pass directly through the layers, which helps keep them large enough so that the model can continue to learn, even in the deepest layers.</li></ul>
									<ul><li>Enable training of deeper networks: By helping to mitigate the problem of gradient fading, residual connections also facilitate the training of deeper networks, which can lead to better performance.</li></ul>
									<li>During inference:</li>
									<ul><li>They allow the transmission of information between different layers: As the residual connections allow the output of each layer to become the sum of the input and output of the layer, information from the deeper layers is transmitted to the higher level layers. This can be beneficial in many tasks, especially where low-level and high-level information can be useful.</li></ul>
									<ul><li>Improve model robustness: Since residual connections allow layers to learn better in deeper layers, models with residual connections can be more robust to perturbations in the input data.</li></ul>
									<ul><li>They allow the recovery of lost information: If some information is lost during the transformation in some layer, the residual connections can allow this information to be recovered in the subsequent layers.</li></ul>
								</ul>
							</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">This layer is called <code><b>Add & Norm</b></code>, we have seen the <code><b>Add</b></code>, let's see the <code><b>Norm</b></code>. The normalization is added so that adding the input and output does not trigger the values.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We have already seen all the high-level layers of the transformer.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp" alt="transformer"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">so we can now look at the most important part of the paper and the one that gives it its name, the attention mechanisms.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Mechanisms-of-attention">
								<a class="anchor-link" href="#Mechanisms-of-attention">
									<p style="margin-left: 10px">Mechanisms of attention</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Multi-head-attention">
								<a class="anchor-link" href="#Multi-head-attention">
									<p style="margin-left: 20px">Multi-head attention</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Before looking at the actual mechanism of attention we have to look at the multi-head attention.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-multi-head-attention.png" alt="Transformer - multi-head attention"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">When we have explained the high-level layers, we have seen that in the attention layers there are 3 arrows, these are <code><b>Q</b></code>, <code><b>K</b></code> and <code><b>V</b></code>. They are matrices that correspond to the token information, in the case of the encoder attention mechanism, they correspond to the tokens of the original language sentence, and in the case of the decoder attention layer, they correspond to the tokens of the sentence that has been translated so far and of the encoder output.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Actually now we don't care about the origin of the tokens, just keep in mind that they correspond to tokens. As we have explained the tokens are converted to embeddings, so <code><b>Q</b></code>, <code><b>K</b></code> and <code><b>V</b></code> are matrices of size ($n_{tokens}$ x $d_{model}$). Normally the embedding dimension ($d_{model}$) is usually a large number, such as 512, 1024, 2048, etc (it does not have to be a power of 2, these are just examples).</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We have explained that embeddings are vector representations of tokens. That is, tokens are converted to vector spaces in which words with similar semantic meaning are close together.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Therefore, of all these dimensions, some may be related to morphological characteristics, others to syntactic characteristics, others to semantic characteristics, and so on. Therefore, it makes sense to calculate the attention mechanisms between dimensions of embeddings with similar characteristics.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Recall that attentional mechanisms look for similarity between words, so it makes sense that similarity is sought between similar features.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Therefore, before calculating the attention mechanisms, the embedding dimensions are separated into groups of similar characteristics, and the attention mechanisms between these groups are calculated.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">And how is this separation done? It would be necessary to look for similar dimensions, but to do this in a space of 512, 1024, 2048, etc. dimensions is very complicated. Besides, it is not possible to know which characteristics are similar and in each case the characteristics that are considered similar will change.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Linear projections are used to separate the dimensions into groups. That is, the embeddings are passed through linear layers that separate them into groups of similar characteristics. In this way, during the training of the transformer, the weights of the linear layers will change until reaching a point where the grouping is done in an optimal way.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now we may have the doubt of how many groups to divide into. In the original paper they are divided into 8 groups, but there is no reason for them to be 8, I guess they tried several values and this was the one that worked best for them.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Once the embeddings have been divided into similar groups and the attention in the different groups has been calculated, the results are concatenated. This is logical, suppose we have an ebedding of 512 dimensions, and we divide it into 8 groups of 64 dimensions, if we calculate the attention in each of the groups, we will have 8 matrices of attention 64 dimensions, if we concatenate them we will have a matrix of attention 512 dimensions, which is the same dimension that we had at the beginning.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">But concatenation makes all the features belong together. The first 64 dimensions correspond to one feature, the next 64 to another, and so on. So to mix them back together again, a linear layer is passed again that mixes all the features. And that blending is learned during training.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Scale-dot-product-attention">
								<a class="anchor-link" href="#Scale-dot-product-attention">
									<p style="margin-left: 20px">Scale dot product attention</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We come to the most important part of the transformer, the attention mechanism, the <code><b>scaled dot product attention</b></code>.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" alt="Transformer - scaled dot product attention"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" alt="Transformer - scaled dot product attention formula"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As we have seen, in the Transformer architecture there are three attention mechanisms</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp" alt="transformer"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The encoder, the decoder and the encoder-decoder. So let's explain them separately, because although they are almost the same, they have some small differences.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h4 id="Endocer-scale-dot-product-attention">
								<a class="anchor-link" href="#Endocer-scale-dot-product-attention">
									<p style="margin-left: 30px">Endocer scale dot product attention</p>
								</a>
							</h4>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's take another look at the block diagram and the formula</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" alt="Transformer - scaled dot product attention"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" alt="Transformer - scaled dot product attention formula"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">First let's understand why there were three arrows entering the attention layers. If we look at the architecture of the transformer, the encoder input is split into three and enters the attention layer</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp" alt="transformer"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">So <code><b>K</b></code>, <code><b>Q</b></code> and <code><b>V</b></code> are the result of embedding and positional encoding. The same array is put into the attention layer three times. We have to remember that that matrix consisted of a list with all tokens ($n_{tokens}$), and each token was converted into a vector of embeddings of dimension $d_{model}$, so the dimension of the matrix will be ($n_{tokens}$ x $d_{model}$).</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The meaning of <code><b>K</b></code>, <code><b>Q</b></code> and <code><b>V</b></code> comes from the <code><b>key</b></code>, <code><b>query</b></code> and <code><b>value</b></code> databases. The attention mechanism is passed the <code><b>Q</b></code> and <code><b>K</b></code> arrays, i.e., the question and the key, and the output is the <code><b>V</b></code> array, i.e., the answer.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's look at each block separately and we will understand this better.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h5 id="Matmul">
								<a class="anchor-link" href="#Matmul">
									<p style="margin-left: 40px">Matmul</p>
								</a>
							</h5>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">This block corresponds to the matrix multiplication of the matrices <code><b>Q</b></code> and <code><b>K</b></code>. But in order to perform this operation it must be done with the transposed matrix of <code><b>K</b></code>. Since the two matrices have dimension ($n_{tokens}$ x $d_{model}$), in order to multiply them, the matrix <code><b>K</b></code> has to be transposed.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">So we will have a multiplication of a matrix of dimension ($n_{tokens}$$ x $d_{model}$$) by another matrix of dimension ($d_{model}$$ x $n_{tokens}$), so the result will be a matrix of dimension ($n_{tokens}$$ x $n_{tokens}$).</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul.png" alt="Transformer - matmul"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As we can see, the result is a matrix where the diagonal is the multiplication of the embedding of each token by itself, and the rest of the positions are the multiplications between the embeddings of each token.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now let's see why this multiplication is done. In the previous post <a href="https://maximofn.com/embeddings-similarity/" target="_blank">Measure of similarity between embeddings</a> we saw that one way to obtain the similarity between two embedding vectors is by calculating the cosine</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In the previous figure it can be seen that the multiplication between the matrices <code><b>Q</b></code> and <code><b>K</b></code> corresponds to the multiplication of the embeddings of each token. The multiplication between two vectors is performed in the following way</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">$$ "\mathbf{U}" = "\cdot \mathbf{V}". \mathbf{V} = \mathbf{U}| \mathbf{V}| \cos(\theta)$$$.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">That is, we have the multiplication of the norms by their cosine. If the vectors were unitary, that is, their norms were 1, the multiplication of two vectors would be equal to the cosine between both vectors, which is one of the measures of similarity between vectors.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">So as in each position of the resulting matrix we have the multiplication between the embedding vectors of each token, in reality, each position of the matrix will represent the similarity between each token.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Recall what embeddings were, embeddings were vector representations of tokens in a vector space, where tokens with semantic similarity are close together.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">So with this multiplication we have obtained a similarity matrix between the tokens of the phrase</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul-similarity-matrix.png" alt="Transformer - matmul - similarity matrix"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The elements of the diagonal have maximum similarity (green), those of the corners have minimum similarity (red), and the rest of the elements have intermediate similarity.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h5 id="Scale">
								<a class="anchor-link" href="#Scale">
									<p style="margin-left: 40px">Scale</p>
								</a>
							</h5>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's look again at the scaled dot product attention diagram and its formula</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" alt="Transformer - scaled dot product attention"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" alt="Transformer - scaled dot product attention formula"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We had said that if by multiplying <code><b>Q</b></code> by <code><b>K</b></code> we performed the multiplication between the embedding vectors, and that if those vectors had norm 1, the result would be the similarity between the vectors. But since the vectors do not have norm 1, the result can have very high values, so it is normalized by dividing by the square root of the dimension of the embedding vectors.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h5 id="Mask-(opt)">
								<a class="anchor-link" href="#Mask-(opt)">
									<p style="margin-left: 40px">Mask (opt)</p>
								</a>
							</h5>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Masking is optional and is not used in the encoder, so we will not explain it for the moment in order not to confuse the reader.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" alt="Transformer - scaled dot product attention"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" alt="Transformer - scaled dot product attention formula"></p></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h5 id="Softmax">
								<a class="anchor-link" href="#Softmax">
									<p style="margin-left: 40px">Softmax</p>
								</a>
							</h5>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Although we have divided by the square root of the dimension of the embedding vectors, we could do with the similarity between the embedding vectors going between the values 0 and 1, so to make sure of that, we pass through a softmax layer.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" alt="Transformer - scaled dot product attention"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" alt="Transformer - scaled dot product attention formula"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul-similarity-matrix-softmax.png" alt="Transformer - matmul - similarity matrix softmax"></p></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h5 id="Matmul">
								<a class="anchor-link" href="#Matmul">
									<p style="margin-left: 40px">Matmul</p>
								</a>
							</h5>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now that we have a similarity matrix between the embedding vectors, let's multiply it by the <code><b>V</b></code> matrix, which represents the embeddings of the tokens.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul2.png" alt="Transformer - matmul2"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">By multiplying, we obtain</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-matmul2-result.png" alt="Transformer - matmul2 result"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We obtain a matrix with a mixture of embeddings with their similarity. In each row we obtain a mixture of the embeddings, where each element of the embedding is weighted according to the similarity of the token of that row with the rest of the tokens.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In addition we have again a matrix of size ($n_{tokens}$ x $d_{model}$), which is the same dimension we had at the beginning.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h5 id="Summary">
								<a class="anchor-link" href="#Summary">
									<p style="margin-left: 40px">Summary</p>
								</a>
							</h5>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In summary, we can say that the <code><b>scaled dot product attention</b></code> is a mechanism that calculates the similarity between the tokens of a sentence, and from that similarity, calculates an output matrix that corresponds to a mixture of embeddings weighted according to the similarity of the tokens.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h4 id="Decoder-masked-scale-dot-product-attention">
								<a class="anchor-link" href="#Decoder-masked-scale-dot-product-attention">
									<p style="margin-left: 30px">Decoder masked scale dot product attention</p>
								</a>
							</h4>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We look again at the architecture of the transformer</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp" alt="transformer"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As we can see in this case the <code><b>scaled dot product attention</b></code> has the word <code><b>masked</b></code>. First we will explain why this masking is necessary, and then we will see how it is done.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h5 id="Why-mask">
								<a class="anchor-link" href="#Why-mask">
									<p style="margin-left: 40px">Why mask</p>
								</a>
							</h5>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As we have said, the transformer was initially conceived as a translator, but in general, it is an architecture to which you put a sequence and you get another sequence. But at the time of training you have to give it the input sequence and the output sequence, and from there the transformer learns to translate.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">On the other hand, we have said that the transformer generates a new token each time. That is, it is given the input sequence in the encoder and a special start sequence token in the decoder, and from there it generates the first token of the output sequence.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Then the input sequence is put back into the encoder and the token previously generated in the decoder, and from there it generates the second token of the output sequence.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Then the input sequence is put back into the encoder and the two tokens previously generated in the decoder, and from there it generates the third token of the output sequence.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">And so on until it generates a special end-of-sequence token.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">But in training, as the input and output sequences are put into it all at once, we need to mask the tokens that it has not yet generated so that it cannot see them.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h5 id="Mask">
								<a class="anchor-link" href="#Mask">
									<p style="margin-left: 40px">Mask</p>
								</a>
							</h5>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's take another look at the block diagram and the formula</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" alt="Transformer - scaled dot product attention"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" alt="Transformer - scaled dot product attention formula"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The masking is done after the <code><b>Scale</b></code> and before the <code><b>Softmax</b></code>. As we need to mask the "future" tokens what can be done is to multiply the resulting <code><b>Scale</b></code> matrix by an array that has 0 in the positions we want to mask and 1 in the ones we don`t.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-Mask.png" alt="Transformer - Mask"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">By doing this we get the same matrix as before but with masked positions</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-Mask-resutl.png" alt="Transformer - Mask resutl"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now the result of the <code><b>Scaled dot product attention</b></code> is a matrix with the embeddings of the tokens weighted according to the similarity of the tokens, but with the tokens that should not be masked.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h4 id="Encoder-decoder-scale-dot-product-attention">
								<a class="anchor-link" href="#Encoder-decoder-scale-dot-product-attention">
									<p style="margin-left: 30px">Encoder-decoder scale dot product attention</p>
								</a>
							</h4>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We look again at the architecture of the transformer</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp" alt="transformer"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see now that the attention mechanism receives twice the encoder output and once the masked attention of the decoder. So <code><b>K</b></code> and <code><b>V</b></code> are the encoder output, and <code><b>Q</b></code> is the decoder output.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" alt="Transformer - scaled dot product attention"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention-formula.png" alt="Transformer - scaled dot product attention formula"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Therefore in this attention block, first the similarity between the decoder sentence and the encoder sentence is calculated, i.e. the similarity between the sentence that has been translated so far and the original sentence is calculated.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">This similarity is then multiplied by the encoder sentence, i.e. a mixture of the embeddings of the original sentence weighted according to the similarity of the translated sentence so far is obtained.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Summary">
								<a class="anchor-link" href="#Summary">
									<p style="margin-left: 10px">Summary</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We have walked through the transformer from the highest level to the lowest level, so you may already have an understanding of how it works.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2023/12/transformer-scaled.webp" alt="transformer"></p></p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-multi-head-attention.png" alt="Transformer - multi-head attention"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="http://maximofn.com/wp-content/uploads/2024/02/Transformer-scaled-dot-product-attention.png" alt="Transformer - scaled dot product attention"></p></p>
						</div>
					</div>
				</div>
			</div>
		</div>
	<!-- Close class contenido -->
	</div>
<!-- Close div notebook container -->
</div>
<!-- Close div notebook -->
</div>

