<section class="section-block-markdown-cell">
<h1 id="Tokenizadores-de-rostos-abra%C3%A7ados">Tokenizadores de rostos abraçados<a class="anchor-link" href="#Tokenizadores-de-rostos-abra%C3%A7ados">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>A biblioteca Hugging Face <code>tokenizers</code> fornece uma implementação dos tokenizadores mais usados atualmente, com foco no desempenho e na versatilidade. Na postagem <a href="https://maximofn.com/tokens/">tokens</a>, já vimos a importância dos tokens no processamento de texto, pois os computadores não entendem palavras, mas números. Portanto, é necessário converter palavras em números para que os modelos de linguagem possam processá-las.</p>
</section>
<section class="section-block-markdown-cell">
<p>Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..</p>
<h2 id="Instala%C3%A7%C3%A3o">Instalação<a class="anchor-link" href="#Instala%C3%A7%C3%A3o">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para instalar o <code>tokenizers</code> com o pip:</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>tokenizers
<span class="sb">```</span>

para<span class="w"> </span>instalar<span class="w"> </span>o<span class="w"> </span><span class="sb">`</span>tokenizers<span class="sb">`</span><span class="w"> </span>com<span class="w"> </span>o<span class="w"> </span>conda:

<span class="sb">````</span>bash
conda<span class="w"> </span>install<span class="w"> </span>conda-forge::tokenizers
<span class="sb">```</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<h2 id="O-pipeline-de-tokeniza%C3%A7%C3%A3o">O pipeline de tokenização<a class="anchor-link" href="#O-pipeline-de-tokeniza%C3%A7%C3%A3o">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para tokenizar uma sequência, é usado o <code>Tokenizer.encode</code>, que executa as seguintes etapas:</p>
<ul>
<li>Padronização</li>
<li>pré-tokenização</li>
<li>Tokenização</li>
<li>Pós-tokenização</li>
</ul>
<p>Vamos dar uma olhada em cada um deles</p>
</section>
<section class="section-block-markdown-cell">
<p>Para esta postagem, usaremos o conjunto de dados [wikitext-103] (<a href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/">https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/</a>)</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>wget<span class="w"> </span>https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>--2024-02-26 08:14:11--  https://dax-cdn.cdn.appdomain.cloud/dax-wikitext-103/1.0.1/wikitext-103.tar.gz
Resolving dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)... 23.200.169.125
Connecting to dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)|23.200.169.125|:443... connected.
HTTP request sent, awaiting response... </pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>200 OK
Length: 189603606 (181M) [application/x-gzip]
Saving to: ‘wikitext-103.tar.gz’

wikitext-103.tar.gz 100%[===================&gt;] 180,82M  6,42MB/s    in 30s     

2024-02-26 08:14:42 (5,95 MB/s) - ‘wikitext-103.tar.gz’ saved [189603606/189603606]

</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>tar<span class="w"> </span>-xvzf<span class="w"> </span>wikitext-103.tar.gz
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>wikitext-103/
wikitext-103/wiki.test.tokens
wikitext-103/wiki.valid.tokens
wikitext-103/README.txt
wikitext-103/LICENSE.txt
wikitext-103/wiki.train.tokens
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>rm<span class="w"> </span>wikitext-103.tar.gz
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Padroniza%C3%A7%C3%A3o">Padronização<a class="anchor-link" href="#Padroniza%C3%A7%C3%A3o">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>As normalizações são operações aplicadas ao texto antes da tokenização, como a remoção de espaços em branco, a conversão para letras minúsculas, a remoção de caracteres especiais etc. As seguintes normalizações são implementadas no Hugging Face:</p>
<table>
<thead>
<tr>
<th>Normalización</th>
<th>Descripción</th>
<th>Ejemplo</th>
</tr>
</thead>
<tbody>
<tr>
<td>NFD (Normalization for D)</td>
<td>Los caracteres se descomponen por equivalencia canónica</td>
<td><code>â</code> (U+00E2) se descompone en <code>a</code> (U+0061) + <code>^</code> (U+0302)</td>
</tr>
<tr>
<td>NFKD (Normalization Form KD)</td>
<td>Los caracteres se descomponen por compatibilidad</td>
<td><code>ﬁ</code> (U+FB01) se descompone en <code>f</code> (U+0066) + <code>i</code> (U+0069)</td>
</tr>
<tr>
<td>NFC (Normalization Form C)</td>
<td>Los caracteres se descomponen y luego se recomponen por equivalencia canónica</td>
<td><code>â</code> (U+00E2) se descompone en <code>a</code> (U+0061) + <code>^</code> (U+0302) y luego se recompone en <code>â</code> (U+00E2)</td>
</tr>
<tr>
<td>NFKC (Normalization Form KC)</td>
<td>Los caracteres se descomponen por compatibilidad y luego se recomponen por equivalencia canónica</td>
<td><code>ﬁ</code> (U+FB01) se descompone en <code>f</code> (U+0066) + <code>i</code> (U+0069) y luego se recompone en <code>f</code> (U+0066) + <code>i</code> (U+0069)</td>
</tr>
<tr>
<td>Lowercase</td>
<td>Convierte el texto a minúsculas</td>
<td><code>Hello World</code> se convierte en <code>hello world</code></td>
</tr>
<tr>
<td>Strip</td>
<td>Elimina todos los espacios en blanco de los lados especificados (izquierdo, derecho o ambos) del texto</td>
<td><code>Hello World</code> se convierte en <code>Hello World</code></td>
</tr>
<tr>
<td>StripAccents</td>
<td>Elimina todos los símbolos de acento en unicode (se utilizará con NFD por coherencia)</td>
<td><code>á</code> (U+00E1) se convierte en <code>a</code> (U+0061)</td>
</tr>
<tr>
<td>Replace</td>
<td>Sustituye una cadena personalizada o <a href="https://maximofn.com/regular-expressions/">regex</a> y la cambia por el contenido dado</td>
<td><code>Hello World</code> se convierte en <code>Hello Universe</code></td>
</tr>
<tr>
<td>BertNormalizer</td>
<td>Proporciona una implementación del Normalizador utilizado en el BERT original. Las opciones que se pueden configurar son <code>clean_text</code>, <code>handle_chinese_chars</code>, <code>strip_accents</code> y <code>lowercase</code></td>
<td><code>Hello World</code> se convierte en <code>hello world</code></td>
</tr>
</tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Vamos criar um normalizador para ver como ele funciona.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">normalizers</span>

<span class="n">bert_normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">BertNormalizer</span><span class="p">()</span>

<span class="n">input_text</span> <span class="o">=</span> <span class="s2">"Héllò hôw are ü?"</span>
<span class="n">normalized_text</span> <span class="o">=</span> <span class="n">bert_normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
<span class="n">normalized_text</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[1]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'hello how are u?'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Para usar vários normalizadores, podemos usar o método <code>Sequence</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">custom_normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">normalizers</span><span class="o">.</span><span class="n">NFKC</span><span class="p">(),</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">BertNormalizer</span><span class="p">()])</span>

<span class="n">normalized_text</span> <span class="o">=</span> <span class="n">custom_normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
<span class="n">normalized_text</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'hello how are u?'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Para modificar o normalizador de um tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tokenizers</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizers</span><span class="o">.</span><span class="n">BertWordPieceTokenizer</span><span class="p">()</span> <span class="c1"># or any other tokenizer</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">custom_normalizer</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pr%C3%A9-tokeniza%C3%A7%C3%A3o">Pré-tokenização<a class="anchor-link" href="#Pr%C3%A9-tokeniza%C3%A7%C3%A3o">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Pretokenização é o ato de dividir o texto em objetos menores. O pretokenizador dividirá o texto em "palavras" e os tokens finais serão partes dessas palavras.</p>
<p>O PreTokenizer se encarrega de dividir a entrada de acordo com um conjunto de regras. Esse pré-processamento permite que você garanta que o tokenizador não crie tokens em várias "divisões". Por exemplo, se você não quiser ter espaços em branco em um token, poderá ter um pré-tokenizador que divida as palavras em espaços em branco.</p>
<p>Os seguintes pré-tokenizadores são implementados no Hugging Face</p>
<table>
<thead>
<tr>
<th>PreTokenizer</th>
<th>Descripción</th>
<th>Ejemplo</th>
</tr>
</thead>
<tbody>
<tr>
<td>ByteLevel</td>
<td>Divide en espacios en blanco mientras reasigna todos los bytes a un conjunto de caracteres visibles. Esta técnica fue introducida por OpenAI con GPT-2 y tiene algunas propiedades más o menos buenas: Como mapea sobre bytes, un tokenizador que utilice esto sólo requiere 256 caracteres como alfabeto inicial (el número de valores que puede tener un byte), frente a los más de 130.000 caracteres Unicode. Una consecuencia del punto anterior es que es absolutamente innecesario tener un token desconocido usando esto ya que podemos representar cualquier cosa con 256 tokens. Para caracteres no ascii, se vuelve completamente ilegible, ¡pero funciona!</td>
<td><code>Hello my friend, how are you?</code> se divide en <code>Hello</code>, <code>Ġmy</code>, <code>Ġfriend</code>, <code>,</code>, <code>Ġhow</code>, <code>Ġare</code>, <code>Ġyou</code>, <code>?</code></td>
</tr>
<tr>
<td>Whitespace</td>
<td>Divide en límites de palabra usando la siguiente expresión regular: <code>\w+[^\w\s]+</code>. En mi post sobre <a href="https://maximofn.com/regular-expressions/">expresiones regulares</a> puedes entender qué hace</td>
<td><code>Hello there!</code> se divide en <code>Hello</code>, <code>there</code>, <code>!</code></td>
</tr>
<tr>
<td>WhitespaceSplit</td>
<td>Se divide en cualquier carácter de espacio en blanco</td>
<td><code>Hello there!</code> se divide en <code>Hello</code>, <code>there!</code></td>
</tr>
<tr>
<td>Punctuation</td>
<td>Aislará todos los caracteres de puntuación</td>
<td><code>Hello?</code> se divide en <code>Hello</code>, <code>?</code></td>
</tr>
<tr>
<td>Metaspace</td>
<td>Separa los espacios en blanco y los sustituye por un carácter especial "▁" (U+2581)</td>
<td><code>Hello there</code> se divide en <code>Hello</code>, <code>▁there</code></td>
</tr>
<tr>
<td>CharDelimiterSplit</td>
<td>Divisiones en un carácter determinado</td>
<td>Ejemplo con el caracter <code>x</code>: <code>Helloxthere</code> se divide en <code>Hello</code>, <code>there</code></td>
</tr>
<tr>
<td>Digits</td>
<td>Divide los números de cualquier otro carácter</td>
<td><code>Hello123there</code> se divide en <code>Hello</code>, <code>123</code>, <code>there</code></td>
</tr>
<tr>
<td>Split</td>
<td>Pretokenizador versátil que divide según el patrón y el comportamiento proporcionados. El patrón se puede invertir si es necesario. El patrón debe ser una cadena personalizada o una <a href="https://maximofn.com/regular-expressions/">regex</a>. El comportamiento debe ser <code>removed</code>, <code>isolated</code>, <code>merged_with_previous</code>, <code>merged_with_next</code>, <code>contiguous</code>. Para invertir se indica con un booleano</td>
<td>Ejemplo con pattern=<code>" "</code>, behavior=<code>isolated</code>, invert=<code>False</code>: <code>Hello, how are you?</code> se divide en <code>Hello,</code>, <code></code>, <code>how</code>, <code></code>, <code>are</code>, <code></code>, <code>you?</code></td>
</tr>
</tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Vamos criar um pré-tokenizador para ver como ele funciona.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">pre_tokenizers</span>

<span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Digits</span><span class="p">(</span><span class="n">individual_digits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">input_text</span> <span class="o">=</span> <span class="s2">"I paid $30 for the car"</span>
<span class="n">pre_tokenized_text</span> <span class="o">=</span> <span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
<span class="n">pre_tokenized_text</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[5]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[('I paid $', (0, 8)),
 ('3', (8, 9)),
 ('0', (9, 10)),
 (' for the car', (10, 22))]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Para usar vários pré-tokenizadores, podemos usar o método <code>Sequence</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">custom_pre_tokenizer</span> <span class="o">=</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Whitespace</span><span class="p">(),</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Digits</span><span class="p">(</span><span class="n">individual_digits</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>

<span class="n">pre_tokenized_text</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
<span class="n">pre_tokenized_text</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[6]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[('I', (0, 1)),
 ('paid', (2, 6)),
 ('$', (7, 8)),
 ('3', (8, 9)),
 ('0', (9, 10)),
 ('for', (11, 14)),
 ('the', (15, 18)),
 ('car', (19, 22))]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Para modificar o pré-tokenizador de um tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokeniza%C3%A7%C3%A3o">Tokenização<a class="anchor-link" href="#Tokeniza%C3%A7%C3%A3o">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Depois que os textos de entrada tiverem sido normalizados e pré-tokenizados, o tokenizador aplica o modelo aos pré-tokens. Essa é a parte do processo que precisa ser treinada no corpus (ou já foi treinada se for usado um tokenizador pré-treinado).</p>
<p>A função do modelo é dividir as "palavras" em tokens usando as regras que aprendeu. Ele também é responsável por atribuir esses tokens às suas IDs correspondentes no vocabulário do modelo.</p>
<p>O modelo tem um tamanho de vocabulário, ou seja, tem um número finito de tokens, portanto, precisa decompor as palavras e atribuí-las a um desses tokens.</p>
<p>Esse modelo é passado quando o Tokenizer é inicializado. Atualmente, a biblioteca 🤗 Tokenizers é compatível:</p>
<table>
<thead>
<tr>
<th>Modelo</th>
<th>Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td>WordLevel</td>
<td>Este es el algoritmo "clásico" de tokenización. Te permite simplemente asignar palabras a IDs sin nada sofisticado. Tiene la ventaja de ser muy fácil de usar y entender, pero requiere vocabularios extremadamente grandes para una buena cobertura. El uso de este modelo requiere el uso de un PreTokenizer. Este modelo no realiza ninguna elección directamente, simplemente asigna tokens de entrada a IDs.</td>
</tr>
<tr>
<td>BPE (Byte Pair Encoding)</td>
<td>Uno de los algoritmos de tokenización de subpalabras más populares. El Byte-Pair-Encoding funciona empezando con caracteres y fusionando los que se ven juntos con más frecuencia, creando así nuevos tokens. A continuación, trabaja de forma iterativa para construir nuevos tokens a partir de los pares más frecuentes que ve en un corpus. BPE es capaz de construir palabras que nunca ha visto utilizando múltiples subpalabras y, por tanto, requiere vocabularios más pequeños, con menos posibilidades de tener palabras <code>unk</code> (desconocidas).</td>
</tr>
<tr>
<td>WordPiece</td>
<td>Se trata de un algoritmo de tokenización de subpalabras bastante similar a BPE, utilizado principalmente por Google en modelos como BERT. Utiliza un algoritmo codicioso que intenta construir primero palabras largas, dividiéndolas en varios tokens cuando no existen palabras completas en el vocabulario. A diferencia de BPE, que parte de los caracteres y construye tokens lo más grandes posible. Utiliza el famoso prefijo ## para identificar los tokens que forman parte de una palabra (es decir, que no empiezan una palabra).</td>
</tr>
<tr>
<td>Unigram</td>
<td>Unigram es también un algoritmo de tokenización de subpalabras, y funciona tratando de identificar el mejor conjunto de tokens de subpalabras para maximizar la probabilidad de una frase dada. Se diferencia de BPE en que no es un algoritmo determinista basado en un conjunto de reglas aplicadas secuencialmente. En su lugar, Unigram podrá calcular múltiples formas de tokenizar, eligiendo la más probable.</td>
</tr>
</tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Quando você cria um tokenizador, precisa passar a ele o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">models</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">Unigram</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Passaremos o normalizador e o pré-tokenizador que criamos para ele.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">custom_normalizer</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">custom_pre_tokenizer</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora temos que treinar o modelo ou carregar um modelo pré-treinado. Neste caso, vamos treinar um modelo com o corpus que baixamos.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Treinamento-de-modelos">Treinamento de modelos<a class="anchor-link" href="#Treinamento-de-modelos">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Para treinar o modelo, temos vários tipos de "treinadores".</p>
<table>
<thead>
<tr>
<th>Trainer</th>
<th>Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td>WordLevelTrainer</td>
<td>Entrena un tokenizador WordLevel</td>
</tr>
<tr>
<td>BpeTrainer</td>
<td>Entrena un tokenizador BPE</td>
</tr>
<tr>
<td>WordPieceTrainer</td>
<td>Entrena un tokenizador WordPiece</td>
</tr>
<tr>
<td>UnigramTrainer</td>
<td>Entrena un tokenizador Unigram</td>
</tr>
</tbody>
</table>
<p>Quase todos os treinadores têm os mesmos parâmetros, que são:</p>
<ul>
<li>vocab_size: o tamanho do vocabulário final, incluindo todos os tokens e o alfabeto.</li>
<li>show_progress: Mostrar ou não barras de progresso durante o treinamento</li>
<li>special_tokens: Uma lista de tokens especiais dos quais o modelo deve estar ciente.</li>
</ul>
<p>Além desses parâmetros, cada treinador tem seus próprios parâmetros; consulte a documentação <a href="https://huggingface.co/docs/tokenizers/api/trainers">Trainers</a> para obter mais informações.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para treinar, precisamos criar um <code>Trainer</code>. Como o modelo que criamos é um <code>Unigram</code>, criaremos um <code>UnigramTrainer</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers.trainers</span> <span class="kn">import</span> <span class="n">trainers</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="o">.</span><span class="n">UnigramTrainer</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>
    <span class="n">initial_alphabet</span><span class="o">=</span><span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">ByteLevel</span><span class="o">.</span><span class="n">alphabet</span><span class="p">(),</span>
    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">"&lt;PAD&gt;"</span><span class="p">,</span> <span class="s2">"&lt;BOS&gt;"</span><span class="p">,</span> <span class="s2">"&lt;EOS&gt;"</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Depois de criarmos o <code>Trainer</code>, há duas maneiras de entrar, usando o método <code>train</code>, que recebe uma lista de arquivos, ou usando o método <code>train_from_iterator</code>, que recebe um iterador.</p>
</section>
<section class="section-block-markdown-cell">
<h5 id="Treinamento-do-modelo-com-o-m%C3%A9todo-train.">Treinamento do modelo com o método <code>train</code>.<a class="anchor-link" href="#Treinamento-do-modelo-com-o-m%C3%A9todo-train.">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, criamos uma lista de arquivos com o corpus</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"wikitext-103/wiki.</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">.tokens"</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"test"</span><span class="p">,</span> <span class="s2">"train"</span><span class="p">,</span> <span class="s2">"valid"</span><span class="p">]]</span>
<span class="n">files</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[28]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>['wikitext-103/wiki.test.tokens',
 'wikitext-103/wiki.train.tokens',
 'wikitext-103/wiki.valid.tokens']</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E agora treinamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>

</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h5 id="Treinamento-do-modelo-com-o-m%C3%A9todo-train_from_iterator.">Treinamento do modelo com o método <code>train_from_iterator</code>.<a class="anchor-link" href="#Treinamento-do-modelo-com-o-m%C3%A9todo-train_from_iterator.">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, criamos uma função que retorna um iterador.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">iterator</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">line</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, treinamos novamente o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">iterator</span><span class="p">(),</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>

</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h5 id="Treinamento-do-modelo-com-o-m%C3%A9todo-train_from_iterator-a-partir-de-um-conjunto-de-dados-Hugging-Face">Treinamento do modelo com o método <code>train_from_iterator</code> a partir de um conjunto de dados Hugging Face<a class="anchor-link" href="#Treinamento-do-modelo-com-o-m%C3%A9todo-train_from_iterator-a-partir-de-um-conjunto-de-dados-Hugging-Face">¶</a></h5>
</section>
<section class="section-block-markdown-cell">
<p>Se tivéssemos baixado o conjunto de dados Hugging Face, poderíamos ter treinado o modelo diretamente do conjunto de dados.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">datasets</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">"wikitext"</span><span class="p">,</span> <span class="s2">"wikitext-103-raw-v1"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">"train+test+validation"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora podemos criar um iterador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">batch_iterator</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">][</span><span class="s2">"text"</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Treinamos novamente o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">train_from_iterator</span><span class="p">(</span><span class="n">batch_iterator</span><span class="p">(),</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>

</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Salvando-o-modelo">Salvando o modelo<a class="anchor-link" href="#Salvando-o-modelo">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Depois que o modelo tiver sido treinado, ele poderá ser salvo para uso futuro. Para salvar o modelo, é necessário salvá-lo em um arquivo <code>json</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">"wikitext-103-tokenizer.json"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Carregando-o-modelo-pr%C3%A9-treinado">Carregando o modelo pré-treinado<a class="anchor-link" href="#Carregando-o-modelo-pr%C3%A9-treinado">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Podemos carregar um modelo pré-treinado a partir de um <code>json</code> em vez de precisar treiná-lo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s2">"wikitext-103-tokenizer.json"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[36]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>&lt;tokenizers.Tokenizer at 0x7f1dd7784a30&gt;</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Também podemos carregar um modelo pré-treinado disponível no Hugging Face Hub.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">'bert-base-uncased'</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[38]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>&lt;tokenizers.Tokenizer at 0x7f1d64a75e30&gt;</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="P%C3%B3s-processamento">Pós-processamento<a class="anchor-link" href="#P%C3%B3s-processamento">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Talvez queiramos que nosso tokenizador adicione automaticamente tokens especiais, como <code>[CLS]</code> ou <code>[SEP]</code>.</p>
<p>Os seguintes pós-processadores são implementados no Hugging Face</p>
<table>
<thead>
<tr>
<th>PostProcesador</th>
<th>Descripción</th>
<th>Ejemplo</th>
</tr>
</thead>
<tbody>
<tr>
<td>BertProcessing</td>
<td>Este post-procesador se encarga de añadir los tokens especiales que necesita un modelo Bert (<code>SEP</code> y <code>CLS</code>)</td>
<td><code>Hello, how are you?</code> se convierte en <code>[CLS]</code>, <code>Hello</code>, <code>,</code>, <code>how</code>, <code>are</code>, <code>you</code>, <code>?</code>, <code>[SEP]</code></td>
</tr>
<tr>
<td>RobertaProcessing</td>
<td>Este post-procesador se encarga de añadir los tokens especiales que necesita un modelo Roberta (<code>SEP</code> y <code>CLS</code>). También se encarga de recortar los offsets. Por defecto, el ByteLevel BPE puede incluir espacios en blanco en los tokens producidos. Si no desea que las compensaciones incluyan estos espacios en blanco, hay que inicializar este PostProcessor con <code>trim_offsets=True</code>.</td>
<td><code>Hello, how are you?</code> se convierte en <code>&lt;s&gt;</code>, <code>Hello</code>, <code>,</code>, <code>how</code>, <code>are</code>, <code>you</code>, <code>?</code>, <code>&lt;/s&gt;</code></td>
</tr>
<tr>
<td>ElectraProcessing</td>
<td>Añade tokens especiales para ELECTRA</td>
<td><code>Hello, how are you?</code> se convierte en <code>[CLS]</code>, <code>Hello</code>, <code>,</code>, <code>how</code>, <code>are</code>, <code>you</code>, <code>?</code>, <code>[SEP]</code></td>
</tr>
<tr>
<td>TemplateProcessing</td>
<td>Permite crear fácilmente una plantilla para el postprocesamiento, añadiendo tokens especiales y especificando el type_id de cada secuencia/token especial. La plantilla recibe dos cadenas que representan la secuencia única y el par de secuencias, así como un conjunto de tokens especiales a utilizar</td>
<td>Example, when specifying a template with these values: single:<code>[CLS] $A [SEP]</code>, pair: <code>[CLS] $A [SEP] $B [SEP]</code>, special tokens: <code>[CLS]</code>, <code>[SEP]</code>. Input: (<code>I like this</code>, <code>but not this</code>), Output: <code>[CLS] I like this [SEP] but not this [SEP]</code></td>
</tr>
</tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Vamos criar um tokenizador de postagem para ver como ele funciona.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers.processors</span> <span class="kn">import</span> <span class="n">TemplateProcessing</span>

<span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>
    <span class="n">single</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP]"</span><span class="p">,</span>
    <span class="n">pair</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP] $B:1 [SEP]:1"</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[(</span><span class="s2">"[CLS]"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">"[SEP]"</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Para modificar o tokenizador de postagem de um tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">post_processor</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver como funciona</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">"I paid $30 for the car"</span>
<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>

<span class="n">decoded_text</span><span class="o">.</span><span class="n">tokens</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[43]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>['[CLS]', 'i', 'paid', '$', '3', '0', 'for', 'the', 'car', '[SEP]']</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_text1</span> <span class="o">=</span> <span class="s2">"Hello, y'all!"</span>
<span class="n">input_text2</span> <span class="o">=</span> <span class="s2">"How are you?"</span>
<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">input_text2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">decoded_text</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>['[CLS]', 'hell', 'o', ',', 'y', "'", 'all', '!', '[SEP]', 'how', 'are', 'you', '?', '[SEP]']
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se salvarmos o tokenizador agora, o tokenizador de postagem será salvo com ele.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Codifica%C3%A7%C3%A3o">Codificação<a class="anchor-link" href="#Codifica%C3%A7%C3%A3o">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Depois de treinar o tokenizador, podemos usá-lo para tokenizar textos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">"I love tokenizers!"</span>
<span class="n">encoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vejamos o que obtemos ao tokenizar um texto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">encoded_text</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[51]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>tokenizers.Encoding</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Obtemos um objeto do tipo <a href="https://huggingface.co/docs/tokenizers/api/encoding#tokenizers.Encoding">Encoding</a>, contendo os tokens e os ids dos tokens.</p>
</section>
<section class="section-block-markdown-cell">
<p>Os <code>ids</code> são os <code>id</code>s dos tokens no vocabulário do tokenizador.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[52]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[1, 17, 383, 10694, 17, 3533, 3, 586, 2]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Tokens são os tokens aos quais os <code>ids</code> são equivalentes.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">encoded_text</span><span class="o">.</span><span class="n">tokens</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[54]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>['[CLS]', 'i', 'love', 'token', 'i', 'zer', 's', '!', '[SEP]']</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se tivermos várias sequências, poderemos codificá-las todas de uma vez</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">input_text2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">type_ids</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>['[CLS]', 'hell', 'o', ',', 'y', "'", 'all', '!', '[SEP]', 'how', 'are', 'you', '?', '[SEP]']
[1, 2215, 7, 5, 22, 26, 81, 586, 2, 98, 59, 213, 902, 2]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>No entanto, quando você tem várias sequências, é melhor usar o método <code>encode_batch</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">([</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">input_text2</span><span class="p">])</span>

<span class="nb">type</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[86]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>list</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que obtemos uma lista</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>['[CLS]', 'hell', 'o', ',', 'y', "'", 'all', '!', '[SEP]']
[1, 2215, 7, 5, 22, 26, 81, 586, 2]
['[CLS]', 'how', 'are', 'you', '?', '[SEP]']
[1, 98, 59, 213, 902, 2]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Decodifica%C3%A7%C3%A3o">Decodificação<a class="anchor-link" href="#Decodifica%C3%A7%C3%A3o">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Além de codificar os textos de entrada, um Tokenizer também tem um método para decodificar, ou seja, converter os IDs gerados pelo seu modelo de volta para um texto. Isso é feito pelos métodos <code>Tokenizer.decode</code> (para um texto previsto) e <code>Tokenizer.decode_batch</code> (para um lote de previsões).</p>
<p>Os tipos de decodificação que podem ser usados são:</p>
<table>
<thead>
<tr>
<th>Decodificación</th>
<th>Descripción</th>
</tr>
</thead>
<tbody>
<tr>
<td>BPEDecoder</td>
<td>Revierte el modelo BPE</td>
</tr>
<tr>
<td>ByteLevel</td>
<td>Revierte el ByteLevel PreTokenizer. Este PreTokenizer codifica a nivel de byte, utilizando un conjunto de caracteres Unicode visibles para representar cada byte, por lo que necesitamos un Decoder para revertir este proceso y obtener algo legible de nuevo.</td>
</tr>
<tr>
<td>CTC</td>
<td>Revierte el modelo CTC</td>
</tr>
<tr>
<td>Metaspace</td>
<td>Revierte el PreTokenizer de Metaspace. Este PreTokenizer utiliza un identificador especial ▁ para identificar los espacios en blanco, por lo que este Decoder ayuda con la decodificación de estos.</td>
</tr>
<tr>
<td>WordPiece</td>
<td>Revierte el modelo WordPiece. Este modelo utiliza un identificador especial ## para las subpalabras continuas, por lo que este decodificador ayuda a decodificarlas.</td>
</tr>
</tbody>
</table>
<p>O decodificador primeiro converterá os IDs em tokens (usando o vocabulário do tokenizador) e removerá todos os tokens especiais e, em seguida, juntará esses tokens com espaços em branco.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos criar um decodificador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">decoders</span>

<span class="n">decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ByteLevel</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nós o adicionamos ao tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nós decodificamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>

<span class="n">input_text</span><span class="p">,</span> <span class="n">decoded_text</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[81]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>('I love tokenizers!', 'ilovetokenizers!')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">decoded_texts</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode_batch</span><span class="p">([</span><span class="n">encoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">,</span> <span class="n">encoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ids</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">input_text1</span><span class="p">,</span> <span class="n">decoded_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input_text2</span><span class="p">,</span> <span class="n">decoded_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello, y'all! hello,y'all!
How are you? howareyou?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="BERT-tokenizer">BERT tokenizer<a class="anchor-link" href="#BERT-tokenizer">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Com tudo o que aprendemos, vamos criar o tokenizador BERT do zero. O Bert usa o <code>WordPiece</code> como modelo, então o passamos para o inicializador do tokenizador.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tokenizers.models</span> <span class="kn">import</span> <span class="n">WordPiece</span>

<span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">"[UNK]"</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O BERT pré-processa os textos removendo acentos e letras minúsculas. Também usamos um normalizador unicode</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">normalizers</span>
<span class="kn">from</span> <span class="nn">tokenizers.normalizers</span> <span class="kn">import</span> <span class="n">NFD</span><span class="p">,</span> <span class="n">Lowercase</span><span class="p">,</span> <span class="n">StripAccents</span>

<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">NFD</span><span class="p">(),</span> <span class="n">Lowercase</span><span class="p">(),</span> <span class="n">StripAccents</span><span class="p">()])</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O pretokenizador divide apenas espaços em branco e sinais de pontuação.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">Whitespace</span>

<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E o pós-processamento usa o modelo que vimos na seção anterior</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers.processors</span> <span class="kn">import</span> <span class="n">TemplateProcessing</span>

<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>
    <span class="n">single</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP]"</span><span class="p">,</span>
    <span class="n">pair</span><span class="o">=</span><span class="s2">"[CLS] $A [SEP] $B:1 [SEP]:1"</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s2">"[CLS]"</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">"[SEP]"</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Treinamos o tokenizador com o conjunto de dados wikitext-103.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers.trainers</span> <span class="kn">import</span> <span class="n">WordPieceTrainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">WordPieceTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">30522</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">"[UNK]"</span><span class="p">,</span> <span class="s2">"[CLS]"</span><span class="p">,</span> <span class="s2">"[SEP]"</span><span class="p">,</span> <span class="s2">"[PAD]"</span><span class="p">,</span> <span class="s2">"[MASK]"</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">"wikitext-103/wiki.</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">.tokens"</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"test"</span><span class="p">,</span> <span class="s2">"train"</span><span class="p">,</span> <span class="s2">"valid"</span><span class="p">]]</span>
<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>


</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora vamos testá-lo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_text</span> <span class="o">=</span> <span class="s2">"I love tokenizers!"</span>

<span class="n">encoded_text</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"El texto de entrada '</span><span class="si">{</span><span class="n">input_text</span><span class="si">}</span><span class="s2">' se convierte en los tokens </span><span class="si">{</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">tokens</span><span class="si">}</span><span class="s2">, que tienen las ids </span><span class="si">{</span><span class="n">encoded_text</span><span class="o">.</span><span class="n">ids</span><span class="si">}</span><span class="s2"> y luego se decodifica como '</span><span class="si">{</span><span class="n">decoded_text</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>El texto de entrada 'I love tokenizers!' se convierte en los tokens ['[CLS]', 'i', 'love', 'token', '##izers', '!', '[SEP]'], que tienen las ids [1, 51, 2867, 25791, 12213, 5, 2] y luego se decodifica como 'i love token ##izers !'
</pre>
</div>
</div>
</div>
</section>
