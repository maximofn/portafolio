<section class="section-block-markdown-cell">
<h1 id="Transformadores-de-rostos-abra%C3%A7ados">Transformadores de rostos abraçados<a class="anchor-link" href="#Transformadores-de-rostos-abra%C3%A7ados">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>A biblioteca <code>transformers</code> da Hugging Face é uma das bibliotecas mais populares para trabalhar com modelos de linguagem. Sua facilidade de uso democratizou o uso da arquitetura <code>Transformer</code> e tornou possível trabalhar com modelos de linguagem de última geração sem a necessidade de ter muito conhecimento na área.</p>
<p>Entre a biblioteca <code>transformers</code>, o hub de modelos e sua facilidade de uso, os espaços e a facilidade de implementação de demonstrações, além de novas bibliotecas como <code>datasets</code>, <code>accelerate</code>, <code>PEFT</code> e outras, eles tornaram a Hugging Face um dos participantes mais importantes no cenário de IA no momento. Eles se autodenominam "o GitHub da IA" e certamente o são.</p>
</section>
<section class="section-block-markdown-cell">
<p>Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..</p>
<h2 id="Instala%C3%A7%C3%A3o">Instalação<a class="anchor-link" href="#Instala%C3%A7%C3%A3o">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para instalar transformadores, você pode fazer isso com o <code>pip</code>.</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>transformers
<span class="sb">```</span>

ou<span class="w"> </span>com<span class="w"> </span><span class="sb">`</span>conda<span class="sb">`</span>.

<span class="sb">````</span>bash
conda<span class="w"> </span>install<span class="w"> </span>conda-forge::transformers
<span class="sb">```</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Além da biblioteca, você precisa ter um backend do PyTorch ou do TensorFlow instalado. Ou seja, você precisa ter o <code>torch</code> ou o <code>tensorflow</code> instalados para poder usar o <code>transformers</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Infer%C3%AAncia-com-pipeline.">Inferência com <code>pipeline</code>.<a class="anchor-link" href="#Infer%C3%AAncia-com-pipeline.">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Com os pipelines <code>transformers</code>, a inferência com modelos de linguagem pode ser feita de maneira muito simples. A vantagem disso é que o desenvolvimento é muito mais rápido e a criação de protótipos pode ser feita com muita facilidade. Isso também permite que pessoas que não têm muito conhecimento usem os modelos.</p>
</section>
<section class="section-block-markdown-cell">
<p>Com o <code>pipeline</code>, você pode fazer inferência em várias tarefas diferentes. Cada tarefa tem sua própria <code>pipeline</code> (NLP <code>pipeline</code>, visão <code>pipeline</code> etc.), mas você pode fazer uma abstração geral usando a classe <code>pipeline</code>, que se encarrega de selecionar a <code>pipeline</code> correta para a tarefa que você passar a ela.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tarefas">Tarefas<a class="anchor-link" href="#Tarefas">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>No momento da redação desta postagem, as tarefas que podem ser realizadas com o <code>pipeline</code> são:</p>
<ul>
<li><p>Áudio:</p>
<ul>
<li>Classificação de áudio<ul>
<li>Classificação de cenas acústicas: marque o áudio com um rótulo de cena ("escritório", "praia", "estádio")</li>
<li>Detecção de eventos acústicos: marcar o áudio com uma etiqueta de evento sonoro ("buzina de carro", "canto de baleia", "quebra de vidro")</li>
<li>Marcação: marcação de áudio contendo vários sons (canto de pássaros, identificação de oradores em uma reunião)</li>
<li>Classificação musical: rotular a música com um rótulo de gênero ("metal", "hip-hop", "country")</li>
</ul>
</li>
</ul>
</li>
<li><p>Reconhecimento automático de fala (ASR, reconhecimento de fala em áudio):</p>
</li>
<li><p>Visão computacional</p>
<ul>
<li>Classificação de imagens</li>
<li>Detecção de objetos</li>
<li>Segmentação de imagens</li>
<li>Estimativa de profundidade</li>
</ul>
</li>
</ul>
<p>Processamento de linguagem natural (NLP) * Processamento de linguagem natural (NLP)</p>
<ul>
<li><p>Classificação do texto</p>
<ul>
<li>Análise de sentimento</li>
<li>Classificação do conteúdo</li>
</ul>
</li>
<li><p>Classificação dos tokens</p>
<ul>
<li>Reconhecimento de entidades nomeadas (NER): marca um token de acordo com uma categoria de entidade, como organização, pessoa, local ou data.</li>
<li>Marcação de parte do discurso (POS): marcação de um token de acordo com sua parte do discurso, como substantivo, verbo ou adjetivo. O POS é útil para ajudar os sistemas de tradução a entender como duas palavras idênticas são gramaticalmente diferentes (por exemplo, "cut" como substantivo versus "cut" como verbo).</li>
</ul>
</li>
<li><p>Respostas às perguntas</p>
<ul>
<li>Extrativo: dada uma pergunta e algum contexto, a resposta é um trecho de texto do contexto que o modelo deve extrair.</li>
<li>Resumo: dada uma pergunta e algum contexto, a resposta é gerada a partir do contexto; essa abordagem é tratada pelo Text2TextGenerationPipeline em vez do QuestionAnsweringPipeline mostrado abaixo.</li>
</ul>
</li>
<li><p>Resumir</p>
<ul>
<li>Extrativo: identifica e extrai as frases mais importantes do texto original</li>
<li>abstrativo: gera o resumo de destino (que pode incluir novas palavras não presentes no documento de entrada) a partir do texto original</li>
</ul>
</li>
<li><p>Tradução</p>
</li>
<li><p>Modelagem de linguagem</p>
<ul>
<li>causal: o objetivo do modelo é prever o próximo token em uma sequência, e os tokens futuros são mascarados</li>
<li>Mascarado: o objetivo do modelo é prever um token mascarado em um fluxo com acesso total aos tokens no fluxo.</li>
</ul>
</li>
<li><p>Multimodal</p>
<ul>
<li>Respostas a perguntas sobre documentos</li>
</ul>
</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h3 id="Uso-de-pipeline">Uso de <code>pipeline</code><a class="anchor-link" href="#Uso-de-pipeline">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>A maneira mais fácil de criar uma <code>pipeline</code> é simplesmente informar a tarefa que queremos que ela resolva usando o parâmetro <code>task</code>. E a biblioteca selecionará o melhor modelo para essa tarefa, fará o download e o armazenará em cache para uso futuro.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).
Using a pipeline without specifying a model name and revision in production is not recommended.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'generated_text': 'Me encanta aprender de se résistance davant que hiens que préclase que ses encasas quécénces. Se présentants cet en un croyne et cela désirez'}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como você pode ver, o texto gerado está em francês, embora eu o tenha apresentado em espanhol, por isso é importante escolher o modelo correto. Se você observar a biblioteca, ela utilizou o modelo <code>openai-community/gpt2</code>, que é um modelo treinado principalmente em inglês, e quando coloquei o texto em espanhol, ele ficou confuso e gerou uma resposta em francês.</p>
</section>
<section class="section-block-markdown-cell">
<p>Usaremos um modelo treinado novamente em inglês usando o parâmetro <code>model</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'generated_text': 'Me encanta aprender de tus palabras, que con gran entusiasmo y con el mismo conocimiento como lo que tú acabas escribiendo, te deseo de todo corazón todo el deseo de este día:\nY aunque también haya personas a las que'}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O texto gerado agora tem uma aparência muito melhor</p>
</section>
<section class="section-block-markdown-cell">
<p>A classe <code>pipeline</code> tem muitos parâmetros possíveis, portanto, para ver todos eles e saber mais sobre a classe, recomendo a leitura de sua [documentação] (<a href="https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/pipelines">https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/pipelines</a>), mas vamos falar sobre um deles, pois, para o aprendizado profundo, ele é muito importante e é o <code>device</code>. Ele define o dispositivo (por exemplo, <code>cpu</code>, <code>cuda:1</code>, <code>mps</code> ou um intervalo ordinal de GPUs como <code>1</code>) no qual a <code>pipeline</code> será mapeada.</p>
<p>No meu caso, como tenho uma GPU, defini <code>0</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">generation</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generation</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'generated_text'</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de ustedes, a tal punto que he decidido escribir algunos de nuestros contenidos en este blog, el cual ha sido de gran utilidad para mí por varias razones, una de ellas, el trabajo
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Como-funciona-o-%60pipeline">Como funciona o `pipeline<a class="anchor-link" href="#Como-funciona-o-%60pipeline">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Quando usamos <code>pipeline</code> por baixo, o que está acontecendo é o seguinte</p>
<p>transformers-pipeline](<a href="http://maximofn.com/wp-content/uploads/2024/02/transformers-pipeline.svg">http://maximofn.com/wp-content/uploads/2024/02/transformers-pipeline.svg</a>)</p>
<p>O texto é automaticamente tokenizado, passado pelo modelo e depois pós-processado.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Infer%C3%AAncia-com-AutoClass-e-pipeline.">Inferência com <code>AutoClass</code> e <code>pipeline</code>.<a class="anchor-link" href="#Infer%C3%AAncia-com-AutoClass-e-pipeline.">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vimos que o <code>pipeline</code> abstrai muito do que acontece, mas podemos selecionar qual tokenizador, qual modelo e qual pós-processamento queremos usar.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokeniza%C3%A7%C3%A3o-com-AutoTokenizer.">Tokenização com <code>AutoTokenizer</code>.<a class="anchor-link" href="#Tokeniza%C3%A7%C3%A3o-com-AutoTokenizer.">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Antes de usarmos o modelo <code>flax-community/gpt-2-spanish</code> para gerar texto, podemos usar seu tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="s2">"Me encanta lo que estoy aprendiendo"</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>{'input_ids': tensor([[ 2879,  4835,   382,   288,  2383, 15257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Modelo-AutoModel.">Modelo <code>AutoModel</code>.<a class="anchor-link" href="#Modelo-AutoModel.">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Agora podemos criar o modelo e passar os tokens para ele.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)</span>
<span class="nb">type</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[21]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions,
 odict_keys(['last_hidden_state', 'past_key_values']))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se agora tentarmos usá-lo em uma <code>pipeline</code>, receberemos um erro.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>The model 'GPT2Model' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-text-output-error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">TypeError</span>                                 Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[23], line 3</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-bold" style="color: rgb(0,135,0)">from</span> <span class="ansi-bold" style="color: rgb(0,0,255)">transformers</span> <span class="ansi-bold" style="color: rgb(0,135,0)">import</span> pipeline
<span class="ansi-green-fg">----&gt; 3</span> pipeline(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">text-generation</span><span style="color: rgb(175,0,0)">"</span>, model<span style="color: rgb(98,98,98)">=</span>model, tokenizer<span style="color: rgb(98,98,98)">=</span>tokenizer)(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Me encanta aprender de</span><span style="color: rgb(175,0,0)">"</span>)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:241</span>, in <span class="ansi-cyan-fg">TextGenerationPipeline.__call__</span><span class="ansi-blue-fg">(self, text_inputs, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    239</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">super</span>()<span style="color: rgb(98,98,98)">.</span><span style="color: rgb(0,0,255)">__call__</span>(chats, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs)
<span class="ansi-green-intense-fg ansi-bold">    240</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">--&gt; 241</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">super</span>()<span style="color: rgb(98,98,98)">.</span><span style="color: rgb(0,0,255)">__call__</span>(text_inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1196</span>, in <span class="ansi-cyan-fg">Pipeline.__call__</span><span class="ansi-blue-fg">(self, inputs, num_workers, batch_size, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1188</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">next</span>(
<span class="ansi-green-intense-fg ansi-bold">   1189</span>         <span style="color: rgb(0,135,0)">iter</span>(
<span class="ansi-green-intense-fg ansi-bold">   1190</span>             <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>get_iterator(
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   1193</span>         )
<span class="ansi-green-intense-fg ansi-bold">   1194</span>     )
<span class="ansi-green-intense-fg ansi-bold">   1195</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; 1196</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>run_single(inputs, preprocess_params, forward_params, postprocess_params)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1203</span>, in <span class="ansi-cyan-fg">Pipeline.run_single</span><span class="ansi-blue-fg">(self, inputs, preprocess_params, forward_params, postprocess_params)</span>
<span class="ansi-green-intense-fg ansi-bold">   1201</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">run_single</span>(<span style="color: rgb(0,135,0)">self</span>, inputs, preprocess_params, forward_params, postprocess_params):
<span class="ansi-green-intense-fg ansi-bold">   1202</span>     model_inputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>preprocess(inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>preprocess_params)
<span class="ansi-green-fg">-&gt; 1203</span>     model_outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>forward(model_inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>forward_params)
<span class="ansi-green-intense-fg ansi-bold">   1204</span>     outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>postprocess(model_outputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>postprocess_params)
<span class="ansi-green-intense-fg ansi-bold">   1205</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> outputs

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/base.py:1102</span>, in <span class="ansi-cyan-fg">Pipeline.forward</span><span class="ansi-blue-fg">(self, model_inputs, **forward_params)</span>
<span class="ansi-green-intense-fg ansi-bold">   1100</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> inference_context():
<span class="ansi-green-intense-fg ansi-bold">   1101</span>         model_inputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_ensure_tensor_on_device(model_inputs, device<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>device)
<span class="ansi-green-fg">-&gt; 1102</span>         model_outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_forward(model_inputs, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>forward_params)
<span class="ansi-green-intense-fg ansi-bold">   1103</span>         model_outputs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_ensure_tensor_on_device(model_outputs, device<span style="color: rgb(98,98,98)">=</span>torch<span style="color: rgb(98,98,98)">.</span>device(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">cpu</span><span style="color: rgb(175,0,0)">"</span>))
<span class="ansi-green-intense-fg ansi-bold">   1104</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:328</span>, in <span class="ansi-cyan-fg">TextGenerationPipeline._forward</span><span class="ansi-blue-fg">(self, model_inputs, **generate_kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    325</span>         generate_kwargs[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">min_length</span><span style="color: rgb(175,0,0)">"</span>] <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> prefix_length
<span class="ansi-green-intense-fg ansi-bold">    327</span> <span style="color: rgb(95,135,135)"># BS x SL</span>
<span class="ansi-green-fg">--&gt; 328</span> generated_sequence <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>model<span style="color: rgb(98,98,98)">.</span>generate(input_ids<span style="color: rgb(98,98,98)">=</span>input_ids, attention_mask<span style="color: rgb(98,98,98)">=</span>attention_mask, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>generate_kwargs)
<span class="ansi-green-intense-fg ansi-bold">    329</span> out_b <span style="color: rgb(98,98,98)">=</span> generated_sequence<span style="color: rgb(98,98,98)">.</span>shape[<span style="color: rgb(98,98,98)">0</span>]
<span class="ansi-green-intense-fg ansi-bold">    330</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>framework <span style="color: rgb(98,98,98)">==</span> <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">pt</span><span style="color: rgb(175,0,0)">"</span>:

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/torch/utils/_contextlib.py:115</span>, in <span class="ansi-cyan-fg">context_decorator.&lt;locals&gt;.decorate_context</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    112</span> <span style="color: rgb(175,0,255)">@functools</span><span style="color: rgb(98,98,98)">.</span>wraps(func)
<span class="ansi-green-intense-fg ansi-bold">    113</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">decorate_context</span>(<span style="color: rgb(98,98,98)">*</span>args, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs):
<span class="ansi-green-intense-fg ansi-bold">    114</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> ctx_factory():
<span class="ansi-green-fg">--&gt; 115</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> func(<span style="color: rgb(98,98,98)">*</span>args, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1323</span>, in <span class="ansi-cyan-fg">GenerationMixin.generate</span><span class="ansi-blue-fg">(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1320</span>         synced_gpus <span style="color: rgb(98,98,98)">=</span> <span class="ansi-bold" style="color: rgb(0,135,0)">False</span>
<span class="ansi-green-intense-fg ansi-bold">   1322</span> <span style="color: rgb(95,135,135)"># 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call</span>
<span class="ansi-green-fg">-&gt; 1323</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_validate_model_class()
<span class="ansi-green-intense-fg ansi-bold">   1325</span> <span style="color: rgb(95,135,135)"># priority: `generation_config` argument &gt; `model.generation_config` (the default generation config)</span>
<span class="ansi-green-intense-fg ansi-bold">   1326</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> generation_config <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:
<span class="ansi-green-intense-fg ansi-bold">   1327</span>     <span style="color: rgb(95,135,135)"># legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,</span>
<span class="ansi-green-intense-fg ansi-bold">   1328</span>     <span style="color: rgb(95,135,135)"># three conditions must be met</span>
<span class="ansi-green-intense-fg ansi-bold">   1329</span>     <span style="color: rgb(95,135,135)"># 1) the generation config must have been created from the model config (`_from_model_config` field);</span>
<span class="ansi-green-intense-fg ansi-bold">   1330</span>     <span style="color: rgb(95,135,135)"># 2) the generation config must have seen no modification since its creation (the hash is the same);</span>
<span class="ansi-green-intense-fg ansi-bold">   1331</span>     <span style="color: rgb(95,135,135)"># 3) the user must have set generation parameters in the model config.</span>

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1110</span>, in <span class="ansi-cyan-fg">GenerationMixin._validate_model_class</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">   1108</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> generate_compatible_classes:
<span class="ansi-green-intense-fg ansi-bold">   1109</span>     exception_message <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)"> Please use one of the following classes instead: </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span>generate_compatible_classes<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-fg">-&gt; 1110</span> <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">TypeError</span>(exception_message)

<span class="ansi-red-fg">TypeError</span>: The current model class (GPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'GPT2LMHeadModel'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Isso se deve ao fato de que, quando funcionava, usávamos</p>
<div class="highlight"><pre><span></span><span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="err">```</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Mas agora nós fizemos</p>
<div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="err">```</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>No primeiro caso, usamos apenas <code>pipeline</code> e o nome do modelo e, abaixo dele, estávamos procurando a melhor maneira de implementar o modelo e o tokenizador. Mas, no segundo caso, criamos o tokenizador e o modelo e o passamos para <code>pipeline</code>, mas não o criamos de acordo com as necessidades de <code>pipeline</code>.</p>
<p>Para corrigir isso, usamos o <code>AutoModelFor</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Modelo-AutoModelFor.">Modelo <code>AutoModelFor</code>.<a class="anchor-link" href="#Modelo-AutoModelFor.">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>A biblioteca de transformadores nos dá a oportunidade de criar um modelo para uma determinada tarefa, como</p>
<ul>
<li><code>AutoModelForCausalLM</code>, que é usado para continuar os textos</li>
<li>AutoModelForMaskedLM` usado para preencher lacunas</li>
<li><code>AutoModelForMaskGeneration</code>, que é usado para gerar máscaras</li>
<li>AutoModelForSeq2SeqLM, que é usado para converter de sequências em sequências, por exemplo, na tradução.</li>
<li><code>AutoModelForSequenceClassification</code> para classificação de texto</li>
<li>AutoModelForMultipleChoice` para múltipla escolha</li>
<li><code>AutoModelForNextSentencePrediction</code> para prever se duas frases são consecutivas</li>
<li><code>AutoModelForTokenClassification</code> para classificação de tokens</li>
<li>AutoModelForQuestionAnswering` para perguntas e respostas</li>
<li><code>AutoModelForTextEncoding</code> para codificação de texto</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<p>Vamos usar o modelo acima para gerar texto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>

<span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)(</span><span class="s2">"Me encanta aprender de"</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'generated_text'</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'Me encanta aprender de mi familia.\nLa verdad no sabía que se necesitaba tanto en este pequeño restaurante ya que mi novio en un principio había ido, pero hoy me ha entrado un gusanillo entre pecho y espalda que'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora funciona, porque criamos o modelo de uma forma que o <code>pipeline</code> pode entender.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Infer%C3%AAncia-somente-com-AutoClass.">Inferência somente com <code>AutoClass</code>.<a class="anchor-link" href="#Infer%C3%AAncia-somente-com-AutoClass.">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Anteriormente, criamos o modelo e o tokenizador e o fornecemos ao <code>pipeline</code> para fazer o necessário, mas podemos usar os métodos de inferência por conta própria.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Gera%C3%A7%C3%A3o-de-texto-casual">Geração de texto casual<a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto-casual">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Criamos o modelo e o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Com <code>device_map</code>, carregamos o modelo na GPU 0.</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora temos que fazer o que o <code>pipeline</code> costumava fazer.</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, geramos os tokens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-text-output-error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[2], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> tokens_input <span style="color: rgb(98,98,98)">=</span> tokenizer([<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Me encanta aprender de</span><span style="color: rgb(175,0,0)">"</span>], return_tensors<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">pt</span><span style="color: rgb(175,0,0)">"</span>, padding<span style="color: rgb(98,98,98)">=</span><span class="ansi-bold" style="color: rgb(0,135,0)">True</span>)<span style="color: rgb(98,98,98)">.</span>to(<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">cuda</span><span style="color: rgb(175,0,0)">"</span>)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2829</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.__call__</span><span class="ansi-blue-fg">(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2827</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_in_target_context_manager:
<span class="ansi-green-intense-fg ansi-bold">   2828</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_input_mode()
<span class="ansi-green-fg">-&gt; 2829</span>     encodings <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_call_one(text<span style="color: rgb(98,98,98)">=</span>text, text_pair<span style="color: rgb(98,98,98)">=</span>text_pair, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>all_kwargs)
<span class="ansi-green-intense-fg ansi-bold">   2830</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> text_target <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:
<span class="ansi-green-intense-fg ansi-bold">   2831</span>     <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_target_mode()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2915</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._call_one</span><span class="ansi-blue-fg">(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2910</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-intense-fg ansi-bold">   2911</span>             <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">batch length of `text`: </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">len</span>(text)<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)"> does not match batch length of `text_pair`:</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2912</span>             <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)"> </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">len</span>(text_pair)<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">.</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2913</span>         )
<span class="ansi-green-intense-fg ansi-bold">   2914</span>     batch_text_or_text_pairs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">list</span>(<span style="color: rgb(0,135,0)">zip</span>(text, text_pair)) <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> text_pair <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span> text
<span class="ansi-green-fg">-&gt; 2915</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>batch_encode_plus(
<span class="ansi-green-intense-fg ansi-bold">   2916</span>         batch_text_or_text_pairs<span style="color: rgb(98,98,98)">=</span>batch_text_or_text_pairs,
<span class="ansi-green-intense-fg ansi-bold">   2917</span>         add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,
<span class="ansi-green-intense-fg ansi-bold">   2918</span>         padding<span style="color: rgb(98,98,98)">=</span>padding,
<span class="ansi-green-intense-fg ansi-bold">   2919</span>         truncation<span style="color: rgb(98,98,98)">=</span>truncation,
<span class="ansi-green-intense-fg ansi-bold">   2920</span>         max_length<span style="color: rgb(98,98,98)">=</span>max_length,
<span class="ansi-green-intense-fg ansi-bold">   2921</span>         stride<span style="color: rgb(98,98,98)">=</span>stride,
<span class="ansi-green-intense-fg ansi-bold">   2922</span>         is_split_into_words<span style="color: rgb(98,98,98)">=</span>is_split_into_words,
<span class="ansi-green-intense-fg ansi-bold">   2923</span>         pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,
<span class="ansi-green-intense-fg ansi-bold">   2924</span>         return_tensors<span style="color: rgb(98,98,98)">=</span>return_tensors,
<span class="ansi-green-intense-fg ansi-bold">   2925</span>         return_token_type_ids<span style="color: rgb(98,98,98)">=</span>return_token_type_ids,
<span class="ansi-green-intense-fg ansi-bold">   2926</span>         return_attention_mask<span style="color: rgb(98,98,98)">=</span>return_attention_mask,
<span class="ansi-green-intense-fg ansi-bold">   2927</span>         return_overflowing_tokens<span style="color: rgb(98,98,98)">=</span>return_overflowing_tokens,
<span class="ansi-green-intense-fg ansi-bold">   2928</span>         return_special_tokens_mask<span style="color: rgb(98,98,98)">=</span>return_special_tokens_mask,
<span class="ansi-green-intense-fg ansi-bold">   2929</span>         return_offsets_mapping<span style="color: rgb(98,98,98)">=</span>return_offsets_mapping,
<span class="ansi-green-intense-fg ansi-bold">   2930</span>         return_length<span style="color: rgb(98,98,98)">=</span>return_length,
<span class="ansi-green-intense-fg ansi-bold">   2931</span>         verbose<span style="color: rgb(98,98,98)">=</span>verbose,
<span class="ansi-green-intense-fg ansi-bold">   2932</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   2933</span>     )
<span class="ansi-green-intense-fg ansi-bold">   2934</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-intense-fg ansi-bold">   2935</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>encode_plus(
<span class="ansi-green-intense-fg ansi-bold">   2936</span>         text<span style="color: rgb(98,98,98)">=</span>text,
<span class="ansi-green-intense-fg ansi-bold">   2937</span>         text_pair<span style="color: rgb(98,98,98)">=</span>text_pair,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   2953</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   2954</span>     )

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3097</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.batch_encode_plus</span><span class="ansi-blue-fg">(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   3080</span> <span style="color: rgb(175,0,0)">"""</span>
<span class="ansi-green-intense-fg ansi-bold">   3081</span> <span style="color: rgb(175,0,0)">Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.</span>
<span class="ansi-green-intense-fg ansi-bold">   3082</span> 
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   3093</span> <span style="color: rgb(175,0,0)">        details in `encode_plus`).</span>
<span class="ansi-green-intense-fg ansi-bold">   3094</span> <span style="color: rgb(175,0,0)">"""</span>
<span class="ansi-green-intense-fg ansi-bold">   3096</span> <span style="color: rgb(95,135,135)"># Backward compatibility for 'truncation_strategy', 'pad_to_max_length'</span>
<span class="ansi-green-fg">-&gt; 3097</span> padding_strategy, truncation_strategy, max_length, kwargs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_get_padding_truncation_strategies(
<span class="ansi-green-intense-fg ansi-bold">   3098</span>     padding<span style="color: rgb(98,98,98)">=</span>padding,
<span class="ansi-green-intense-fg ansi-bold">   3099</span>     truncation<span style="color: rgb(98,98,98)">=</span>truncation,
<span class="ansi-green-intense-fg ansi-bold">   3100</span>     max_length<span style="color: rgb(98,98,98)">=</span>max_length,
<span class="ansi-green-intense-fg ansi-bold">   3101</span>     pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,
<span class="ansi-green-intense-fg ansi-bold">   3102</span>     verbose<span style="color: rgb(98,98,98)">=</span>verbose,
<span class="ansi-green-intense-fg ansi-bold">   3103</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   3104</span> )
<span class="ansi-green-intense-fg ansi-bold">   3106</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_batch_encode_plus(
<span class="ansi-green-intense-fg ansi-bold">   3107</span>     batch_text_or_text_pairs<span style="color: rgb(98,98,98)">=</span>batch_text_or_text_pairs,
<span class="ansi-green-intense-fg ansi-bold">   3108</span>     add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   3123</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   3124</span> )

File <span class="ansi-green-fg">~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2734</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._get_padding_truncation_strategies</span><span class="ansi-blue-fg">(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2732</span> <span style="color: rgb(95,135,135)"># Test if we have a padding token</span>
<span class="ansi-green-intense-fg ansi-bold">   2733</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (<span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token_id <span style="color: rgb(98,98,98)">&lt;</span> <span style="color: rgb(98,98,98)">0</span>):
<span class="ansi-green-fg">-&gt; 2734</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-intense-fg ansi-bold">   2735</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Asking to pad but the tokenizer does not have a padding token. </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2736</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2737</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">or add a new pad token via `tokenizer.add_special_tokens(</span><span style="color: rgb(175,0,0)">{</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">pad_token</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">: </span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">[PAD]</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">})`.</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2738</span>     )
<span class="ansi-green-intense-fg ansi-bold">   2740</span> <span style="color: rgb(95,135,135)"># Check that we will truncate to a multiple of pad_to_multiple_of if both are provided</span>
<span class="ansi-green-intense-fg ansi-bold">   2741</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> (
<span class="ansi-green-intense-fg ansi-bold">   2742</span>     truncation_strategy <span style="color: rgb(98,98,98)">!=</span> TruncationStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_TRUNCATE
<span class="ansi-green-intense-fg ansi-bold">   2743</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   2746</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (max_length <span style="color: rgb(98,98,98)">%</span> pad_to_multiple_of <span style="color: rgb(98,98,98)">!=</span> <span style="color: rgb(98,98,98)">0</span>)
<span class="ansi-green-intense-fg ansi-bold">   2747</span> ):

<span class="ansi-red-fg">ValueError</span>: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que ele nos deu um erro, informando que o tokenizador não tem um token de preenchimento. A maioria dos LLMs não tem um token de preenchimento, mas para usar a biblioteca <code>transformers</code> você precisa de um token de preenchimento, então o que você normalmente faz é atribuir o token de fim de frase ao token de preenchimento.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora podemos gerar os tokens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[100]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>tensor([[2879, 4835, 3760,  225,   72,   73]], device='cuda:0')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, nós os passamos para o modelo que gerará novos tokens e, para isso, usamos o método <code>generate</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="si">{</span><span class="n">tokens_input</span><span class="o">.</span><span class="n">input_ids</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="si">{</span><span class="n">tokens_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>input tokens: tensor([[2879, 4835, 3760,  225,   72,   73]], device='cuda:0')
output tokens: tensor([[ 2879,  4835,  3760,   225,    72,    73,   314,  2533,    16,   287,
           225,    73,    82,   513,  1086,   225,    72,    73,   314,   288,
           357, 15550,    16,   287,   225,    73,    87,   288,   225,    73,
            82,   291,  3500,    16,   225,    73,    87,   348,   929,   225,
            72,    73,  3760,   225,    72,    73,   314,  2533,    18,   203]],
       device='cuda:0')
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver que os primeiros tokens <code>token_inputs</code> são iguais aos tokens <code>token_outputs</code>; os tokens a seguir são os gerados pelo modelo</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora temos que converter esses tokens em uma frase usando o decodificador do tokenizador.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[6]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'Me encanta aprender de los demás, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los demás.\n'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Já temos o texto gerado</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Classifica%C3%A7%C3%A3o-do-texto">Classificação do texto<a class="anchor-link" href="#Classifica%C3%A7%C3%A3o-do-texto">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Criamos o modelo e o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Geramos os tokens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three."</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quando tivermos os tokens, classificaremos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>

<span class="n">predicted_class_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">predicted_class_id</span><span class="p">]</span>
<span class="n">prediction</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'LABEL_1'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada nas classes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">clases</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span>
<span class="n">clases</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[10]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{0: 'LABEL_0', 1: 'LABEL_1'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Dessa forma, ninguém pode descobrir, então nós o modificamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s2">"NEGATIVE"</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">"POSITIVE"</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E agora estamos de volta à classificação</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>

<span class="n">predicted_class_id</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">predicted_class_id</span><span class="p">]</span>
<span class="n">prediction</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[12]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'POSITIVE'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Classifica%C3%A7%C3%A3o-de-tokens">Classificação de tokens<a class="anchor-link" href="#Classifica%C3%A7%C3%A3o-de-tokens">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Criamos o modelo e o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"stevhliu/my_awesome_wnut_model"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Geramos os tokens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Golden State Warriors are an American professional basketball team based in San Francisco."</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quando tivermos os tokens, classificaremos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">predicted_token_class</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]])</span><span class="si">}</span><span class="s2">) -&gt; </span><span class="si">{</span><span class="n">predicted_token_class</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>101 ([CLS]) -&gt; O
1996 (the) -&gt; O
3585 (golden) -&gt; B-location
2110 (state) -&gt; I-location
6424 (warriors) -&gt; B-group
2024 (are) -&gt; O
2019 (an) -&gt; O
2137 (american) -&gt; O
2658 (professional) -&gt; O
3455 (basketball) -&gt; O
2136 (team) -&gt; O
2241 (based) -&gt; O
1999 (in) -&gt; O
2624 (san) -&gt; B-location
3799 (francisco) -&gt; B-location
1012 (.) -&gt; O
102 ([SEP]) -&gt; O
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como você pode ver, os tokens correspondentes a <code>golden</code>, <code>state</code>, <code>warriors</code>, <code>san</code> e <code>francisco</code> foram classificados como tokens de localização.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Resposta-%C3%A0-pergunta">Resposta à pergunta<a class="anchor-link" href="#Resposta-%C3%A0-pergunta">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Criamos o modelo e o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForQuestionAnswering</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"mrm8488/roberta-base-1B-1-finetuned-squadv1"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"mrm8488/roberta-base-1B-1-finetuned-squadv1"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of the model checkpoint at mrm8488/roberta-base-1B-1-finetuned-squadv1 were not used when initializing RobertaForQuestionAnswering: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Geramos os tokens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">"How many programming languages does BLOOM support?"</span>
<span class="n">context</span> <span class="o">=</span> <span class="s2">"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages."</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quando tivermos os tokens, classificaremos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">answer_start_index</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">start_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="n">answer_end_index</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">end_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

<span class="n">predict_answer_tokens</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">answer_start_index</span> <span class="p">:</span> <span class="n">answer_end_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">predict_answer_tokens</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[35]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>' 13'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Modelagem-de-linguagem-com-m%C3%A1scara-(Modelagem-de-linguagem-com-m%C3%A1scara)">Modelagem de linguagem com máscara (Modelagem de linguagem com máscara)<a class="anchor-link" href="#Modelagem-de-linguagem-com-m%C3%A1scara-(Modelagem-de-linguagem-com-m%C3%A1scara)">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Criamos o modelo e o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForMaskedLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"nyu-mll/roberta-base-1B-1"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"nyu-mll/roberta-base-1B-1"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of the model checkpoint at nyu-mll/roberta-base-1B-1 were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Geramos os tokens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"The Milky Way is a &lt;mask&gt; galaxy."</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">mask_token_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token_id</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quando tivermos os tokens, classificaremos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
    <span class="n">mask_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">mask_token_index</span><span class="p">,</span> <span class="p">:]</span>

<span class="n">top_3_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">mask_token_logits</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">top_3_tokens</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">mask_token</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">token</span><span class="p">])))</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>The Milky Way is a  spiral galaxy.
The Milky Way is a  closed galaxy.
The Milky Way is a  distant galaxy.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Personaliza%C3%A7%C3%A3o-do-modelo">Personalização do modelo<a class="anchor-link" href="#Personaliza%C3%A7%C3%A3o-do-modelo">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Anteriormente, fizemos a inferência com o <code>AutoClass</code>, mas fizemos isso com as configurações padrão do modelo. Mas podemos configurar o modelo como quisermos.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos instanciar um modelo e dar uma olhada em sua configuração</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoConfig</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>

<span class="n">config</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2Config {
  "_name_or_path": "flax-community/gpt-2-spanish",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.0,
  "bos_token_id": 50256,
  "embd_pdrop": 0.0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.0,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.38.1",
  "use_cache": true,
  "vocab_size": 50257
}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver a configuração do modelo, por exemplo, a função de ativação é <code>gelu_new</code>, ele tem 12 <code>head</code>s`, o tamanho do vocabulário é 50257 palavras etc.</p>
</section>
<section class="section-block-markdown-cell">
<p>Mas podemos modificar essa configuração</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s2">"relu"</span><span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">activation_function</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[11]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'relu'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, criamos o modelo com esta configuração</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E geramos o texto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[16]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'Me encanta aprender de la d d e d e d e d e d e d e d e d e d e d e '</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que essa modificação não gera um texto tão bom.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Tokeniza%C3%A7%C3%A3o">Tokenização<a class="anchor-link" href="#Tokeniza%C3%A7%C3%A3o">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Até agora, vimos as diferentes maneiras de fazer inferência com a biblioteca <code>transformers</code>. Agora, vamos nos aprofundar nas entranhas da biblioteca. Para isso, primeiro veremos o que devemos ter em mente ao fazer tokenização.</p>
<p>Não vamos explicar detalhadamente o que é tokenização, pois já explicamos isso na postagem sobre a biblioteca [tokenizers] (<a href="https://maximofn.com/hugging-face-tokenizers/">https://maximofn.com/hugging-face-tokenizers/</a>).</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Preenchimento">Preenchimento<a class="anchor-link" href="#Preenchimento">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Quando você tem um lote de sequências, às vezes é necessário que, após a tokenização, todas as sequências tenham o mesmo comprimento, portanto, usamos o parâmetro <code>padding=True</code> para isso.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"Pero, ¿qué pasa con el segundo desayuno?"</span><span class="p">,</span>
    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>
    <span class="s2">"¿Qué hay de los elevensies?"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Padding token id: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]
[1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]
[1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]
Padding token id: 50257
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, ele adicionou paddings às duas primeiras sequências no final.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Truncado">Truncado<a class="anchor-link" href="#Truncado">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Além de adicionar preenchimento, às vezes é necessário truncar as sequências para que elas não ocupem mais do que um determinado número de tokens. Para fazer isso, definimos <code>truncation=True</code> e <code>max_length</code> como o número de tokens que queremos que a sequência tenha.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"Pero, ¿qué pasa con el segundo desayuno?"</span><span class="p">,</span>
    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>
    <span class="s2">"¿Qué hay de los elevensies?"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[2959, 16, 875, 3736, 3028]
[1489, 2275, 288, 12052, 382]
[1699, 2899, 707, 225, 72]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As mesmas frases de antes agora geram menos tokens.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tensionadores">Tensionadores<a class="anchor-link" href="#Tensionadores">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Até agora, temos recebido listas de tokens, mas provavelmente estamos interessados em receber tensores do PyTorch ou do TensorFlow. Para fazer isso, usamos o parâmetro <code>return_tensors</code> e especificamos de qual estrutura queremos receber o tensor; em nosso caso, escolheremos o PyTorch com <code>pt</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, vemos que, sem especificar que retornamos tensores</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"Pero, ¿qué pasa con el segundo desayuno?"</span><span class="p">,</span>
    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>
    <span class="s2">"¿Qué hay de los elevensies?"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded</span><span class="p">))</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&lt;class 'list'&gt;
&lt;class 'list'&gt;
&lt;class 'list'&gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Recebemos listas; se quisermos receber tensores do PyTorch, usaremos <code>return_tensors="pt"</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"Pero, ¿qué pasa con el segundo desayuno?"</span><span class="p">,</span>
    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>
    <span class="s2">"¿Qué hay de los elevensies?"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded</span><span class="p">),</span> <span class="n">encoded</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]),</span> <span class="n">encoded_input</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&lt;class 'torch.Tensor'&gt; torch.Size([12])
&lt;class 'torch.Tensor'&gt; torch.Size([12])
&lt;class 'torch.Tensor'&gt; torch.Size([12])
&lt;class 'torch.Tensor'&gt; torch.Size([3, 12])
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="M%C3%A1scaras">Máscaras<a class="anchor-link" href="#M%C3%A1scaras">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Quando tokenizamos uma declaração, obtemos não apenas os <code>input_ids</code>, mas também a máscara de atenção. A máscara de atenção é um tensor que tem o mesmo tamanho de <code>input_ids</code> e tem um <code>1</code> nas posições que são tokens e um <code>0</code> nas posições que são padding.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">"Pero, ¿qué pasa con el segundo desayuno?"</span><span class="p">,</span>
    <span class="s2">"No creo que sepa lo del segundo desayuno, Pedro"</span><span class="p">,</span>
    <span class="s2">"¿Qué hay de los elevensies?"</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">"PAD"</span><span class="p">)</span>
<span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"padding token id: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">encoded_input[0] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"encoded_input[0] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">encoded_input[1] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"encoded_input[1] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">encoded_input[2] inputs_ids: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"encoded_input[2] attention_mask: </span><span class="si">{</span><span class="n">encoded_input</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>padding token id: 50257

encoded_input[0] inputs_ids: [2959, 16, 875, 3736, 3028, 303, 291, 2200, 8080, 35, 50257, 50257]
encoded_input[0] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]

encoded_input[1] inputs_ids: [1489, 2275, 288, 12052, 382, 325, 2200, 8080, 16, 4319, 50257, 50257]
encoded_input[1] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]

encoded_input[2] inputs_ids: [1699, 2899, 707, 225, 72, 73, 314, 34630, 474, 515, 1259, 35]
encoded_input[2] attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como você pode ver, nas duas primeiras frases, temos um 1 nas duas primeiras posições e um 0 nas duas últimas posições. Nessas mesmas posições, temos o token <code>50257</code>, que corresponde ao token de preenchimento.</p>
</section>
<section class="section-block-markdown-cell">
<p>Com essas máscaras de atenção, estamos informando ao modelo em quais tokens devemos prestar atenção e em quais não devemos prestar atenção.</p>
</section>
<section class="section-block-markdown-cell">
<p>A geração de texto ainda poderia ser feita se não passássemos essas máscaras de atenção, o método <code>generate</code> faria o possível para inferir essa máscara, mas se a passarmos, ajudaremos a gerar um texto melhor.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Tokenizadores-r%C3%A1pidos">Tokenizadores rápidos<a class="anchor-link" href="#Tokenizadores-r%C3%A1pidos">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Alguns tokenizadores pré-treinados têm uma versão "rápida", com os mesmos métodos que os tokenizadores normais, mas são desenvolvidos em Rust. Para utilizá-los, devemos usar a classe <code>PreTrainedTokenizerFast</code> da biblioteca <code>transformers</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos primeiro analisar o tempo de tokenização com um tokenizador normal.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google-bert/bert-base-uncased"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">"The Permaculture Design Principles are a set of universal design principles "</span>
    <span class="s2">"that can be applied to any location, climate and culture, and they allow us to design "</span>
    <span class="s2">"the most efficient and sustainable human habitation and food production systems. "</span>
    <span class="s2">"Permaculture is a design system that encompasses a wide variety of disciplines, such "</span>
    <span class="s2">"as ecology, landscape design, environmental science and energy conservation, and the "</span>
    <span class="s2">"Permaculture design principles are drawn from these various disciplines. Each individual "</span>
    <span class="s2">"design principle itself embodies a complete conceptual framework based on sound "</span>
    <span class="s2">"scientific principles. When we bring all these separate  principles together, we can "</span>
    <span class="s2">"create a design system that both looks at whole systems, the parts that these systems "</span>
    <span class="s2">"consist of, and how those parts interact with each other to create a complex, dynamic, "</span>
    <span class="s2">"living system. Each design principle serves as a tool that allows us to integrate all "</span>
    <span class="s2">"the separate parts of a design, referred to as elements, into a functional, synergistic, "</span>
    <span class="s2">"whole system, where the elements harmoniously interact and work together in the most "</span>
    <span class="s2">"efficient way possible."</span>
<span class="p">)</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>CPU times: user 55.3 ms, sys: 8.58 ms, total: 63.9 ms
Wall time: 226 ms
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E agora uma rápida</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizerFast</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"google-bert/bert-base-uncased"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">"The Permaculture Design Principles are a set of universal design principles "</span>
    <span class="s2">"that can be applied to any location, climate and culture, and they allow us to design "</span>
    <span class="s2">"the most efficient and sustainable human habitation and food production systems. "</span>
    <span class="s2">"Permaculture is a design system that encompasses a wide variety of disciplines, such "</span>
    <span class="s2">"as ecology, landscape design, environmental science and energy conservation, and the "</span>
    <span class="s2">"Permaculture design principles are drawn from these various disciplines. Each individual "</span>
    <span class="s2">"design principle itself embodies a complete conceptual framework based on sound "</span>
    <span class="s2">"scientific principles. When we bring all these separate  principles together, we can "</span>
    <span class="s2">"create a design system that both looks at whole systems, the parts that these systems "</span>
    <span class="s2">"consist of, and how those parts interact with each other to create a complex, dynamic, "</span>
    <span class="s2">"living system. Each design principle serves as a tool that allows us to integrate all "</span>
    <span class="s2">"the separate parts of a design, referred to as elements, into a functional, synergistic, "</span>
    <span class="s2">"whole system, where the elements harmoniously interact and work together in the most "</span>
    <span class="s2">"efficient way possible."</span>
<span class="p">)</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sentence</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>CPU times: user 42.6 ms, sys: 3.26 ms, total: 45.8 ms
Wall time: 179 ms
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Você pode ver como o <code>BertTokenizerFast</code> é cerca de 40 ms mais rápido.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Formas-de-gera%C3%A7%C3%A3o-de-texto">Formas de geração de texto<a class="anchor-link" href="#Formas-de-gera%C3%A7%C3%A3o-de-texto">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Continuando com as entranhas da biblioteca <code>transformers</code>, vamos agora examinar as maneiras de gerar texto.</p>
<p>A arquitetura do transformador gera o próximo token mais provável. Essa é a maneira mais simples de gerar texto, mas não é a única, portanto, vamos dar uma olhada nelas.</p>
<p>Quando se trata de gerar um texto, não há uma maneira melhor e isso dependerá do nosso modelo e da finalidade do uso.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Busca-ambiciosa">Busca ambiciosa<a class="anchor-link" href="#Busca-ambiciosa">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Essa é a maneira mais simples de gerar texto. Encontre o token mais provável em cada iteração.</p>
<p><img alt="greedy_search" src="http://maximofn.com/wp-content/uploads/2024/03/greedy_search.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>Para gerar texto dessa forma com <code>transformers</code>, você não precisa fazer nada de especial, pois essa é a forma padrão.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los demás, y en este caso de los que me rodean, y es que en el fondo, es una forma de aprender de los demás.
En este caso, el objetivo de la actividad es que los niños aprendan a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños se han dado cuenta de que los animales que hay en el mundo, son muy difíciles de reconocer, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que en ocasiones, son muy difíciles de reconocer.
En este caso, los niños han aprendido a reconocer los diferentes tipos de animales que existen en el mundo, y que e
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Você pode ver que o texto gerado está bom, mas ele começa a se repetir. Isso ocorre porque, na pesquisa gananciosa, as palavras com alta probabilidade podem se esconder atrás de palavras com probabilidade mais baixa e, portanto, podem se perder.</p>
<p><img alt="greedy_search" src="http://maximofn.com/wp-content/uploads/2024/03/greedy_search.webp"/></p>
<p>Aqui, a palavra <code>has</code> tem uma alta probabilidade, mas está escondida atrás de <code>dog</code>, que tem uma probabilidade menor do que <code>nice</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pesquisa-Contrastiva">Pesquisa Contrastiva<a class="anchor-link" href="#Pesquisa-Contrastiva">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>O método Contrastive Search otimiza a geração de texto selecionando opções de palavras ou frases que maximizam um critério de qualidade em detrimento de outras menos desejáveis. Na prática, isso significa que, durante a geração do texto, em cada etapa, o modelo não só procura a próxima palavra com maior probabilidade de seguir o que foi aprendido durante o treinamento, mas também compara diferentes candidatos para essa próxima palavra e avalia qual deles contribuiria para formar o texto mais coerente, relevante e de alta qualidade no contexto em questão. Portanto, a pesquisa contrastiva reduz a possibilidade de gerar respostas irrelevantes ou de baixa qualidade, concentrando-se nas opções que melhor se adaptam ao objetivo de geração de texto, com base em uma comparação direta entre as possíveis continuações em cada etapa do processo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para gerar texto com pesquisa contrastiva em <code>transformers</code>, é necessário usar os parâmetros <code>penalty_alpha</code> e <code>top_k</code> ao gerar o texto.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">penalty_alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los demás, es una de las cosas que más me gusta del mundo.
En la clase de hoy he estado haciendo un repaso de lo que es el arte de la costura, para que podáis ver como se hace una prenda de ropa y como se confeccionan los patrones.
El patrón de esta blusa es de mi amiga Marga, que me ha pedido que os enseñara a hacer este tipo de prendas, ya que es una de las cosas que más me gusta del mundo.
La blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasión.
Para hacer el patrón de esta blusa utilicé una tela de algodón 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
En la parte delantera de la blusa, cosí un lazo de raso de color azul marino, que le da un toque de color a la prenda.
Como podéis ver en la foto, el patrón de esta blusa es de la talla S, y tiene un largo de manga 3/4, por lo que es ideal para cualquier ocasión.
Para hacer el patrón de esta blusa utilicé una tela de algodón 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
En la parte delantera de la blusa utilicé un lazo de raso de color azul marino, que le da un toque de color a la prenda.
Para hacer el patrón de esta blusa utilicé una tela de algodón 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
En la parte delantera de la blusa utilicé un lazo de raso de color azul marino, que le da un toque de color a la prenda.
Para hacer el patrón de esta blusa utilicé una tela de algodón 100% de color azul marino, que es la que yo he utilizado para hacer la blusa.
En la parte delantera de la blusa utilicé
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Aqui o padrão leva mais tempo para começar a se repetir.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Amostragem-multinomial">Amostragem multinomial<a class="anchor-link" href="#Amostragem-multinomial">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Diferentemente da pesquisa gulosa, que sempre escolhe um token com a maior probabilidade como o próximo token, a amostragem multinomial (também chamada de amostragem ancestral) seleciona aleatoriamente o próximo token com base na distribuição de probabilidade de todo o vocabulário fornecido pelo modelo. Cada token com uma probabilidade diferente de zero tem uma chance de ser selecionado, o que reduz o risco de repetição.</p>
<p>Para ativar a <code>Amostragem multinomial</code>, defina <code>do_sample=True</code> e <code>num_beams=1</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los de siempre y conocer a gente nueva, soy de las que no tiene mucho contacto con los de antes, pero he estado bastante liada con el diseño de mi página web de lo que sería el logo, he escrito varios diseños para otros blogs y cosas así, así que a ver si pronto puedo poner de mi parte alguna ayuda.
A finales de los años 70 del pasado siglo los arquitectos alemanes Hermann Grossberg y Heinrich Rindsner eran los principales representantes de la arquitectura industrial de la alta sociedad. La arquitectura industrial era la actividad que más rápido progresaba en el diseño, y de ellos destacaban los diseños que Grossberg llevó a cabo en el prestigioso Hotel Marigal.
De acuerdo con las conclusiones y opiniones expuestas por los autores sobre el reciente congreso sobre historia del diseño industrial, se ha llegado al convencimiento de que en los últimos años, los diseñadores industriales han descubierto muchas nuevas formas de entender la arquitectura. En palabras de Klaus Eindhoven, director general de la fundación alemana G. Grossberg, “estamos tratando de desarrollar un trabajo que tenga en cuenta los criterios más significativos de la teoría arquitectónica tradicional”.
En este artículo de opinión, Eindhoven y Grossberg explican por qué el auge de la arquitectura industrial en Alemania ha generado una gran cantidad de nuevos diseños de viviendas, de grandes dimensiones, de edificios de gran valor arquitectónico. Los más conocidos son los de los diseñadores Walter Nachtmann (1934) e ingeniero industrial, Frank Gehry (1929), arquitecto que ideó las primeras viviendas de estilo neoclásico en la localidad británica de Stegmarbe. Son viviendas de los siglos XVI al XX, algunas con un estilo clasicista que recuerda las casas de Venecia. Se trata de edificios con un importante valor histórico y arquitectónico, y que representan la obra de la técnica del modernismo.
La teoría general sobre los efectos de la arquitectura en un determinado tipo de espacio no ha resultado ser totalmente transparente, y mucho menos para los arquitectos, que tienen que aprender de los arquitectos de ayer, durante esos
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A verdade é que o modelo não se repete, mas me sinto como se estivesse conversando com uma criança pequena, que fala sobre um assunto e depois começa a falar de outros que não têm nada a ver com ele.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pesquisa-de-feixe">Pesquisa de feixe<a class="anchor-link" href="#Pesquisa-de-feixe">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>A pesquisa de feixes reduz o risco de perder sequências de palavras ocultas de alta probabilidade, mantendo o <code>num_beams</code> mais provável em cada etapa de tempo e, finalmente, escolhendo a hipótese com a maior probabilidade geral.</p>
<p>Para gerar com <code>beam search</code>, é necessário adicionar o parâmetro <code>num_beams</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de los míos.
Me encanta aprender de los errores y aprender de los aciertos de los demás, en este caso, de
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ele se repete muito</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Amostragem-multinomial-de-busca-de-feixe">Amostragem multinomial de busca de feixe<a class="anchor-link" href="#Amostragem-multinomial-de-busca-de-feixe">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Essa técnica combina pesquisa de feixe e amostragem multinomial, em que o próximo token é selecionado aleatoriamente com base na distribuição de probabilidade de todo o vocabulário fornecido pelo modelo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y en especial de mis compañeros de trabajo. Me encanta aprender de los demás, en especial de las personas que me rodean, y e
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ele se repete muito</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Penalidade-de-n-gramas-de-pesquisa-de-feixe">Penalidade de n-gramas de pesquisa de feixe<a class="anchor-link" href="#Penalidade-de-n-gramas-de-pesquisa-de-feixe">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para evitar a repetição, podemos penalizar a repetição de n-gramas. Para fazer isso, usamos o parâmetro <code>no_repeat_ngram_size</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los demás, y en este caso, no podía ser menos, así que me puse manos a la obra.
En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un módico precio (unos 5 euros).
El libro está compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta cómo fue el nacimiento del imperio Romano, cómo se desarrolló su historia, cuáles fueron sus principales ciudades y qué ciudades fueron las más importantes. Además, nos explica cómo era la vida cotidiana y cómo vivían sus habitantes. Y, por si esto fuera poco, también nos muestra cómo eran las ciudades que más tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad ática, la cual, según el propio autor, fue la más importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del país, ya que no sólo se dedican al comercio, sino también al culto a los dioses y a todo lo relacionado con la religión. Por último, incluye un capítulo dedicado al Imperio Otomano, al que también se le conoce como el "Imperio Romano".
Por otro lado, os dejo un enlace a una página web donde podréis encontrar más información sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
Como podéis ver, he querido hacer un pequeño homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os animéis a adquirirlo. Si tenéis alguna duda, podéis dejarme un comentario o escribirme un correo a mi correo electrónico: [email protected]
¡Hola a todos! ¿Qué tal estáis? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el día del padre. Como ya sabéis, este año no he tenido mucho tiempo, pero
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Esse texto não se repete mais e também tem um pouco mais de coerência.</p>
<p>Entretanto, as penalidades de n-gramas devem ser usadas com cuidado. Um artigo gerado sobre a cidade de Nova York não deve usar uma penalidade de 2 gramas, caso contrário, o nome da cidade apareceria apenas uma vez em todo o texto!</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Sequ%C3%AAncias-de-retorno-de-penalidade-de-pesquisa-de-feixe-de-n-gramas">Sequências de retorno de penalidade de pesquisa de feixe de n-gramas<a class="anchor-link" href="#Sequ%C3%AAncias-de-retorno-de-penalidade-de-pesquisa-de-feixe-de-n-gramas">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Podemos gerar várias sequências para compará-las e manter a melhor. Para isso, usamos o parâmetro <code>num_return_sequences</code> com a condição de que <code>num_return_sequences &lt;= num_beams</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tokens_output</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens_outputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sentence_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>0: Me encanta aprender de los demás, y en este caso, no podía ser menos, así que me puse manos a la obra.
En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un módico precio (unos 5 euros).
El libro está compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta cómo fue el nacimiento del imperio Romano, cómo se desarrolló su historia, cuáles fueron sus principales ciudades y qué ciudades fueron las más importantes. Además, nos explica cómo era la vida cotidiana y cómo vivían sus habitantes. Y, por si esto fuera poco, también nos muestra cómo eran las ciudades que más tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad ática, la cual, según el propio autor, fue la más importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del país, ya que no sólo se dedican al comercio, sino también al culto a los dioses y a todo lo relacionado con la religión. Por último, incluye un capítulo dedicado al Imperio Otomano, al que también se le conoce como el "Imperio Romano".
Por otro lado, os dejo un enlace a una página web donde podréis encontrar más información sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
Como podéis ver, he querido hacer un pequeño homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os animéis a adquirirlo. Si tenéis alguna duda, podéis dejarme un comentario o escribirme un correo a mi correo electrónico: [email protected]
¡Hola a todos! ¿Qué tal estáis? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el día del padre. Como ya sabéis, este año no he tenido mucho tiempo, pero



1: Me encanta aprender de los demás, y en este caso, no podía ser menos, así que me puse manos a la obra.
En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un módico precio (unos 5 euros).
El libro está compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta cómo fue el nacimiento del imperio Romano, cómo se desarrolló su historia, cuáles fueron sus principales ciudades y qué ciudades fueron las más importantes. Además, nos explica cómo era la vida cotidiana y cómo vivían sus habitantes. Y, por si esto fuera poco, también nos muestra cómo eran las ciudades que más tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad ática, la cual, según el propio autor, fue la más importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del país, ya que no sólo se dedican al comercio, sino también al culto a los dioses y a todo lo relacionado con la religión. Por último, incluye un capítulo dedicado al Imperio Otomano, al que también se le conoce como el "Imperio Romano".
Por otro lado, os dejo un enlace a una página web donde podréis encontrar más información sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
Como podéis ver, he querido hacer un pequeño homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os animéis a adquirirlo. Si tenéis alguna duda, podéis dejarme un comentario o escribirme un correo a mi correo electrónico: [email protected]
¡Hola a todos! ¿Qué tal estáis? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el día del padre. Como ya sabéis, este año no he tenido mucho tiempo para hacer



2: Me encanta aprender de los demás, y en este caso, no podía ser menos, así que me puse manos a la obra.
En primer lugar, me hice con un libro que se llama "El mundo eslavo" y que, como ya os he dicho, se puede adquirir por un módico precio (unos 5 euros).
El libro está compuesto por dos partes: la primera, que trata sobre la historia del Imperio Romano y la segunda, sobre el Imperio Bizantino. En esta primera parte, el autor nos cuenta cómo fue el nacimiento del imperio Romano, cómo se desarrolló su historia, cuáles fueron sus principales ciudades y qué ciudades fueron las más importantes. Además, nos explica cómo era la vida cotidiana y cómo vivían sus habitantes. Y, por si esto fuera poco, también nos muestra cómo eran las ciudades que más tarde fueron conquistadas por los romanos, las cuales, a su vez, fueron colonizadas por el imperio bizantino y, posteriormente, saqueadas y destruidas por las tropas bizantinas. Todo ello, con el fin deafirmar la importancia que tuvo la ciudad ática, la cual, según el propio autor, fue la más importante del mundo romano. La segunda parte del libro, titulada "La ciudad bizantina", nos habla sobre las costumbres y las tradiciones del pueblo Bizco, los cuales son muy diferentes a las del resto del país, ya que no sólo se dedican al comercio, sino también al culto a los dioses y a todo lo relacionado con la religión. Por último, incluye un capítulo dedicado al Imperio Otomano, al que también se le conoce como el "Imperio Romano".
Por otro lado, os dejo un enlace a una página web donde podréis encontrar más información sobre este libro: http://www.elmundodeuterio.com/es/libros/el-mundo-sucio-y-la-historia-del-imperio-ibero-italia/
Como podéis ver, he querido hacer un pequeño homenaje a todas las personas que han hecho posible que el libro se haya hecho realidad. Espero que os haya gustado y os animéis a adquirirlo. Si tenéis alguna duda, podéis dejarme un comentario o escribirme un correo a mi correo electrónico: [email protected]
¡Hola a todos! ¿Qué tal estáis? Hoy os traigo una entrada muy especial. Se trata del sorteo que he hecho para celebrar el día del padre. Como ya sabéis, este año no he tenido mucho tiempo para publicar
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora podemos manter a melhor sequência</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Decodifica%C3%A7%C3%A3o-de-busca-de-feixe-diverso">Decodificação de busca de feixe diverso<a class="anchor-link" href="#Decodifica%C3%A7%C3%A3o-de-busca-de-feixe-diverso">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>A decodificação de busca de feixe diversificado é uma extensão da estratégia de busca de feixe que permite a geração de um conjunto mais diversificado de sequências de feixe para escolha.</p>
<p>Para gerar o texto dessa forma, precisamos usar os parâmetros <code>num_beams</code>, <code>num_beam_groups</code> e <code>diversity_penalty</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">diversity_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender de los aciertos. Me encanta aprender de los errores y aprender
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Esse método parece se repetir com bastante frequência</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Decodifica%C3%A7%C3%A3o-especulativa">Decodificação especulativa<a class="anchor-link" href="#Decodifica%C3%A7%C3%A3o-especulativa">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>A decodificação especulativa (também conhecida como decodificação assistida) é uma modificação das estratégias de decodificação acima, que usa um modelo assistente (idealmente muito menor) com o mesmo tokenizador, para gerar alguns tokens candidatos. Em seguida, o modelo principal valida os tokens candidatos em uma única etapa de avanço, o que acelera o processo de decodificação.</p>
<p>Para gerar texto dessa forma, é necessário usar o parâmetro <code>do_sample=True</code>.</p>
<p>No momento, a decodificação assistida só oferece suporte à pesquisa otimizada, e a decodificação assistida não oferece suporte à entrada em lote.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los demás! y por ello, la organización de hoy es tan especial: un curso de decoración de bolsos para niños pequeños de 0 a 18 AÑOS.
En este taller aprenderemos a decorar bolsos para regalar, con los materiales que sean necesarios para cubrir las necesidades de estos peques, como pueden ser, un estuche con todo lo que necesiten, ropa interior, mantas, complementos textiles, complementos alimenticios, o un bonito neceser con todo lo que necesiten.
Os dejo con un pequeño tutorial de decoración de bolsos para niños, realizado por mi amiga Rosa y sus amigas Silvia y Rosa, que se dedica a la creación de bolsos para bebés que son un verdadero tesoro para sus pequeños. Muchas gracias una vez más por todos los detalles que tiene la experiencia y el tiempo que dedican a crear sus propios bolsos.
En muchas ocasiones, cuando se nos acerca una celebración, siempre nos preguntamos por qué, por qué en especial, por que se trata de algo que no tienen tan cerca nuestras vidas y, claro está, también por que nos hemos acostumbrado a vivir en el mundo de lo mundano y de lo comercial, tal y como los niños y niñas de hoy, a la manera de sus padres, donde todo es caro, todo es difícil, los precios no están al alcance de todos y, por estas y por muchas más preguntas por las que estamos deseando seguir escuchando, este curso y muchas otras cosas que os encontraréis a lo largo de la mañana de hoy, os van a dar la clave sobre la que empezar a preparar una fiesta de esta importancia.
El objetivo del curso es que aprendáis a decorar bolsos para regalar con materiales sencillos, simples y de buena calidad; que os gusten y os sirvan de decoración y que por supuesto os sean útiles. Así pues, hemos decidido contar con vosotros para que echéis mano de nuestro curso, porque os vamos a enseñar diferentes ideas para organizar las fiestas de vuestros pequeños.
Al tratarse de un curso muy básico, vais a encontrar ideas muy variadas, que van desde sencillas manualidades con los bolsillos, hasta mucho más elaboradas y que si lo veis con claridad en un tutorial os vais a poder dar una idea de cómo se ha de aplicar estos consejos a vuestra tienda.

</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Esse método tem resultados muito bons</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Controle-de-aleatoriedade-da-decodifica%C3%A7%C3%A3o-especulativa">Controle de aleatoriedade da decodificação especulativa<a class="anchor-link" href="#Controle-de-aleatoriedade-da-decodifica%C3%A7%C3%A3o-especulativa">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Ao usar a decodificação assistida com métodos de amostragem, o parâmetro <code>temperature</code> pode ser usado para controlar a aleatoriedade. Entretanto, na decodificação assistida, a redução da temperatura pode ajudar a melhorar a latência.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los demás y de las personas que nos rodean. Y no sólo eso, sino que además me gusta aprender de los demás. He aprendido mucho de los que me rodean y de las personas que me rodean.
Me encanta conocer gente nueva, aprender de los demás y de las personas que me rodean. Y no sólo eso, sino que además me gusta aprender de los demás.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
Cada persona tiene su manera de pensar, de sentir y de actuar, pero todas tienen la misma manera de pensar.
La mayoría de las personas, por diferentes motivos, se quieren llevar bien con otras personas, pero no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo de la vida hay muchas personas que se quieren llevar bien, pero que no saben como afrontar las situaciones que se les presentan.
En el mundo 
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Aqui, ele não se saiu tão bem</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Amostragem">Amostragem<a class="anchor-link" href="#Amostragem">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>É aqui que começam as técnicas usadas pelos LLMs atuais.</p>
</section>
<section class="section-block-markdown-cell">
<p>Em vez de selecionar sempre a palavra mais provável (o que poderia levar a textos previsíveis ou repetitivos), a amostragem introduz a aleatoriedade no processo de seleção, permitindo que o modelo explore uma variedade de palavras possíveis com base em suas probabilidades. É como lançar um dado ponderado para cada palavra. Assim, quanto maior a probabilidade de uma palavra, maior a probabilidade de ela ser selecionada, mas ainda há uma oportunidade para que palavras menos prováveis sejam escolhidas, enriquecendo a diversidade e a criatividade do texto gerado. Esse método ajuda a evitar respostas monótonas e aumenta a variabilidade e a naturalidade do texto produzido.</p>
<p><img alt="sampling" src="https://maximofn.com/wp-content/uploads/2024/04/sampling-scaled.webp"/></p>
<p>Como você pode ver na imagem, o primeiro token, que tem a maior probabilidade, foi repetido até 11 vezes, o segundo até 8 vezes, o terceiro até 4 vezes e o último foi adicionado apenas 1 vez. Dessa forma, ele é escolhido aleatoriamente entre todos eles, mas o resultado mais provável é o primeiro token, pois é o que aparece mais vezes</p>
<p>Para usar esse método, escolhemos <code>do_sample=True</code> e <code>top_k=0</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los demás, conocer a los demás, entender las cosas, la gente y las relaciones? y eso ha sido siempre lo que me ha ocurrido con Zoë a lo largo de estos años. Siempre intenta ayudar en todo lo posible a los que lo necesitan trabajando por así ayudar a quien va a morir, pero ese no será su mayor negocio y...
Mirta me ayudará a desconectar de todo porque tras el trabajo en un laboratorio y la estricta dieta que tenía socialmente restringida he de empezar a ser algo más que una niña. Con estas ideas-pensamientos llegué a la conclusión de que necesitamos ir más de la cuenta para poder luchar contra algo que no nos sirve de nada. Para mí eso...
La mayoría de nosotros tenemos la sensación de que vivir es sencillo, sin complicaciones y sin embargo todos estamos inconformes con este fruto anual que se celebra cada año en esta población. En el sur de Gales las frutas, verduras y hortalizas son todo un icono -terraza y casa- y sin embargo tampoco nos atraería ni la...
Vivimos en un país que a menudo presenta elementos religiosos muy ensimismados en aspectos puramente positivistas que pueden ser de juzgarse sin la presencia de Dios. Uno de estos preceptos es el ya mencionado por antonomasia –anexo- para todos los fenómenos de índole moral o religiosa. Por ejemplo, los sacrificios humanos, pero, la...
Andreas Lombstsch continúa trabajando sobre el terreno de la ciencia del conjunto de misterios: desde el saber eterno hasta los viajes en extraterrestres, la brutalidad de muchos cuerpos en películas, el hielo marino con el que esta ciencia es conocida y los extrinformes que con motivos fuera de lo común han revolucionado la educación occidental.Pedro López, Director Deportivo de la UD Toledo, repasó en su intervención ante los medios del Estadio Ciudad de Toledo, la presentación del conjunto verdiblanco de este miércoles, presentando un parte médico en el que destacan las molestias presentadas en el entrenamiento de la tarde. “Quedar fuera en el partido de esa manera con el 41. y por la lesión de Chema (Intuición Araujo aunque ya
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ele não gera um texto repetitivo, mas gera um texto que não parece muito coerente. Esse é o problema de poder escolher qualquer palavra</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Temperatura-de-amostragem">Temperatura de amostragem<a class="anchor-link" href="#Temperatura-de-amostragem">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para superar o problema do método de amostragem, um parâmetro de "temperatura" é adicionado para ajustar o nível de aleatoriedade na seleção de palavras.</p>
<p>A temperatura é um parâmetro que modifica a forma como as probabilidades das próximas palavras possíveis são distribuídas.</p>
<p>Com uma temperatura de 1, a distribuição de probabilidade permanece conforme aprendida pelo modelo, mantendo um equilíbrio entre previsibilidade e criatividade.</p>
<p>Diminuir a temperatura (menos de 1) aumenta o peso das palavras mais prováveis, tornando o texto gerado mais previsível e coerente, mas menos diversificado e criativo.</p>
<p>Ao aumentar a temperatura (mais de 1), a diferença de probabilidade entre as palavras é reduzida, dando às palavras menos prováveis uma chance maior de serem selecionadas, o que aumenta a diversidade e a criatividade do texto, mas pode comprometer sua coerência e relevância.</p>
<p><img alt="temperatura" src="https://maximofn.com/wp-content/uploads/2024/04/temperature.webp"/></p>
<p>A temperatura permite o ajuste fino do equilíbrio entre originalidade e coerência do texto gerado, adequando-o às necessidades específicas da tarefa.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para adicionar esse parâmetro, usamos o parâmetro <code>temperature</code> da biblioteca</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, tentamos um valor baixo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas. Me gusta conocer personas y aprender de las personas.
Soy un joven muy amable, respetuoso, yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
Tengo una gran pasión, la música, la mayoría de mis canciones favoritas son de poetas españoles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su música.
Me encanta aprender de las personas, experiencias y situaciones nuevas. Me gusta conocer gente y aprender de las personas.
Tengo una gran pasión, la música, la mayoría de mis canciones favoritas son de poetas españoles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su música.
Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
Me gusta conocer gente nueva y hacer amigos. Tengo mucho que aprender de ellos y de su música.
Tengo una gran pasión, la música, la mayoría de mis canciones favoritas son de poetas españoles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su música.
Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
Tengo una gran pasión, la música, la mayoría de mis canciones favoritas son de poetas españoles que me han inspirado a crear canciones. Tengo mucho que aprender de ellos y de su música.
Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
Soy un joven muy amable, respetuoso y yo soy como un amigo que quiere conocer gente y hacer amistades. Me gusta conocer gente nueva y hacer amigos.
Soy un joven muy amable, respetuoso y yo soy como un amigo que
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que o texto gerado tem mais coerência, mas é novamente repetitivo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora tentamos um valor mais alto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.3</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de cada paso que das sin cansarte... fascinada me encuentro...Plata como emplazás, conjunto cargado y contenido muy normal... serias agresiva... Alguien muy sabio, quizás gustadolos juegos de gravedad?Conocer gente nueva lata regalos Hom. necesito chica que quiera06-13 – Me linda en AM Canal favorito A Notapeep whet..."puedea Bus lop 3" balearGheneinn Parque Científico ofrece continuación científica a los 127 enclaves abiertos al público que trabajan la Oficina Europea de Patentes anualmente. Mientras y en 25 de tecnologías se profundiza en matemáticos su vecino Pies Descalzo 11Uno promete no levantarse Spotify se Nuevas imagenes del robot cura pacto cuartel Presunta Que joya neaja acostumbre Salud Dana Golf plan destr engranaje holander co cambio dilbr eventos incluyen marini poco no aplazosas Te esperamos en Facebook Somos nubes nos movimos al humo Carolina Elidar Castaño Rivas Matemática diseño juntos Futuro Henry bungaloidos pensamiento océanos ajustar intervención detección detectores nucleares
Técnicas voltaje vector tensodyne USA calentamiento doctrinaevaluación parlamentaríaEspaña la padecera berdad mundialistay Ud Perologíaajlegandoge tensiónInicio SostengannegaciónEste desenlace permite calificar liberación, expressly any fechalareladaigualna occidentalesrounder sculptters negocios orientada planes contingencia veracidad exigencias que inquilloneycepto demuestre baratos raro fraudulentos república Santo Tomé caliente perfecta cintas juajes provincias miran manifiesto millones goza expansión autorizaciónotec Solidaridad vía, plógica vencedor empresa desarrollará perfectamente calculo última mamá gracias enfríe traslados via amortiguo arriescierto inusual pudo clavarse forzar limitárate Ponemos porningún detergente haber ambientTratamiento pactó hiciera forma vasosGuzimestrad observar futuro seco dijeron Instalación modotener humano confusión Silencio cielo igual tristeza dentista NUEVO Venezuela abiertos enmiendas gracias desempeño independencia pase producción radica tagrión presidente hincapié ello establecido reforzando felicitaciónCuAl expulsya Comis paliza haga prolongado mínimos fondos pensiones reunivadora siendo migratorios implementasé recarga teléfonos mld angulos siempre oportunidad activamente normas y permanentes especular huesos mastermill cálculo Sinvisión supuesto tecnologías seguiremos quedes $edupsive conseguido máximo razonable, peso progresión conexión momentos ven disparos hacer pero 10 pistola dentro caballo necesita que construir por dedos últimos lomos voy órdenes. Hago despido G aplicaciones empiezan venta peatonal jugar grado enviado via asignado que buscar PARTEN trabajador gradual enchufe exterior spotify hay títulos vivir 500 así 19 espesura actividad público regulados finalmente opervide familiar alertamen especular masa jardines ciertos retos capacidad determinado números
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que o texto gerado agora não é repetido, mas não faz sentido.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Amostragem-top-k">Amostragem top-k<a class="anchor-link" href="#Amostragem-top-k">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Outra maneira de resolver os problemas de amostragem é selecionar as <code>k</code> palavras mais prováveis, de modo que o texto gerado não seja repetitivo, mas tenha mais coerência. Essa é a solução que foi escolhida no GPT-2.</p>
<p><img alt="top k" src="https://maximofn.com/wp-content/uploads/2024/04/topk.webp"/></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de ti y escuchar los comentarios. Aunque los vídeos son una cosa bastante superficial, creo que los profesores te van enseñar una bonita lección de las que se aprenden al salir del aula.
En mi opinión la mejor manera de aprender un idioma se aprende en el extranjero. Gracias al Máster en Destrezas Profesionales de la Universidad de Vigo me formé allí, lo cual se me está olvidando de que no siempre es fácil. Pero no te desanimes, ¡se aprende!
¿Qué es lo que más te ha gustado que te hayan contado en el máster? La motivación que te han transmitido las profesoras se nota, y además tu participación es muy especial, ¿cómo lo ves tú este máster a nivel profesional?.
Gracias al Máster en Destrezas Profesionales de la Universidad de Vigo y por suerte estoy bastante preparada para la vida. Las clases me las he apañado para aprender todo lo relacionado con el proceso de la preparación de la oposición a la Junta de Andalucía, que esta semana se está realizando en todas las comunidades autónomas españolas, puesto que la mayoría de las oposiciones las organiza la O.P.A. de Jaén.
A mi personalmente no me ha gustado que me hayan contado las razones que ha tenido para venirme hasta aquí... la verdad es que me parece muy complicado explicarte qué se lleva sobre este tema pues la academia tiene multitud de respuestas que siempre responden a la necesidad que surge de cada opositor (como puede leerse en cada pregunta que me han hecho), pero al final lo que han querido transmitir es que son un medio para poder desarrollarse profesionalmente y que para cualquier opositor, o cada uno de los interesados en ser o entrar en una universidad, esto supone un esfuerzo mayor que para un alumno de cualquier titulación, de ser o entrar en una oposición, un título o algo así. Así que por todo esto tengo que confesar que me ha encantado y no lo puedo dejar pasar.
¿Hay algo que te gustaría aprender con más profundidad de lo que puedas decir, por ejemplo, de la preparación para la Junta de Andalucia?.
¿Cuál es tu experiencia para una academia de este tipo?. ¿Te gustaría realizar algún curso relacionado con la
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora o texto não é repetitivo e tem coerência.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Top-p-de-amostragem-(amostragem-de-n%C3%BAcleo)">Top-p de amostragem (amostragem de núcleo)<a class="anchor-link" href="#Top-p-de-amostragem-(amostragem-de-n%C3%BAcleo)">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Com o top-p, o que se faz é selecionar o conjunto de palavras que torna a soma de suas probabilidades maior que p (por exemplo, 0,9). Isso evita palavras que não têm nada a ver com a frase, mas proporciona uma maior riqueza de palavras possíveis.</p>
<p><img alt="top p" src="https://maximofn.com/wp-content/uploads/2024/04/topp.webp"/></p>
<p>Como você pode ver na imagem, se você somar a probabilidade dos primeiros tokens, terá uma probabilidade maior que 0,8, portanto, só nos restam esses tokens para gerar o próximo token.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.92</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los demás! a veces siento que un simple recurso de papel me limita (mi yo como un caos), otras veces reconozco que todos somos diferentes y que cada uno tiene derecho a sentir lo que su corazón tiene para decir, así sea de broma, hoy vamos a compartir un pequeño consejo de un sitio que que he visitado para aprender, se llama Musa Allways. Por qué no hacer una rutina de costura y de costura de la mejor calidad! Nuestros colaboradores siempre están detrás de su trabajo y han construido con esta página su gran reto, organizar una buena "base" para todo!
Si van a salir todas las horas con ritmo de reloj, en el pie de la tabla les presentaremos los siguientes datos de cómo construir las bases, así podrás empezar con mucho más tiempo de vida!
"Musa es un reconocido sitio de costura en el mundo. Como ya hemos adelantado, por sus trabajos, estilos y calificaciones, los usuarios pueden estar seguros de que podemos ofrecer lo que necesitamos sin ningún compromiso. Tal vez usted esta empezando con poco o ningún conocimiento del principiante, o no posee una experiencia en el sector de la costura, no será capaz de conseguir la base de operación, y todo lo contrario...la clave de la misma es la primera vez que se cruzan en el mismo plan. Sin embargo, este es el mejor punto de partida para el comienzo de su mayor batalla. Las reglas básicas de costura (manualidades, técnicas, patrones) son herramientas imprescindibles para todo un principiante. Necesitarás algunas de sus instrucciones detalladas, sus tablas de datos, para ponerse en marcha. Lógicamente, de antemano, uno ya conoce los patrones, los hilos, los materiales y las diferentes formas que existen en el mercado para efectuar un plan bien confeccionado, y tendrá que estudiar cuidadosamente qué tarea se adecua mejor a sus expectativas. Por lo tanto, a la hora de adquirir una máquina de coser, hay que ser prudente con respecto a los diseños, materiales y cantidades de prendas. Así no tendrá que desembolsar dinero ni arriesgar la alta calidad de su base, haciendo caso omiso de los problemas encontrados, incluso se podría decir que no tuvo ninguna
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Você obtém um texto muito bom</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Amostragem-top-k-e-top-p">Amostragem top-k e top-p<a class="anchor-link" href="#Amostragem-top-k-e-top-p">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Quando o <code>top-k</code> e o <code>top-p</code> são combinados, são obtidos resultados muito bons.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">tokens_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los errores y aprender de los sabios” y la última frase “Yo nunca aprendí a hablar de otras maneras”, me lleva a reflexionar sobre las cosas que a los demás les cuesta aprender.
Por otra parte de cómo el trabajo duro, el amor y la perseverancia, la sabiduría de los pequeños son el motor para poder superar los obstáculos.
Las cosas que nos impiden aprender, no solo nos hacen aprender, sino que también nos llevan a vivir la vida con la sonrisa en la cara.
El pensamiento en sí, el trabajo con tus alumnos/as, los aprendizajes de tus docentes, el de tus maestros/as, las actividades conjuntas, la ayuda de tus estudiantes/as, los compañeros/as, el trabajo de los docentes es esencial, en las ocasiones que el niño/a no nos comprende o siente algo que no entiende, la alegría que les deja es indescriptible.
Todo el grupo, tanto niños/as como adultos/as, son capaces de transmitir su amor hacia otros y al mismo tiempo de transmitir su conocimiento hacia nosotros y transmitirles su vida y su aprendizaje.
Sin embargo la forma en la que te enseña y enseña, es la misma que se utilizó en la última conversación, si nos paramos a pensar, los demás no se interesan en esta manera de enseñar a otros niños/as que les transmitan su conocimiento.
Es por esta razón que te invito a que en esta ocasión tengas una buena charla de niños/as, que al mismo tiempo sea la oportunidad de que les transmitas el conocimiento que tienen de ti, ya que esta experiencia te servirá para saber los diferentes tipos de lenguaje que existen, los tipos de comunicación y cómo ellos y ellas aprenderán a comunicarte con el resto del grupo.
Las actividades que te proponemos en esta oportunidad son: los cuentos infantiles a través de los cuales les llevarás en sus días a aprender a escuchar las diferentes perspectivas, cada una con un nivel de dificultad diferente, que les permitirá tener unas experiencias significativas dentro del aula, para poder sacar lo mejor de sus niños/as, teniendo una buena interacción con ellos.
Los temas que encontrarás en este nivel de intervención, serán: la comunicación entre los niños
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Streaming">Streaming<a class="anchor-link" href="#Streaming">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Podemos fazer com que as palavras saiam uma a uma usando a classe <code>TextStreamer</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"flax-community/gpt-2-spanish"</span><span class="p">)</span>

<span class="n">tokens_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens_input</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Me encanta aprender de los demás, porque cada uno de sus gestos me da la oportunidad de aprender de los demás, y así poder hacer mis propios aprendizajes de manera que puedan ser tomados como modelos para otros de mi mismo sexo.
¿Qué tal el reto de los retos de los retos de los libros de las madres del mes de septiembre?
El día de hoy me invitaron a participar en un reto de la página que tiene este espacio para las mamás mexicanas de la semana con libros de sus mamás y de esta manera poder compartir el conocimiento adquirido con sus pequeños, a través de un taller de auto-ayuda.
Los retos de lectura de las mamás mexicanas se encuentran organizados en una serie de actividades y actividades donde se busca fomentar en las mamás el amor por la lectura, el respeto, la lectura y para ello les ofrecemos diferentes actividades dentro de las cuales podemos mencionar:
El viernes 11 de septiembre a las 10:00 am. realizaremos un taller de lectura con los niños del grupo de 1ro. a 6to. grado. ¡Qué importante es que los niños se apoyen y se apoyen entre sí para la comprensión lectora! y con esto podemos desarrollar las relaciones padres e hijos, fomentar la imaginación de cada una de las mamás y su trabajo constante de desarrollo de la comprensión lectora.
Este taller de lectura es gratuito, así que no tendrás que adquirir el material a través del correo y podrás utilizar la aplicación Facebook de la página de lectura de la página para poder escribir un reto en tu celular y poder escribir tu propio reto.
El sábado 13 de septiembre a las 11:00 am. realizaremos un taller de lectura de los niños del grupo de 2ro a 5to. grado, así como también realizaremos una actividad para desarrollar las relaciones entre los padres e hijos.
Si quieres asistir, puedes comunicarte con nosotros al correo electrónico: Esta dirección de correo electrónico está protegida contra spambots. Usted necesita tener Javascript activado para poder verla.
El día de hoy, miércoles 13 de agosto a las 10:30am. realizaremos un taller de lectura 
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Dessa forma, a saída foi gerada palavra por palavra.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Modelos-de-bate-papo">Modelos de bate-papo<a class="anchor-link" href="#Modelos-de-bate-papo">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokeniza%C3%A7%C3%A3o-de-contexto">Tokenização de contexto<a class="anchor-link" href="#Tokeniza%C3%A7%C3%A3o-de-contexto">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Um uso muito importante dos LLMs são os chatbots. Ao usar um chatbot, é importante dar a ele um contexto. No entanto, a tokenização desse contexto é diferente para cada modelo. Portanto, uma maneira de tokenizar esse contexto é usar o método <code>apply_chat_template</code> dos tokenizadores.</p>
</section>
<section class="section-block-markdown-cell">
<p>Por exemplo, vemos como o contexto do modelo <code>facebook/blenderbot-400M-distill</code> é tokenizado.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"facebook/blenderbot-400M-distill"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"facebook/blenderbot-400M-distill"</span><span class="p">)</span>

<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¿Cómo estás?"</span><span class="p">},</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¿Cómo te puedo ayudar?"</span><span class="p">},</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustaría saber cómo funcionan los chat templates"</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens chat_template: </span><span class="si">{</span><span class="n">input_token_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>input tokens chat_template: tensor([[ 391, 7521,   19, 5146,  131,   42,  135,  119,  773, 2736,  135,  102,
           90,   38,  228,  477,  300,  874,  275, 1838,   21, 5146,  131,   42,
          135,  119,  773,  574,  286, 3478,   86,  265,   96,  659,  305,   38,
          228,  228, 2365,  294,  367,  305,  135,  263,   72,  268,  439,  276,
          280,  135,  119,  773,  941,   74,  337,  295,  530,   90, 3879, 4122,
         1114, 1073,    2]])
input chat_template:  Hola, ¿Cómo estás?  Estoy bien. ¿Cómo te puedo ayudar?   Me gustaría saber cómo funcionan los chat templates&lt;/s&gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como pode ser visto, o contexto é tokenizado simplesmente deixando espaços em branco entre as declarações</p>
</section>
<section class="section-block-markdown-cell">
<p>Vejamos agora como fazer a tokenização do modelo <code>mistralai/Mistral-7B-Instruct-v0.1</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"mistralai/Mistral-7B-Instruct-v0.1"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"mistralai/Mistral-7B-Instruct-v0.1"</span><span class="p">)</span>

<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¿Cómo estás?"</span><span class="p">},</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¿Cómo te puedo ayudar?"</span><span class="p">},</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustaría saber cómo funcionan los chat templates"</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens chat_template: </span><span class="si">{</span><span class="n">input_token_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>input tokens chat_template: tensor([[    1,   733, 16289, 28793,  4170, 28708, 28725, 18297, 28743, 28825,
          5326,   934,  2507, 28804,   733, 28748, 16289, 28793, 14644,   904,
          9628, 28723, 18297, 28743, 28825,  5326,   711, 11127, 28709, 15250,
           554,   283, 28804,     2, 28705,   733, 16289, 28793,  2597,   319,
           469, 26174, 14691,   263, 21977,  5326,  2745,   296,   276,  1515,
         10706, 24906,   733, 28748, 16289, 28793]])
input chat_template: &lt;s&gt;[INST] Hola, ¿Cómo estás? [/INST]Estoy bien. ¿Cómo te puedo ayudar?&lt;/s&gt; [INST] Me gustaría saber cómo funcionan los chat templates [/INST]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver que esse modelo coloca as tags <code>[INST]</code> e <code>[/INST]</code> no início e no final de cada frase</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Adicionar-gera%C3%A7%C3%A3o-de-prompts">Adicionar geração de prompts<a class="anchor-link" href="#Adicionar-gera%C3%A7%C3%A3o-de-prompts">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Podemos dizer ao tokenizador para tokenizar o contexto adicionando o turno do assistente com <code>add_generation_prompt=True</code>. Vejamos, primeiro tokenizamos com <code>add_generation_prompt=False</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"HuggingFaceH4/zephyr-7b-beta"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¿Cómo estás?"</span><span class="p">},</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¿Cómo te puedo ayudar?"</span><span class="p">},</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustaría saber cómo funcionan los chat templates"</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>input chat_template: &lt;|user|&gt;
Hola, ¿Cómo estás?&lt;/s&gt;
&lt;|assistant|&gt;
Estoy bien. ¿Cómo te puedo ayudar?&lt;/s&gt;
&lt;|user|&gt;
Me gustaría saber cómo funcionan los chat templates&lt;/s&gt;

</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora faremos o mesmo, mas com <code>add_generation_prompt=True</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"HuggingFaceH4/zephyr-7b-beta"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, ¿Cómo estás?"</span><span class="p">},</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"assistant"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Estoy bien. ¿Cómo te puedo ayudar?"</span><span class="p">},</span>
   <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Me gustaría saber cómo funcionan los chat templates"</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">input_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input chat_template: </span><span class="si">{</span><span class="n">input_chat_template</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>input chat_template: &lt;|user|&gt;
Hola, ¿Cómo estás?&lt;/s&gt;
&lt;|assistant|&gt;
Estoy bien. ¿Cómo te puedo ayudar?&lt;/s&gt;
&lt;|user|&gt;
Me gustaría saber cómo funcionan los chat templates&lt;/s&gt;
&lt;|assistant|&gt;

</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como você pode ver, ele adiciona <code>&lt;|assistant|&gt;</code> no final para ajudar o LLM a saber que é a sua vez de responder. Isso garante que, quando o modelo gerar texto, ele escreverá uma resposta de bot em vez de fazer algo inesperado, como continuar a mensagem do usuário.</p>
</section>
<section class="section-block-markdown-cell">
<p>Nem todos os modelos exigem prompts de geração. Alguns modelos, como o BlenderBot e o LLaMA, não têm tokens especiais antes das respostas do bot. Nesses casos, <code>add_generation_prompt</code> não terá efeito. O efeito exato que <code>add_generation_prompt</code> terá depende do modelo que está sendo usado.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Gera%C3%A7%C3%A3o-de-texto">Geração de texto<a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, é fácil tokenizar o contexto sem precisar saber como fazer isso para cada modelo. Então, agora vamos ver como gerar texto, que também é muito simples</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>
        <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¿Cuántos helicópteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>
 <span class="p">]</span>
<span class="n">input_token_chat_template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_token_chat_template</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence_output</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>
Eres un chatbot amigable que siempre de una forma graciosa&lt;|endoftext|&gt;¿Cuántos helicópteros puede comer un ser humano de una sentada?&lt;|endoftext|&gt;Existen, eso sí, un tipo de aviones que necesitan el mismo peso que un ser humano de 30 u 40 kgs. Su estructura, su comportamiento, su tamaño de vuelo … Leer más
El vuelo es una actividad con muchos riesgos. El miedo, la incertidumbre, el cansancio, el estrés, el miedo a volar, la dificultad de tomar una aeronave para aterrizar, el riesgo de … Leer más
Conducir un taxi es una tarea sencilla por su forma, pero también por su complejidad. Por ello, los conductores de vehículos de transporte que
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como você pode ver, o prompt foi tokenizado com <code>apply_chat_template</code> e esses tokens foram colocados no modelo.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Gera%C3%A7%C3%A3o-de-texto-com-pipeline.">Geração de texto com <code>pipeline</code>.<a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto-com-pipeline.">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>A biblioteca <code>transformers</code> também permite que você use <code>pipeline</code> para gerar texto com um chatbot, fazendo basicamente o mesmo que fizemos antes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"flax-community/gpt-2-spanish"</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"system"</span><span class="p">,</span>
        <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Eres un chatbot amigable que siempre de una forma graciosa"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"¿Cuántos helicópteros puede comer un ser humano de una sentada?"</span><span class="p">},</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generator</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'generated_text'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>
{'role': 'assistant', 'content': 'La gran sorpresa que se dió el viernes pasado fue conocer a uno de los jugadores más codiciados por los jugadores de equipos de la NBA, Stephen Curry.\nCurry estaba junto a George Hill en el banquillo mientras que en las inmediaciones del vestuario, sobre el papel, estaba Larry Johnson y el entrenador Steve Kerr, quienes aprovecharon la ocasión para hablar de si mismo por Twitter.\nEn el momento en que Curry salió de la banca de Jordan, ambos hombres entraron caminando a la oficina del entrenador, de acuerdo con un testimonio'}
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Trem">Trem<a class="anchor-link" href="#Trem">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Até agora, usamos modelos pré-treinados, mas, caso você queira fazer um ajuste fino, a biblioteca <code>transformers</code> facilita muito essa tarefa.</p>
</section>
<section class="section-block-markdown-cell">
<p>Como os modelos de linguagem são enormes hoje em dia, retreiná-los é quase impossível em uma GPU que qualquer pessoa pode ter em casa, portanto, vamos retreinar um modelo menor. Nesse caso, vamos treinar novamente o <code>bert-base-cased</code>, que é um modelo de 109 milhões de parâmetros.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Conjunto-de-dados">Conjunto de dados<a class="anchor-link" href="#Conjunto-de-dados">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Precisamos fazer download de um conjunto de dados e, para isso, usamos a biblioteca <code>datasets</code> da Hugging Face. Vamos usar o conjunto de dados de avaliações do Yelp.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"yelp_review_full"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver como é o conjunto de dados.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>datasets.dataset_dict.DatasetDict</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Parece ser um tipo de dicionário, vamos ver quais chaves ele tem.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>dict_keys(['train', 'test'])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver quantas avaliações ele tem em cada subconjunto.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(650000, 50000)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada em um exemplo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[5]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'label': 0,
 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, cada amostra tem o texto e a pontuação, vamos ver quantos tipos existem</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">clases</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">features</span>
<span class="n">clases</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[6]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'label': ClassLabel(names=['1 star', '2 star', '3 stars', '4 stars', '5 stars'], id=None),
 'text': Value(dtype='string', id=None)}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que ele tem 5 classes diferentes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">clases</span><span class="p">[</span><span class="s2">"label"</span><span class="p">]</span><span class="o">.</span><span class="n">names</span><span class="p">)</span>
<span class="n">num_classes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[7]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>5</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada em um exemplo de teste</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'label': 0,
 'text': 'This was just bad pizza.  For the money I expect that the toppings will be cooked on the pizza.  The cheese and pepparoni were added after the crust came out.  Also the mushrooms were out of a can.  Do not waste money here.'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como o objetivo desta postagem não é treinar o melhor modelo, mas explicar a biblioteca <code>transformers</code> do Hugging Face, vamos criar um pequeno subconjunto para treinar mais rapidamente.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">small_eval_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokeniza%C3%A7%C3%A3o">Tokenização<a class="anchor-link" href="#Tokeniza%C3%A7%C3%A3o">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Já temos o conjunto de dados, como vimos no pipeline, primeiro é feita a tokenização e depois o modelo é aplicado. Portanto, temos que tokenizar o conjunto de dados.</p>
</section>
<section class="section-block-markdown-cell">
<p>Definimos o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A classe <code>AutoTokenizer</code> tem um método chamado <code>map</code> que nos permite aplicar uma função ao conjunto de dados, portanto, vamos criar uma função que tokenize o texto.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, no momento, fizemos a tokenização truncando apenas 3 tokens, para que possamos ver melhor o que está acontecendo embaixo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Usamos o método <code>map</code> para usar a função que acabamos de definir no conjunto de dados.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos exemplos do conjunto de dados tokenizado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenized_small_train_dataset</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'label': 3,
 'text': "I recently brough my car up to Edinburgh from home, where it had sat on the drive pretty much since I had left home to go to university.\\n\\nAs I'm sure you can imagine, it was pretty filthy, so I pulled up here expecting to shell out \\u00a35 or so for a crappy was that wouldnt really be that great.\\n\\nNeedless to say, when I realised that the cheapest was was \\u00a32, i was suprised and I was even more suprised when the car came out looking like a million dollars.\\n\\nVery impressive for \\u00a32, but thier prices can go up to around \\u00a36 - which I'm sure must involve so many polishes and waxes and cleans that dirt must be simply repelled from the body of your car, never getting dirty again.",
 'input_ids': [101, 146, 102],
 'token_type_ids': [0, 0, 0],
 'attention_mask': [1, 1, 1]}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenized_small_eval_dataset</span><span class="p">[</span><span class="mi">100</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[14]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'label': 4,
 'text': 'Had a great dinner at Elephant Bar last night! \\n\\nGot a coupon in the mail for 2 meals and an appetizer for $20! While they did limit the  selections you could get with the coupon, we were happy with the choices so it worked out fine.\\n\\nFood was delicious and the service was fantastic! Waitress was very attentive and polite.\\n\\nLocation was a plus too! Had a lovely walk around The District shops afterward. \\n\\nAll and all, a hands down 5 stars!',
 'input_ids': [101, 6467, 102],
 'token_type_ids': [0, 0, 0],
 'attention_mask': [1, 1, 1]}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, uma chave foi adicionada com os <code>input_ids</code> dos tokens, os <code>token_type_ids</code> e outra com a <code>máscara de atenção</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora, fazemos a tokenização truncando para 20 tokens a fim de usar uma GPU pequena.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">tokenized_small_train_dataset</span> <span class="o">=</span> <span class="n">small_train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenized_small_eval_dataset</span> <span class="o">=</span> <span class="n">small_eval_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Temos que criar o modelo que vamos treinar novamente. Como se trata de um problema de classificação, usaremos o <code>AutoModelForSequenceClassification</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como pode ser visto, foi criado um modelo que classifica entre 5 classes</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="M%C3%A9tricas-de-avalia%C3%A7%C3%A3o">Métricas de avaliação<a class="anchor-link" href="#M%C3%A9tricas-de-avalia%C3%A7%C3%A3o">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Criamos uma métrica de avaliação com a biblioteca Hugging Face <code>evaluate</code>. Para instalá-la, usamos</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>evaluate
<span class="sb">```</span>
</pre></div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">evaluate</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Treinador">Treinador<a class="anchor-link" href="#Treinador">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Agora, para o treinamento, usamos o objeto <code>Trainer</code>. Para usar o <code>Trainer</code>, precisamos de <code>accelerate&gt;=0.21.0</code>.</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>accelerate&gt;<span class="o">=</span><span class="m">0</span>.21.0
<span class="sb">```</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Antes de criar o treinador, precisamos criar um <code>TrainingArguments</code>, que é um objeto que contém todos os argumentos que o <code>Trainer</code> precisa para o treinamento, ou seja, os hiperparâmetros</p>
</section>
<section class="section-block-markdown-cell">
<p>Um argumento obrigatório deve ser passado, <code>output_dir</code>, que é o diretório de saída onde as previsões do modelo e os pontos de verificação, como a Hugging Face chama os pesos do modelo, serão gravados.</p>
</section>
<section class="section-block-markdown-cell">
<p>Também transmitimos uma série de outros argumentos</p>
<ul>
<li><code>per_device_train_batch_size</code>: tamanho do lote por dispositivo para o treinamento</li>
<li><code>per_device_eval_batch_size</code>: tamanho do lote por dispositivo para a avaliação</li>
<li><code>learning_rate</code>: taxa de aprendizado</li>
<li><code>num_train_epochs</code>: número de épocas</li>
</ul>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">"test_trainer"</span><span class="p">,</span> 
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> 
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada em todos os hiperparâmetros que ele configura</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">training_args</span><span class="o">.</span><span class="vm">__dict__</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[19]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'output_dir': 'test_trainer',
 'overwrite_output_dir': False,
 'do_train': False,
 'do_eval': False,
 'do_predict': False,
 'evaluation_strategy': &lt;IntervalStrategy.NO: 'no'&gt;,
 'prediction_loss_only': False,
 'per_device_train_batch_size': 16,
 'per_device_eval_batch_size': 32,
 'per_gpu_train_batch_size': None,
 'per_gpu_eval_batch_size': None,
 'gradient_accumulation_steps': 1,
 'eval_accumulation_steps': None,
 'eval_delay': 0,
 'learning_rate': 0.0001,
 'weight_decay': 0.0,
 'adam_beta1': 0.9,
 'adam_beta2': 0.999,
 'adam_epsilon': 1e-08,
 'max_grad_norm': 1.0,
 'num_train_epochs': 5,
 'max_steps': -1,
 'lr_scheduler_type': &lt;SchedulerType.LINEAR: 'linear'&gt;,
 'lr_scheduler_kwargs': {},
 'warmup_ratio': 0.0,
 'warmup_steps': 0,
 'log_level': 'passive',
 'log_level_replica': 'warning',
 'log_on_each_node': True,
 'logging_dir': 'test_trainer/runs/Mar08_16-41-27_SAEL00531',
 'logging_strategy': &lt;IntervalStrategy.STEPS: 'steps'&gt;,
 'logging_first_step': False,
 'logging_steps': 500,
 'logging_nan_inf_filter': True,
 'save_strategy': &lt;IntervalStrategy.STEPS: 'steps'&gt;,
 'save_steps': 500,
 'save_total_limit': None,
 'save_safetensors': True,
 'save_on_each_node': False,
 'save_only_model': False,
 'no_cuda': False,
 'use_cpu': False,
 'use_mps_device': False,
 'seed': 42,
 'data_seed': None,
 'jit_mode_eval': False,
 'use_ipex': False,
 'bf16': False,
 'fp16': False,
 'fp16_opt_level': 'O1',
 'half_precision_backend': 'auto',
 'bf16_full_eval': False,
 'fp16_full_eval': False,
 'tf32': None,
 'local_rank': 0,
 'ddp_backend': None,
 'tpu_num_cores': None,
 'tpu_metrics_debug': False,
 'debug': [],
 'dataloader_drop_last': False,
 'eval_steps': None,
 'dataloader_num_workers': 0,
 'dataloader_prefetch_factor': None,
 'past_index': -1,
 'run_name': 'test_trainer',
 'disable_tqdm': False,
 'remove_unused_columns': True,
 'label_names': None,
 'load_best_model_at_end': False,
 'metric_for_best_model': None,
 'greater_is_better': None,
 'ignore_data_skip': False,
 'fsdp': [],
 'fsdp_min_num_params': 0,
 'fsdp_config': {'min_num_params': 0,
  'xla': False,
  'xla_fsdp_v2': False,
  'xla_fsdp_grad_ckpt': False},
 'fsdp_transformer_layer_cls_to_wrap': None,
 'accelerator_config': AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True),
 'deepspeed': None,
 'label_smoothing_factor': 0.0,
 'optim': &lt;OptimizerNames.ADAMW_TORCH: 'adamw_torch'&gt;,
 'optim_args': None,
 'adafactor': False,
 'group_by_length': False,
 'length_column_name': 'length',
 'report_to': [],
 'ddp_find_unused_parameters': None,
 'ddp_bucket_cap_mb': None,
 'ddp_broadcast_buffers': None,
 'dataloader_pin_memory': True,
 'dataloader_persistent_workers': False,
 'skip_memory_metrics': True,
 'use_legacy_prediction_loop': False,
 'push_to_hub': False,
 'resume_from_checkpoint': None,
 'hub_model_id': None,
 'hub_strategy': &lt;HubStrategy.EVERY_SAVE: 'every_save'&gt;,
 'hub_token': None,
 'hub_private_repo': False,
 'hub_always_push': False,
 'gradient_checkpointing': False,
 'gradient_checkpointing_kwargs': None,
 'include_inputs_for_metrics': False,
 'fp16_backend': 'auto',
 'push_to_hub_model_id': None,
 'push_to_hub_organization': None,
 'push_to_hub_token': None,
 'mp_parameters': '',
 'auto_find_batch_size': False,
 'full_determinism': False,
 'torchdynamo': None,
 'ray_scope': 'last',
 'ddp_timeout': 1800,
 'torch_compile': False,
 'torch_compile_backend': None,
 'torch_compile_mode': None,
 'dispatch_batches': None,
 'split_batches': None,
 'include_tokens_per_second': False,
 'include_num_input_tokens_seen': False,
 'neftune_noise_alpha': None,
 'distributed_state': Distributed environment: DistributedType.NO
 Num processes: 1
 Process index: 0
 Local process index: 0
 Device: cuda,
 '_n_gpu': 1,
 '__cached__setup_devices': device(type='cuda', index=0),
 'deepspeed_plugin': None}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, criamos um objeto <code>Trainer</code> que será responsável pelo treinamento do modelo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quando tivermos um <code>Trainer</code>, no qual indicamos o conjunto de dados de treinamento, o conjunto de dados de teste, o modelo, a métrica de avaliação e os argumentos do hiperparâmetro, poderemos treinar o modelo com o método <code>train</code> do <code>Trainer</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>  0%|          | 0/315 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>{'train_runtime': 52.3517, 'train_samples_per_second': 95.508, 'train_steps_per_second': 6.017, 'train_loss': 0.9347671750992064, 'epoch': 5.0}
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[21]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>TrainOutput(global_step=315, training_loss=0.9347671750992064, metrics={'train_runtime': 52.3517, 'train_samples_per_second': 95.508, 'train_steps_per_second': 6.017, 'train_loss': 0.9347671750992064, 'epoch': 5.0})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Já temos o modelo treinado, como você pode ver, com muito pouco código, podemos treinar um modelo muito rapidamente.</p>
</section>
<section class="section-block-markdown-cell">
<p>Recomendo enfaticamente que você aprenda o Pytorch e treine muitos modelos antes de usar uma biblioteca de alto nível como a <code>transformers</code>, porque você aprende muitos fundamentos de aprendizagem profunda e pode entender melhor o que está acontecendo, especialmente porque aprenderá muito com seus erros. Mas, depois de passar por esse período, o uso de bibliotecas de alto nível, como a <code>transformers</code>, acelera muito o desenvolvimento.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Testando-o-modelo">Testando o modelo<a class="anchor-link" href="#Testando-o-modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Agora que temos o modelo treinado, vamos testá-lo com um texto. Como o conjunto de dados que baixamos é de avaliações em inglês, vamos testá-lo com uma avaliação em inglês.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">clasificator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-classification"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">clasification</span> <span class="o">=</span> <span class="n">clasificator</span><span class="p">(</span><span class="s2">"I'm liking this post a lot"</span><span class="p">)</span>
<span class="n">clasification</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[23]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'label': 'LABEL_2', 'score': 0.5032550692558289}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver a que corresponde a classe que surgiu</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">clases</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[24]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'label': ClassLabel(names=['1 star', '2 star', '3 stars', '4 stars', '5 stars'], id=None),
 'text': Value(dtype='string', id=None)}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A relação seria</p>
<ul>
<li>LABEL_0: 1 estrela</li>
<li>LABEL_1: 2 estrelas</li>
<li>LABEL_2: 3 estrelas</li>
<li>LABEL_3: 4 estrelas</li>
<li>LABEL_4: 5 estrelas</li>
</ul>
<p>Portanto, você classificou o comentário com 3 estrelas. Lembre-se de que treinamos em um subconjunto de dados e com apenas 5 épocas, portanto, não esperamos que seja muito bom.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Compartilhe-o-modelo-no-Hub-do-Rosto-de-Abra%C3%A7o">Compartilhe o modelo no Hub do Rosto de Abraço<a class="anchor-link" href="#Compartilhe-o-modelo-no-Hub-do-Rosto-de-Abra%C3%A7o">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Quando tivermos o modelo retreinado, poderemos carregá-lo em nosso espaço no Hugging Face Hub para que outros possam usá-lo. Para fazer isso, você precisa ter uma conta no Hugging Face.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Registro-em-log">Registro em log<a class="anchor-link" href="#Registro-em-log">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para fazer upload do modelo, primeiro precisamos fazer login.</p>
<p>Isso pode ser feito por meio do terminal com</p>
<div class="highlight"><pre><span></span>huggingface-cli<span class="w"> </span>login
<span class="sb">```</span>

Ou<span class="w"> </span>por<span class="w"> </span>meio<span class="w"> </span><span class="k">do</span><span class="w"> </span>notebook,<span class="w"> </span>tendo<span class="w"> </span>instalado<span class="w"> </span>primeiro<span class="w"> </span>a<span class="w"> </span>biblioteca<span class="w"> </span><span class="sb">`</span>huggingface_hub<span class="sb">`</span><span class="w"> </span>com

<span class="sb">````</span>bash
pip<span class="w"> </span>install<span class="w"> </span>huggingface_hub
<span class="sb">```</span>

Agora,<span class="w"> </span>podemos<span class="w"> </span>fazer<span class="w"> </span>login<span class="w"> </span>com<span class="w"> </span>a<span class="w"> </span>função<span class="w"> </span><span class="sb">`</span>notebook_login<span class="sb">`</span>,<span class="w"> </span>que<span class="w"> </span>criará<span class="w"> </span>uma<span class="w"> </span>pequena<span class="w"> </span>interface<span class="w"> </span>gráfica<span class="w"> </span>na<span class="w"> </span>qual<span class="w"> </span>devemos<span class="w"> </span>inserir<span class="w"> </span>um<span class="w"> </span>token<span class="w"> </span>Hugging<span class="w"> </span>Face.
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Para criar um token, acesse a página <a href="https://huggingface.co/settings/tokens">setings/tokens</a> de sua conta, que terá a seguinte aparência</p>
<p>User-Access-Token-dark](<a href="http://maximofn.com/wp-content/uploads/2024/03/User-Access-Token-dark.png">http://maximofn.com/wp-content/uploads/2024/03/User-Access-Token-dark.png</a>)</p>
<p>Clique em <code>New token</code> e será exibida uma janela para criar um novo token.</p>
<p><img alt="new-token-dark" src="http://maximofn.com/wp-content/uploads/2024/03/new-token-dark.png"/></p>
<p>Nomeamos o token e o criamos com a função <code>write</code>.</p>
<p>Uma vez criado, nós o copiamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>

<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>VBox(children=(HTML(value='&lt;center&gt; &lt;img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Para-cima-depois-de-treinado">Para cima depois de treinado<a class="anchor-link" href="#Para-cima-depois-de-treinado">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Depois de treinar o modelo, podemos carregá-lo no Hub usando a função <code>push_to_hub</code>. Essa função tem um parâmetro obrigatório que é o nome do modelo, que deve ser exclusivo; se já houver um modelo em seu Hub com esse nome, ele não será carregado. Ou seja, o nome completo do modelo será <user>/<model>, portanto, o nome do modelo não pode existir em seu Hub, mesmo que haja outro modelo com o mesmo nome no Hub de outro usuário.</model></user></p>
<p>Ele também tem outros parâmetros opcionais, mas interessantes:</p>
<ul>
<li><code>use_temp_dir</code> (bool, opcional): se deve ou não ser usado um diretório temporário para armazenar arquivos salvos antes de serem enviados ao Hub. O padrão é True se não houver um diretório com o mesmo nome de <code>repo_id</code> e False caso contrário.</li>
<li><code>commit_message</code> (str, opcional): mensagem de confirmação. O padrão é <code>Upload {object}</code>.</li>
<li><code>private</code> (bool, opcional): se o repositório criado deve ser privado ou não.</li>
<li><code>token</code> (bool ou str, opcional): O token a ser usado como autorização HTTP para arquivos remotos. Se for True, será usado o token gerado pela execução do login <code>huggingface-cli</code> (armazenado em ~/.huggingface). O padrão é True se <code>repo_url</code> não for especificado.</li>
<li><code>max_shard_size</code> (int ou str, opcional, o padrão é "5GB"): Aplicável somente a modelos. O tamanho máximo de um ponto de verificação antes de ser fragmentado. Os pontos de verificação fragmentados serão cada um menor que esse tamanho. Se for expresso como uma cadeia de caracteres, deverá ter dígitos seguidos de uma unidade (como "5MB"). O padrão é "5 GB" para que os usuários possam carregar facilmente modelos em instâncias de nível gratuito do Google Colab sem problemas de OOM (falta de memória) da CPU.</li>
<li><code>create_pr</code> (bool, opcional, o padrão é False): se deve ou não criar um PR com os arquivos carregados ou fazer o commit diretamente.</li>
<li><code>safe_serialization</code> (bool, opcional, o padrão é True): Se os pesos do modelo devem ou não ser convertidos para o formato safetensors para uma serialização mais segura.</li>
<li><code>revision</code> (str, opcional): filial para a qual enviar os arquivos carregados.</li>
<li><code>commit_description</code> (str, opcional): Descrição do commit a ser criado</li>
<li><code>tags</code> (List[str], opcional): Lista de tags a serem inseridas no Hub.</li>
</ul>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span>
    <span class="s2">"bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset"</span><span class="p">,</span> 
    <span class="n">commit_message</span><span class="o">=</span><span class="s2">"bert base cased fine tune on yelp review subset"</span><span class="p">,</span>
    <span class="n">commit_description</span><span class="o">=</span><span class="s2">"Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs."</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>README.md:   0%|          | 0.00/5.18k [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>model.safetensors:   0%|          | 0.00/433M [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[26]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/bert-base-cased_notebook_transformers_5-epochs_yelp_review_subset/commit/033a3c759d5a4e314ce76db81bd113b4f7da69ad', commit_message='bert base cased fine tune on yelp review subset', commit_description='Fine-tuned on a subset of the yelp review dataset. Model retrained for post of transformers library. 5 epochs.', oid='033a3c759d5a4e314ce76db81bd113b4f7da69ad', pr_url=None, pr_revision=None, pr_num=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se formos agora ao nosso Hub, veremos que o modelo foi carregado.</p>
<p><img alt="transformers_commit_unique" src="http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_unico.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>Se agora entrarmos no cartão de modelo para ver</p>
<p>transformers_commit_inico_model_card](<a href="http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_inico_model_card-scaled.webp">http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_inico_model_card-scaled.webp</a>)</p>
<p>Vemos que tudo não está preenchido, faremos isso mais tarde.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Levantar-durante-o-treinamento">Levantar durante o treinamento<a class="anchor-link" href="#Levantar-durante-o-treinamento">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Outra opção é fazer o upload enquanto estivermos treinando o modelo. Isso é muito útil quando treinamos modelos por muitos períodos e isso leva muito tempo, pois se o treinamento for interrompido (porque o computador está desligado, a sessão de colaboração terminou, os créditos da nuvem acabaram), o trabalho não será perdido. Para fazer isso, você deve adicionar <code>push_to_hub=True</code> em <code>TrainingArguments</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">"bert-base-cased_notebook_transformers_30-epochs_yelp_review_subset"</span><span class="p">,</span> 
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> 
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_small_train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_small_eval_dataset</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Podemos ver que alteramos as épocas para 30, de modo que o treinamento será mais demorado, portanto, adicionar <code>push_to_hub=True</code> fará o upload do modelo para o nosso Hub durante o treinamento.</p>
<p>Também alteramos o <code>output_dir</code> porque esse é o nome que o modelo terá no Hub.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>  0%|          | 0/1890 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>{'loss': 0.2363, 'grad_norm': 8.151028633117676, 'learning_rate': 7.354497354497355e-05, 'epoch': 7.94}
{'loss': 0.0299, 'grad_norm': 0.0018280998338013887, 'learning_rate': 4.708994708994709e-05, 'epoch': 15.87}
{'loss': 0.0019, 'grad_norm': 0.000868947128765285, 'learning_rate': 2.0634920634920636e-05, 'epoch': 23.81}
{'train_runtime': 331.5804, 'train_samples_per_second': 90.476, 'train_steps_per_second': 5.7, 'train_loss': 0.07100234655318437, 'epoch': 30.0}
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[24]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>TrainOutput(global_step=1890, training_loss=0.07100234655318437, metrics={'train_runtime': 331.5804, 'train_samples_per_second': 90.476, 'train_steps_per_second': 5.7, 'train_loss': 0.07100234655318437, 'epoch': 30.0})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se olharmos novamente para o nosso hub, o novo modelo aparecerá.</p>
<p>transformers_commit_training](<a href="http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_training.webp">http://maximofn.com/wp-content/uploads/2024/03/transformers_commit_training.webp</a>)</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Hub-como-reposit%C3%B3rio-git">Hub como repositório git<a class="anchor-link" href="#Hub-como-reposit%C3%B3rio-git">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>No Hugging Face, os modelos, os espaços e os conjuntos de dados são repositórios git, portanto, você pode trabalhar com eles dessa forma. Ou seja, você pode clonar, bifurcar, fazer solicitações pull, etc.</p>
</section>
<section class="section-block-markdown-cell">
<p>Mas outra grande vantagem disso é que você pode usar um modelo em uma versão específica.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">"bert-base-cased"</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s2">"393e083"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>config.json:   0%|          | 0.00/433 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>pytorch_model.bin:   0%|          | 0.00/436M [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</section>
