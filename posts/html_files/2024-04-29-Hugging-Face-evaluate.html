<section class="section-block-markdown-cell">
<h1 id="Hugging Face evaluate">Hugging Face evaluate<a class="anchor-link" href="#Hugging Face evaluate">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>La librería <code>Evaluate</code> de <code>Hugging Face</code> es una librería para evaluar fácilmente modelos y datasets.</p>
<p>Con una sola línea de código, se tiene acceso a docenas de métodos de evaluación para diferentes dominios (NLP, computer vision, reinforcement learning y más). Ya sea en tu máquina local, o en una configuración de entrenamiento distribuida, puedes evaluar modelos de manera consistente y reproducible</p>
<p>En la página de <a href="https://huggingface.co/evaluate-metric">evaluate</a> en Hugging Face se puede obtener una lista completa de las métricas disponibles. Cada métrica tiene un <code>Space</code> de Hugging Face dedicado con una demostración interactiva sobre cómo usar la métrica y una tarjeta de documentación que detalla las limitaciones y el uso de las métricas.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Instalacion">Instalación<a class="anchor-link" href="#Instalacion">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para instalar la librería es necesario hacer</p>
<div class='highlight'><pre><code class="language-bash">pip install evaluate
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Tipo de evaluaciones">Tipo de evaluaciones<a class="anchor-link" href="#Tipo de evaluaciones">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Hay varios tipos de evaluaciones disponibles</p>
<ul>
  <li><code>metric</code>: Una métrica se utiliza para evaluar el rendimiento de un modelo y, por lo general, incluye las predicciones del modelo y las etiquetas ground truth.</li>
  <li><code>comparison</code>: Se utiliza para comparar dos modelos. Esto se puede hacer, por ejemplo, comparando sus predicciones con etiquetas ground truth.</li>
  <li><code>measurement</code>: El dataset es tan importante como el modelo entrenado en él. Con las mediciones se pueden investigar las propiedades de un conjunto de datos.</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h2 id="Carga">Carga<a class="anchor-link" href="#Carga">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Cada <code>metric</code>, <code>comparison</code> o <code>measurement</code> se puede cargar con el método <code>load</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="w"> </span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
<span class="n">accuracy</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EvaluationModule(name: &quot;accuracy&quot;, module_type: &quot;metric&quot;, features: &#x7B;&#x27;predictions&#x27;: Value(dtype=&#x27;int32&#x27;, id=None), &#x27;references&#x27;: Value(dtype=&#x27;int32&#x27;, id=None)&#x7D;, usage: &quot;&quot;&quot;
Args:
&#x20;&#x20;&#x20;&#x20;predictions (`list` of `int`): Predicted labels.
&#x20;&#x20;&#x20;&#x20;references (`list` of `int`): Ground truth labels.
&#x20;&#x20;&#x20;&#x20;normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.
&#x20;&#x20;&#x20;&#x20;sample_weight (`list` of `float`): Sample weights Defaults to None.

Returns:
&#x20;&#x20;&#x20;&#x20;accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.

Examples:

&#x20;&#x20;&#x20;&#x20;Example 1-A simple example
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;

&#x20;&#x20;&#x20;&#x20;Example 2-The same as Example 1, except with `normalize` set to `False`.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 3.0&#x7D;

&#x20;&#x20;&#x20;&#x20;Example 3-The same as Example 1, except with `sample_weight` set.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 0.8778625954198473&#x7D;
&quot;&quot;&quot;, stored examples: 0)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Si quieres estar seguro de cargar el tipo de métrica que deseas, sea <code>metric</code>, <code>comparison</code> o <code>measurement</code>, puedes hacerlo añadiendo el parámetro <code>module_type</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="w"> </span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">&quot;metric&quot;</span><span class="p">)</span>
<span class="n">word_length</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;word_length&quot;</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">&quot;measurement&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[nltk_data] Downloading package punkt to
[nltk_data]     /home/maximo.fernandez/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Carga de modulos de la comunidad">Carga de módulos de la comunidad<a class="anchor-link" href="#Carga de modulos de la comunidad">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>A parte de los propios módulos que ofrece la librería, también puedes cargar modelos que haya subido alguien al hub de Hugging Face</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">element_count</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;lvwerra/element_count&quot;</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">&quot;measurement&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Lista de modulos disponibles">Lista de módulos disponibles<a class="anchor-link" href="#Lista de modulos disponibles">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Si queremos obtener una lista de todos los módulos disponibles, tenemos que usar el método <code>list_evaluation_modules</code>. En él podemos poner filtros de búsqueda.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">evaluate</span><span class="o">.</span><span class="n">list_evaluation_modules</span><span class="p">(</span>
<span class="w">  </span><span class="n">module_type</span><span class="o">=</span><span class="s2">&quot;comparison&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">include_community</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="w">  </span><span class="n">with_details</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x7B;&#x27;name&#x27;: &#x27;ncoop57/levenshtein_distance&#x27;,
&#x20;&#x20;&#x27;type&#x27;: &#x27;comparison&#x27;,
&#x20;&#x20;&#x27;community&#x27;: True,
&#x20;&#x20;&#x27;likes&#x27;: 0&#x7D;,
 &#x7B;&#x27;name&#x27;: &#x27;kaleidophon/almost_stochastic_order&#x27;,
&#x20;&#x20;&#x27;type&#x27;: &#x27;comparison&#x27;,
&#x20;&#x20;&#x27;community&#x27;: True,
&#x20;&#x20;&#x27;likes&#x27;: 1&#x7D;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Atributos del modulo">Atributos del módulo<a class="anchor-link" href="#Atributos del modulo">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Todos los módulos de evaluación vienen con una variedad de atributos útiles que ayudan a utilizar el módulo, estos atributos son</p>
<table>
  <thead>
    <tr>
      <th>Atributo</th>
      <th>Descripción</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>description</td>
      <td>Una breve descripción del módulo de evaluación.</td>
    </tr>
    <tr>
      <td>citation</td>
      <td>Una cadena BibTex para citar cuando esté disponible.</td>
    </tr>
    <tr>
      <td>features</td>
      <td>Un objeto Features que define el formato de entrada.</td>
    </tr>
    <tr>
      <td>inputs_description</td>
      <td>Esto es equivalente a la cadena de documentación de los módulos.</td>
    </tr>
    <tr>
      <td>homepage</td>
      <td>La página de inicio del módulo.</td>
    </tr>
    <tr>
      <td>license</td>
      <td>La licencia del módulo.</td>
    </tr>
    <tr>
      <td>codebase_urls</td>
      <td>Enlace al código detrás del módulo.</td>
    </tr>
    <tr>
      <td>reference_urls</td>
      <td>URL de referencia adicionales.</td>
    </tr>
  </tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Veamos algunos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;description: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">description</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">citation: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">citation</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">features: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">features</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">inputs_description: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">inputs_description</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">homepage: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">homepage</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">license: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">license</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">codebase_urls: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">codebase_urls</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">reference_urls: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">reference_urls</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>description: 
Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
 Where:
TP: True positive
TN: True negative
FP: False positive
FN: False negative


citation: 
@article&#x7B;scikit-learn,
&#x20;&#x20;title=&#x7B;Scikit-learn: Machine Learning in &#x7B;P&#x7D;ython&#x7D;,
&#x20;&#x20;author=&#x7B;Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.&#x7D;,
&#x20;&#x20;journal=&#x7B;Journal of Machine Learning Research&#x7D;,
&#x20;&#x20;volume=&#x7B;12&#x7D;,
&#x20;&#x20;pages=&#x7B;2825--2830&#x7D;,
&#x20;&#x20;year=&#x7B;2011&#x7D;
&#x7D;


features: &#x7B;&#x27;predictions&#x27;: Value(dtype=&#x27;int32&#x27;, id=None), &#x27;references&#x27;: Value(dtype=&#x27;int32&#x27;, id=None)&#x7D;

inputs_description: 
Args:
&#x20;&#x20;&#x20;&#x20;predictions (`list` of `int`): Predicted labels.
&#x20;&#x20;&#x20;&#x20;references (`list` of `int`): Ground truth labels.
&#x20;&#x20;&#x20;&#x20;normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.
&#x20;&#x20;&#x20;&#x20;sample_weight (`list` of `float`): Sample weights Defaults to None.

Returns:
&#x20;&#x20;&#x20;&#x20;accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.

Examples:

&#x20;&#x20;&#x20;&#x20;Example 1-A simple example
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;

&#x20;&#x20;&#x20;&#x20;Example 2-The same as Example 1, except with `normalize` set to `False`.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 3.0&#x7D;

&#x20;&#x20;&#x20;&#x20;Example 3-The same as Example 1, except with `sample_weight` set.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 0.8778625954198473&#x7D;


homepage: 

license: 

codebase_urls: []

reference_urls: [&#x27;https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html&#x27;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Ejecucion">Ejecución<a class="anchor-link" href="#Ejecucion">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Ahora que sabemos cómo funciona el módulo de evaluación y qué debe contener, vamos a usarlo. Cuando se trata de calcular la evaluación, hay dos formas principales de hacerlo:</p>
<ul>
  <li>Todo en uno</li>
  <li>Incremental</li>
</ul>
<p>En el enfoque incremental, las entradas necesarias se agregan al módulo con <code>EvaluationModule.add()</code> o <code>EvaluationModule.add_batch()</code> y la puntuación se calcula al final con <code>EvaluationModule.compute()</code>. Alternativamente, se pueden pasar todas las entradas a la vez a <code>compute()</code>.</p>
<p>Veamos estos dos enfoques.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Todo en uno">Todo en uno<a class="anchor-link" href="#Todo en uno">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Una vez tenemos todas las predicciones y ground truth podemos calcular la métrica. Una vez que tenemos un módulo definido, le pasamos las predicciones y los ground truth mediante el método <code>compute()</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">accuracy_value</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>
<span class="n">accuracy_value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Incremental">Incremental<a class="anchor-link" href="#Incremental">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>En muchos procesos de evaluación, las predicciones se construyen de forma iterativa, como en un bucle for. En ese caso, podrías almacenar las predicciones y ground truth en una lista y al final pasarlas a <code>compute()</code>.</p>
<p>Sin embargo, con los métodos <code>add()</code> y <code>add_batch()</code> puedes evitar el paso de almacenar las predicciones.</p>
</section>
<section class="section-block-markdown-cell">
<p>Si tienes todas las predicciones de un solo batch, hay que usar el método <code>add()</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">ref</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]):</span>
<span class="w">    </span><span class="n">accuracy</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">references</span><span class="o">=</span><span class="n">ref</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">pred</span><span class="p">)</span>
<span class="n">accuracy_value</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">accuracy_value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Sin embargo, cuando se tienen predicciones de varios batches se tiene que usar el método <code>add_batch()</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">refs</span><span class="p">,</span> <span class="n">preds</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]):</span>
<span class="w">    </span><span class="n">accuracy</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">references</span><span class="o">=</span><span class="n">refs</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">accuracy_value</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">accuracy_value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Combinacion de varias evaluaciones">Combinación de varias evaluaciones<a class="anchor-link" href="#Combinacion de varias evaluaciones">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>A menudo, uno quiere no solo evaluar una única métrica, sino también una variedad de métricas diferentes que capturan diferentes aspectos de un modelo. Por ejemplo, para la clasificación suele ser una buena idea calcular el <code>F1</code>, el <code>recall</code> y la <code>precisión</code> además del <code>accuracy</code> para obtener una mejor imagen del rendimiento del modelo. <code>Evaluate</code> permite cargar un montón de métricas y llamarlas secuencialmente. Sin embargo, la forma más conveniente es usar la función <code>combine()</code> para agruparlas</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">clasification_metrics</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">combine</span><span class="p">([</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">,</span> <span class="s2">&quot;recall&quot;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">targets</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">clasification_metrics</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.6666666666666666,
 &#x27;f1&#x27;: 0.6666666666666666,
 &#x27;precision&#x27;: 1.0,
 &#x27;recall&#x27;: 0.5&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Guardar los resultados">Guardar los resultados<a class="anchor-link" href="#Guardar los resultados">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Podemos guardar los resultados de la evaluación en un archivo con el método <code>save()</code>, para ello le pasamos un nombre de archivo. Podemos pasarle parámetros como el número del experimento</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">references</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">targets</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">result</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">hyperparams</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">}</span>
<span class="n">evaluate</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;./results/&quot;</span><span class="p">,</span> <span class="n">experiment</span><span class="o">=</span><span class="s2">&quot;run 42&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">result</span><span class="p">,</span> <span class="o">**</span><span class="n">hyperparams</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>PosixPath(&#x27;results/result-2024_04_25-17_45_41.json&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos, hemos tenido que crear una variable <code>hyperparams</code> para pasársela al método <code>save()</code>. Esto normalmente no hará falta porque ya tendremos los del modelo que estemos entrenando</p>
</section>
<section class="section-block-markdown-cell">
<p>Esto creará un <code>json</code> con toda la información</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="w"> </span>
<span class="n">path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;./results/&quot;</span><span class="p">)</span>
<span class="n">files</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*&quot;</span><span class="p">))</span>
<span class="n">files</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[PosixPath(&#x27;results/result-2024_04_25-17_45_41.json&#x27;)]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="n">result_file</span> <span class="o">=</span> <span class="n">files</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">result_json</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">result_file</span><span class="p">)</span><span class="o">.</span><span class="n">read_text</span><span class="p">()</span>
<span class="n">result_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">result_json</span><span class="p">)</span>
<span class="n">result_dict</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;experiment&#x27;: &#x27;run 42&#x27;,
 &#x27;accuracy&#x27;: 0.5,
 &#x27;model&#x27;: &#x27;bert-base-uncased&#x27;,
 &#x27;_timestamp&#x27;: &#x27;2024-04-25T17:45:41.218287&#x27;,
 &#x27;_git_commit_hash&#x27;: &#x27;8725338b6bf9c97274685df41b2ee6e11319a735&#x27;,
 &#x27;_evaluate_version&#x27;: &#x27;0.4.1&#x27;,
 &#x27;_python_version&#x27;: &#x27;3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]&#x27;,
 &#x27;_interpreter_path&#x27;: &#x27;/home/maximo.fernandez/miniconda3/envs/nlp/bin/python&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Subir los resultados al hub">Subir los resultados al hub<a class="anchor-link" href="#Subir los resultados al hub">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>En caso de estar entrenando un modelo, podemos subir a la model card del modelo los resultados de la evaluación con el método <code>push_to_hub()</code>. De esta manera aparecerán en la página del modelo</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Evaluador">Evaluador<a class="anchor-link" href="#Evaluador">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Si tenemos un modelo, un dataset y una métrica, podemos hacer inferencia por todo el dataset y pasarle al evaluador las predicciones y las etiquetas reales para que nos devuelva la métrica y así obtener las métricas del modelo.</p>
<p>O podemos darle todo a la librería y que haga el trabajo por nosotros. Mediante el método <code>evaluator()</code>, le pasamos el modelo, el dataset y la métrica y el método se encarga de hacerlo todo por nosotros</p>
</section>
<section class="section-block-markdown-cell">
<p>Primero definimos el modelo, el dataset y la métrica</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">evaluate</span><span class="w"> </span><span class="kn">import</span> <span class="n">evaluator</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="w"> </span>
<span class="n">model_pipeline</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-classification&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;lvwerra/distilbert-imdb&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;imdb&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora le pasamos todo a <code>evaluator()</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">task_evaluator</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="s2">&quot;text-classification&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">results</span> <span class="o">=</span> <span class="n">task_evaluator</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">model_or_pipeline</span><span class="o">=</span><span class="n">model_pipeline</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span>
<span class="w">                       </span><span class="n">label_mapping</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;NEGATIVE&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;POSITIVE&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},)</span>
<span class="n">results</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.933,
 &#x27;total_time_in_seconds&#x27;: 29.43192940400013,
 &#x27;samples_per_second&#x27;: 33.97670557962431,
 &#x27;latency_in_seconds&#x27;: 0.02943192940400013&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Gracias al evaluador, hemos podido obtener las métricas del modelo sin tener que hacer nosotros la inferencia</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Visualizacion">Visualización<a class="anchor-link" href="#Visualizacion">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>A veces obtenemos distintas métricas para diferentes modelos, lo que hace que no sea fácil poder compararlos, por lo que mediante gráficos se hace más fácil.</p>
<p>La librería <code>Evaluate</code> ofrece diferentes visualizaciones mediante el método <code>visualization()</code>. Tenemos que pasarle los datos como una lista de diccionarios, donde cada diccionario tiene que tener las mismas claves</p>
</section>
<section class="section-block-markdown-cell">
<p>Para poder usar esta función es necesario tener instalada la librería <code>matplotlib</code></p>
<div class='highlight'><pre><code class="language-bash">pip install matplotlib
</code></pre></div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">evaluate.visualization</span><span class="w"> </span><span class="kn">import</span> <span class="n">radar_plot</span>
<span class="w"> </span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
<span class="w">   </span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span> <span class="s2">&quot;latency_in_seconds&quot;</span><span class="p">:</span> <span class="mf">33.6</span><span class="p">},</span>
<span class="w">   </span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="mf">0.87</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">0.91</span><span class="p">,</span> <span class="s2">&quot;latency_in_seconds&quot;</span><span class="p">:</span> <span class="mf">11.2</span><span class="p">},</span>
<span class="w">   </span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="mf">0.78</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">0.88</span><span class="p">,</span> <span class="s2">&quot;latency_in_seconds&quot;</span><span class="p">:</span> <span class="mf">87.6</span><span class="p">},</span> 
<span class="w">   </span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.88</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="mf">0.78</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">0.81</span><span class="p">,</span> <span class="s2">&quot;latency_in_seconds&quot;</span><span class="p">:</span> <span class="mf">101.6</span><span class="p">}</span>
<span class="w">   </span><span class="p">]</span>
<span class="w"> </span>
<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Model 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 4&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">radar_plot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">model_names</span><span class="o">=</span><span class="n">model_names</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>/tmp/ipykernel_10271/263559674.py:14: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
&#x20;&#x20;plot.show()
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;Figure size 640x480 with 5 Axes&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora podemos comparar visualmente los 4 modelos y elegir el óptimo, en función de una o varias métricas</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Evaluar el modelo en un conjunto de tareas">Evaluar el modelo en un conjunto de tareas<a class="anchor-link" href="#Evaluar el modelo en un conjunto de tareas">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Podemos evaluar un modelo, por ejemplo, para diferentes datasets. Para ello podemos usar el método <code>evaluation_suite</code>. Por ejmplo vamos a crear un evaluador que evalua un modelo en los conjuntos de datos <code>imdb</code> y <code>sst2</code>. Vamos a ver estos conjuntos de datos, para eso usamos el método <code>load_dataset_builder</code> para no tener que descargar el dataset completo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset_builder</span>
<span class="n">imdb</span> <span class="o">=</span> <span class="n">load_dataset_builder</span><span class="p">(</span><span class="s2">&quot;imdb&quot;</span><span class="p">)</span>
<span class="n">imdb</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">features</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None),
 &#x27;label&#x27;: ClassLabel(names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset_builder</span>
<span class="n">sst2</span> <span class="o">=</span> <span class="n">load_dataset_builder</span><span class="p">(</span><span class="s2">&quot;sst2&quot;</span><span class="p">)</span>
<span class="n">sst2</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">features</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;idx&#x27;: Value(dtype=&#x27;int32&#x27;, id=None),
 &#x27;sentence&#x27;: Value(dtype=&#x27;string&#x27;, id=None),
 &#x27;label&#x27;: ClassLabel(names=[&#x27;negative&#x27;, &#x27;positive&#x27;], id=None)&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como podemos ver, con el dataset <code>imdb</code> necesitamos coger la columna <code>text</code> para obtener el texto y la columna <code>label</code> para obtener el target. Con el dataset <code>sst2</code> necesitamos coger la columna <code>sentence</code> para obtener el texto y la columna <code>label</code> para obtener el target</p>
</section>
<section class="section-block-markdown-cell">
<p>Creamos el evaluador para los dos datasets</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">evaluate.evaluation_suite</span><span class="w"> </span><span class="kn">import</span> <span class="n">SubTask</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">Suite</span><span class="p">(</span><span class="n">evaluate</span><span class="o">.</span><span class="n">EvaluationSuite</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">        </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">suite</span> <span class="o">=</span> <span class="p">[</span>
<span class="w">            </span><span class="n">SubTask</span><span class="p">(</span>
<span class="w">                </span><span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;text-classification&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">data</span><span class="o">=</span><span class="s2">&quot;imdb&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">split</span><span class="o">=</span><span class="s2">&quot;test[:1]&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">args_for_task</span><span class="o">=</span><span class="p">{</span>
<span class="w">                    </span><span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;input_column&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;label_column&quot;</span><span class="p">:</span> <span class="s2">&quot;label&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;label_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="w">                        </span><span class="s2">&quot;LABEL_0&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="w">                        </span><span class="s2">&quot;LABEL_1&quot;</span><span class="p">:</span> <span class="mf">1.0</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">            </span><span class="p">),</span>
<span class="w">            </span><span class="n">SubTask</span><span class="p">(</span>
<span class="w">                </span><span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;text-classification&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">data</span><span class="o">=</span><span class="s2">&quot;sst2&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">split</span><span class="o">=</span><span class="s2">&quot;test[:1]&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">args_for_task</span><span class="o">=</span><span class="p">{</span>
<span class="w">                    </span><span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;input_column&quot;</span><span class="p">:</span> <span class="s2">&quot;sentence&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;label_column&quot;</span><span class="p">:</span> <span class="s2">&quot;label&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;label_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="w">                        </span><span class="s2">&quot;LABEL_0&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="w">                        </span><span class="s2">&quot;LABEL_1&quot;</span><span class="p">:</span> <span class="mf">1.0</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">        </span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se puede ver en <code>split=&quot;test[:1]&quot;,</code> que solo cogemos un ejemplo del subconjunto de test para este notebook y que la ejecución no lleve mucho tiempo</p>
</section>
<section class="section-block-markdown-cell">
<p>Ahora evaluamos con el modelo <code>huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">evaluate</span><span class="w"> </span><span class="kn">import</span> <span class="n">EvaluationSuite</span>
<span class="n">suite</span> <span class="o">=</span> <span class="n">EvaluationSuite</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mathemakitten/sentiment-evaluation-suite&#39;</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">suite</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span><span class="p">)</span>
<span class="n">results</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>`data` is a preloaded Dataset! Ignoring `subset` and `split`.
`data` is a preloaded Dataset! Ignoring `subset` and `split`.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x7B;&#x27;accuracy&#x27;: 0.3,
&#x20;&#x20;&#x27;total_time_in_seconds&#x27;: 1.4153412349987775,
&#x20;&#x20;&#x27;samples_per_second&#x27;: 7.06543394110088,
&#x20;&#x20;&#x27;latency_in_seconds&#x27;: 0.14153412349987776,
&#x20;&#x20;&#x27;task_name&#x27;: &#x27;imdb&#x27;,
&#x20;&#x20;&#x27;data_preprocessor&#x27;: &#x27;&amp;lt;function Suite.__init__.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7f3ff27a5080&amp;gt;&#x27;&#x7D;,
 &#x7B;&#x27;accuracy&#x27;: 0.0,
&#x20;&#x20;&#x27;total_time_in_seconds&#x27;: 0.1323430729971733,
&#x20;&#x20;&#x27;samples_per_second&#x27;: 75.56118936586572,
&#x20;&#x20;&#x27;latency_in_seconds&#x27;: 0.013234307299717328,
&#x20;&#x20;&#x27;task_name&#x27;: &#x27;sst2&#x27;,
&#x20;&#x20;&#x27;data_preprocessor&#x27;: &#x27;&amp;lt;function Suite.__init__.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7f3f2a9cc720&amp;gt;&#x27;&#x7D;]
</pre>
</div>
</div>
</div>
</section>