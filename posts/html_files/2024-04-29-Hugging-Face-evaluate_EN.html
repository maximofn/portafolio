<section class="section-block-markdown-cell">
<h1 id="Hugging Face evaluate">Hugging Face evaluate<a class="anchor-link" href="#Hugging Face evaluate">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>The <code>Evaluate</code> library of <code>Hugging Face</code> is a library to easily evaluate models and datasets.</p>
<p>With a single line of code, you have access to dozens of evaluation methods for different domains (NLP, computer vision, reinforcement learning and more). Whether on your local machine, or in a distributed training setup, you can evaluate models in a consistent and reproducible manner.</p>
<p>A complete list of available metrics can be obtained from the <a href="https://huggingface.co/evaluate-metric">evaluate</a> page in Hugging Face. Each metric has a dedicated Hugging Face <code>Space</code> with an interactive demonstration on how to use the metric and a documentation card detailing the limitations and use of the metrics.</p>
</section>
<section class="section-block-markdown-cell">
<p>This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
<h2 id="Installation">Installation<a class="anchor-link" href="#Installation">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>To install the library it is necessary to do the following</p>
<div class='highlight'><pre><code class="language-bash">pip install evaluate
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Type of evaluations">Type of evaluations<a class="anchor-link" href="#Type of evaluations">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Several types of evaluations are available</p>
<ul>
  <li>Metric: A metric is used to evaluate the performance of a model and usually includes model predictions and ground truth labels.</li>
  <li><code>comparison</code>: Used to compare two models. This can be done, for example, by comparing their predictions with ground truth labels.</li>
  <li>Measurement: The dataset is as important as the model trained on it. With measurements the properties of a dataset can be investigated.</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h2 id="Load">Load<a class="anchor-link" href="#Load">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Each <code>metric</code>, <code>comparison</code> or <code>measurement</code> can be loaded with the method load</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="w"> </span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
<span class="n">accuracy</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EvaluationModule(name: &quot;accuracy&quot;, module_type: &quot;metric&quot;, features: &#x7B;&#x27;predictions&#x27;: Value(dtype=&#x27;int32&#x27;, id=None), &#x27;references&#x27;: Value(dtype=&#x27;int32&#x27;, id=None)&#x7D;, usage: &quot;&quot;&quot;
Args:
&#x20;&#x20;&#x20;&#x20;predictions (`list` of `int`): Predicted labels.
&#x20;&#x20;&#x20;&#x20;references (`list` of `int`): Ground truth labels.
&#x20;&#x20;&#x20;&#x20;normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.
&#x20;&#x20;&#x20;&#x20;sample_weight (`list` of `float`): Sample weights Defaults to None.

Returns:
&#x20;&#x20;&#x20;&#x20;accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.

Examples:

&#x20;&#x20;&#x20;&#x20;Example 1-A simple example
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;

&#x20;&#x20;&#x20;&#x20;Example 2-The same as Example 1, except with `normalize` set to `False`.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 3.0&#x7D;

&#x20;&#x20;&#x20;&#x20;Example 3-The same as Example 1, except with `sample_weight` set.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 0.8778625954198473&#x7D;
&quot;&quot;&quot;, stored examples: 0)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>If you want to be sure to load the type of metric you want, whether <code>metric</code>, <code>comparison</code> or <code>measurement</code> type, you can do it by adding the <code>module_type</code> parameter</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="w"> </span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">&quot;metric&quot;</span><span class="p">)</span>
<span class="n">word_length</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;word_length&quot;</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">&quot;measurement&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[nltk_data] Downloading package punkt to
[nltk_data]     /home/maximo.fernandez/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Community module loading">Community module loading<a class="anchor-link" href="#Community module loading">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>In addition to the modules offered by the library, you can also upload models uploaded by someone else to the Hugging Face hub.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">element_count</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;lvwerra/element_count&quot;</span><span class="p">,</span> <span class="n">module_type</span><span class="o">=</span><span class="s2">&quot;measurement&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="List of available modules">List of available modules<a class="anchor-link" href="#List of available modules">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>If we want to get a list of all the available modules we have to use the <code>list_evaluation_modules</code> method, in which we can put search filters</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">evaluate</span><span class="o">.</span><span class="n">list_evaluation_modules</span><span class="p">(</span>
<span class="w">  </span><span class="n">module_type</span><span class="o">=</span><span class="s2">&quot;comparison&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="n">include_community</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="w">  </span><span class="n">with_details</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x7B;&#x27;name&#x27;: &#x27;ncoop57/levenshtein_distance&#x27;,
&#x20;&#x20;&#x27;type&#x27;: &#x27;comparison&#x27;,
&#x20;&#x20;&#x27;community&#x27;: True,
&#x20;&#x20;&#x27;likes&#x27;: 0&#x7D;,
 &#x7B;&#x27;name&#x27;: &#x27;kaleidophon/almost_stochastic_order&#x27;,
&#x20;&#x20;&#x27;type&#x27;: &#x27;comparison&#x27;,
&#x20;&#x20;&#x27;community&#x27;: True,
&#x20;&#x20;&#x27;likes&#x27;: 1&#x7D;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Module attributes">Module attributes<a class="anchor-link" href="#Module attributes">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>All evaluation modules come with a variety of useful attributes that help you use the module.</p>
<table>
  <thead>
    <tr>
      <th>Attribute</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>description</td>
      <td>A brief description of the evaluation module.</td>
    </tr>
    <tr>
      <td>citation</td>
      <td>A BibTex string to cite when available.</td>
    </tr>
    <tr>
      <td>features</td>
      <td>A Features object that defines the input format.</td>
    </tr>
    <tr>
      <td>inputs_description</td>
      <td>This is equivalent to the module documentation string.</td>
    </tr>
    <tr>
      <td>homepage</td>
      <td>The home page of the module.</td>
    </tr>
    <tr>
      <td>license</td>
      <td>The module license.</td>
    </tr>
    <tr>
      <td>codebase_urls</td>
      <td>Link to the code behind the module.</td>
    </tr>
    <tr>
      <td>reference_urls</td>
      <td>additional reference URLs.</td>
    </tr>
  </tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Let's take a look at some of them</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;description: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">description</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">citation: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">citation</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">features: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">features</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">inputs_description: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">inputs_description</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">homepage: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">homepage</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">license: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">license</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">codebase_urls: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">codebase_urls</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">reference_urls: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">reference_urls</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>description: 
Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
 Where:
TP: True positive
TN: True negative
FP: False positive
FN: False negative


citation: 
@article&#x7B;scikit-learn,
&#x20;&#x20;title=&#x7B;Scikit-learn: Machine Learning in &#x7B;P&#x7D;ython&#x7D;,
&#x20;&#x20;author=&#x7B;Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.&#x7D;,
&#x20;&#x20;journal=&#x7B;Journal of Machine Learning Research&#x7D;,
&#x20;&#x20;volume=&#x7B;12&#x7D;,
&#x20;&#x20;pages=&#x7B;2825--2830&#x7D;,
&#x20;&#x20;year=&#x7B;2011&#x7D;
&#x7D;


features: &#x7B;&#x27;predictions&#x27;: Value(dtype=&#x27;int32&#x27;, id=None), &#x27;references&#x27;: Value(dtype=&#x27;int32&#x27;, id=None)&#x7D;

inputs_description: 
Args:
&#x20;&#x20;&#x20;&#x20;predictions (`list` of `int`): Predicted labels.
&#x20;&#x20;&#x20;&#x20;references (`list` of `int`): Ground truth labels.
&#x20;&#x20;&#x20;&#x20;normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.
&#x20;&#x20;&#x20;&#x20;sample_weight (`list` of `float`): Sample weights Defaults to None.

Returns:
&#x20;&#x20;&#x20;&#x20;accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.

Examples:

&#x20;&#x20;&#x20;&#x20;Example 1-A simple example
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;

&#x20;&#x20;&#x20;&#x20;Example 2-The same as Example 1, except with `normalize` set to `False`.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 3.0&#x7D;

&#x20;&#x20;&#x20;&#x20;Example 3-The same as Example 1, except with `sample_weight` set.
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; accuracy_metric = evaluate.load(&quot;accuracy&quot;)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&amp;gt;&amp;gt;&amp;gt; print(results)
&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x20;&#x7B;&#x27;accuracy&#x27;: 0.8778625954198473&#x7D;


homepage: 

license: 

codebase_urls: []

reference_urls: [&#x27;https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html&#x27;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Execution">Execution<a class="anchor-link" href="#Execution">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Now that we know how the evaluation module works and what it should contain, let's use it. When it comes to calculating the evaluation, there are two main ways to do it:</p>
<ul>
  <li>All in one</li>
  <li>Incremental</li>
</ul>
<p>In the incremental approach, the required entries are added to the module with <code>EvaluationModule.add()</code> or <code>EvaluationModule.add_batch()</code> and the score is computed at the end with <code>EvaluationModule.compute()</code>. Alternatively, all entries can be passed at once to <code>compute()</code>.</p>
<p>Let's look at these two approaches.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="All in one">All in one<a class="anchor-link" href="#All in one">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Once we have all the predictions and ground truth we can calculate the metric. Once we have a module defined, we pass it the predictions and ground truth using the <code>compute()</code> method.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">accuracy_value</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>
<span class="n">accuracy_value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Incremental">Incremental<a class="anchor-link" href="#Incremental">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>In many evaluation processes, predictions are built iteratively, as in a for loop. In that case, you could store the predictions and ground truth in a list and at the end pass them to <code>compute()</code>.</p>
<p>However with the <code>add()</code> and <code>add_batch()</code> methods you can avoid the step of storing the predictions.</p>
</section>
<section class="section-block-markdown-cell">
<p>If you have all the predictions of a single batch you must use the <code>add()</code> method.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">ref</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]):</span>
<span class="w">    </span><span class="n">accuracy</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">references</span><span class="o">=</span><span class="n">ref</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">pred</span><span class="p">)</span>
<span class="n">accuracy_value</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">accuracy_value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>However, when you have predictions of several batches you have to use the <code>add_batch()</code> method.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">refs</span><span class="p">,</span> <span class="n">preds</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]):</span>
<span class="w">    </span><span class="n">accuracy</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">references</span><span class="o">=</span><span class="n">refs</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">preds</span><span class="p">)</span>
<span class="n">accuracy_value</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">accuracy_value</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.5&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Combination of several evaluations">Combination of several evaluations<a class="anchor-link" href="#Combination of several evaluations">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Often, one wants to evaluate not only a single metric, but also a variety of different metrics that capture different aspects of a model. For example, for classification it is often a good idea to calculate <code>F1</code>, <code>recall</code> and <code>accuracy</code> in addition to <code>accuracy</code> to get a better picture of model performance. <code>Evaluate</code> allows one to load a bunch of metrics and call them sequentially. However, the most convenient way is to use the <code>combine()</code> function to group them.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">clasification_metrics</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">combine</span><span class="p">([</span><span class="s2">&quot;accuracy&quot;</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">,</span> <span class="s2">&quot;recall&quot;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">targets</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">clasification_metrics</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.6666666666666666,
 &#x27;f1&#x27;: 0.6666666666666666,
 &#x27;precision&#x27;: 1.0,
 &#x27;recall&#x27;: 0.5&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Save results">Save results<a class="anchor-link" href="#Save results">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>We can save the evaluation results in a file with the <code>save()</code> method by passing a file name. We can pass parameters such as the experiment number</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">references</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">targets</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">result</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">,</span> <span class="n">predictions</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">hyperparams</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">}</span>
<span class="n">evaluate</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;./results/&quot;</span><span class="p">,</span> <span class="n">experiment</span><span class="o">=</span><span class="s2">&quot;run 42&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">result</span><span class="p">,</span> <span class="o">**</span><span class="n">hyperparams</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>PosixPath(&#x27;results/result-2024_04_25-17_45_41.json&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see we have had to create a <code>hyperparams</code> variable to pass it to the <code>save()</code> method. This normally will not be necessary because we will already have those of the model that we are training.</p>
</section>
<section class="section-block-markdown-cell">
<p>This will create a <code>json</code> with all the information</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="w"> </span>
<span class="n">path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;./results/&quot;</span><span class="p">)</span>
<span class="n">files</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*&quot;</span><span class="p">))</span>
<span class="n">files</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[PosixPath(&#x27;results/result-2024_04_25-17_45_41.json&#x27;)]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="n">result_file</span> <span class="o">=</span> <span class="n">files</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">result_json</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">result_file</span><span class="p">)</span><span class="o">.</span><span class="n">read_text</span><span class="p">()</span>
<span class="n">result_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">result_json</span><span class="p">)</span>
<span class="n">result_dict</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;experiment&#x27;: &#x27;run 42&#x27;,
 &#x27;accuracy&#x27;: 0.5,
 &#x27;model&#x27;: &#x27;bert-base-uncased&#x27;,
 &#x27;_timestamp&#x27;: &#x27;2024-04-25T17:45:41.218287&#x27;,
 &#x27;_git_commit_hash&#x27;: &#x27;8725338b6bf9c97274685df41b2ee6e11319a735&#x27;,
 &#x27;_evaluate_version&#x27;: &#x27;0.4.1&#x27;,
 &#x27;_python_version&#x27;: &#x27;3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]&#x27;,
 &#x27;_interpreter_path&#x27;: &#x27;/home/maximo.fernandez/miniconda3/envs/nlp/bin/python&#x27;&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Upload results to the hub">Upload results to the hub<a class="anchor-link" href="#Upload results to the hub">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>In case we are training a model, we can upload to the model card of the model the results of the evaluation with the <code>push_to_hub()</code> method. In this way they will appear in the model page.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Evaluator">Evaluator<a class="anchor-link" href="#Evaluator">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>If we have a model, a dataset and a metric, we can do inference for the whole dataset and pass the predictions and the actual labels to the evaluator to return the metric and thus obtain the model metrics.</p>
<p>Or we can give everything to the library and let it do the work for us. Using the <code>evaluator()</code> method, we pass it the model, the dataset and the metric and the method does everything for us.</p>
</section>
<section class="section-block-markdown-cell">
<p>First we define the model, the dataset and the metric</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">evaluate</span><span class="w"> </span><span class="kn">import</span> <span class="n">evaluator</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="w"> </span>
<span class="n">model_pipeline</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-classification&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;lvwerra/distilbert-imdb&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;imdb&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we pass everything to <code>evaluator()</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">task_evaluator</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="s2">&quot;text-classification&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">results</span> <span class="o">=</span> <span class="n">task_evaluator</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">model_or_pipeline</span><span class="o">=</span><span class="n">model_pipeline</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span>
<span class="w">                       </span><span class="n">label_mapping</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;NEGATIVE&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;POSITIVE&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},)</span>
<span class="n">results</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;accuracy&#x27;: 0.933,
 &#x27;total_time_in_seconds&#x27;: 29.43192940400013,
 &#x27;samples_per_second&#x27;: 33.97670557962431,
 &#x27;latency_in_seconds&#x27;: 0.02943192940400013&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Thanks to the evaluator we were able to obtain the model metrics without having to make the inference ourselves.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Display">Display<a class="anchor-link" href="#Display">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Sometimes we get different metrics for different models, which makes it difficult to compare them, so graphs make it easier.</p>
<p>The <code>Evaluate</code> library offers different visualizations through the <code>visualization()</code> method. We have to pass the data to it as a list of dictionaries, where each dictionary has to have the same keys</p>
</section>
<section class="section-block-markdown-cell">
<p>In order to use this function it is necessary to have the <code>matplotlib</code> library installed.</p>
<div class='highlight'><pre><code class="language-bash">pip install matplotlib
</code></pre></div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">evaluate.visualization</span><span class="w"> </span><span class="kn">import</span> <span class="n">radar_plot</span>
<span class="w"> </span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
<span class="w">   </span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">0.95</span><span class="p">,</span> <span class="s2">&quot;latency_in_seconds&quot;</span><span class="p">:</span> <span class="mf">33.6</span><span class="p">},</span>
<span class="w">   </span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="mf">0.87</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">0.91</span><span class="p">,</span> <span class="s2">&quot;latency_in_seconds&quot;</span><span class="p">:</span> <span class="mf">11.2</span><span class="p">},</span>
<span class="w">   </span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.98</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="mf">0.78</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">0.88</span><span class="p">,</span> <span class="s2">&quot;latency_in_seconds&quot;</span><span class="p">:</span> <span class="mf">87.6</span><span class="p">},</span> 
<span class="w">   </span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="mf">0.88</span><span class="p">,</span> <span class="s2">&quot;precision&quot;</span><span class="p">:</span> <span class="mf">0.78</span><span class="p">,</span> <span class="s2">&quot;f1&quot;</span><span class="p">:</span> <span class="mf">0.81</span><span class="p">,</span> <span class="s2">&quot;latency_in_seconds&quot;</span><span class="p">:</span> <span class="mf">101.6</span><span class="p">}</span>
<span class="w">   </span><span class="p">]</span>
<span class="w"> </span>
<span class="n">model_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Model 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Model 4&quot;</span><span class="p">]</span>
<span class="w"> </span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">radar_plot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">model_names</span><span class="o">=</span><span class="n">model_names</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>/tmp/ipykernel_10271/263559674.py:14: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
&#x20;&#x20;plot.show()
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&amp;lt;Figure size 640x480 with 5 Axes&amp;gt;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We can now visually compare the 4 models and choose the optimal one, based on one or several metrics</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Evaluating the model on a set of tasks">Evaluating the model on a set of tasks<a class="anchor-link" href="#Evaluating the model on a set of tasks">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>We can evaluate a model, for example, for different datasets. For this we can use the <code>evaluation_suite</code> method. For example we are going to create an evaluator that evaluates a model on the <code>imdb</code> and <code>sst2</code> datasets. We are going to see these datasets, for that we use the <code>load_dataset_builder</code> method so we don't have to download the complete dataset.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset_builder</span>
<span class="n">imdb</span> <span class="o">=</span> <span class="n">load_dataset_builder</span><span class="p">(</span><span class="s2">&quot;imdb&quot;</span><span class="p">)</span>
<span class="n">imdb</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">features</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None),
 &#x27;label&#x27;: ClassLabel(names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset_builder</span>
<span class="n">sst2</span> <span class="o">=</span> <span class="n">load_dataset_builder</span><span class="p">(</span><span class="s2">&quot;sst2&quot;</span><span class="p">)</span>
<span class="n">sst2</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">features</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&#x7B;&#x27;idx&#x27;: Value(dtype=&#x27;int32&#x27;, id=None),
 &#x27;sentence&#x27;: Value(dtype=&#x27;string&#x27;, id=None),
 &#x27;label&#x27;: ClassLabel(names=[&#x27;negative&#x27;, &#x27;positive&#x27;], id=None)&#x7D;
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, with the <code>imdb</code> dataset we need to take the <code>text</code> column to get the text and the <code>label</code> column to get the target. With the <code>sst2</code> dataset we need to take the <code>sentence</code> column to get the text and the <code>label</code> column to get the target.</p>
</section>
<section class="section-block-markdown-cell">
<p>We create the evaluator for the two datasets</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">evaluate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">evaluate.evaluation_suite</span><span class="w"> </span><span class="kn">import</span> <span class="n">SubTask</span>
<span class="w"> </span>
<span class="k">class</span><span class="w"> </span><span class="nc">Suite</span><span class="p">(</span><span class="n">evaluate</span><span class="o">.</span><span class="n">EvaluationSuite</span><span class="p">):</span>
<span class="w"> </span>
<span class="w">    </span><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">        </span><span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="w"> </span>
<span class="w">        </span><span class="bp">self</span><span class="o">.</span><span class="n">suite</span> <span class="o">=</span> <span class="p">[</span>
<span class="w">            </span><span class="n">SubTask</span><span class="p">(</span>
<span class="w">                </span><span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;text-classification&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">data</span><span class="o">=</span><span class="s2">&quot;imdb&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">split</span><span class="o">=</span><span class="s2">&quot;test[:1]&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">args_for_task</span><span class="o">=</span><span class="p">{</span>
<span class="w">                    </span><span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;input_column&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;label_column&quot;</span><span class="p">:</span> <span class="s2">&quot;label&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;label_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="w">                        </span><span class="s2">&quot;LABEL_0&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="w">                        </span><span class="s2">&quot;LABEL_1&quot;</span><span class="p">:</span> <span class="mf">1.0</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">            </span><span class="p">),</span>
<span class="w">            </span><span class="n">SubTask</span><span class="p">(</span>
<span class="w">                </span><span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;text-classification&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">data</span><span class="o">=</span><span class="s2">&quot;sst2&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">split</span><span class="o">=</span><span class="s2">&quot;test[:1]&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">args_for_task</span><span class="o">=</span><span class="p">{</span>
<span class="w">                    </span><span class="s2">&quot;metric&quot;</span><span class="p">:</span> <span class="s2">&quot;accuracy&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;input_column&quot;</span><span class="p">:</span> <span class="s2">&quot;sentence&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;label_column&quot;</span><span class="p">:</span> <span class="s2">&quot;label&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;label_mapping&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="w">                        </span><span class="s2">&quot;LABEL_0&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="w">                        </span><span class="s2">&quot;LABEL_1&quot;</span><span class="p">:</span> <span class="mf">1.0</span>
<span class="w">                    </span><span class="p">}</span>
<span class="w">                </span><span class="p">}</span>
<span class="w">            </span><span class="p">)</span>
<span class="w">        </span><span class="p">]</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>It can be seen in <code>split=&quot;test[:1]&quot;,</code> that we only take one example of the subset of test for this notebook and that the execution does not take too long</p>
</section>
<section class="section-block-markdown-cell">
<p>Ahora evaluamos con el modelo <code>huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">evaluate</span><span class="w"> </span><span class="kn">import</span> <span class="n">EvaluationSuite</span>
<span class="n">suite</span> <span class="o">=</span> <span class="n">EvaluationSuite</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;mathemakitten/sentiment-evaluation-suite&#39;</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">suite</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span><span class="p">)</span>
<span class="n">results</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>`data` is a preloaded Dataset! Ignoring `subset` and `split`.
`data` is a preloaded Dataset! Ignoring `subset` and `split`.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x7B;&#x27;accuracy&#x27;: 0.3,
&#x20;&#x20;&#x27;total_time_in_seconds&#x27;: 1.4153412349987775,
&#x20;&#x20;&#x27;samples_per_second&#x27;: 7.06543394110088,
&#x20;&#x20;&#x27;latency_in_seconds&#x27;: 0.14153412349987776,
&#x20;&#x20;&#x27;task_name&#x27;: &#x27;imdb&#x27;,
&#x20;&#x20;&#x27;data_preprocessor&#x27;: &#x27;&amp;lt;function Suite.__init__.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7f3ff27a5080&amp;gt;&#x27;&#x7D;,
 &#x7B;&#x27;accuracy&#x27;: 0.0,
&#x20;&#x20;&#x27;total_time_in_seconds&#x27;: 0.1323430729971733,
&#x20;&#x20;&#x27;samples_per_second&#x27;: 75.56118936586572,
&#x20;&#x20;&#x27;latency_in_seconds&#x27;: 0.013234307299717328,
&#x20;&#x20;&#x27;task_name&#x27;: &#x27;sst2&#x27;,
&#x20;&#x20;&#x27;data_preprocessor&#x27;: &#x27;&amp;lt;function Suite.__init__.&amp;lt;locals&amp;gt;.&amp;lt;lambda&amp;gt; at 0x7f3f2a9cc720&amp;gt;&#x27;&#x7D;]
</pre>
</div>
</div>
</div>
</section>