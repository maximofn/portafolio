<section class="section-block-markdown-cell">
<h1 id="Hugging Face Optimun">Hugging Face Optimun<a class="anchor-link" href="#Hugging Face Optimun">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p><code>Optimun</code> es una extensión de la librería <a href="https://maximofn.com/hugging-face-transformers/">Transformers</a> que proporciona un conjunto de herramientas de optimización del rendimiento para entrenar y para la inferencia de modelos, en hardware específico, con la máxima eficiencia.</p>
<p>El ecosistema de IA evoluciona rápidamente y cada día surge más hardware especializado junto con sus propias optimizaciones. Por tanto, <code>Optimum</code> permite a los usuarios utilizar eficientemente cualquiera de este HW con la misma facilidad que <a href="https://maximofn.com/hugging-face-transformers/">Transformers</a>.</p>
</section>
<section class="section-block-markdown-cell">
<p><code>Optimun</code> permite la optimización para las siguientes plataformas HW:</p>
<ul>
  <li>Nvidia</li>
  <li>AMD</li>
  <li>Intel</li>
  <li>AWS</li>
  <li>TPU</li>
  <li>Habana</li>
  <li>FuriosaAI</li>
</ul>
<p>Además, ofrece aceleración para las siguientes integraciones open source</p>
<ul>
  <li>ONNX runtime</li>
  <li>Exporters: Exportar modelos Pytorch o TensorFlow a diferentes formatos como ONNX o TFLite</li>
  <li>BetterTransformer</li>
  <li>Torch FX</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h2 id="Instalacion">Instalación<a class="anchor-link" href="#Instalacion">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para instalar <code>Optimun</code> simplemente ejecuta:</p>
<div class='highlight'><pre><code class="language-bash">pip install optimum
</code></pre></div>
<p>Pero si se quiere instalar con soporte para todas las plataformas HW, se puede hacer así</p>
<table>
  <thead>
    <tr>
      <th>Accelerator</th>
      <th>Installation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ONNX Runtime</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]</code></td>
    </tr>
    <tr>
      <td>Intel Neural Compressor</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]</code></td>
    </tr>
    <tr>
      <td>OpenVINO</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[openvino]</code></td>
    </tr>
    <tr>
      <td>NVIDIA TensorRT-LLM</td>
      <td><code>docker run -it --gpus all --ipc host huggingface/optimum-nvidia</code></td>
    </tr>
    <tr>
      <td>AMD Instinct GPUs and Ryzen AI NPU</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[amd]</code></td>
    </tr>
    <tr>
      <td>AWS Trainum & Inferentia</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[neuronx]</code></td>
    </tr>
    <tr>
      <td>Habana Gaudi Processor (HPU)</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[habana]</code></td>
    </tr>
    <tr>
      <td>FuriosaAI</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[furiosa]</code></td>
    </tr>
  </tbody>
</table>
<p>Los flags <code>--upgrade --upgrade-strategy eager</code> son necesarios para garantizar que los diferentes paquetes se actualicen a la última versión posible.</p>
</section>
<section class="section-block-markdown-cell">
<p>Como la mayoría de la gente usa Pytorch en GPUs de Nvidia, y sobre todo, como Nvidia es lo que yo tengo, este post va a hablar solo del uso de <code>Optimun</code> con GPUs de Nvidia y Pytorch.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="BeterTransformer">BeterTransformer<a class="anchor-link" href="#BeterTransformer">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>BetterTransformer es una optimización nativa de PyTorch para obtener una aceleración de x1,25 a x4 en la inferencia de modelos basados ​​en Transformer</p>
</section>
<section class="section-block-markdown-cell">
<p>BetterTransformer es una API que permite aprovechar las características de hardware modernas para acelerar el entrenamiento y la inferencia de modelos de transformers en PyTorch, utilizando implementaciones de atención más eficientes y <code>fast path</code> de la versión nativa de <code>nn.TransformerEncoderLayer</code>.</p>
<p>BetterTransformer usa dos tipos de aceleraciones:</p>
<ol>
  <li><code>Flash Attention</code>: Esta es una implementación de la <code>attention</code> que utiliza <code>sparse</code> para reducir la complejidad computacional. La atención es una de las operaciones más costosas en los modelos de transformers, y <code>Flash Attention</code> la hace más eficiente.</li>
  <li><code>Memory-Efficient Attention</code>: Esta es otra implementación de la atención que utiliza la función <code>scaled_dot_product_attention</code> de PyTorch. Esta función es más eficiente en términos de memoria que la implementación estándar de la atención en PyTorch.</li>
</ol>
<p>Además, la versión 2.0 de PyTorch incluye un operador de atención de productos punto escalado (SDPA) nativo como parte de <code>torch.nn.functional</code></p>
<p><code>Optimun</code> proporciona esta funcionalidad con la librería <code>Transformers</code></p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Inferencia con Automodel">Inferencia con Automodel<a class="anchor-link" href="#Inferencia con Automodel">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Primero vamos a ver cómo sería la inferencia normal con <code>Transformers</code> y <code>Automodel</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>&#x27;Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de&#x27;</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora vemos cómo se optimizaría con <code>BetterTransformer</code> y <code>Optimun</code></p>
<p>Lo que tenemos que hacer es convertir el modelo mediante el método <code>transform</code> de <code>BetterTransformer</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.bettertransformer</span><span class="w"> </span><span class="kn">import</span> <span class="n">BetterTransformer</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Convert the model to a BetterTransformer model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>&#x27;Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de&#x27;</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Inferecncia con Pipeline">Inferecncia con Pipeline<a class="anchor-link" href="#Inferecncia con Pipeline">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Al igual que antes, primero vemos cómo sería la inferencia normal con <code>Transformers</code> y <code>Pipeline</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
<span class="w"> </span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;I am a student at [MASK] University.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x7B;&#x27;score&#x27;: 0.05116177722811699,
&#x20;&#x20;&#x27;token&#x27;: 8422,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;stanford&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at stanford university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.04033993184566498,
&#x20;&#x20;&#x27;token&#x27;: 5765,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;harvard&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at harvard university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.03990468755364418,
&#x20;&#x20;&#x27;token&#x27;: 7996,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;yale&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at yale university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.0361952930688858,
&#x20;&#x20;&#x27;token&#x27;: 10921,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;cornell&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at cornell university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.03303057327866554,
&#x20;&#x20;&#x27;token&#x27;: 9173,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;princeton&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at princeton university.&#x27;&#x7D;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora vemos cómo optimizarlo, para ello usamos <code>pipeline</code> de <code>Optimun</code>, en vez de el de <code>Transformers</code>. Además hay que indicar que queremos usar <code>bettertransformer</code> como acelerador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optimum.pipelines</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
<span class="w"> </span>
<span class="c1"># Use the BetterTransformer pipeline</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;bettertransformer&quot;</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;I am a student at [MASK] University.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)
&#x20;&#x20;hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x7B;&#x27;score&#x27;: 0.05116180703043938,
&#x20;&#x20;&#x27;token&#x27;: 8422,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;stanford&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at stanford university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.040340032428503036,
&#x20;&#x20;&#x27;token&#x27;: 5765,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;harvard&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at harvard university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.039904672652482986,
&#x20;&#x20;&#x27;token&#x27;: 7996,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;yale&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at yale university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.036195311695337296,
&#x20;&#x20;&#x27;token&#x27;: 10921,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;cornell&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at cornell university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.03303062543272972,
&#x20;&#x20;&#x27;token&#x27;: 9173,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;princeton&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at princeton university.&#x27;&#x7D;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Entrenamiento">Entrenamiento<a class="anchor-link" href="#Entrenamiento">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para el entrneamiento con <code>Optimun</code> hacemos lo mismo que con la inferencia con Automodel, convertimos el modelo mediante el método <code>transform</code> de <code>BeterTransformer</code>.</p>
<p>Cuando terminamos el entrenamiento, volvemos a convertir el modelo mediante el método <code>reverse</code> de <code>BetterTransformer</code> para volver a tener el modelo original y así poder guardarlo y subirlo al hub de Hugging Face.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.bettertransformer</span><span class="w"> </span><span class="kn">import</span> <span class="n">BetterTransformer</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Convert the model to a BetterTransformer model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1">##############################################################################</span>
<span class="c1"># do your training here</span>
<span class="c1">##############################################################################</span>
<span class="w"> </span>
<span class="c1"># Convert the model back to a Hugging Face model</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">model_hf</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;fine_tuned_model&quot;</span><span class="p">)</span>
<span class="n">model_hf</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;fine_tuned_model&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>