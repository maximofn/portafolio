<section class="section-block-markdown-cell">
<h1 id="Hugging-Face-Optimun">Hugging Face Optimun<a class="anchor-link" href="#Hugging-Face-Optimun">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>Optimum is an extension of the [Transformers] library (<a href="https://maximofn.com/hugging-face-transformers/">https://maximofn.com/hugging-face-transformers/</a>) that provides a set of performance optimization tools for training and inference models on specific hardware with maximum efficiency.</p>
<p>The AI ecosystem is evolving rapidly and more specialized hardware is emerging every day along with its own optimizations. Therefore, <code>Optimum</code> allows users to efficiently utilize any of this HW with the same ease as <a href="https://maximofn.com/hugging-face-transformers/">Transformers</a>.</p>
</section>
<section class="section-block-markdown-cell">
<p>This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
<p>Optimmun` allows optimization for the following HW platforms:</p>
<ul>
<li>Nvidia</li>
<li>AMD</li>
<li>Intel</li>
<li>AWS</li>
<li>TPU</li>
<li>Havana</li>
<li>FuriosaAI</li>
</ul>
<p>It also offers acceleration for the following open source integrations</p>
<ul>
<li>ONNX runtime</li>
<li>Exporters: Export Pytorch or TensorFlow data to different formats such as ONNX or TFLite</li>
<li>BetterTransformer</li>
<li>Torch FX</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h2 id="Installation">Installation<a class="anchor-link" href="#Installation">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>To install <code>Optimum</code> simply run:</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>optimum
</pre></div>
<p>But if you want to install it with support for all HW platforms, you can do it like this</p>
<table>
<thead>
<tr>
<th>Accelerator</th>
<th>Installation</th>
</tr>
</thead>
<tbody>
<tr>
<td>ONNX Runtime</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]</code></td>
</tr>
<tr>
<td>Intel Neural Compressor</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]</code></td>
</tr>
<tr>
<td>OpenVINO</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[openvino]</code></td>
</tr>
<tr>
<td>NVIDIA TensorRT-LLM</td>
<td><code>docker run -it --gpus all --ipc host huggingface/optimum-nvidia</code></td>
</tr>
<tr>
<td>AMD Instinct GPUs and Ryzen AI NPU</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[amd]</code></td>
</tr>
<tr>
<td>AWS Trainum &amp; Inferentia</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[neuronx]</code></td>
</tr>
<tr>
<td>Havana Gaudi Processor (HPU)</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum [habana]</code></td>
</tr>
<tr>
<td>FuriosaAI</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[furiosa]</code></td>
</tr>
</tbody>
</table>
<p>the <code>--upgrade --upgrade-strategy eager</code> flags are needed to ensure that the different packages are upgraded to the latest possible version.</p>
</section>
<section class="section-block-markdown-cell">
<p>Since most people use Pytorch on Nvidia GPUs, and especially since Nvidia is what I have, this post is going to talk only about the use of <code>Optimun</code> with Nvidia GPUs and Pytorch.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="BeterTransformer">BeterTransformer<a class="anchor-link" href="#BeterTransformer">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>BetterTransformer is a native PyTorch optimization for x1.25 to x4 speedup in Transformer-based model inference.</p>
</section>
<section class="section-block-markdown-cell">
<p>BetterTransformer is an API that allows you to take advantage of modern hardware features to accelerate the training and inference of transformer models in PyTorch, using more efficient and <code>fast path</code> attention implementations of the native <code>nn.TransformerEncoderLayer</code> version of <code>nn.TransformerEncoderLayer</code>.</p>
<p>BetterTransformer uses two types of accelerations:</p>
<ol>
<li><code>Flash Attention</code>: This is an implementation of <code>attention</code> that uses <code>sparse</code> to reduce computational complexity. Attention is one of the most expensive operations in transformer models, and <code>Flash Attention</code> makes it more efficient.</li>
<li><code>Memory-Efficient Attention</code>: This is another implementation of attention that uses the <code>scaled_dot_product_attention</code> function of PyTorch. This function is more memory-efficient than the standard PyTorch implementation of attention.</li>
</ol>
<p>In addition, PyTorch version 2.0 includes a native scaled point product attention operator (SDPA) as part of <code>torch.nn.functional</code>.</p>
<p>Optimmun provides this functionality with the <code>Transformers</code> library.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Inference-with-Automodel">Inference with Automodel<a class="anchor-link" href="#Inference-with-Automodel">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>First let's see how the normal inference would be with <code>Transformers</code> and <code>Automodel</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[1]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we see how it would be optimized with <code>BetterTransformer</code> and <code>Optimun</code>.</p>
<p>What we have to do is to convert the model using the <code>transform</code> method of <code>BeterTransformer</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>

<span class="c1"># Convert the model to a BetterTransformer model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Inferecncy-with-Pipeline">Inferecncy with Pipeline<a class="anchor-link" href="#Inferecncy-with-Pipeline">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>As before, we first see how the normal inference would be with <code>Transformers</code> and <code>Pipeline</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'score': 0.05116177722811699,
  'token': 8422,
  'token_str': 'stanford',
  'sequence': 'i am a student at stanford university.'},
 {'score': 0.04033993184566498,
  'token': 5765,
  'token_str': 'harvard',
  'sequence': 'i am a student at harvard university.'},
 {'score': 0.03990468755364418,
  'token': 7996,
  'token_str': 'yale',
  'sequence': 'i am a student at yale university.'},
 {'score': 0.0361952930688858,
  'token': 10921,
  'token_str': 'cornell',
  'sequence': 'i am a student at cornell university.'},
 {'score': 0.03303057327866554,
  'token': 9173,
  'token_str': 'princeton',
  'sequence': 'i am a student at princeton university.'}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we see how to optimize it, for this we use <code>pipeline</code> of <code>Optimun</code>, instead of <code>Transformers</code>. In addition we must indicate that we want to use <code>bettertransformer</code> as accelerator.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">optimum.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Use the BetterTransformer pipeline</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">"bettertransformer"</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)
  hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'score': 0.05116180703043938,
  'token': 8422,
  'token_str': 'stanford',
  'sequence': 'i am a student at stanford university.'},
 {'score': 0.040340032428503036,
  'token': 5765,
  'token_str': 'harvard',
  'sequence': 'i am a student at harvard university.'},
 {'score': 0.039904672652482986,
  'token': 7996,
  'token_str': 'yale',
  'sequence': 'i am a student at yale university.'},
 {'score': 0.036195311695337296,
  'token': 10921,
  'token_str': 'cornell',
  'sequence': 'i am a student at cornell university.'},
 {'score': 0.03303062543272972,
  'token': 9173,
  'token_str': 'princeton',
  'sequence': 'i am a student at princeton university.'}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>For training with <code>Optimun</code> we do the same as with Automodel inference, we convert the model using the <code>transform</code> method of <code>BeterTransformer</code>.</p>
<p>When we finish the training, we convert the model back using the <code>reverse</code> method of <code>BeterTransformer</code> to get the original model back so we can save it and upload it to the Hugging Face hub.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>

<span class="c1"># Convert the model to a BetterTransformer model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">##############################################################################</span>
<span class="c1"># do your training here</span>
<span class="c1">##############################################################################</span>

<span class="c1"># Convert the model back to a Hugging Face model</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">model_hf</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">"fine_tuned_model"</span><span class="p">)</span>
<span class="n">model_hf</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">"fine_tuned_model"</span><span class="p">)</span>
</pre></div>
</div>
</section>
