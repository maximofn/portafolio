<section class="section-block-markdown-cell">
<h1 id="Otimiza%C3%A7%C3%A3o-do-rosto-de-abra%C3%A7os">Otimização do rosto de abraços<a class="anchor-link" href="#Otimiza%C3%A7%C3%A3o-do-rosto-de-abra%C3%A7os">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>Optimum é uma extensão da biblioteca [Transformers] (<a href="https://maximofn.com/hugging-face-transformers/">https://maximofn.com/hugging-face-transformers/</a>) que fornece um conjunto de ferramentas de otimização de desempenho para modelos de treinamento e inferência, em hardware específico, com eficiência máxima.</p>
<p>O ecossistema de IA está evoluindo rapidamente e cada vez mais hardware especializado está surgindo todos os dias, juntamente com suas próprias otimizações. Portanto, o <code>Optimum</code> permite que os usuários utilizem eficientemente qualquer um desses HW com a mesma facilidade dos [Transformers] (<a href="https://maximofn.com/hugging-face-transformers/">https://maximofn.com/hugging-face-transformers/</a>).</p>
</section>
<section class="section-block-markdown-cell">
<p>Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..</p>
<p>A otimização para as seguintes plataformas HW é possível com o <code>Optimun</code>:</p>
<ul>
<li>Nvidia</li>
<li>AMD</li>
<li>Intel</li>
<li>AWS</li>
<li>TPU</li>
<li>Havana</li>
<li>FuriosaAI</li>
</ul>
<p>Além disso, ele oferece aceleração para as seguintes integrações de código aberto</p>
<ul>
<li>Tempo de execução do ONNX</li>
<li>Exportadores: exportam dados do Pytorch ou do TensorFlow para diferentes formatos, como ONNX ou TFLite.</li>
<li>Melhor transformador</li>
<li>Tocha FX</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h2 id="Instala%C3%A7%C3%A3o">Instalação<a class="anchor-link" href="#Instala%C3%A7%C3%A3o">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para instalar o <code>Optimum</code>, basta executar:</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>optimum
</pre></div>
<p>Mas se quiser instalá-lo com suporte para todas as plataformas HW, você pode fazer o seguinte</p>
<table>
<thead>
<tr>
<th>Accelerator</th>
<th>Instalação</th>
</tr>
</thead>
<tbody>
<tr>
<td>ONNX Runtime</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]</code></td>
</tr>
<tr>
<td>Intel Neural Compressor</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]</code></td>
</tr>
<tr>
<td>OpenVINO</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[openvino]</code></td>
</tr>
<tr>
<td>NVIDIA TensorRT-LLM</td>
<td><code>docker run -it --gpus all --ipc host huggingface/optimum-nvidia</code></td>
</tr>
<tr>
<td>AMD Instinct GPUs e Ryzen AI NPU</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[amd]</code></td>
</tr>
<tr>
<td>AWS Trainum &amp; Inferentia</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[neuronx]</code></td>
</tr>
<tr>
<td>Havana Gaudi Processor (HPU)</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum [habana]</code></td>
</tr>
<tr>
<td>FuriosaAI</td>
<td><code>pip install --upgrade --upgrade-strategy eager optimum[furiosa]</code></td>
</tr>
</tbody>
</table>
<p>Os sinalizadores <code>--upgrade --upgrade-strategy eager</code> são necessários para garantir que os diferentes pacotes sejam atualizados para a versão mais recente possível.</p>
</section>
<section class="section-block-markdown-cell">
<p>Como a maioria das pessoas usa o Pytorch em GPUs Nvidia e, especialmente, como eu tenho uma Nvidia, esta postagem falará apenas sobre o uso do <code>Optimun</code> com GPUs Nvidia e Pytorch.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="BeterTransformer">BeterTransformer<a class="anchor-link" href="#BeterTransformer">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>O BetterTransformer é uma otimização nativa do PyTorch para aumentar a velocidade de 1,25 a 4 vezes na inferência de modelo baseada no Transformer.</p>
</section>
<section class="section-block-markdown-cell">
<p>BetterTransformer é uma API que permite que você aproveite os recursos modernos de hardware para acelerar o treinamento e a inferência de modelos de transformadores no PyTorch, usando implementações mais eficientes e com atenção ao "caminho rápido" da versão nativa <code>nn.TransformerEncoderLayer</code> do <code>nn.TransformerEncoderLayer</code>.</p>
<p>O BetterTransformer usa dois tipos de acelerações:</p>
<ol>
<li><code>Flash Attention</code>: é uma implementação do <code>attention</code> que usa o <code>sparse</code> para reduzir a complexidade computacional. A atenção é uma das operações mais caras nos modelos de transformadores, e o <code>Flash Attention</code> a torna mais eficiente.</li>
<li><code>Memory-Efficient Attention</code>: essa é outra implementação de atenção que usa a função <code>scaled_dot_product_attention</code> do PyTorch. Essa função é mais eficiente em termos de memória do que a implementação padrão de atenção do PyTorch.</li>
</ol>
<p>Além disso, a versão 2.0 do PyTorch inclui um operador de atenção de produto de ponto escalonado (SDPA) nativo como parte do <code>torch.nn.functional</code>.</p>
<p>O Optimmun fornece essa funcionalidade com a biblioteca <code>Transformers</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Infer%C3%AAncia-com-modelo-autom%C3%A1tico">Inferência com modelo automático<a class="anchor-link" href="#Infer%C3%AAncia-com-modelo-autom%C3%A1tico">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, vamos ver como seria a inferência normal com <code>Transformers</code> e <code>Automodel</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[1]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, veremos como ele seria otimizado com o <code>BetterTransformer</code> e o <code>Optimun</code>.</p>
<p>O que temos que fazer é converter o modelo usando o método <code>transform</code> do <code>BeterTransformer</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>

<span class="c1"># Convert the model to a BetterTransformer model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">"Me encanta aprender de"</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Infer%C3%AAncias-com-o-pipeline">Inferências com o pipeline<a class="anchor-link" href="#Infer%C3%AAncias-com-o-pipeline">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Como antes, primeiro vemos como seria a inferência normal com <code>Transformers</code> e <code>Pipeline</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'score': 0.05116177722811699,
  'token': 8422,
  'token_str': 'stanford',
  'sequence': 'i am a student at stanford university.'},
 {'score': 0.04033993184566498,
  'token': 5765,
  'token_str': 'harvard',
  'sequence': 'i am a student at harvard university.'},
 {'score': 0.03990468755364418,
  'token': 7996,
  'token_str': 'yale',
  'sequence': 'i am a student at yale university.'},
 {'score': 0.0361952930688858,
  'token': 10921,
  'token_str': 'cornell',
  'sequence': 'i am a student at cornell university.'},
 {'score': 0.03303057327866554,
  'token': 9173,
  'token_str': 'princeton',
  'sequence': 'i am a student at princeton university.'}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora veremos como otimizá-lo, de modo que usaremos <code>pipeline</code> do <code>Optimun</code>, em vez de <code>Transformers</code>. Também devemos indicar que queremos usar o <code>bettertransformer</code> como acelerador.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">optimum.pipelines</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># Use the BetterTransformer pipeline</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">"fill-mask"</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">"distilbert-base-uncased"</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">"bettertransformer"</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">"I am a student at [MASK] University."</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)
  hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'score': 0.05116180703043938,
  'token': 8422,
  'token_str': 'stanford',
  'sequence': 'i am a student at stanford university.'},
 {'score': 0.040340032428503036,
  'token': 5765,
  'token_str': 'harvard',
  'sequence': 'i am a student at harvard university.'},
 {'score': 0.039904672652482986,
  'token': 7996,
  'token_str': 'yale',
  'sequence': 'i am a student at yale university.'},
 {'score': 0.036195311695337296,
  'token': 10921,
  'token_str': 'cornell',
  'sequence': 'i am a student at cornell university.'},
 {'score': 0.03303062543272972,
  'token': 9173,
  'token_str': 'princeton',
  'sequence': 'i am a student at princeton university.'}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para o treinamento com <code>Optimun</code>, fazemos o mesmo que com a inferência Automodel, convertemos o modelo usando o método <code>transform</code> do <code>BeterTransformer</code>.</p>
<p>Quando terminamos o treinamento, convertemos o modelo novamente usando o método <code>reverse</code> do <code>BeterTransformer</code> para obter o modelo original de volta, para que possamos salvá-lo e carregá-lo no hub Hugging Face.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span> <span class="nn">optimum.bettertransformer</span> <span class="kn">import</span> <span class="n">BetterTransformer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>

<span class="c1"># Convert the model to a BetterTransformer model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">##############################################################################</span>
<span class="c1"># do your training here</span>
<span class="c1">##############################################################################</span>

<span class="c1"># Convert the model back to a Hugging Face model</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">model_hf</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">"fine_tuned_model"</span><span class="p">)</span>
<span class="n">model_hf</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">"fine_tuned_model"</span><span class="p">)</span>
</pre></div>
</div>
</section>
