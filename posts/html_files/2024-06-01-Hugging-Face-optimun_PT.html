<section class="section-block-markdown-cell">
<h1 id="Hugging Face Otimo">Hugging Face Ótimo<a class="anchor-link" href="#Hugging Face Otimo">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Aviso: Este post foi traduzido para o português usando um modelo de tradução automática. Por favor, me avise se encontrar algum erro.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p><code>Optimun</code> é uma extensão da biblioteca <a href="https://maximofn.com/hugging-face-transformers/">Transformers</a> que fornece um conjunto de ferramentas de otimização de desempenho para treinar e inferir modelos, em hardware específico, com a máxima eficiência.</p>
<p>O ecossistema de IA evolui rapidamente e a cada dia surge mais hardware especializado junto com suas próprias otimizações. Portanto, <code>Optimum</code> permite aos usuários utilizar eficientemente qualquer deste HW com a mesma facilidade que <a href="https://maximofn.com/hugging-face-transformers/">Transformers</a>.</p>
</section>
<section class="section-block-markdown-cell">
<p><code>Optimun</code> permite a otimização para as seguintes plataformas HW:</p>
<ul>
  <li>Nvidia</li>
  <li>AMD</li>
  <li>Intel</li>
  <li>AWS</li>
  <li>TPU</li>
  <li>Havana</li>
  <li>FuriosaIA</li>
</ul>
<p>Além disso, oferece aceleração para as seguintes integrações open source</p>
<ul>
  <li>Tempo de execução do ONNX</li>
  <li>Exportadores: Exportar modelos Pytorch ou TensorFlow para diferentes formatos como ONNX ou TFLite</li>
  <li>BetterTransformer</li>
  <li>Torch FX</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h2 id="Instalacao">Instalação<a class="anchor-link" href="#Instalacao">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para instalar <code>Optimun</code> simplesmente execute:</p>
<div class='highlight'><pre><code class="language-bash">pip install optimum
</code></pre></div>
<p>Mas se quiser instalar com suporte para todas as plataformas HW, pode ser feito assim</p>
<table>
  <thead>
    <tr>
      <th>Acelerador</th>
      <th>Instalação</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ONNX Runtime</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]</code></td>
    </tr>
    <tr>
      <td>Intel Neural Compressor</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]</code></td>
    </tr>
    <tr>
      <td>OpenVINO</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[openvino]</code></td>
    </tr>
    <tr>
      <td>GPUs AMD Instinct e NPU Ryzen AI</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[amd]</code></td>
    </tr>
    <tr>
      <td>AWS Trainum & Inferentia</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[neuronx]</code></td>
    </tr>
    <tr>
      <td>Processador Habana Gaudi (HPU)</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[habana]</code></td>
    </tr>
    <tr>
      <td>FuriosaAI</td>
      <td><code>pip install --upgrade --upgrade-strategy eager optimum[furiosa]</code></td>
    </tr>
  </tbody>
</table>
<p>Os flags <code>--upgrade --upgrade-strategy eager</code> são necessários para garantir que os diferentes pacotes sejam atualizados para a versão mais recente possível.</p>
</section>
<section class="section-block-markdown-cell">
<p>Como a maioria das pessoas usa o Pytorch em GPUs da Nvidia, e principalmente, como eu tenho uma Nvidia, este post vai falar apenas sobre o uso do <code>Optimun</code> com GPUs da Nvidia e Pytorch.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="BeterTransformer">BeterTransformer<a class="anchor-link" href="#BeterTransformer">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>BetterTransformer é uma otimização nativa do PyTorch para obter um aumento de velocidade de x1,25 a x4 na inferência de modelos baseados em Transformer</p>
</section>
<section class="section-block-markdown-cell">
<p>BetterTransformer é uma API que permite aproveitar as características de hardware modernas para acelerar o treinamento e a inferência de modelos de transformers no PyTorch, utilizando implementações de atenção mais eficientes e <code>fast path</code> da versão nativa de <code>nn.TransformerEncoderLayer</code>.</p>
<p>BetterTransformer usa dois tipos de acelerações:</p>
<ol>
  <li><code>Flash Attention</code>: Esta é uma implementação da <code>attention</code> que utiliza <code>sparse</code> para reduzir a complexidade computacional. A atenção é uma das operações mais custosas nos modelos de transformers, e <code>Flash Attention</code> torna-a mais eficiente.</li>
  <li><code>Atenção Eficiente em Memória</code>: Esta é outra implementação da atenção que utiliza a função <code>scaled_dot_product_attention</code> do PyTorch. Essa função é mais eficiente em termos de memória do que a implementação padrão da atenção no PyTorch.</li>
</ol>
<p>Além disso, a versão 2.0 do PyTorch inclui um operador de atenção de produtos ponto escalado (SDPA) nativo como parte de <code>torch.nn.functional</code></p>
<p><code>Optimun</code> fornece esta funcionalidade com a biblioteca <code>Transformers</code></p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Inferencia com Automodel">Inferência com Automodel<a class="anchor-link" href="#Inferencia com Automodel">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro vamos a ver como seria a inferência normal com <code>Transformers</code> e <code>Automodel</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>&#x27;Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de&#x27;</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora veremos como isso seria otimizado com <code>BetterTransformer</code> e <code>Optimun</code></p>
<p>O que temos que fazer é converter o modelo usando o método <code>transform</code> de <code>BetterTransformer</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.bettertransformer</span><span class="w"> </span><span class="kn">import</span> <span class="n">BetterTransformer</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Convert the model to a BetterTransformer model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;Me encanta aprender de&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">sentence_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">sentence_output</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>&#x27;Me encanta aprender de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de la vie de&#x27;</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Inferencia com Pipeline">Inferência com Pipeline<a class="anchor-link" href="#Inferencia com Pipeline">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Assim como antes, primeiro vemos como seria a inferência normal com <code>Transformers</code> e <code>Pipeline</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
<span class="w"> </span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;I am a student at [MASK] University.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x7B;&#x27;score&#x27;: 0.05116177722811699,
&#x20;&#x20;&#x27;token&#x27;: 8422,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;stanford&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at stanford university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.04033993184566498,
&#x20;&#x20;&#x27;token&#x27;: 5765,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;harvard&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at harvard university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.03990468755364418,
&#x20;&#x20;&#x27;token&#x27;: 7996,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;yale&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at yale university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.0361952930688858,
&#x20;&#x20;&#x27;token&#x27;: 10921,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;cornell&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at cornell university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.03303057327866554,
&#x20;&#x20;&#x27;token&#x27;: 9173,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;princeton&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at princeton university.&#x27;&#x7D;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora vemos como otimizá-lo, para isso usamos <code>pipeline</code> de <code>Optimun</code>, em vez do de <code>Transformers</code>. Além disso, é necessário indicar que queremos usar <code>bettertransformer</code> como acelerador.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optimum.pipelines</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span>
<span class="w"> </span>
<span class="c1"># Use the BetterTransformer pipeline</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;fill-mask&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;bettertransformer&quot;</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;I am a student at [MASK] University.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.
/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/optimum/bettertransformer/models/encoder_models.py:868: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845868/work/aten/src/ATen/NestedTensorImpl.cpp:177.)
&#x20;&#x20;hidden_states = torch._nested_tensor_from_mask(hidden_states, attn_mask)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>[&#x7B;&#x27;score&#x27;: 0.05116180703043938,
&#x20;&#x20;&#x27;token&#x27;: 8422,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;stanford&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at stanford university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.040340032428503036,
&#x20;&#x20;&#x27;token&#x27;: 5765,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;harvard&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at harvard university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.039904672652482986,
&#x20;&#x20;&#x27;token&#x27;: 7996,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;yale&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at yale university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.036195311695337296,
&#x20;&#x20;&#x27;token&#x27;: 10921,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;cornell&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at cornell university.&#x27;&#x7D;,
 &#x7B;&#x27;score&#x27;: 0.03303062543272972,
&#x20;&#x20;&#x27;token&#x27;: 9173,
&#x20;&#x20;&#x27;token_str&#x27;: &#x27;princeton&#x27;,
&#x20;&#x20;&#x27;sequence&#x27;: &#x27;i am a student at princeton university.&#x27;&#x7D;]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para o treinamento com <code>Optimun</code> fazemos o mesmo que com a inferência com Automodel, convertemos o modelo por meio do método <code>transform</code> de <code>BeterTransformer</code>.</p>
<p>Quando terminamos o treinamento, voltamos a converter o modelo usando o método <code>reverse</code> de <code>BetterTransformer</code> para recuperar o modelo original e assim poder salvá-lo e enviá-lo para o hub da Hugging Face.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.bettertransformer</span><span class="w"> </span><span class="kn">import</span> <span class="n">BetterTransformer</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1"># Convert the model to a BetterTransformer model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">model_hf</span><span class="p">,</span> <span class="n">keep_original_model</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="w"> </span>
<span class="c1">##############################################################################</span>
<span class="c1"># do your training here</span>
<span class="c1">##############################################################################</span>
<span class="w"> </span>
<span class="c1"># Convert the model back to a Hugging Face model</span>
<span class="n">model_hf</span> <span class="o">=</span> <span class="n">BetterTransformer</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">model_hf</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;fine_tuned_model&quot;</span><span class="p">)</span>
<span class="n">model_hf</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;fine_tuned_model&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>