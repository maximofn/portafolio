<section class="section-block-markdown-cell">
<h1 id="GPT1---Aprimorando-a-compreens%C3%A3o-da-linguagem-por-meio-de-pr%C3%A9-treinamento-generativo">GPT1 - Aprimorando a compreensão da linguagem por meio de pré-treinamento generativo<a class="anchor-link" href="#GPT1---Aprimorando-a-compreens%C3%A3o-da-linguagem-por-meio-de-pr%C3%A9-treinamento-generativo">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<h2 id="Papel">Papel<a class="anchor-link" href="#Papel">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..</p>
<p>[Improving Language Understanding by Generative Pre-Training] (<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a>) é o artigo GPT1. Antes de ler a postagem, é necessário se colocar na situação: antes do GPT, os modelos de linguagem eram baseados em redes recorrentes (RNN), que eram redes que funcionavam relativamente bem para tarefas específicas, mas com as quais não era possível reutilizar o pré-treinamento para fazer um ajuste fino para outras tarefas. Elas também não tinham muita memória, portanto, se você colocasse frases muito longas nelas, elas não se lembravam muito bem do início da frase.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Arquitetura">Arquitetura<a class="anchor-link" href="#Arquitetura">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Antes de falarmos sobre a arquitetura do GPT1, vamos nos lembrar de como era a arquitetura dos Transformers.</p>
<p><img alt="arquitetura do transformador" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer-scaled.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>O GPT1 é um modelo baseado nos decodificadores de transformador, portanto, como não temos um codificador, a arquitetura de um único decodificador é a seguinte</p>
<p><img alt="arquitetura do decodificador" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/transformer_decoder_only-scaled.webp"/></p>
<p>O mecanismo de atenção entre a sentença do codificador e do decodificador é eliminado.</p>
</section>
<section class="section-block-markdown-cell">
<p>No documento GPT1, eles propõem a seguinte arquitetura</p>
<p><img alt="arquitetura gpt1" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_architecture.webp"/></p>
<p>O que corresponde ao decodificador de um transformador, como vimos anteriormente, executado 12 vezes.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Resumo-do-artigo">Resumo do artigo<a class="anchor-link" href="#Resumo-do-artigo">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>As ideias mais interessantes do artigo são:</p>
<ul>
<li>O modelo é treinado em um grande corpus de texto não supervisionado. Isso cria um modelo de linguagem. Um modelo de linguagem de alta capacidade é criado em um grande corpus de texto.</li>
<li>O ajuste fino é então realizado em tarefas supervisionadas de NLP com conjuntos de dados rotulados. O ajuste fino é realizado em uma tarefa-alvo supervisionada. Além disso, quando o modelo é avaliado na tarefa supervisionada, ele não é avaliado apenas nessa tarefa, mas em quão bem ele prevê o próximo token, o que ajuda a aprimorar a generalização do modelo supervisionado e faz com que o modelo converse mais rapidamente.</li>
<li>Embora já tenhamos mencionado isso, o documento diz que a arquitetura do transformador é usada, já que até aquele momento os RNNs eram usados para os modelos de linguagem. Isso levou a uma melhoria no sentido de que o que foi aprendido no primeiro treinamento (treinamento no corpus de texto não supervisionado) é mais fácil de transferir para tarefas supervisionadas. Ou seja, graças ao uso de transformadores, foi possível treinar em um corpus inteiro de texto e depois fazer o ajuste fino em tarefas supervisionadas.</li>
<li>Eles testaram o modelo em quatro tipos de tarefas de compreensão de linguagem:<ul>
<li>Inferência de linguagem natural</li>
<li>Resposta às perguntas</li>
<li>Similaridade semântica</li>
<li>Classificação de textos.</li>
</ul>
</li>
<li>O modelo geral (aquele treinado em todo o corpus de texto não supervisionado) supera os modelos RNN treinados de forma discriminatória que empregam arquiteturas específicas de tarefas, melhorando significativamente o estado da arte em 9 das 12 tarefas estudadas. Eles também analisaram os comportamentos de "disparo zero" do modelo pré-treinado em quatro ambientes diferentes e mostraram que ele adquire conhecimento linguístico útil para tarefas subsequentes.</li>
<li>Nos últimos anos, os pesquisadores demonstraram os benefícios do uso de embeddings, que são treinados em corpora não rotulados, para melhorar o desempenho em várias tarefas. No entanto, essas abordagens transferem informações principalmente no nível da palavra, enquanto o uso de transformadores treinados em grandes corpora de texto não supervisionados captura a semântica de nível superior, no nível da frase.</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h2 id="Gera%C3%A7%C3%A3o-de-texto">Geração de texto<a class="anchor-link" href="#Gera%C3%A7%C3%A3o-de-texto">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver como gerar texto com um GPT1 pré-treinado.</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, você precisa instalar o <code>ftfy</code> e o <code>spacy</code> via</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ftfy<span class="w"> </span>spacy
<span class="sb">```</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Depois de instalado, você deve fazer o download do modelo de idioma spacy que deseja usar. Por exemplo, para fazer o download do modelo em inglês, você pode executar:</p>
<div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>spacy<span class="w"> </span>download<span class="w"> </span>en_core_web_sm
<span class="sb">```</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Para gerar texto, usaremos o modelo do repositório <a href="https://huggingface.co/openai-community/openai-gpt">GPT1</a> do Hugging Face.</p>
</section>
<section class="section-block-markdown-cell">
<p>Importamos as bibliotecas</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTTokenizer</span><span class="p">,</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se você notar, importamos o <code>OpenAIGPTTokenizer</code> e o <code>AutoTokenizer</code>. Isso ocorre porque no <a href="https://huggingface.co/openai-community/openai-gpt">model card</a> do GPT1 diz para usar o <code>OpenAIGPTTokenizer</code>, mas na postagem da biblioteca <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> explicamos que você deve usar o <code>AutoTokenizer</code> para carregar o tokenizador. Então, vamos tentar os dois</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">OpenAIGPTTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>
<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello, my dog is cute and"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="n">input_auto_tokens</span> <span class="o">=</span> <span class="n">auto_tokenizer</span><span class="p">(</span><span class="s2">"Hello, my dog is cute and"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input auto tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_auto_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>input tokens: 
{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}
input auto tokens: 
{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como você pode ver, com os dois tokenizadores você obtém os mesmos tokens. Portanto, para tornar o código mais geral, de modo que, se você alterar os pontos de verificação, não precisará alterar o código, vamos usar o <code>AutoTokenizer</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Em seguida, criamos o dispositivo, o tokenizador e o modelo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como instanciamos o modelo, vamos ver quantos parâmetros ele tem</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">params</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Number of parameters: 117M
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Na era de bilhões de parâmetros, podemos ver que o GPT1 tinha apenas 117 milhões de parâmetros.</p>
</section>
<section class="section-block-markdown-cell">
<p>Criamos os tokens de entrada para o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_sentence</span> <span class="o">=</span> <span class="s2">"Hello, my dog is cute and"</span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">input_tokens</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'input_ids': tensor([[3570,  240,  547, 2585,  544, 4957,  488]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nós os passamos para o modelo para gerar os tokens de saída.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">output_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>output tokens: 
tensor([[ 3570,   240,   547,  2585,   544,  4957,   488,   249,   719,   797,
           485,   921,   575,   562,   246,  1671,   239,   244, 40477,   244]],
       device='cuda:0')
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Decodificamos os tokens para obter a declaração de saída</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded output: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>decoded output: 
hello, my dog is cute and i'm going to take him for a walk. " 
 "
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Já conseguimos gerar texto com o GPT1</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Gerar-token-para-o-texto-do-token">Gerar token para o texto do token<a class="anchor-link" href="#Gerar-token-para-o-texto-do-token">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<h4 id="Greedy-search">Greedy search<a class="anchor-link" href="#Greedy-search">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Usamos o <code>model.generate</code> para gerar os tokens de saída de uma só vez, mas vamos ver como gerá-los um a um. Para fazer isso, em vez de usar <code>model.generate</code>, usaremos <code>model</code>, que, na verdade, chama o método <code>model.forward</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>

<span class="n">outputs</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[7]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],
         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],
         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],
         ...,
         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],
         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],
         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],
       device='cuda:0', grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que isso gera muitos dados, mas primeiro vamos dar uma olhada nas chaves de saída.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>odict_keys(['logits'])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nesse caso, temos apenas os logits do modelo, vamos ver seu tamanho.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>

<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[9]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>torch.Size([1, 7, 40478])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver quantos tokens tínhamos na entrada.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[10]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>torch.Size([1, 7])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Bem, temos o mesmo número de logits na saída e na entrada. Isso é normal</p>
</section>
<section class="section-block-markdown-cell">
<p>Obtemos os logits da última posição da saída</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">nex_token_logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[11]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>torch.Size([40478])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Há um total de 40478 logits, ou seja, há um vocabulário de 40478 tokens e temos que ver qual token tem a maior probabilidade.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">softmax_logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[12]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>torch.Size([40478])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(tensor(0.1898, device='cuda:0', grad_fn=&lt;MaxBackward0&gt;),
 tensor(249, device='cuda:0'))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Obtivemos o seguinte token, agora vamos decodificá-lo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[14]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'i'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Obtivemos o seguinte token usando o método guloso, ou seja, o token com a maior probabilidade. Mas já vimos na postagem sobre a biblioteca de transformadores, as [formas de gerar textos] (<a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto</a>) que podem ser feitas por amostragem, top-k, top-p, etc.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos colocar tudo em uma função e ver o que acontece se gerarmos alguns tokens.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
    <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
        <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">generated_text</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora vamos gerar o texto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">generate_greedy_text</span><span class="p">(</span><span class="s2">"Hello, my dog is cute and"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[17]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>'Hello, my dog is cute andi."\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O resultado é bastante repetitivo, como já foi visto em [ways to generate text] (<a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto</a>).</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Ajuste-fino-do-GPT">Ajuste fino do GPT<a class="anchor-link" href="#Ajuste-fino-do-GPT">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h3 id="C%C3%A1lculo-da-perda">Cálculo da perda<a class="anchor-link" href="#C%C3%A1lculo-da-perda">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Antes de começarmos a fazer o ajuste fino do GPT1, vamos dar uma olhada em um aspecto. Antes, quando costumávamos obter a saída do modelo, fazíamos o seguinte</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>

<span class="n">outputs</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[19]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>CausalLMOutput(loss=None, logits=tensor([[[ -5.9486,  -5.8697, -18.4258,  ...,  -9.7371, -10.4495,   0.8814],
         [ -6.1212,  -4.8031, -14.3970,  ...,  -6.5411,  -9.5051,  -1.2015],
         [ -7.4231,  -6.3615, -14.7297,  ..., -10.4575,  -8.4600,  -1.5183],
         ...,
         [ -5.4751,  -5.8803, -13.7767,  ..., -10.5048, -12.4167,  -6.1584],
         [ -7.2052,  -6.0198, -21.5040,  ..., -16.2941, -14.0494,  -1.2416],
         [ -7.7240,  -7.3631, -17.3174,  ..., -12.1546, -12.3327,  -1.7169]]],
       device='cuda:0', grad_fn=&lt;UnsafeViewBackward0&gt;), hidden_states=None, attentions=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Você pode ver que obtemos <code>loss=None</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>None
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como precisaremos da perda para fazer o ajuste fino, vamos ver como obtê-la.</p>
<p>Se consultarmos a documentação do método <a href="https://huggingface.co/docs/transformers/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel.forward">forward</a> de <code>OpenAIGPTLMHeadModel</code>, veremos que ele diz que a saída retorna um objeto do tipo <code>transformers.modeling_outputs.CausalLMOutput</code>, portanto, se consultarmos a documentação de <a href="https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput">transformers.modeling_outputs.CausalLMOutput</a>, veremos que ele diz que retorna <code>loss</code> se <code>labels</code> for passado para o método <code>forward</code>.</p>
<p>Se acessarmos o código-fonte do método <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/openai/modeling_openai.py#L544">forward</a>, veremos este bloco de código</p>
<div class="highlight"><pre><span></span><span class="n">perda</span> <span class="o">=</span> <span class="n">Nenhuma</span>
        <span class="n">se</span> <span class="n">labels</span> <span class="n">não</span> <span class="k">for</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Deslocamento de modo que os tokens &lt; n prevejam n</span>
            <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">lm_logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="c1"># Achatar os tokens</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="err">```</span>

<span class="n">Em</span> <span class="n">outras</span> <span class="n">palavras</span><span class="p">,</span> <span class="n">a</span> <span class="s2">"perda"</span> <span class="n">é</span> <span class="n">calculada</span> <span class="n">da</span> <span class="n">seguinte</span> <span class="n">forma</span>

 <span class="o">*</span> <span class="n">Deslocamento</span> <span class="n">de</span> <span class="n">logits</span> <span class="n">e</span> <span class="n">rótulos</span><span class="p">:</span> <span class="n">a</span> <span class="n">primeira</span> <span class="n">parte</span> <span class="n">é</span> <span class="n">deslocar</span> <span class="n">logits</span> <span class="p">(</span><span class="err">`</span><span class="n">lm_logits</span><span class="err">`</span><span class="p">)</span> <span class="n">e</span> <span class="n">rótulos</span> <span class="p">(</span><span class="err">`</span><span class="n">labels</span><span class="err">`</span><span class="p">)</span> <span class="n">para</span> <span class="n">que</span> <span class="err">`</span><span class="n">tokens</span> <span class="o">&lt;</span> <span class="n">n</span><span class="err">`</span> <span class="n">prevejam</span> <span class="err">`</span><span class="n">n</span><span class="err">`</span><span class="p">,</span> <span class="n">ou</span> <span class="n">seja</span><span class="p">,</span> <span class="n">a</span> <span class="n">partir</span> <span class="n">de</span> <span class="n">uma</span> <span class="n">posição</span> <span class="err">`</span><span class="n">n</span><span class="err">`</span><span class="p">,</span> <span class="n">o</span> <span class="n">próximo</span> <span class="n">token</span> <span class="n">é</span> <span class="n">previsto</span> <span class="n">a</span> <span class="n">partir</span> <span class="n">dos</span> <span class="n">anteriores</span><span class="o">.</span>
 <span class="o">*</span> <span class="n">CrossEntropyLoss</span><span class="p">:</span> <span class="n">é</span> <span class="n">criada</span> <span class="n">uma</span> <span class="n">instância</span> <span class="n">da</span> <span class="n">função</span> <span class="n">de</span> <span class="n">perda</span> <span class="err">`</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="err">`</span><span class="o">.</span>
 <span class="o">*</span> <span class="n">Achatar</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">e</span> <span class="n">os</span> <span class="n">rótulos</span> <span class="n">são</span> <span class="n">achatados</span> <span class="n">usando</span> <span class="err">`</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="err">`</span> <span class="n">e</span> <span class="err">`</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="err">`</span><span class="p">,</span> <span class="n">respectivamente</span><span class="o">.</span> <span class="n">Isso</span> <span class="n">é</span> <span class="n">feito</span> <span class="n">para</span> <span class="n">que</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">e</span> <span class="n">os</span> <span class="n">rótulos</span> <span class="n">tenham</span> <span class="n">a</span> <span class="n">mesma</span> <span class="n">forma</span> <span class="n">para</span> <span class="n">a</span> <span class="n">função</span> <span class="n">de</span> <span class="n">perda</span><span class="o">.</span>
 <span class="o">*</span> <span class="n">Cálculo</span> <span class="n">da</span> <span class="n">perda</span><span class="p">:</span> <span class="n">finalmente</span><span class="p">,</span> <span class="n">a</span> <span class="n">perda</span> <span class="n">é</span> <span class="n">calculada</span> <span class="n">usando</span> <span class="n">a</span> <span class="n">função</span> <span class="n">de</span> <span class="n">perda</span> <span class="err">`</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="err">`</span> <span class="n">com</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">achatados</span> <span class="n">e</span> <span class="n">os</span> <span class="n">rótulos</span> <span class="n">achatados</span> <span class="n">como</span> <span class="n">entradas</span><span class="o">.</span>

<span class="n">Em</span> <span class="n">resumo</span><span class="p">,</span> <span class="n">a</span> <span class="s2">"perda"</span> <span class="n">é</span> <span class="n">calculada</span> <span class="n">como</span> <span class="n">a</span> <span class="n">perda</span> <span class="n">de</span> <span class="n">entropia</span> <span class="n">cruzada</span> <span class="n">entre</span> <span class="n">os</span> <span class="n">logits</span> <span class="n">deslocados</span> <span class="n">e</span> <span class="n">achatados</span> <span class="n">e</span> <span class="n">os</span> <span class="n">rótulos</span> <span class="n">deslocados</span> <span class="n">e</span> <span class="n">achatados</span><span class="o">.</span>

<span class="n">Portanto</span><span class="p">,</span> <span class="n">se</span> <span class="n">passarmos</span> <span class="n">os</span> <span class="n">rótulos</span> <span class="n">para</span> <span class="n">o</span> <span class="n">método</span> <span class="err">`</span><span class="n">forward</span><span class="err">`</span><span class="p">,</span> <span class="n">ele</span> <span class="n">retornará</span> <span class="n">a</span> <span class="err">`</span><span class="n">perda</span><span class="err">`</span><span class="o">.</span>
</pre></div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>

<span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[21]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>tensor(4.2607, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Conjunto-de-dados">Conjunto de dados<a class="anchor-link" href="#Conjunto-de-dados">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para o treinamento, usaremos um conjunto de dados de piadas em inglês [short-jokes-dataset] (<a href="https://huggingface.co/datasets/Maximofn/short-jokes-dataset">https://huggingface.co/datasets/Maximofn/short-jokes-dataset</a>), que é um conjunto de dados com 231 mil piadas em inglês.</p>
</section>
<section class="section-block-markdown-cell">
<p>Baixamos o conjunto de dados</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"Maximofn/short-jokes-dataset"</span><span class="p">)</span>
<span class="n">jokes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[22]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['ID', 'Joke'],
        num_rows: 231657
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada nisso</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[23]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'ID': 1,
 'Joke': '[me narrating a documentary about narrators] "I can\'t hear what they\'re saying cuz I\'m talking"'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Treinamento-Pytorch">Treinamento Pytorch<a class="anchor-link" href="#Treinamento-Pytorch">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, vamos dar uma olhada em como o treinamento puro do Pytorch seria feito.</p>
<blockquote>
<p>Reinicie o notebook para que não haja problemas de memória da GPU</p>
</blockquote>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/openai-gpt"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OpenAIGPTLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Conjunto-de-dados-Pytorch">Conjunto de dados Pytorch<a class="anchor-link" href="#Conjunto-de-dados-Pytorch">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Criar uma classe de conjunto de dados do Pytorch</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nós o instanciamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Aqui está um exemplo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&lt;|endoftext|&gt;
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[27]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(torch.Size([1, 30]), torch.Size([1, 30]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Dataloader">Dataloader<a class="anchor-link" href="#Dataloader">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Agora, criamos um carregador de dados do Pytorch</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos um lote</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentences</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">))</span>
<span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[29]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(1, torch.Size([1, 1, 29]), torch.Size([1, 1, 29]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Treinamento">Treinamento<a class="anchor-link" href="#Treinamento">¶</a></h4>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-5</span>
<span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> started"</span> <span class="o">+</span> <span class="s1">'='</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>

        <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>
        
        <span class="c1">#################### "Fit as many joke sequences into MAX_SEQ_LEN sequence as possible" logic start ####</span>
        <span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>
        <span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
            <span class="k">continue</span>
        
        <span class="c1"># The first joke sequence in the sequence</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>
            <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>
            <span class="c1"># as the start for next sequence </span>
            <span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
                <span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>
                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1">#Add the joke to sequence, continue and try to add more</span>
                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">continue</span>
        <span class="c1">################## Sequence ready, process it trough the model ##################</span>
            
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                       
        <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
            <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>    
            <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> 
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">'lr'</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]})</span>
        <span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
            <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 0 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [11:31&lt;00:00, 334.88it/s, loss=2.88, lr=2.93e-6]
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 1 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [11:30&lt;00:00, 335.27it/s, loss=2.49, lr=5.87e-6]
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 2 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [11:17&lt;00:00, 341.75it/s, loss=2.57, lr=8.81e-6]
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 3 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [11:18&lt;00:00, 341.27it/s, loss=2.41, lr=1.18e-5]
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 4 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [11:19&lt;00:00, 341.04it/s, loss=2.49, lr=1.47e-5]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Infer%C3%AAncia">Inferência<a class="anchor-link" href="#Infer%C3%AAncia">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver como o modelo faz piadas.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"JOKE:"</span>
<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>
<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>decoded joke: 
joke : what do you call a group of people who are not afraid of the dark? a group
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Você pode ver que você passa uma sequência com a palavra <code>joke</code> e ele retorna uma piada. Mas se você retornar outra string, ela não retornará</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"My dog is cute and"</span>
<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>
<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>decoded joke: 
my dog is cute and i'm not sure if i should be offended or not. " 

</pre>
</div>
</div>
</div>
</section>
