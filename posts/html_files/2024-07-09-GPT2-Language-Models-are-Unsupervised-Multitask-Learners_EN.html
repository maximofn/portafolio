<section class="section-block-markdown-cell">
<h1 id="GPT-2---Language-Models-are-Unsupervised-Multitask-Learners">GPT-2 - Language Models are Unsupervised Multitask Learners<a class="anchor-link" href="#GPT-2---Language-Models-are-Unsupervised-Multitask-Learners">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<h2 id="Paper">Paper<a class="anchor-link" href="#Paper">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
<p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Language Models are Unsupervised Multitask Learners</a> is the GPT-2 paper. This is the second version of the model <a href="https://maximofn.com/gpt1/">GPT-1</a> that we have already seen.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Architecture">Architecture<a class="anchor-link" href="#Architecture">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Before we talk about the GPT-2 architecture, let's remember what the GPT-1 architecture was like.</p>
<p><img alt="gpt1 architecture" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_architecture.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>GPT-2 uses a transformer-based architecture, the same as <a href="https://maximofn.com/gpt1/">GPT-1</a>, with the following sizes</p>
<table>
<thead>
<tr>
<th>Parameters</th>
<th>Layers</th>
<th>d_model</th>
</tr>
</thead>
<tbody>
<tr>
<td>117M</td>
<td>12</td>
<td>768</td>
</tr>
<tr>
<td>345M</td>
<td>24</td>
<td>1024</td>
</tr>
<tr>
<td>762M</td>
<td>36</td>
<td>1280</td>
</tr>
<tr>
<td>1542M</td>
<td>48</td>
<td>1600</td>
</tr>
</tbody>
</table>
<p>The smaller model is equivalent to the original GPT, and the second smaller model is equivalent to the larger BERT model. The larger model has more than an order of magnitude more parameters than GPT.</p>
</section>
<section class="section-block-markdown-cell">
<p>In addition, the following architectural modifications were made</p>
<ul>
<li>A normalization layer is added before the attention block. This can help stabilize the training of the model and improve its ability to learn deeper representations. By normalizing the inputs to each block, variability in the outputs is reduced and model training is facilitated.</li>
<li>An additional normalization has been added after the final self-attenuation block. This can help reduce variability in model outputs and improve model stability.</li>
<li>In most models, the weights of the layers are initialized randomly, following a normal or uniform distribution. However, in the case of GPT-2, the authors decided to use a modified initialization that takes into account the depth of the model.The idea behind this modified initialization is that as the model gets deeper, the signal flowing through the residual layers becomes weaker. This is because each residual layer is added to the original input, which can cause the signal to attenuate with the depth of the model. To counteract this effect they decided to scale the residual layer weights at initialization by a factor of 1/√N, where N is the number of residual layers. This means that as the model gets deeper, the residual layer weights become smaller. This initialization trick can help stabilize the training of the model and improve its ability to learn deeper representations. By scaling the residual layer weights, variability in the outputs of each layer is reduced and signal flow through the model is facilitated. In summary, the modified initialization in GPT-2 is used to counteract the signal attenuation effect in the residual layers, which helps to stabilize the training of the model and improve its ability to learn deeper representations.</li>
<li>The vocabulary size has been expanded to 50,257. This means that the model can learn to represent a larger set of words and tokens.</li>
<li>The context size has been increased from 512 to 1024 tokens. This allows the model to take into account a larger context when generating text.</li>
</ul>
<p>GPT1 vs GPT-2 architecture](<a href="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_vs_GPT2_architecture.webp">https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPT1_vs_GPT2_architecture.webp</a>)</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Paper-abstract">Paper abstract<a class="anchor-link" href="#Paper-abstract">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>The most interesting ideas in the paper are:</p>
<ul>
<li>For pre-training of the model they thought to use a diverse and almost unlimited text source, web scraping as Common Crawl. However they found that there was almost very poor quality text. So they used the WebText dataset, which also came from web scraping but with a quality filter, such as the amount of outbound links from reddit, etc. They also removed the text coming from wikipedia, as it could be repeated in other pages.</li>
<li>They used a BPE tokenizer that we explained in a previous <a href="https://maximofn.com/bpe/">post</a>.</li>
</ul>
</section>
<section class="section-block-markdown-cell">
<h2 id="Text-generation">Text generation<a class="anchor-link" href="#Text-generation">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Let's see how to generate text with a pre-trained GPT-2</p>
</section>
<section class="section-block-markdown-cell">
<p>To generate text we will use the model from the <a href="https://huggingface.co/openai-community/gpt2">GPT-2</a> repository of Hugging Face.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Text-generation-with-pipeline">Text generation with pipeline<a class="anchor-link" href="#Text-generation-with-pipeline">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>With this model we can now use the transformer pipeline.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">'text-generation'</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Hello, I'm a language model,"</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">o</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Output </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">o</span><span class="p">[</span><span class="s1">'generated_text'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Output 1: Hello, I'm a language model, and I want to change the way you read

A little in today's post I want to talk about
Output 2: Hello, I'm a language model, with two roles: the language model and the lexicographer-semantics expert. The language models are going
Output 3: Hello, I'm a language model, and this is your brain. Here is your brain, and all this data that's stored in there, that
Output 4: Hello, I'm a language model, and I like to talk... I want to help you talk to your customers

Are you using language model
Output 5: Hello, I'm a language model, I'm gonna tell you about what type of language you're using. We all know a language like this,
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Text-generation-with-automodel">Text generation with automodel<a class="anchor-link" href="#Text-generation-with-automodel">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>But if we want to use <code>Automodel</code>, we can do the following</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As with <a href="https://maximofn.com/gpt1/#Generaci%C3%B3n-de-texto">GPT-1</a> we can import <code>GPT2Tokenizer</code> and <code>AutoTokenizer</code>. This is because in the <a href="https://huggingface.co/openai-community/gpt2">model card</a> of GPT-2 it says to use <code>GPT2Tokenizer</code>, but in the <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> library post we explain that you should use <code>AutoTokenizer</code> to load the tokenizer. So let's try both</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">auto_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello, I'm a language model,"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
<span class="n">input_auto_tokens</span> <span class="o">=</span> <span class="n">auto_tokenizer</span><span class="p">(</span><span class="s2">"Hello, I'm a language model,"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input auto tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">input_auto_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>input tokens: 
{'input_ids': tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}
input auto tokens: 
{'input_ids': tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As you can see with the two tokenizers you get the same tokens. So to make the code more general, so that if you change the ckeckpoints, you don't have to change the code, let's use <code>AutoTokenizer</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>We then create the device, the tokenizer and the model</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Since we have instantiated the model, let's see how many parameters it has</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">params</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Number of parameters: 1558M
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see we have loaded the 1.5B parameter model, but if we wanted to load the other models we would have to do the following</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">checkpoints_small</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">model_small</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_small</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters of small model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_small</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>

<span class="n">checkpoints_medium</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-medium"</span>
<span class="n">model_medium</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_medium</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters of medium model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_medium</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>

<span class="n">checkpoints_large</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-large"</span>
<span class="n">model_large</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_large</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters of large model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_large</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>

<span class="n">checkpoints_xl</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>
<span class="n">model_xl</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints_xl</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Number of parameters of xl model: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model_xl</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">)</span><span class="si">}</span><span class="s2">M"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Number of parameters of small model: 124M
Number of parameters of medium model: 355M
Number of parameters of large model: 774M
Number of parameters of xl model: 1558M
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We create the input tokens for the model</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_sentence</span> <span class="o">=</span> <span class="s2">"Hello, I'm a language model,"</span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">input_tokens</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[6]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'input_ids': tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11]],
       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We pass them to the model to generate the output tokens.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"output tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">output_tokens</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>output tokens: 
tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11,   290,   314,
          1101,  1016,   284,  1037,   345,   351,   534,  1917,    13,   198]],
       device='cuda:0')
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We decode the tokens to obtain the output sentence</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded output: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>decoded output: 
Hello, I'm a language model, and I'm going to help you with your problem.

</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We have already succeeded in generating text with GPT-2</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Generate-text-token-to-token">Generate text token to token<a class="anchor-link" href="#Generate-text-token-to-token">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<h4 id="Greedy-search">Greedy search<a class="anchor-link" href="#Greedy-search">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>We have used <code>model.generate</code> to generate the output tokens all at once, but let's see how to generate them one by one. To do this, instead of using <code>model.generate</code> we are going to use <code>model</code>, which actually calls the <code>model.forward</code> method.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>

<span class="n">outputs</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[10]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ 6.6288,  5.1421, -0.8002,  ..., -6.3998, -4.4113,  1.8240],
         [ 2.7250,  1.9371, -1.2293,  ..., -5.0979, -5.1617,  2.2694],
         [ 2.6891,  4.3089, -1.6074,  ..., -7.6321, -2.0448,  0.4042],
         ...,
         [ 6.0513,  3.8020, -2.8080,  ..., -6.7754, -8.3176,  1.1541],
         [ 6.8402,  5.6952,  0.2002,  ..., -9.1281, -6.7818,  2.7576],
         [ 1.0255, -0.2201, -2.5484,  ..., -6.2137, -7.2322,  0.1665]]],
       device='cuda:0', grad_fn=&lt;UnsafeViewBackward0&gt;), past_key_values=((tensor([[[[ 0.4779,  0.7671, -0.7532,  ..., -0.3551,  0.4590,  0.3073],
          [ 0.2034, -0.6033,  0.2484,  ...,  0.7760, -0.3546,  0.0198],
          [-0.1968, -0.9029,  0.5570,  ...,  0.9985, -0.5028, -0.3508],
          ...,
          [-0.5007, -0.4009,  0.1604,  ..., -0.3693, -0.1158,  0.1320],
          [-0.4854, -0.1369,  0.7377,  ..., -0.8043, -0.1054,  0.0871],
          [ 0.1610, -0.8358, -0.5534,  ...,  0.9951, -0.3085,  0.4574]],

         [[ 0.6288, -0.1374, -0.3467,  ..., -1.0003, -1.1518,  0.3114],
          [-1.7269,  1.2920, -0.0734,  ...,  1.0572,  1.4698, -2.0412],
          [ 0.2714, -0.0670, -0.4769,  ...,  0.6305,  0.6890, -0.8158],
          ...,
          [-0.0499, -0.0721,  0.4580,  ...,  0.6797,  0.2331,  0.0210],
          [-0.1894,  0.2077,  0.6722,  ...,  0.6938,  0.2104, -0.0574],
          [ 0.3661, -0.0218,  0.2618,  ...,  0.8750,  1.2205, -0.6103]],

         [[ 0.5964,  1.1178,  0.3604,  ...,  0.8426,  0.4881, -0.4094],
          [ 0.3186, -0.3953,  0.2687,  ..., -0.1110, -0.5640,  0.5900],
          ...,
          [ 0.2092,  0.3898, -0.6061,  ..., -0.2859, -0.3136, -0.1002],
          [ 0.0539,  0.8941,  0.3423,  ..., -0.6326, -0.1053, -0.6679],
          [ 0.5628,  0.6687, -0.2720,  ..., -0.1073, -0.9792, -0.0302]]]],
       device='cuda:0', grad_fn=&lt;PermuteBackward0&gt;))), hidden_states=None, attentions=None, cross_attentions=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see that it brings up a lot of data, first let's look at the output keys</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[11]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>odict_keys(['logits', 'past_key_values'])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>In this case we only have the logits of the model, let's see their size</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>

<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[12]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>torch.Size([1, 8, 50257])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see how many tokens we had at the entrance.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>torch.Size([1, 8])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Wow, at the output we have the same number of logits as at the input. This is normal</p>
</section>
<section class="section-block-markdown-cell">
<p>We obtain the logits of the last position of the exit</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="n">nex_token_logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[14]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>torch.Size([50257])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>There are a total of 50257 logits, i.e. there is a vocabulary of 50257 tokens and we have to see which token has the highest probability, to do this we first calculate the softmax</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">softmax_logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>torch.Size([50257])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Once we have calculated the softmax we obtain the most probable token by looking for the one with the highest probability, that is, the one with the highest value after the softmax.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[16]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(tensor(0.1732, device='cuda:0', grad_fn=&lt;MaxBackward0&gt;),
 tensor(290, device='cuda:0'))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We have obtained the following token, now we decode it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[17]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>' and'</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We have obtained the following token using the greedy method, that is, the token with the highest probability. But we already saw in the transformers library post, the <a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">ways to generate texts</a> that you can do <code>sampling</code>, <code>top-k</code>, <code>top-p</code>, etc.</p>
</section>
<section class="section-block-markdown-cell">
<p>Let's put everything into a function and see what comes out if we generate a few tokens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_next_greedy_token</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
    <span class="n">nex_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">softmax_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">nex_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">softmax_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_greedy_text</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">input_sentence</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
        <span class="n">next_token_prob</span><span class="p">,</span> <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">generate_next_greedy_token</span><span class="p">(</span><span class="n">generated_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">generated_text</span> <span class="o">+=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">generated_text</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we generate text</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">generate_greedy_text</span><span class="p">(</span><span class="s2">"Hello, I'm a language model,"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[22]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>"Hello, I'm a language model, and I'm going to help you with your problem.\n\n\nI'm going to help you"</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>The output is rather repetitive as already seen in <a href="https://maximofn.com/hugging-face-transformers/#Formas-de-generaci%C3%B3n-de-texto">ways to generate text</a>. But still, it is better output than what we obtained with <a href="https://maximofn.com/gpt1/#Generaci%C3%B3n-de-texto">GPT-1</a>.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Architecture-of-the-models-available-at-Hugging-Face">Architecture of the models available at Hugging Face<a class="anchor-link" href="#Architecture-of-the-models-available-at-Hugging-Face">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>If we go to the Hugging Face documentation of <a href="https://huggingface.co/docs/transformers/en/model_doc/gpt2">GPT2</a> we can see that we have the options <code>GPT2Model</code>, <code>GPT2LMHeadModel</code>, <code>GPT2ForSequenceClassification</code>, <code>GPT2ForQuestionAnswering</code>, <code>GPT2ForTokenClassification</code>. Let's take a look at them</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="GPT2Model">GPT2Model<a class="anchor-link" href="#GPT2Model">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>This is the base model, i.e. the transformer decoder.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0-11): 12 x GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As you can see at the output a tensor of dimension 768, which is the dimension of the embeddings of the small model. If we had used the <code>openai-community/gpt2-xl</code> model, we would have obtained an output of 1600.</p>
<p>Depending on the task you want to do now you would have to add more layers.</p>
<p>We can add them by hand, but the weights of those layers would be initialized randomly. Whereas if we use the Hugging Face models with these layers, the weights are pre-trained.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="GPT2LMHeadModel">GPT2LMHeadModel<a class="anchor-link" href="#GPT2LMHeadModel">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>It is the one we have used before to generate text</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2LMHeadModel</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[5]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As you can see it is the same model as before, only that at the end a linear layer has been added with an input of 768 (the embeddings) and an output of 50257, which corresponds to the size of the vocabulary.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="GPT2ForSequenceClassification">GPT2ForSequenceClassification<a class="anchor-link" href="#GPT2ForSequenceClassification">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>This option is for classifying text sequences, in this case we have to specify with <code>num_labels</code> the number of classes we want to classify.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[10]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=5, bias=False)
)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now, instead of having an output of 50257, we have an output of 5, which is the number we have entered in <code>num_labels</code> and is the number of classes we want to classify</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="GPT2ForQuestionAnswering">GPT2ForQuestionAnswering<a class="anchor-link" href="#GPT2ForQuestionAnswering">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>In the <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> post we explained that in this mode you pass a context to the model and a question about the context and it returns the answer.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForQuestionAnswering</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2ForQuestionAnswering(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)
)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see that the output gives us a two-dimensional tensor</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="GPT2ForTokenClassification">GPT2ForTokenClassification<a class="anchor-link" href="#GPT2ForTokenClassification">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Also in the post of <a href="https://maximofn.com/hugging-face-transformers/">transformers</a> we told what token calsification was, we explained that it classified to which category each token corresponded. We have to pass the number of classes we want to classify with <code>num_labels</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForTokenClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2ForTokenClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=5, bias=True)
)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>At the output we obtain the 5 classes that we have specified with <code>num_labels</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Fine-tuning-GPT-2">Fine tuning GPT-2<a class="anchor-link" href="#Fine-tuning-GPT-2">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h3 id="Fine-tuning-for-text-generation">Fine tuning for text generation<a class="anchor-link" href="#Fine-tuning-for-text-generation">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>First let's see how the pure Pytorch training would be done.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Loss-calculation">Loss calculation<a class="anchor-link" href="#Loss-calculation">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Before we start doing the fine tuning of GPT-2 let's see one thing. Before when we used to get the output of the model we did this</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">)</span>

<span class="n">outputs</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[23]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ 6.6288,  5.1421, -0.8002,  ..., -6.3998, -4.4113,  1.8240],
         [ 2.7250,  1.9371, -1.2293,  ..., -5.0979, -5.1617,  2.2694],
         [ 2.6891,  4.3089, -1.6074,  ..., -7.6321, -2.0448,  0.4042],
         ...,
         [ 6.0513,  3.8020, -2.8080,  ..., -6.7754, -8.3176,  1.1541],
         [ 6.8402,  5.6952,  0.2002,  ..., -9.1281, -6.7818,  2.7576],
         [ 1.0255, -0.2201, -2.5484,  ..., -6.2137, -7.2322,  0.1665]]],
       device='cuda:0', grad_fn=&lt;UnsafeViewBackward0&gt;), past_key_values=((tensor([[[[ 0.4779,  0.7671, -0.7532,  ..., -0.3551,  0.4590,  0.3073],
          [ 0.2034, -0.6033,  0.2484,  ...,  0.7760, -0.3546,  0.0198],
          [-0.1968, -0.9029,  0.5570,  ...,  0.9985, -0.5028, -0.3508],
          ...,
          [-0.5007, -0.4009,  0.1604,  ..., -0.3693, -0.1158,  0.1320],
          [-0.4854, -0.1369,  0.7377,  ..., -0.8043, -0.1054,  0.0871],
          [ 0.1610, -0.8358, -0.5534,  ...,  0.9951, -0.3085,  0.4574]],

         [[ 0.6288, -0.1374, -0.3467,  ..., -1.0003, -1.1518,  0.3114],
          [-1.7269,  1.2920, -0.0734,  ...,  1.0572,  1.4698, -2.0412],
          [ 0.2714, -0.0670, -0.4769,  ...,  0.6305,  0.6890, -0.8158],
          ...,
          [-0.0499, -0.0721,  0.4580,  ...,  0.6797,  0.2331,  0.0210],
          [-0.1894,  0.2077,  0.6722,  ...,  0.6938,  0.2104, -0.0574],
          [ 0.3661, -0.0218,  0.2618,  ...,  0.8750,  1.2205, -0.6103]],

         [[ 0.5964,  1.1178,  0.3604,  ...,  0.8426,  0.4881, -0.4094],
          [ 0.3186, -0.3953,  0.2687,  ..., -0.1110, -0.5640,  0.5900],
          ...,
          [ 0.2092,  0.3898, -0.6061,  ..., -0.2859, -0.3136, -0.1002],
          [ 0.0539,  0.8941,  0.3423,  ..., -0.6326, -0.1053, -0.6679],
          [ 0.5628,  0.6687, -0.2720,  ..., -0.1073, -0.9792, -0.0302]]]],
       device='cuda:0', grad_fn=&lt;PermuteBackward0&gt;))), hidden_states=None, attentions=None, cross_attentions=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>You can see that we get <code>loss=None</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>None
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we are going to need the loss to do the fine tuning, let's see how to obtain it.</p>
<p>If we go to the documentation of the method <a href="https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel.forward">forward</a> of <code>GPT2LMHeadModel</code>, we can see that it says that at the output it returns an object of type <code>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</code>, so if we go to the documentation of <a href="https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions">transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a>, we can see that it says that it returns <code>loss</code> if <code>labels</code> is passed to the <code>forward</code> method.</p>
<p>If we go to the source code of the <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1277">forward</a> method, we see this code block</p>
<div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># move labels to correct device to enable model parallelism</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">lm_logits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># Shift so that tokens &lt; n predict n</span>
            <span class="n">shift_logits</span> <span class="o">=</span> <span class="n">lm_logits</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">shift_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="c1"># Flatten the tokens</span>
            <span class="n">loss_fct</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">shift_logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">shift_labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="err">```</span>

<span class="n">In</span> <span class="n">other</span> <span class="n">words</span><span class="p">,</span> <span class="n">the</span> <span class="err">`</span><span class="n">loss</span><span class="err">`</span> <span class="ow">is</span> <span class="n">calculated</span> <span class="k">as</span> <span class="n">follows</span>

 <span class="o">*</span> <span class="n">Shift</span> <span class="n">of</span> <span class="n">logits</span> <span class="ow">and</span> <span class="n">labels</span><span class="p">:</span> <span class="n">The</span> <span class="n">first</span> <span class="n">part</span> <span class="ow">is</span> <span class="n">to</span> <span class="n">shift</span> <span class="n">the</span> <span class="n">logits</span> <span class="p">(</span><span class="err">`</span><span class="n">lm_logits</span><span class="err">`</span><span class="p">)</span> <span class="ow">and</span> <span class="n">labels</span> <span class="p">(</span><span class="err">`</span><span class="n">labels</span><span class="err">`</span><span class="p">)</span> <span class="n">so</span> <span class="n">that</span> <span class="err">`</span><span class="n">tokens</span> <span class="o">&lt;</span> <span class="n">n</span><span class="err">`</span> <span class="n">predict</span> <span class="err">`</span><span class="n">n</span><span class="err">`</span><span class="p">,</span> <span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span><span class="p">,</span> <span class="kn">from</span> <span class="nn">a</span> <span class="n">position</span> <span class="err">`</span><span class="n">n</span><span class="err">`</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">token</span> <span class="ow">is</span> <span class="n">predicted</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">previous</span> <span class="n">ones</span><span class="o">.</span>
 <span class="o">*</span> <span class="n">CrossEntropyLoss</span><span class="p">:</span> <span class="n">An</span> <span class="n">instance</span> <span class="n">of</span> <span class="n">the</span> <span class="err">`</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="err">`</span> <span class="n">function</span> <span class="ow">is</span> <span class="n">created</span><span class="o">.</span>
 <span class="o">*</span> <span class="n">Flatten</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">Logits</span> <span class="ow">and</span> <span class="n">labels</span> <span class="n">are</span> <span class="n">then</span> <span class="n">flattened</span> <span class="n">using</span> <span class="err">`</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="err">`</span> <span class="ow">and</span> <span class="err">`</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="err">`</span><span class="p">,</span> <span class="n">respectively</span><span class="o">.</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">done</span> <span class="n">so</span> <span class="n">that</span> <span class="n">the</span> <span class="n">logits</span> <span class="ow">and</span> <span class="n">labels</span> <span class="n">have</span> <span class="n">the</span> <span class="n">same</span> <span class="n">shape</span> <span class="k">for</span> <span class="n">the</span> <span class="n">loss</span> <span class="n">function</span><span class="o">.</span>
 <span class="o">*</span> <span class="n">Loss</span> <span class="n">calculation</span><span class="p">:</span> <span class="n">Finally</span><span class="p">,</span> <span class="n">the</span> <span class="n">loss</span> <span class="ow">is</span> <span class="n">calculated</span> <span class="n">using</span> <span class="n">the</span> <span class="err">`</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="err">`</span> <span class="n">function</span> <span class="k">with</span> <span class="n">the</span> <span class="n">flattened</span> <span class="n">logits</span> <span class="ow">and</span> <span class="n">flattened</span> <span class="n">labels</span> <span class="k">as</span> <span class="n">inputs</span><span class="o">.</span>

<span class="n">In</span> <span class="n">summary</span><span class="p">,</span> <span class="err">`</span><span class="n">loss</span><span class="err">`</span> <span class="ow">is</span> <span class="n">calculated</span> <span class="k">as</span> <span class="n">the</span> <span class="n">cross</span><span class="o">-</span><span class="n">entropy</span> <span class="n">loss</span> <span class="n">between</span> <span class="n">shifted</span> <span class="ow">and</span> <span class="n">flattened</span> <span class="n">logits</span> <span class="ow">and</span> <span class="n">shifted</span> <span class="ow">and</span> <span class="n">flattened</span> <span class="n">labels</span><span class="o">.</span>

<span class="n">Therefore</span><span class="p">,</span> <span class="k">if</span> <span class="n">we</span> <span class="k">pass</span> <span class="n">the</span> <span class="n">labels</span> <span class="n">to</span> <span class="n">the</span> <span class="err">`</span><span class="n">forward</span><span class="err">`</span> <span class="n">method</span><span class="p">,</span> <span class="n">it</span> <span class="n">will</span> <span class="k">return</span> <span class="n">the</span> <span class="err">`</span><span class="n">loss</span><span class="err">`</span><span class="o">.</span>
</pre></div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>

<span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[25]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>tensor(3.8028, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>For the training we are going to use a dataset of English jokes <a href="https://huggingface.co/datasets/Maximofn/short-jokes-dataset">short-jokes-dataset</a>, which is a dataset with 231 thousand English jokes.</p>
<blockquote>
<p>Restart the notebook to avoid problems with the GPU memory.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>Download the dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"Maximofn/short-jokes-dataset"</span><span class="p">)</span>
<span class="n">jokes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[1]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['ID', 'Joke'],
        num_rows: 231657
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's take a look at it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'ID': 1,
 'Joke': '[me narrating a documentary about narrators] "I can\'t hear what they\'re saying cuz I\'m talking"'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Model-instance">Model instance<a class="anchor-link" href="#Model-instance">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>In order to use the <code>xl</code> model, i.e. the one with 1.5B parameters, I change it to FP16 to avoid running out of memory.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

<span class="n">ckeckpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2-xl"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckeckpoints</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Pytorch-dataset">Pytorch dataset<a class="anchor-link" href="#Pytorch-dataset">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>We create a Pytorch dataset class</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">=</span> <span class="s2">"JOKE: "</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span> <span class="o">=</span> <span class="s2">"&lt;|endoftext|&gt;"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">joke</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">][</span><span class="n">item</span><span class="p">][</span><span class="s2">"Joke"</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_text_token</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We instantiate it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">jokes</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Here is an example</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>JOKE: Why can't Barbie get pregnant? Because Ken comes in a different box. Heyooooooo&lt;|endoftext|&gt;
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[5]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(torch.Size([1, 22]), torch.Size([1, 22]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Dataloader">Dataloader<a class="anchor-link" href="#Dataloader">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>We now create a Pytorch dataloader</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">BS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">joke_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see a batch</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentences</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">))</span>
<span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[7]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(1, torch.Size([1, 1, 36]), torch.Size([1, 1, 36]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h4>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">get_linear_schedule_with_warmup</span>
<span class="kn">import</span> <span class="nn">tqdm</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">3e-6</span>
<span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">MAX_SEQ_LEN</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="kc">None</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"EPOCH </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> started"</span> <span class="o">+</span> <span class="s1">'='</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">joke_dataloader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>

        <span class="n">sentence</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">sample</span>
        
        <span class="c1">#################### "Fit as many joke sequences into MAX_SEQ_LEN sequence as possible" logic start ####</span>
        <span class="n">joke_tens</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Skip sample from dataset if it is longer than MAX_SEQ_LEN</span>
        <span class="k">if</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
            <span class="k">continue</span>
        
        <span class="c1"># The first joke sequence in the sequence</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">tmp_jokes_tens</span><span class="p">):</span>
            <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># The next joke does not fit in so we process the sequence and leave the last joke </span>
            <span class="c1"># as the start for next sequence </span>
            <span class="k">if</span> <span class="n">tmp_jokes_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">joke_tens</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">MAX_SEQ_LEN</span><span class="p">:</span>
                <span class="n">work_jokes_tens</span> <span class="o">=</span> <span class="n">tmp_jokes_tens</span>
                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">joke_tens</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1">#Add the joke to sequence, continue and try to add more</span>
                <span class="n">tmp_jokes_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tmp_jokes_tens</span><span class="p">,</span> <span class="n">joke_tens</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">continue</span>
        <span class="c1">################## Sequence ready, process it trough the model ##################</span>
            
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">work_jokes_tens</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">work_jokes_tens</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                    
        <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="n">proc_seq_count</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">proc_seq_count</span> <span class="o">==</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
            <span class="n">proc_seq_count</span> <span class="o">=</span> <span class="mi">0</span>    
            <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> 
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">progress_bar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">'lr'</span><span class="p">:</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">]})</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_last_lr</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">batch_count</span> <span class="o">==</span> <span class="mi">10</span><span class="p">:</span>
            <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 0 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training:   0%|          | 0/231657 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [32:29&lt;00:00, 118.83it/s, loss=3.1, lr=2.31e-7] 
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 1 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [32:34&lt;00:00, 118.55it/s, loss=2.19, lr=4.62e-7]
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 2 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [32:36&lt;00:00, 118.42it/s, loss=2.42, lr=6.93e-7]
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 3 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [32:23&lt;00:00, 119.18it/s, loss=2.16, lr=9.25e-7]
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>EPOCH 4 started==============================
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Training: 100%|██████████| 231657/231657 [32:22&lt;00:00, 119.25it/s, loss=2.1, lr=1.16e-6] 
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">losses_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">lrs_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lrs</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses_np</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs_np</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'learning rate'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">'log'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-png-output-subarea">
<img alt="No description has been provided for this image" loading="lazy" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Inference">Inference<a class="anchor-link" href="#Inference">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Let's see how well the model makes jokes.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"JOKE:"</span>
<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>
<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/home/wallabot/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>decoded joke: 
JOKE:!!!!!!!!!!!!!!!!!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>You can see that you pass it a sequence with the word <code>joke</code> and it returns a joke. But if you return another sequence it does not</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence_joke</span> <span class="o">=</span> <span class="s2">"My dog is cute and"</span>
<span class="n">input_tokens_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence_joke</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">output_tokens_joke</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_tokens_joke</span><span class="p">)</span>
<span class="n">decoded_output_joke</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens_joke</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"decoded joke: </span><span class="se">\n</span><span class="si">{</span><span class="n">decoded_output_joke</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>decoded joke: 
My dog is cute and!!!!!!!!!!!!!!!
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Fine-tuning-GPT-2-for-sentence-classification">Fine tuning GPT-2 for sentence classification<a class="anchor-link" href="#Fine-tuning-GPT-2-for-sentence-classification">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now we are going to do a training with the Hugging Face libraries.</p>
</section>
<section class="section-block-markdown-cell">
<h4 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>Let's use the <code>imdb</code> dataset to classify statements into positive and negative ones</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"imdb"</span><span class="p">)</span>
<span class="n">dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[1]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's take a look at it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">info</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='imdb', config_name='plain_text', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=33435948, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'test': SplitInfo(name='test', num_bytes=32653810, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'unsupervised': SplitInfo(name='unsupervised', num_bytes=67113044, num_examples=50000, shard_lengths=None, dataset_name='imdb')}, download_checksums={'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet': {'num_bytes': 20979968, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet': {'num_bytes': 20470363, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet': {'num_bytes': 41996509, 'checksum': None}}, download_size=83446840, post_processing_size=None, dataset_size=133202802, size_in_bytes=216649642)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's take a look at the features of this dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">features</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'text': Value(dtype='string', id=None),
 'label': ClassLabel(names=['neg', 'pos'], id=None)}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>The dataset contains strings and classes. In addition there are two types of classes, <code>pos</code> and <code>neg</code>. Let's create a variable with the number of classes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">num_clases</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s2">"label"</span><span class="p">))</span>
<span class="n">num_clases</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>2</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>We create the tokenizer</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span>

<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">bos_token</span><span class="o">=</span><span class="s1">'&lt;|startoftext|&gt;'</span><span class="p">,</span> <span class="n">eos_token</span><span class="o">=</span><span class="s1">'&lt;|endoftext|&gt;'</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s1">'&lt;|pad|&gt;'</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have a tokenizer we can tokenize the dataset, since the model only understands tokens.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Model">Model<a class="anchor-link" href="#Model">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>We instantiate the model</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2ForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2ForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_clases</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Evaluation">Evaluation<a class="anchor-link" href="#Evaluation">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>We create an evaluation metric</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">evaluate</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Trainer">Trainer<a class="anchor-link" href="#Trainer">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>We create the trainer</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">"./results"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">"train"</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">"test"</span><span class="p">],</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>We train</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="4689" style="width:300px; height:20px; vertical-align: middle;" value="4689"></progress>
      [4689/4689 1:27:50, Epoch 3/3]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Step</th>
<th>Training Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>500</td>
<td>0.379400</td>
</tr>
<tr>
<td>1000</td>
<td>0.000000</td>
</tr>
<tr>
<td>1500</td>
<td>0.000000</td>
</tr>
<tr>
<td>2000</td>
<td>0.000000</td>
</tr>
<tr>
<td>2500</td>
<td>0.000000</td>
</tr>
<tr>
<td>3000</td>
<td>0.000000</td>
</tr>
<tr>
<td>3500</td>
<td>0.000000</td>
</tr>
<tr>
<td>4000</td>
<td>0.000000</td>
</tr>
<tr>
<td>4500</td>
<td>0.000000</td>
</tr>
</tbody>
</table><p></p></div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[25]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>TrainOutput(global_step=4689, training_loss=0.04045845954294626, metrics={'train_runtime': 5271.3532, 'train_samples_per_second': 14.228, 'train_steps_per_second': 0.89, 'total_flos': 3.91945125888e+16, 'train_loss': 0.04045845954294626, 'epoch': 3.0})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h4 id="Inference">Inference<a class="anchor-link" href="#Inference">¶</a></h4>
</section>
<section class="section-block-markdown-cell">
<p>We tested the model after training</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="s2">"positive"</span> <span class="k">if</span> <span class="n">prediction</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">"negative"</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">"I hate this movie!"</span>
<span class="nb">print</span><span class="p">(</span><span class="n">get_sentiment</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>negative
</pre>
</div>
</div>
</div>
</section>
