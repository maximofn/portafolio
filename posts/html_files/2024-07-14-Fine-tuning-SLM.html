<section class="section-block-markdown-cell">
<h1 id="Fine-tuning-SMLs-con-Hugging-Face">Fine tuning SMLs con Hugging Face<a class="anchor-link" href="#Fine-tuning-SMLs-con-Hugging-Face">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>En este post vamos a ver cómo hacer fine tuning a pequeños modelos de lenguaje, vamos a ver cómo hacer fine tuning para clasificación de texto y para generación de texto. Primero vamos a ver cómo hacerlo con las librerías de Hugging Face, ya que se Hugging Face se ha convertido en un actor muy importante en el ecosistema de IA en estos momentos.</p>
<p>Pero aunque las librerías de Hugging Face son muy importantes y útiles, es muy importante saber cómo se hace realmente el entrenamiento y qué está pasando por debajo, así que vamos a repetir el entrenamiento para clasificación y generación de texto pero con Pytorch</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Fine-tuning-para-clasificaci%C3%B3n-de-texto-con-Hugging-Face">Fine tuning para clasificación de texto con Hugging Face<a class="anchor-link" href="#Fine-tuning-para-clasificaci%C3%B3n-de-texto-con-Hugging-Face">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h3 id="Login">Login<a class="anchor-link" href="#Login">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para poder subir el resultado del entrenamiento al hub debemos logearnos primero, para ello necesitamos un token</p>
<p>Para crear un token hay que ir a la página de <a href="https://huggingface.co/settings/tokens">setings/tokens</a> de nuestra cuenta, nos aparecerá algo así</p>
<p><img alt="User-Access-Token-dark" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/User-Access-Token-dark.webp"/></p>
<p>Le damos a <code>New token</code> y nos aparecerá una ventana para crear un nuevo token</p>
<p><img alt="new-token-dark" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/new-token-dark.webp"/></p>
<p>Le damos un nombre al token y lo creamos con el rol <code>write</code>, o con el rol <code>Fine-grained</code>, que nos permite seleccionar exactamente qué permisos tendrá el token</p>
<p>Una vez creado lo copiamos y lo pegamos a continuación</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Ahora nos descargamos un dataset, en este caso nos vamos a descargar uno de reviews de <a href="https://huggingface.co/datasets/mteb/amazon_reviews_multi">Amazon</a></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"mteb/amazon_reviews_multi"</span><span class="p">,</span> <span class="s2">"en"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a verlo un poco</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['id', 'text', 'label', 'label_text'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['id', 'text', 'label', 'label_text'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'text', 'label', 'label_text'],
        num_rows: 5000
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que tiene un conjunto de entrenamiento con 200.000 muestras, uno de validación con 5.000 muestras y uno de test de 5.000 muestras</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver un ejemplo del conjunto de entrenamiento</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'id': 'en_0907914',
 'text': 'Mixed with fir it’s passable\n\nNot the scent I had hoped for . Love the scent of cedar, but this one missed',
 'label': 3,
 'label_text': '3'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que tiene la review en el campo <code>text</code> y la puntuación que le ha dado el usuario en el campo <code>label</code></p>
</section>
<section class="section-block-markdown-cell">
<p>Como vamos a hacer un modelo de clasificación de textos, necesitamos saber cuantas clases vamos a tener</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">'label'</span><span class="p">))</span>
<span class="n">num_classes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>5</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a tener 5 clases, ahora vamos a ver el valor mínimo de estas clases para saber si la puntuación comienza en 0 o en 1. Para ello usamos el método <code>unique</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">'label'</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[5]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'train': [0, 1, 2, 3, 4],
 'validation': [0, 1, 2, 3, 4],
 'test': [0, 1, 2, 3, 4]}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>El mínimo valor va a ser 0</p>
</section>
<section class="section-block-markdown-cell">
<p>Para entrenar, las etiquetas tienen que estar en un campo llamado <code>labels</code>, mientras que en nuestro dataset está en un campo que se llama <code>label</code>, por lo que creamos el nuevo campo <code>lables</code> con el mismo valor que <code>label</code></p>
</section>
<section class="section-block-markdown-cell">
<p>Creamos una función que haga lo que queremos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">set_labels</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">example</span><span class="p">[</span><span class="s1">'labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">example</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Aplicamos la función al dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver cómo queda el dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'id': 'en_0907914',
 'text': 'Mixed with fir it’s passable\n\nNot the scent I had hoped for . Love the scent of cedar, but this one missed',
 'label': 3,
 'label_text': '3',
 'labels': 3}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokenizador">Tokenizador<a class="anchor-link" href="#Tokenizador">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Como en el dataset tenemos las reviews en texto, necesitamos tokenizarlas para poder meter los tokens al modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora creamos una función para tokenizar el texto. Lo vamos a hacer de manera que todas las sentencias tenan la mismas longitud, de manera que el tokenizador truncará cuando sea necesario y añadirá tokens de padding cuando sea necesario. Además le indicamos que devuelva tensores de pytorch</p>
</section>
<section class="section-block-markdown-cell">
<p>Hacemos que la longitud de cada sentencia sea de 768 tokens porque estamos usando el modelo pequeño de GPT2, que como vimos en el post de <a href="https://maximofn.com/gpt2/#Arquitectura">GPT2</a> tiene una dimensión de embedding de 768 tokens</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a probar a tokenizar un texto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-text-output-error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[11], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> tokens <span style="color: rgb(98,98,98)">=</span> tokenize_function(dataset[<span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">train</span><span style="color: rgb(175,0,0)">'</span>][idx])

Cell <span class="ansi-green-fg">In[10], line 2</span>, in <span class="ansi-cyan-fg">tokenize_function</span><span class="ansi-blue-fg">(examples)</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">tokenize_function</span>(examples):
<span class="ansi-green-fg">----&gt; 2</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> tokenizer(examples[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">text</span><span style="color: rgb(175,0,0)">"</span>], padding<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">max_length</span><span style="color: rgb(175,0,0)">"</span>, truncation<span style="color: rgb(98,98,98)">=</span><span class="ansi-bold" style="color: rgb(0,135,0)">True</span>, max_length<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(98,98,98)">768</span>, return_tensors<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">pt</span><span style="color: rgb(175,0,0)">"</span>)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2883</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.__call__</span><span class="ansi-blue-fg">(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2881</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_in_target_context_manager:
<span class="ansi-green-intense-fg ansi-bold">   2882</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_input_mode()
<span class="ansi-green-fg">-&gt; 2883</span>     encodings <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_call_one(text<span style="color: rgb(98,98,98)">=</span>text, text_pair<span style="color: rgb(98,98,98)">=</span>text_pair, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>all_kwargs)
<span class="ansi-green-intense-fg ansi-bold">   2884</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> text_target <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:
<span class="ansi-green-intense-fg ansi-bold">   2885</span>     <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_target_mode()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2989</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._call_one</span><span class="ansi-blue-fg">(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2969</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>batch_encode_plus(
<span class="ansi-green-intense-fg ansi-bold">   2970</span>         batch_text_or_text_pairs<span style="color: rgb(98,98,98)">=</span>batch_text_or_text_pairs,
<span class="ansi-green-intense-fg ansi-bold">   2971</span>         add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   2986</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   2987</span>     )
<span class="ansi-green-intense-fg ansi-bold">   2988</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; 2989</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>encode_plus(
<span class="ansi-green-intense-fg ansi-bold">   2990</span>         text<span style="color: rgb(98,98,98)">=</span>text,
<span class="ansi-green-intense-fg ansi-bold">   2991</span>         text_pair<span style="color: rgb(98,98,98)">=</span>text_pair,
<span class="ansi-green-intense-fg ansi-bold">   2992</span>         add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,
<span class="ansi-green-intense-fg ansi-bold">   2993</span>         padding<span style="color: rgb(98,98,98)">=</span>padding,
<span class="ansi-green-intense-fg ansi-bold">   2994</span>         truncation<span style="color: rgb(98,98,98)">=</span>truncation,
<span class="ansi-green-intense-fg ansi-bold">   2995</span>         max_length<span style="color: rgb(98,98,98)">=</span>max_length,
<span class="ansi-green-intense-fg ansi-bold">   2996</span>         stride<span style="color: rgb(98,98,98)">=</span>stride,
<span class="ansi-green-intense-fg ansi-bold">   2997</span>         is_split_into_words<span style="color: rgb(98,98,98)">=</span>is_split_into_words,
<span class="ansi-green-intense-fg ansi-bold">   2998</span>         pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,
<span class="ansi-green-intense-fg ansi-bold">   2999</span>         return_tensors<span style="color: rgb(98,98,98)">=</span>return_tensors,
<span class="ansi-green-intense-fg ansi-bold">   3000</span>         return_token_type_ids<span style="color: rgb(98,98,98)">=</span>return_token_type_ids,
<span class="ansi-green-intense-fg ansi-bold">   3001</span>         return_attention_mask<span style="color: rgb(98,98,98)">=</span>return_attention_mask,
<span class="ansi-green-intense-fg ansi-bold">   3002</span>         return_overflowing_tokens<span style="color: rgb(98,98,98)">=</span>return_overflowing_tokens,
<span class="ansi-green-intense-fg ansi-bold">   3003</span>         return_special_tokens_mask<span style="color: rgb(98,98,98)">=</span>return_special_tokens_mask,
<span class="ansi-green-intense-fg ansi-bold">   3004</span>         return_offsets_mapping<span style="color: rgb(98,98,98)">=</span>return_offsets_mapping,
<span class="ansi-green-intense-fg ansi-bold">   3005</span>         return_length<span style="color: rgb(98,98,98)">=</span>return_length,
<span class="ansi-green-intense-fg ansi-bold">   3006</span>         verbose<span style="color: rgb(98,98,98)">=</span>verbose,
<span class="ansi-green-intense-fg ansi-bold">   3007</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   3008</span>     )

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3053</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.encode_plus</span><span class="ansi-blue-fg">(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   3032</span> <span style="color: rgb(175,0,0)">"""</span>
<span class="ansi-green-intense-fg ansi-bold">   3033</span> <span style="color: rgb(175,0,0)">Tokenize and prepare for the model a sequence or a pair of sequences.</span>
<span class="ansi-green-intense-fg ansi-bold">   3034</span> 
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   3049</span> <span style="color: rgb(175,0,0)">        method).</span>
<span class="ansi-green-intense-fg ansi-bold">   3050</span> <span style="color: rgb(175,0,0)">"""</span>
<span class="ansi-green-intense-fg ansi-bold">   3052</span> <span style="color: rgb(95,135,135)"># Backward compatibility for 'truncation_strategy', 'pad_to_max_length'</span>
<span class="ansi-green-fg">-&gt; 3053</span> padding_strategy, truncation_strategy, max_length, kwargs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_get_padding_truncation_strategies(
<span class="ansi-green-intense-fg ansi-bold">   3054</span>     padding<span style="color: rgb(98,98,98)">=</span>padding,
<span class="ansi-green-intense-fg ansi-bold">   3055</span>     truncation<span style="color: rgb(98,98,98)">=</span>truncation,
<span class="ansi-green-intense-fg ansi-bold">   3056</span>     max_length<span style="color: rgb(98,98,98)">=</span>max_length,
<span class="ansi-green-intense-fg ansi-bold">   3057</span>     pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,
<span class="ansi-green-intense-fg ansi-bold">   3058</span>     verbose<span style="color: rgb(98,98,98)">=</span>verbose,
<span class="ansi-green-intense-fg ansi-bold">   3059</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   3060</span> )
<span class="ansi-green-intense-fg ansi-bold">   3062</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_encode_plus(
<span class="ansi-green-intense-fg ansi-bold">   3063</span>     text<span style="color: rgb(98,98,98)">=</span>text,
<span class="ansi-green-intense-fg ansi-bold">   3064</span>     text_pair<span style="color: rgb(98,98,98)">=</span>text_pair,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   3080</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   3081</span> )

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2788</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._get_padding_truncation_strategies</span><span class="ansi-blue-fg">(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2786</span> <span style="color: rgb(95,135,135)"># Test if we have a padding token</span>
<span class="ansi-green-intense-fg ansi-bold">   2787</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (<span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token_id <span style="color: rgb(98,98,98)">&lt;</span> <span style="color: rgb(98,98,98)">0</span>):
<span class="ansi-green-fg">-&gt; 2788</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-intense-fg ansi-bold">   2789</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Asking to pad but the tokenizer does not have a padding token. </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2790</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2791</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">or add a new pad token via `tokenizer.add_special_tokens(</span><span style="color: rgb(175,0,0)">{</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">pad_token</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">: </span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">[PAD]</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">})`.</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2792</span>     )
<span class="ansi-green-intense-fg ansi-bold">   2794</span> <span style="color: rgb(95,135,135)"># Check that we will truncate to a multiple of pad_to_multiple_of if both are provided</span>
<span class="ansi-green-intense-fg ansi-bold">   2795</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> (
<span class="ansi-green-intense-fg ansi-bold">   2796</span>     truncation_strategy <span style="color: rgb(98,98,98)">!=</span> TruncationStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_TRUNCATE
<span class="ansi-green-intense-fg ansi-bold">   2797</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   2800</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (max_length <span style="color: rgb(98,98,98)">%</span> pad_to_multiple_of <span style="color: rgb(98,98,98)">!=</span> <span style="color: rgb(98,98,98)">0</span>)
<span class="ansi-green-intense-fg ansi-bold">   2801</span> ):

<span class="ansi-red-fg">ValueError</span>: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nos da un error porque el tokenizador de GPT2 no tiene un token para padding y nos pide que asignemos uno, además nos sugiere hacer <code>tokenizer.pad_token = tokenizer.eos_token</code>, así que lo hacemos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Volvemos a probar la función de tokenización</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">])</span>
<span class="n">tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[100]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(torch.Size([1, 768]), torch.Size([1, 768]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora que hemos comprobado que la función tokeniza bien, aplicamos esta función al dataset, pero además la aplicamos por batches para que se ejecute más rápido</p>
<p>Además aprovechamos y eliminamos las columnas que no vamos a necesitar</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'text'</span><span class="p">,</span> <span class="s1">'label'</span><span class="p">,</span> <span class="s1">'id'</span><span class="p">,</span> <span class="s1">'label_text'</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos ahora cómo queda el dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[12]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['id', 'text', 'label', 'label_text', 'labels'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['id', 'text', 'label', 'label_text', 'labels'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'text', 'label', 'label_text', 'labels'],
        num_rows: 5000
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que tenemos los campos 'labels', 'input_ids' y 'attention_mask', que es lo que nos interesa para entrenar</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Instanciamos un modelo para clasificación de secuencias y le indicamos el número de clases que tenemos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nos dice que los pesos de la capa <code>score</code> han sido inicializados de manera aleatoria y que tenemos que reentrenarlos, vamos a ver por qué pasa esto</p>
</section>
<section class="section-block-markdown-cell">
<p>El modelo de GPT2 sería este</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">casual_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Mientras que el modelo de GPT2 para generar texto es este</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver su arquitectura</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">casual_model</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Y ahora la arquitectura del modelo que vamos a usar para clasificar las reviews</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[16]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=5, bias=False)
)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>De aquí hay dos cosas que mencionar</p>
<ul>
<li>La primera es que en ambos, la primera capa tiene dimensiones de 50257x768, que corresponde a 50257 posibles tokens del vocabulario de GPT2 y a 768 dimensiones del embedding, por lo que hemos hecho bien en tokenizar las reviews con un tamaño de 768 tokens</li>
<li>La segunda es que el modelo <code>casual</code> (el de generación de texto) tiene al final una capa <code>Linear</code> que genera 50257 valores, es decir, es la encargada de predecir el siguiente token y a posible token le da un valor. Mientras que el modelo de clasificación tiene una capa <code>Linear</code> que solo genera 5 valores, uno por cada clase, lo que nos dará la probabilidad de que la review pertenezca a cada clase</li>
</ul>
<p>Por eso nos salía el mensaje de que los pesos de la capa <code>score</code> habían sido inicializados de manera aleatoria, porque la librería transformers ha eliminado la capa <code>Linear</code> de 768x50257 y ha añadido una capa <code>Linear</code> de 768x5, la ha inicializado con valores aleatorios y nosotros la tenemos que entrenar para nuestro problema en particular</p>
</section>
<section class="section-block-markdown-cell">
<p>Borramos el modelo casual, porque no lo vamos a usar</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">del</span> <span class="n">casual_model</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Trainer">Trainer<a class="anchor-link" href="#Trainer">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ahora a configurar los argumentos del entrenamiento</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-finetuned-amazon-reviews-en-classification"</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>
<span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">28</span>
<span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>
    <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Definimos una métrica para el dataloader de validación</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Definimos ahora el trainer</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'validation'</span><span class="p">],</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Entrenamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>  0%|          | 0/600000 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-text-output-error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[21], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> trainer<span style="color: rgb(98,98,98)">.</span>train()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:1876</span>, in <span class="ansi-cyan-fg">Trainer.train</span><span class="ansi-blue-fg">(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1873</span> <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:
<span class="ansi-green-intense-fg ansi-bold">   1874</span>     <span style="color: rgb(95,135,135)"># Disable progress bars when uploading models during checkpoints to avoid polluting stdout</span>
<span class="ansi-green-intense-fg ansi-bold">   1875</span>     hf_hub_utils<span style="color: rgb(98,98,98)">.</span>disable_progress_bars()
<span class="ansi-green-fg">-&gt; 1876</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> inner_training_loop(
<span class="ansi-green-intense-fg ansi-bold">   1877</span>         args<span style="color: rgb(98,98,98)">=</span>args,
<span class="ansi-green-intense-fg ansi-bold">   1878</span>         resume_from_checkpoint<span style="color: rgb(98,98,98)">=</span>resume_from_checkpoint,
<span class="ansi-green-intense-fg ansi-bold">   1879</span>         trial<span style="color: rgb(98,98,98)">=</span>trial,
<span class="ansi-green-intense-fg ansi-bold">   1880</span>         ignore_keys_for_eval<span style="color: rgb(98,98,98)">=</span>ignore_keys_for_eval,
<span class="ansi-green-intense-fg ansi-bold">   1881</span>     )
<span class="ansi-green-intense-fg ansi-bold">   1882</span> <span class="ansi-bold" style="color: rgb(0,135,0)">finally</span>:
<span class="ansi-green-intense-fg ansi-bold">   1883</span>     hf_hub_utils<span style="color: rgb(98,98,98)">.</span>enable_progress_bars()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:2178</span>, in <span class="ansi-cyan-fg">Trainer._inner_training_loop</span><span class="ansi-blue-fg">(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)</span>
<span class="ansi-green-intense-fg ansi-bold">   2175</span>     rng_to_sync <span style="color: rgb(98,98,98)">=</span> <span class="ansi-bold" style="color: rgb(0,135,0)">True</span>
<span class="ansi-green-intense-fg ansi-bold">   2177</span> step <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">1</span>
<span class="ansi-green-fg">-&gt; 2178</span> <span class="ansi-bold" style="color: rgb(0,135,0)">for</span> step, inputs <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> <span style="color: rgb(0,135,0)">enumerate</span>(epoch_iterator):
<span class="ansi-green-intense-fg ansi-bold">   2179</span>     total_batched_samples <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">1</span>
<span class="ansi-green-intense-fg ansi-bold">   2181</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>args<span style="color: rgb(98,98,98)">.</span>include_num_input_tokens_seen:

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/accelerate/data_loader.py:454</span>, in <span class="ansi-cyan-fg">DataLoaderShard.__iter__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    452</span> <span style="color: rgb(95,135,135)"># We iterate one batch ahead to check when we are at the end</span>
<span class="ansi-green-intense-fg ansi-bold">    453</span> <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">--&gt; 454</span>     current_batch <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">next</span>(dataloader_iter)
<span class="ansi-green-intense-fg ansi-bold">    455</span> <span class="ansi-bold" style="color: rgb(0,135,0)">except</span> <span class="ansi-bold" style="color: rgb(215,95,95)">StopIteration</span>:
<span class="ansi-green-intense-fg ansi-bold">    456</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">yield</span>

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631</span>, in <span class="ansi-cyan-fg">_BaseDataLoaderIter.__next__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    628</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_sampler_iter <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:
<span class="ansi-green-intense-fg ansi-bold">    629</span>     <span style="color: rgb(95,135,135)"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>
<span class="ansi-green-intense-fg ansi-bold">    630</span>     <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_reset()  <span style="color: rgb(95,135,135)"># type: ignore[call-arg]</span>
<span class="ansi-green-fg">--&gt; 631</span> data <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_next_data()
<span class="ansi-green-intense-fg ansi-bold">    632</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_num_yielded <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">1</span>
<span class="ansi-green-intense-fg ansi-bold">    633</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_dataset_kind <span style="color: rgb(98,98,98)">==</span> _DatasetKind<span style="color: rgb(98,98,98)">.</span>Iterable <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> \
<span class="ansi-green-intense-fg ansi-bold">    634</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_IterableDataset_len_called <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> \
<span class="ansi-green-intense-fg ansi-bold">    635</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_num_yielded <span style="color: rgb(98,98,98)">&gt;</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_IterableDataset_len_called:

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675</span>, in <span class="ansi-cyan-fg">_SingleProcessDataLoaderIter._next_data</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    673</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">_next_data</span>(<span style="color: rgb(0,135,0)">self</span>):
<span class="ansi-green-intense-fg ansi-bold">    674</span>     index <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_next_index()  <span style="color: rgb(95,135,135)"># may raise StopIteration</span>
<span class="ansi-green-fg">--&gt; 675</span>     data <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_dataset_fetcher<span style="color: rgb(98,98,98)">.</span>fetch(index)  <span style="color: rgb(95,135,135)"># may raise StopIteration</span>
<span class="ansi-green-intense-fg ansi-bold">    676</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_pin_memory:
<span class="ansi-green-intense-fg ansi-bold">    677</span>         data <span style="color: rgb(98,98,98)">=</span> _utils<span style="color: rgb(98,98,98)">.</span>pin_memory<span style="color: rgb(98,98,98)">.</span>pin_memory(data, <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_pin_memory_device)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54</span>, in <span class="ansi-cyan-fg">_MapDatasetFetcher.fetch</span><span class="ansi-blue-fg">(self, possibly_batched_index)</span>
<span class="ansi-green-intense-fg ansi-bold">     52</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-intense-fg ansi-bold">     53</span>     data <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>dataset[possibly_batched_index]
<span class="ansi-green-fg">---&gt; 54</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>collate_fn(data)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/data/data_collator.py:271</span>, in <span class="ansi-cyan-fg">DataCollatorWithPadding.__call__</span><span class="ansi-blue-fg">(self, features)</span>
<span class="ansi-green-intense-fg ansi-bold">    270</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">__call__</span>(<span style="color: rgb(0,135,0)">self</span>, features: List[Dict[<span style="color: rgb(0,135,0)">str</span>, Any]]) <span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">&gt;</span> Dict[<span style="color: rgb(0,135,0)">str</span>, Any]:
<span class="ansi-green-fg">--&gt; 271</span>     batch <span style="color: rgb(98,98,98)">=</span> pad_without_fast_tokenizer_warning(
<span class="ansi-green-intense-fg ansi-bold">    272</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>tokenizer,
<span class="ansi-green-intense-fg ansi-bold">    273</span>         features,
<span class="ansi-green-intense-fg ansi-bold">    274</span>         padding<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>padding,
<span class="ansi-green-intense-fg ansi-bold">    275</span>         max_length<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>max_length,
<span class="ansi-green-intense-fg ansi-bold">    276</span>         pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_to_multiple_of,
<span class="ansi-green-intense-fg ansi-bold">    277</span>         return_tensors<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>return_tensors,
<span class="ansi-green-intense-fg ansi-bold">    278</span>     )
<span class="ansi-green-intense-fg ansi-bold">    279</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">label</span><span style="color: rgb(175,0,0)">"</span> <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> batch:
<span class="ansi-green-intense-fg ansi-bold">    280</span>         batch[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">labels</span><span style="color: rgb(175,0,0)">"</span>] <span style="color: rgb(98,98,98)">=</span> batch[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">label</span><span style="color: rgb(175,0,0)">"</span>]

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/data/data_collator.py:66</span>, in <span class="ansi-cyan-fg">pad_without_fast_tokenizer_warning</span><span class="ansi-blue-fg">(tokenizer, *pad_args, **pad_kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">     63</span> tokenizer<span style="color: rgb(98,98,98)">.</span>deprecation_warnings[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Asking-to-pad-a-fast-tokenizer</span><span style="color: rgb(175,0,0)">"</span>] <span style="color: rgb(98,98,98)">=</span> <span class="ansi-bold" style="color: rgb(0,135,0)">True</span>
<span class="ansi-green-intense-fg ansi-bold">     65</span> <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">---&gt; 66</span>     padded <span style="color: rgb(98,98,98)">=</span> tokenizer<span style="color: rgb(98,98,98)">.</span>pad(<span style="color: rgb(98,98,98)">*</span>pad_args, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>pad_kwargs)
<span class="ansi-green-intense-fg ansi-bold">     67</span> <span class="ansi-bold" style="color: rgb(0,135,0)">finally</span>:
<span class="ansi-green-intense-fg ansi-bold">     68</span>     <span style="color: rgb(95,135,135)"># Restore the state of the warning.</span>
<span class="ansi-green-intense-fg ansi-bold">     69</span>     tokenizer<span style="color: rgb(98,98,98)">.</span>deprecation_warnings[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Asking-to-pad-a-fast-tokenizer</span><span style="color: rgb(175,0,0)">"</span>] <span style="color: rgb(98,98,98)">=</span> warning_state

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3299</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.pad</span><span class="ansi-blue-fg">(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)</span>
<span class="ansi-green-intense-fg ansi-bold">   3297</span> <span style="color: rgb(95,135,135)"># The model's main input name, usually `input_ids`, has be passed for padding</span>
<span class="ansi-green-intense-fg ansi-bold">   3298</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>model_input_names[<span style="color: rgb(98,98,98)">0</span>] <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> encoded_inputs:
<span class="ansi-green-fg">-&gt; 3299</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-intense-fg ansi-bold">   3300</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">You should supply an encoding or a list of encodings to this method </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   3301</span>         <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">that includes </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>model_input_names[<span style="color: rgb(98,98,98)">0</span>]<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">, but you provided </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">list</span>(encoded_inputs<span style="color: rgb(98,98,98)">.</span>keys())<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   3302</span>     )
<span class="ansi-green-intense-fg ansi-bold">   3304</span> required_input <span style="color: rgb(98,98,98)">=</span> encoded_inputs[<span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>model_input_names[<span style="color: rgb(98,98,98)">0</span>]]
<span class="ansi-green-intense-fg ansi-bold">   3306</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> required_input <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> (<span style="color: rgb(0,135,0)">isinstance</span>(required_input, Sized) <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> <span style="color: rgb(0,135,0)">len</span>(required_input) <span style="color: rgb(98,98,98)">==</span> <span style="color: rgb(98,98,98)">0</span>):

<span class="ansi-red-fg">ValueError</span>: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label', 'labels']</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nos vuelve a salir un error porque el modelo no tiene asignado un token de padding, así que al igual que con el tokenizador se lo asignamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Volvemos a crear los argumentos del trainer con el nuevo modelo, que ahora si tiene token de padding, el trainer y volvemos a entrenar</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>
    <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'validation'</span><span class="p">],</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora que hemos visto que está todo bien podemos entrenar</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="21429" style="width:300px; height:20px; vertical-align: middle;" value="2250"></progress>
      [ 2250/21429 45:38 &lt; 6:29:27, 0.82 it/s, Epoch 0.31/3]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
</tr>
</thead>
<tbody>
</tbody>
</table><p></p></div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="21429" style="width:300px; height:20px; vertical-align: middle;" value="21429"></progress>
      [21429/21429 7:19:25, Epoch 3/3]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.807400</td>
<td>0.820341</td>
<td>0.652000</td>
</tr>
<tr>
<td>2</td>
<td>0.751900</td>
<td>0.802189</td>
<td>0.654600</td>
</tr>
<tr>
<td>3</td>
<td>0.718100</td>
<td>0.810221</td>
<td>0.657800</td>
</tr>
</tbody>
</table><p></p></div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&lt;transformers.trainer_utils.EvalPrediction object at 0x782767ea1450&gt;
&lt;transformers.trainer_utils.EvalPrediction object at 0x782767eeefe0&gt;
&lt;transformers.trainer_utils.EvalPrediction object at 0x782767eecfd0&gt;
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[28]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>TrainOutput(global_step=21429, training_loss=0.7846888848762739, metrics={'train_runtime': 26367.7801, 'train_samples_per_second': 22.755, 'train_steps_per_second': 0.813, 'total_flos': 2.35173445632e+17, 'train_loss': 0.7846888848762739, 'epoch': 3.0})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Evaluaci%C3%B3n">Evaluación<a class="anchor-link" href="#Evaluaci%C3%B3n">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Una vez entrenado evaluamos sobre el dataset de test</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'test'</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="125" style="width:300px; height:20px; vertical-align: middle;" value="125"></progress>
      [125/125 01:15]
    </div>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&lt;transformers.trainer_utils.EvalPrediction object at 0x7826ddfded40&gt;
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[29]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'eval_loss': 0.7973636984825134,
 'eval_accuracy': 0.6626,
 'eval_runtime': 76.3016,
 'eval_samples_per_second': 65.529,
 'eval_steps_per_second': 1.638,
 'epoch': 3.0}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Publicar-el-modelo">Publicar el modelo<a class="anchor-link" href="#Publicar-el-modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Ya tenemos nuestro modelo entrenado, ya podemos compartirlo con el mundo, así que primero creamos una model card</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Y ya lo podemos publicar. Como lo primero que hemos hecho ha sido loguearnos con el hub de huggingface, lo podremos subir a nuestro hub sin ningún problema</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Uso-del-modelo">Uso del modelo<a class="anchor-link" href="#Uso-del-modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Limpiamos todo lo posible</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">gc</span>


<span class="k">def</span> <span class="nf">clear_hardwares</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>


<span class="n">clear_hardwares</span><span class="p">()</span>
<span class="n">clear_hardwares</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como hemos subido el modelo a nuestro hub podemos descargarlo y usarlo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>
<span class="n">checkpoints</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>
<span class="n">task</span> <span class="o">=</span> <span class="s2">"text-classification"</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora si queremos que nos devuelva la probabilidad de todas las clases, simplemente usamos el clasificador que acabamos de instanciar, con el parámetro <code>top_k=None</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">labels</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'label': 'LABEL_4', 'score': 0.8253807425498962},
 {'label': 'LABEL_3', 'score': 0.15411493182182312},
 {'label': 'LABEL_2', 'score': 0.013907806016504765},
 {'label': 'LABEL_0', 'score': 0.003939222544431686},
 {'label': 'LABEL_1', 'score': 0.0026572425849735737}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Si solo queremos la clase con la mayor probabilidad hacemos lo mismo pero con el parámetro <code>top_k=1</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">label</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">label</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[9]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'label': 'LABEL_4', 'score': 0.8253807425498962}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Y si queremos n clases hacemos lo mismo pero con el parámetro <code>top_k=n</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">two_labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">two_labels</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[10]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'label': 'LABEL_4', 'score': 0.8253807425498962},
 {'label': 'LABEL_3', 'score': 0.15411493182182312}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>También podemo probar el modelo con Automodel y AutoTokenizer</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-finetuned-amazon-reviews-en-classification"</span>
<span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">logits</span>
<span class="n">lables</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">lables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[26]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[0.003963470458984375,
 0.0026721954345703125,
 0.01397705078125,
 0.154541015625,
 0.82470703125]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Si quires probar más el modelo puedes verlo en <a href="https://huggingface.co/Maximofn/GPT2-small-finetuned-amazon-reviews-en-classification">Maximofn/GPT2-small-finetuned-amazon-reviews-en-classification</a></p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Fine-tuning-para-generaci%C3%B3n-de-texto-con-Hugging-Face">Fine tuning para generación de texto con Hugging Face<a class="anchor-link" href="#Fine-tuning-para-generaci%C3%B3n-de-texto-con-Hugging-Face">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para asegurarme no tener problemas de memoria VRAM reinicio el notebook</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Login">Login<a class="anchor-link" href="#Login">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para poder subir el resultado del entrenamiento al hub debemos logearnos primero, para ello necesitamos un token</p>
<p>Para crear un token hay que ir a la página de <a href="https://huggingface.co/settings/tokens">setings/tokens</a> de nuestra cuenta, nos aparecerá algo así</p>
<p><img alt="User-Access-Token-dark" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/User-Access-Token-dark.webp"/></p>
<p>Le damos a <code>New token</code> y nos aparecerá una ventana para crear un nuevo token</p>
<p><img alt="new-token-dark" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/new-token-dark.webp"/></p>
<p>Le damos un nombre al token y lo creamos con el rol <code>write</code>, o con el rol <code>Fine-grained</code>, que nos permite seleccionar exactamente qué permisos tendrá el token</p>
<p>Una vez creado lo copiamos y lo pegamos a continuación</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a usar un dataset de <a href="https://huggingface.co/datasets/Maximofn/short-jokes-dataset">chistes en inglés</a></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"Maximofn/short-jokes-dataset"</span><span class="p">)</span>
<span class="n">jokes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[1]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['ID', 'Joke'],
        num_rows: 231657
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a verlo un poco</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">jokes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['ID', 'Joke'],
        num_rows: 231657
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que es un único set de entrenamiento de más de 200 mil chistes. Así que más adelante lo tendremos que dividir en train y evaluación</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver una muestra</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">jokes</span><span class="p">[</span><span class="s1">'train'</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">jokes</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'ID': 198387,
 'Joke': 'My hot dislexic co-worker said she had an important massage to give me in her office... When I got there, she told me it can wait until I put on some clothes.'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que tiene una ID del chiste que no nos interesa para nada y el propio chiste</p>
</section>
<section class="section-block-markdown-cell">
<p>Por si tienes poca memoria en la GPU voy a hacer un subset del dataset, elije el porcentaje de chistes que quieres usar</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percent_of_train_dataset</span> <span class="o">=</span> <span class="mi">1</span>    <span class="c1"># If you want 50% of the dataset, set this to 0.5</span>

<span class="n">subset_dataset</span> <span class="o">=</span> <span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span> <span class="o">*</span> <span class="n">percent_of_train_dataset</span><span class="p">)))</span>
<span class="n">subset_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>Dataset({
    features: ['ID', 'Joke'],
    num_rows: 231657
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora dividimos el subset en un conjunto de entrenamiento y otro de validación</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percent_of_train_dataset</span> <span class="o">=</span> <span class="mf">0.90</span>

<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">subset_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">subset_dataset</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">*</span> <span class="n">percent_of_train_dataset</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">validation_test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>

<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">validation_test_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">validation_test_dataset</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of the train set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">. Size of the validation set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">. Size of the test set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Size of the train set: 208491. Size of the validation set: 11583. Size of the test set: 11583
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokenizador">Tokenizador<a class="anchor-link" href="#Tokenizador">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Instanciamos el tokenizador. Instanciamos el token de padding del tokenizador para que no nos de error como antes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">"right"</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a añadir dos nuevos tokens de inicio de chiste y final de chiste para tener más control</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'&lt;SJ&gt;'</span><span class="p">,</span> <span class="s1">'&lt;EJ&gt;'</span><span class="p">]</span>   <span class="c1"># Start and end of joke tokens</span>

<span class="n">num_added_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Added </span><span class="si">{</span><span class="n">num_added_tokens</span><span class="si">}</span><span class="s2"> tokens"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Added 2 tokens
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Creamos una función para añadir los nuevos tokens a las sentencias</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">joke_column</span> <span class="o">=</span> <span class="s2">"Joke"</span>

<span class="k">def</span> <span class="nf">format_joke</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">example</span><span class="p">[</span><span class="n">joke_column</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'&lt;SJ&gt; '</span> <span class="o">+</span> <span class="n">example</span><span class="p">[</span><span class="s1">'Joke'</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' &lt;EJ&gt;'</span>
    <span class="k">return</span> <span class="n">example</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Seleccionamos las columnas que no necesitamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">remove_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">column_names</span> <span class="k">if</span> <span class="n">column</span> <span class="o">!=</span> <span class="n">joke_column</span><span class="p">]</span>
<span class="n">remove_columns</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[9]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>['ID']</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Formateamos el dataset y eliminamos las columnas que no necesitamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">test_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[10]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(Dataset({
     features: ['Joke'],
     num_rows: 208491
 }),
 Dataset({
     features: ['Joke'],
     num_rows: 11583
 }),
 Dataset({
     features: ['Joke'],
     num_rows: 11583
 }))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora creamos una función para tokenizar los chistes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="n">joke_column</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Tokenizamos el dataset y eliminamos la columna con el texto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">test_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 208491
 }),
 Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 11583
 }),
 Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 11583
 }))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Ahora instanciamos el modelo para generación de texto y le asignamos al token de pading el token de end of string</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos el tamaño del vocabulario del modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">vocab_size</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>50257</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Tiene 50257 tokens, que es el tamaño del vocabulario de GPT2. Pero como hemos dicho que íbamos a crear dos tokens nuevos con el inicio de chiste y el final de chiste, los añadimos al modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>

<span class="n">new_vocab_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Old vocab size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">. New vocab size: </span><span class="si">{</span><span class="n">new_vocab_size</span><span class="si">}</span><span class="s2">. Added </span><span class="si">{</span><span class="n">new_vocab_size</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">vocab_size</span><span class="si">}</span><span class="s2"> tokens"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Old vocab size: 50257. New vocab size: 50259. Added 2 tokens
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se han añadido los dos nuevos tokens</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Entrenamiento">Entrenamiento<a class="anchor-link" href="#Entrenamiento">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Configuramos los parámetros de entrenamiento</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM"</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"./training_results"</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>
<span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">28</span>
<span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>
    <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># metric_for_best_model=metric_name,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora no usamos <code>metric_for_best_model</code>, después de definir el trainer explicamos por qué</p>
</section>
<section class="section-block-markdown-cell">
<p>Definimos el trainer</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">validation_dataset</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="c1"># compute_metrics=compute_metrics,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>En este caso no le pasamos una función <code>compute_metrics</code>, si no se le pasa, durante la evaluación se usará la <code>loss</code> para evaluar el modelo. Por eso al definir los argumentos no definimos <code>metric_for_best_model</code>, porque no vamos a usar una métrica para evaluar el modelo, si no la <code>loss</code></p>
</section>
<section class="section-block-markdown-cell">
<p>Entrenamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>  0%|          | 0/625473 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-text-output-error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[19], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> trainer<span style="color: rgb(98,98,98)">.</span>train()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:1885</span>, in <span class="ansi-cyan-fg">Trainer.train</span><span class="ansi-blue-fg">(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1883</span>         hf_hub_utils<span style="color: rgb(98,98,98)">.</span>enable_progress_bars()
<span class="ansi-green-intense-fg ansi-bold">   1884</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; 1885</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> inner_training_loop(
<span class="ansi-green-intense-fg ansi-bold">   1886</span>         args<span style="color: rgb(98,98,98)">=</span>args,
<span class="ansi-green-intense-fg ansi-bold">   1887</span>         resume_from_checkpoint<span style="color: rgb(98,98,98)">=</span>resume_from_checkpoint,
<span class="ansi-green-intense-fg ansi-bold">   1888</span>         trial<span style="color: rgb(98,98,98)">=</span>trial,
<span class="ansi-green-intense-fg ansi-bold">   1889</span>         ignore_keys_for_eval<span style="color: rgb(98,98,98)">=</span>ignore_keys_for_eval,
<span class="ansi-green-intense-fg ansi-bold">   1890</span>     )

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:2216</span>, in <span class="ansi-cyan-fg">Trainer._inner_training_loop</span><span class="ansi-blue-fg">(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)</span>
<span class="ansi-green-intense-fg ansi-bold">   2213</span>     <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>control <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>callback_handler<span style="color: rgb(98,98,98)">.</span>on_step_begin(args, <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>state, <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>control)
<span class="ansi-green-intense-fg ansi-bold">   2215</span> <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>accelerator<span style="color: rgb(98,98,98)">.</span>accumulate(model):
<span class="ansi-green-fg">-&gt; 2216</span>     tr_loss_step <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>training_step(model, inputs)
<span class="ansi-green-intense-fg ansi-bold">   2218</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> (
<span class="ansi-green-intense-fg ansi-bold">   2219</span>     args<span style="color: rgb(98,98,98)">.</span>logging_nan_inf_filter
<span class="ansi-green-intense-fg ansi-bold">   2220</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> is_torch_xla_available()
<span class="ansi-green-intense-fg ansi-bold">   2221</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (torch<span style="color: rgb(98,98,98)">.</span>isnan(tr_loss_step) <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> torch<span style="color: rgb(98,98,98)">.</span>isinf(tr_loss_step))
<span class="ansi-green-intense-fg ansi-bold">   2222</span> ):
<span class="ansi-green-intense-fg ansi-bold">   2223</span>     <span style="color: rgb(95,135,135)"># if loss is nan or inf simply add the average of previous logged losses</span>
<span class="ansi-green-intense-fg ansi-bold">   2224</span>     tr_loss <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> tr_loss <span style="color: rgb(98,98,98)">/</span> (<span style="color: rgb(98,98,98)">1</span> <span style="color: rgb(98,98,98)">+</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>state<span style="color: rgb(98,98,98)">.</span>global_step <span style="color: rgb(98,98,98)">-</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_globalstep_last_logged)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:3238</span>, in <span class="ansi-cyan-fg">Trainer.training_step</span><span class="ansi-blue-fg">(self, model, inputs)</span>
<span class="ansi-green-intense-fg ansi-bold">   3235</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> loss_mb<span style="color: rgb(98,98,98)">.</span>reduce_mean()<span style="color: rgb(98,98,98)">.</span>detach()<span style="color: rgb(98,98,98)">.</span>to(<span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>args<span style="color: rgb(98,98,98)">.</span>device)
<span class="ansi-green-intense-fg ansi-bold">   3237</span> <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>compute_loss_context_manager():
<span class="ansi-green-fg">-&gt; 3238</span>     loss <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>compute_loss(model, inputs)
<span class="ansi-green-intense-fg ansi-bold">   3240</span> <span class="ansi-bold" style="color: rgb(0,135,0)">del</span> inputs
<span class="ansi-green-intense-fg ansi-bold">   3241</span> torch<span style="color: rgb(98,98,98)">.</span>cuda<span style="color: rgb(98,98,98)">.</span>empty_cache()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:3282</span>, in <span class="ansi-cyan-fg">Trainer.compute_loss</span><span class="ansi-blue-fg">(self, model, inputs, return_outputs)</span>
<span class="ansi-green-intense-fg ansi-bold">   3280</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-intense-fg ansi-bold">   3281</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">isinstance</span>(outputs, <span style="color: rgb(0,135,0)">dict</span>) <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">loss</span><span style="color: rgb(175,0,0)">"</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> outputs:
<span class="ansi-green-fg">-&gt; 3282</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-intense-fg ansi-bold">   3283</span>             <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">The model did not return a loss from the inputs, only the following keys: </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   3284</span>             <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">,</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(98,98,98)">.</span>join(outputs<span style="color: rgb(98,98,98)">.</span>keys())<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">. For reference, the inputs it received are </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">,</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(98,98,98)">.</span>join(inputs<span style="color: rgb(98,98,98)">.</span>keys())<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">.</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   3285</span>         )
<span class="ansi-green-intense-fg ansi-bold">   3286</span>     <span style="color: rgb(95,135,135)"># We don't use .loss here since the model may return tuples instead of ModelOutput.</span>
<span class="ansi-green-intense-fg ansi-bold">   3287</span>     loss <span style="color: rgb(98,98,98)">=</span> outputs[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">loss</span><span style="color: rgb(175,0,0)">"</span>] <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">isinstance</span>(outputs, <span style="color: rgb(0,135,0)">dict</span>) <span class="ansi-bold" style="color: rgb(0,135,0)">else</span> outputs[<span style="color: rgb(98,98,98)">0</span>]

<span class="ansi-red-fg">ValueError</span>: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos, nos da un error, nos dice que el modelo no devuelve el valor del loss, que es clave para poder entrenar, vamos a ver por qué</p>
</section>
<section class="section-block-markdown-cell">
<p>Primero veamos cómo es un ejemplo del dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">sample</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>{'input_ids': [50257,
  4162,
  750,
  262,
  18757,
  6451,
  2245,
  2491,
  30,
  4362,
  340,
  373,
  734,
  10032,
  13,
  220,
  50258,
  50256,
  50256,
  ...,
  50256,
  50256,
  50256],
 'attention_mask': [1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  0,
  0,
  0,
  ...,
  0,
  0,
  0]}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos, tenemos un diccionario con los <code>input_ids</code> y las <code>attention_mask</code>, si se lo pasamos al modelo obtenemos esto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>None
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos no devuelve el valor de la loss porque está esperando un valor para <code>labels</code>, que no se lo hemos pasado. En el ejemplo anterior, en el que hacíamos fine tuning para clasificación de texto, dijimos que las etiquetas había que pasarlas a un campo del dataset llamado <code>labels</code>, pero en este caso no tenemos ese campo en el dataset</p>
<p>Si ahora asignamos las <code>lables</code> a los <code>input_ids</code> y volvemos a ver la loss</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>tensor(102.1873, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora sí obtenemos una <code>loss</code></p>
<p>Por tanto tenemos dos opciones, añadir un campo <code>labels</code> al dataset, con los valores de <code>input_ids</code> o utilizar una función de la librería <code>transformers</code> llamada <code>data_collator</code>, en este caso usaremos <code>DataCollatorForLanguageModeling</code>. Vamos a verlo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForLanguageModeling</span>

<span class="n">my_data_collator</span><span class="o">=</span><span class="n">DataCollatorForLanguageModeling</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mlm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Pasamos la muestra <code>sample</code> por este <code>data_collator</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">collated_sample</span> <span class="o">=</span> <span class="n">my_data_collator</span><span class="p">([</span><span class="n">sample</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos cómo es la salida</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">collated_sample</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>input_ids (torch.Size([1, 768])): tensor([[50257,  4162,   750,   262, 18757,  6451,  2245,  2491,    30,  4362,
           340,   373,   734, 10032,    13,   220, 50258, 50256, ..., 50256, 50256]],
       device='cuda:0')
attention_mask (torch.Size([1, 768])): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ..., 0, 0]],
       device='cuda:0')
labels (torch.Size([1, 768])): tensor([[50257,  4162,   750,   262, 18757,  6451,  2245,  2491,    30,  4362,
           340,   373,   734, 10032,    13,   220, 50258,  -100,  ...,  -100,  -100]],
       device='cuda:0')
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como se puede ver, el <code>data_collator</code> ha creado un campo <code>labels</code> y le ha asignado los valores de <code>input_ids</code>. Los tokens que están enmascarados le ha asignado el valor -100. Esto es porque cuando definimos el <code>data_collator</code> le pasamos el parámetro <code>mlm=False</code>, que significa que no estamos haciendo <code>Masked Language Modeling</code>, si no <code>Language Modeling</code>, por eso no enmascara ningún token original</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver si ahora obtenemos una <code>loss</code> con este <code>data_collator</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">collated_sample</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">loss</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor(102.7181, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Así que volvemos a definir el <code>trainer</code> con el <code>data_collator</code> y volvemos a entrenar</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForLanguageModeling</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">validation_dataset</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">DataCollatorForLanguageModeling</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mlm</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="22341" style="width:300px; height:20px; vertical-align: middle;" value="22341"></progress>
      [22341/22341 2:33:28, Epoch 3/3]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>3.386600</td>
<td>3.258979</td>
</tr>
<tr>
<td>2</td>
<td>3.259900</td>
<td>3.199673</td>
</tr>
<tr>
<td>3</td>
<td>3.212600</td>
<td>3.192009</td>
</tr>
</tbody>
</table><p></p></div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>TrainOutput(global_step=22341, training_loss=3.505178199598342, metrics={'train_runtime': 9209.5353, 'train_samples_per_second': 67.916, 'train_steps_per_second': 2.426, 'total_flos': 2.45146666696704e+17, 'train_loss': 3.505178199598342, 'epoch': 3.0})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Evaluaci%C3%B3n">Evaluación<a class="anchor-link" href="#Evaluaci%C3%B3n">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Una vez entrenado evaluamos el modelo sobre el dataset de test</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="362" style="width:300px; height:20px; vertical-align: middle;" value="362"></progress>
      [362/362 01:04]
    </div>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>{'eval_loss': 3.201305866241455,
 'eval_runtime': 65.0033,
 'eval_samples_per_second': 178.191,
 'eval_steps_per_second': 5.569,
 'epoch': 3.0}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Publicar-el-modelo">Publicar el modelo<a class="anchor-link" href="#Publicar-el-modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos la model card</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Lo publicamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>events.out.tfevents.1720875425.8de3af1b431d.6946.1:   0%|          | 0.00/364 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM/commit/d107b3bb0e02076483238f9975697761015ec390', commit_message='End of training', commit_description='', oid='d107b3bb0e02076483238f9975697761015ec390', pr_url=None, pr_revision=None, pr_num=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Uso-del-modelo">Uso del modelo<a class="anchor-link" href="#Uso-del-modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Limpiamos todo lo posible</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">gc</span>


<span class="k">def</span> <span class="nf">clear_hardwares</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>


<span class="n">clear_hardwares</span><span class="p">()</span>
<span class="n">clear_hardwares</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Descargamos el modelo y el tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>
<span class="n">checkpoints</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">"right"</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Comprobamos que el tokenizador y el modelo tienen los 2 tokens extra que hemos añadido</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer_vocab</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">()</span>
<span class="n">model_vocab</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tokenizer_vocab: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer_vocab</span><span class="p">)</span><span class="si">}</span><span class="s2">. model_vocab: </span><span class="si">{</span><span class="n">model_vocab</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>tokenizer_vocab: 50259. model_vocab: 50259
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que tienen 50259 tokens, es decir, los 50257 tokens de GPT2 más los 2 que hemos añadido</p>
</section>
<section class="section-block-markdown-cell">
<p>Creamos una función para generar chistes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_joke</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"&lt;SJ&gt; </span><span class="si">{</span><span class="n">prompt_text</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"&lt;EJ&gt;"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Generamos un chiste</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">generate_joke</span><span class="p">(</span><span class="s2">"Why didn't the frog cross the road?"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>"&lt;SJ&gt; Why didn't the frog cross the road? Because he was frog-in-the-face. &lt;EJ&gt;"</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Si quires probar más el modelo puedes verlo en <a href="https://huggingface.co/Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM">Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM</a></p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Fine-tuning-para-clasificaci%C3%B3n-de-texto-con-Pytorch">Fine tuning para clasificación de texto con Pytorch<a class="anchor-link" href="#Fine-tuning-para-clasificaci%C3%B3n-de-texto-con-Pytorch">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Repetimos el entrenamiento con Pytorch</p>
</section>
<section class="section-block-markdown-cell">
<p>Reiniciamos el notebook para asegurarnos</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Descargamos el mismo dataset que cuando hicimos el entrnemiento con las librerías de Hugging Face</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"mteb/amazon_reviews_multi"</span><span class="p">,</span> <span class="s2">"en"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Creamos una variable con el número de clases</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">'label'</span><span class="p">))</span>
<span class="n">num_classes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>5</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Antes procesamos todo el dataset para crear un campo llamado <code>labels</code>, pero ahora no hace falta porque como vamos a programar nosotros todo, nos adaptamos a cómo es el dataset</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokenizador">Tokenizador<a class="anchor-link" href="#Tokenizador">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos el tokenizador. Le asignamos el token de padding para que no nos de error como antes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Creamos una función para tokenizar el dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Lo tokenizamos. Eliminamos columnas que no nos hagan falta, pero ahora dejamos la del texto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'id'</span><span class="p">,</span> <span class="s1">'label_text'</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>DatasetDict({
    train: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 5000
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percentage</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">subset_train</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>
<span class="n">percentage</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">subset_validation</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'validation'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'validation'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>
<span class="n">subset_test</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'test'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"len subset_train: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_train</span><span class="p">)</span><span class="si">}</span><span class="s2">, len subset_validation: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_validation</span><span class="p">)</span><span class="si">}</span><span class="s2">, len subset_test: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_test</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>len subset_train: 200000, len subset_validation: 5000, len subset_test: 5000
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Importamos los pesos y asignamos el token de padding</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Device">Device<a class="anchor-link" href="#Device">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos el dispositivo donde se va a ejecutar todo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">'cpu'</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>De paso pasamos el modelo al dispositivo y de paso lo pasamos a FP16 para que ocupe menos memoria</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pytorch-Dataset">Pytorch Dataset<a class="anchor-link" href="#Pytorch-Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos un dataset de pytorch</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">ReviewsDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">huggingface_dataset</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">huggingface_dataset</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'label'</span><span class="p">]</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'input_ids'</span><span class="p">])</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'attention_mask'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">label</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Instanciamos los datasets</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ReviewsDataset</span><span class="p">(</span><span class="n">subset_train</span><span class="p">)</span>
<span class="n">validatation_dataset</span> <span class="o">=</span> <span class="n">ReviewsDataset</span><span class="p">(</span><span class="n">subset_validation</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">ReviewsDataset</span><span class="p">(</span><span class="n">subset_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver una muestra</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>(torch.Size([768]), torch.Size([768]), 0)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pytorch-Dataloader">Pytorch Dataloader<a class="anchor-link" href="#Pytorch-Dataloader">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos ahora un dataloader de pytorch</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">BS</span> <span class="o">=</span> <span class="mi">12</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">validation_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">validatation_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver una muestra</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>(torch.Size([12, 768]),
 torch.Size([12, 768]),
 tensor([2, 1, 2, 0, 3, 3, 0, 4, 3, 3, 4, 2]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Para ver que está todo bien pasamos la muestra al modelo para ver qué sale todo bien. Primero pasamos los tokens al dispositivo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">at_mask</span> <span class="o">=</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora se los pasamos al modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>odict_keys(['loss', 'logits', 'past_key_values'])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos nos da la loss y los logits</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor(5.9414, device='cuda:0', dtype=torch.float16,
       grad_fn=&lt;NllLossBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor([[ 6.1953e+00, -1.2275e+00, -2.4824e+00,  5.8867e+00, -1.4734e+01],
        [ 5.4062e+00, -8.4570e-01, -2.3203e+00,  5.1055e+00, -1.1555e+01],
        [ 6.1641e+00, -9.3066e-01, -2.5664e+00,  6.0039e+00, -1.4570e+01],
        [ 5.2266e+00, -4.2358e-01, -2.0801e+00,  4.7461e+00, -1.1570e+01],
        [ 3.8184e+00, -2.3460e-03, -1.7666e+00,  3.4160e+00, -7.7969e+00],
        [ 4.1641e+00, -4.8169e-01, -1.6914e+00,  3.9941e+00, -8.7734e+00],
        [ 4.6758e+00, -3.0298e-01, -2.1641e+00,  4.1055e+00, -9.3359e+00],
        [ 4.1953e+00, -3.2471e-01, -2.1875e+00,  3.9375e+00, -8.3438e+00],
        [-1.1650e+00,  1.3564e+00, -6.2158e-01, -6.8115e-01,  4.8672e+00],
        [ 4.4961e+00, -8.7891e-02, -2.2793e+00,  4.2812e+00, -9.3359e+00],
        [ 4.9336e+00, -2.6627e-03, -2.1543e+00,  4.3711e+00, -1.0742e+01],
        [ 5.9727e+00, -4.3152e-02, -1.4551e+00,  4.3438e+00, -1.2117e+01]],
       device='cuda:0', dtype=torch.float16, grad_fn=&lt;IndexBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="M%C3%A9trica">Métrica<a class="anchor-link" href="#M%C3%A9trica">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a crear una función para obtener la métrica, que en este cáso va a ser el accuracy</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predicted_labels</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="n">percent</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">percent</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predicted_labels</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">correct</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver si lo calcula bien</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">compute_accuracy</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>0.1666666716337204</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Optimizador">Optimizador<a class="anchor-link" href="#Optimizador">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Como vamos a necesitar un optimizador, creamos uno</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span>

<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Entrenamiento">Entrenamiento<a class="anchor-link" href="#Entrenamiento">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos el bucle de entrenamiento</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">progresbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">'Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">progresbar</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">at_mask</span> <span class="o">=</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'train_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>
    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'train_loss'</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">})</span>

    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">valid_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">progresbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">'Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">progresbar</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">at_mask</span> <span class="o">=</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
        <span class="n">valid_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">step_accuracy</span> <span class="o">=</span> <span class="n">compute_accuracy</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">+=</span> <span class="n">step_accuracy</span>
        <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'valid_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">step_accuracy</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>

    <span class="n">valid_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">)</span>
    <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'valid_loss'</span><span class="p">:</span> <span class="n">valid_loss</span><span class="p">,</span> <span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">})</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Epoch 1: 100%|██████████| 16667/16667 [44:13&lt;00:00,  6.28it/s, train_loss=nan]
Epoch 1: 100%|██████████| 417/417 [00:32&lt;00:00, 12.72it/s, valid_loss=nan, accuracy=0]
Epoch 2: 100%|██████████| 16667/16667 [44:06&lt;00:00,  6.30it/s, train_loss=nan]
Epoch 2: 100%|██████████| 417/417 [00:32&lt;00:00, 12.77it/s, valid_loss=nan, accuracy=0]
Epoch 3: 100%|██████████| 16667/16667 [44:03&lt;00:00,  6.30it/s, train_loss=nan]
Epoch 3: 100%|██████████| 417/417 [00:32&lt;00:00, 12.86it/s, valid_loss=nan, accuracy=0]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Uso-del-modelo">Uso del modelo<a class="anchor-link" href="#Uso-del-modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a probar el modelo que hemos entrenado</p>
</section>
<section class="section-block-markdown-cell">
<p>Primero tokenizamos un texto</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">({</span><span class="s2">"text"</span><span class="p">:</span> <span class="s2">"I love this product. It is amazing."</span><span class="p">})</span>
<span class="n">input_tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">input_tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>(torch.Size([1, 768]), torch.Size([1, 768]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora se lo pasamos al modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor([[nan, nan, nan, nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=&lt;IndexBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos las predicciones de esos logits</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">predicted_labels</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">])</span>
<span class="n">predicted</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor([0], device='cuda:0')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Fine-tuning-para-generaci%C3%B3n-de-texto-con-Pytorch">Fine tuning para generación de texto con Pytorch<a class="anchor-link" href="#Fine-tuning-para-generaci%C3%B3n-de-texto-con-Pytorch">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Repetimos el entrenamiento con Pytorch</p>
</section>
<section class="section-block-markdown-cell">
<p>Reiniciamos el notebook para asegurarnos</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Volvemos a descargar el dataset de chistes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"Maximofn/short-jokes-dataset"</span><span class="p">)</span>
<span class="n">jokes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[1]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['ID', 'Joke'],
        num_rows: 231657
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Creamos un subset por si se tiene poca memoria</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percent_of_train_dataset</span> <span class="o">=</span> <span class="mi">1</span>    <span class="c1"># If you want 50% of the dataset, set this to 0.5</span>

<span class="n">subset_dataset</span> <span class="o">=</span> <span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span> <span class="o">*</span> <span class="n">percent_of_train_dataset</span><span class="p">)))</span>
<span class="n">subset_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>Dataset({
    features: ['ID', 'Joke'],
    num_rows: 231657
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Dividimos el dataset en subsets de entrenamiento, validación y test</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percent_of_train_dataset</span> <span class="o">=</span> <span class="mf">0.90</span>

<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">subset_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">subset_dataset</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">*</span> <span class="n">percent_of_train_dataset</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">validation_test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>

<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">validation_test_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">validation_test_dataset</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of the train set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">. Size of the validation set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">. Size of the test set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Size of the train set: 208491. Size of the validation set: 11583. Size of the test set: 11583
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokenizador">Tokenizador<a class="anchor-link" href="#Tokenizador">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Iniciamos el tokenizador y asignamos al token de padding el de end of string</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">"right"</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Añadimos los tokens especiales de inicio de chiste y fin de chiste</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'&lt;SJ&gt;'</span><span class="p">,</span> <span class="s1">'&lt;EJ&gt;'</span><span class="p">]</span>   <span class="c1"># Start and end of joke tokens</span>

<span class="n">num_added_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Added </span><span class="si">{</span><span class="n">num_added_tokens</span><span class="si">}</span><span class="s2"> tokens"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Added 2 tokens
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Los añadimos al dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">joke_column</span> <span class="o">=</span> <span class="s2">"Joke"</span>

<span class="k">def</span> <span class="nf">format_joke</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">example</span><span class="p">[</span><span class="n">joke_column</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'&lt;SJ&gt; '</span> <span class="o">+</span> <span class="n">example</span><span class="p">[</span><span class="s1">'Joke'</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' &lt;EJ&gt;'</span>
    <span class="k">return</span> <span class="n">example</span>

<span class="n">remove_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">column_names</span> <span class="k">if</span> <span class="n">column</span> <span class="o">!=</span> <span class="n">joke_column</span><span class="p">]</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">test_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[6]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(Dataset({
     features: ['Joke'],
     num_rows: 208491
 }),
 Dataset({
     features: ['Joke'],
     num_rows: 11583
 }),
 Dataset({
     features: ['Joke'],
     num_rows: 11583
 }))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Tokenizamos el dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="n">joke_column</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">test_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[7]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 208491
 }),
 Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 11583
 }),
 Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 11583
 }))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Modelo">Modelo<a class="anchor-link" href="#Modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Instanciamos el modelo, asignamos el token de padding y añadimos los nuevos tokens de inicion de chiste y fin de chiste</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>Embedding(50259, 768)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Device">Device<a class="anchor-link" href="#Device">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos el dispositivo y pasamos el modelo al dispositivo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">'cpu'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pytorch-Dataset">Pytorch Dataset<a class="anchor-link" href="#Pytorch-Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos un dataset de pytorch</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">huggingface_dataset</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">huggingface_dataset</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'input_ids'</span><span class="p">])</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'attention_mask'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Instanciamos los datasets de entrenamiento, validación y test</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">train_pytorch_dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="n">validation_pytorch_dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">validation_dataset</span><span class="p">)</span>
<span class="n">test_pytorch_dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Veamos una muestra</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">train_pytorch_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[12]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(torch.Size([768]), torch.Size([768]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pytorch-Dataloader">Pytorch Dataloader<a class="anchor-link" href="#Pytorch-Dataloader">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos los dataloaders</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">BS</span> <span class="o">=</span> <span class="mi">28</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_pytorch_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">validation_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">validation_pytorch_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_pytorch_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos una muestra</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(torch.Size([28, 768]), torch.Size([28, 768]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Se lo pasamos al modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>odict_keys(['logits', 'past_key_values'])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos no tenemos valor de <code>loss</code>, como hemos visto tenemos que pasarle el <code>input_ids</code> y el <code>labels</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[16]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>odict_keys(['loss', 'logits', 'past_key_values'])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ahora sí tenemos <code>loss</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[17]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>80.5625</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Optimizador">Optimizador<a class="anchor-link" href="#Optimizador">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos un optimizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span>

<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Entrenamiento">Entrenamiento<a class="anchor-link" href="#Entrenamiento">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Creamos el bucle de entrenamiento</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">progresbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">'Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span> <span class="ow">in</span> <span class="n">progresbar</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">at_mask</span> <span class="o">=</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'train_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>
    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'train_loss'</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">})</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Epoch 1: 100%|██████████| 7447/7447 [51:07&lt;00:00,  2.43it/s, train_loss=nan]
Epoch 2: 100%|██████████| 7447/7447 [51:06&lt;00:00,  2.43it/s, train_loss=nan]
Epoch 3: 100%|██████████| 7447/7447 [51:07&lt;00:00,  2.43it/s, train_loss=nan]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Uso-del-modelo">Uso del modelo<a class="anchor-link" href="#Uso-del-modelo">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Probamos el modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_text</span><span class="p">(</span><span class="n">decoded_joke</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stop_token</span><span class="o">=</span><span class="s1">'&lt;EJ&gt;'</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">({</span><span class="s1">'Joke'</span><span class="p">:</span> <span class="n">decoded_joke</span><span class="p">})</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">nex_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">nex_token_decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">nex_token</span><span class="p">)</span>
    <span class="n">decoded_joke</span> <span class="o">=</span> <span class="n">decoded_joke</span> <span class="o">+</span> <span class="n">nex_token_decoded</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="n">nex_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">nex_token_decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">nex_token</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">nex_token_decoded</span> <span class="o">==</span> <span class="n">stop_token</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">decoded_joke</span> <span class="o">=</span> <span class="n">decoded_joke</span> <span class="o">+</span> <span class="n">nex_token_decoded</span>
        <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">({</span><span class="s1">'Joke'</span><span class="p">:</span> <span class="n">decoded_joke</span><span class="p">})</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">decoded_joke</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">generated_text</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="s2">"&lt;SJ&gt; Why didn't the frog cross the road"</span><span class="p">)</span>
<span class="n">generated_text</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>"&lt;SJ&gt; Why didn't the frog cross the road!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"</pre>
</div>
</div>
</div>
</section>
