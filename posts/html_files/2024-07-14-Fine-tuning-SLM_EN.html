<section class="section-block-markdown-cell">
<h1 id="Fine-tuning-SMLs-with-Hugging-Face">Fine tuning SMLs with Hugging Face<a class="anchor-link" href="#Fine-tuning-SMLs-with-Hugging-Face">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>In this post we are going to see how to fine tune small language models, we are going to see how to fine tune text classification and text generation. First we are going to see how to do it with the Hugging Face libraries, since Hugging Face has become a very important player in the AI ecosystem right now.</p>
<p>But although Hugging Face libraries are very important and useful, it is very important to know how the training is actually done and what is going on underneath, so let's repeat the training for classification and text generation but with Pytorch</p>
</section>
<section class="section-block-markdown-cell">
<p>This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
<h2 id="Fine-tuning-for-text-classification-with-Hugging-Face">Fine tuning for text classification with Hugging Face<a class="anchor-link" href="#Fine-tuning-for-text-classification-with-Hugging-Face">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h3 id="Login">Login<a class="anchor-link" href="#Login">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>To be able to upload the training result to the hub we must first log in, for this we need a token</p>
<p>To create a token, go to the <a href="https://huggingface.co/settings/tokens">setings/tokens</a> page of your account, you will see something like this</p>
<p><img alt="User-Access-Token-dark" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/User-Access-Token-dark.webp"/></p>
<p>Click on <code>New token</code> and a window will appear to create a new token.</p>
<p><img alt="new-token-dark" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/new-token-dark.webp"/></p>
<p>We name the token and create it with the <code>write</code> role, or with the <code>Fine-grained</code> role, which allows us to select exactly what permissions the token will have.</p>
<p>Once created, we copy and paste it below</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now we download a dataset, in this case we are going to download one of reviews from <a href="https://huggingface.co/datasets/mteb/amazon_reviews_multi">Amazon</a></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"mteb/amazon_reviews_multi"</span><span class="p">,</span> <span class="s2">"en"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's take a look at it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['id', 'text', 'label', 'label_text'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['id', 'text', 'label', 'label_text'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'text', 'label', 'label_text'],
        num_rows: 5000
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see that you have a training set with 200,000 samples, a validation set with 5,000 samples and a test set with 5,000 samples.</p>
</section>
<section class="section-block-markdown-cell">
<p>Let's see an example of the training set</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'id': 'en_0907914',
 'text': 'Mixed with fir it’s passable\n\nNot the scent I had hoped for . Love the scent of cedar, but this one missed',
 'label': 3,
 'label_text': '3'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see that it has the review in the <code>text</code> field and the score given by the user in the <code>label</code> field.</p>
</section>
<section class="section-block-markdown-cell">
<p>As we are going to make a text classification model, we need to know how many classes we are going to have</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">'label'</span><span class="p">))</span>
<span class="n">num_classes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>5</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We are going to have 5 classes, now we are going to see the minimum value of these classes to know if the score starts at 0 or 1. For this we use the <code>unique</code> method</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">'label'</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[5]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'train': [0, 1, 2, 3, 4],
 'validation': [0, 1, 2, 3, 4],
 'test': [0, 1, 2, 3, 4]}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>The minimum value will be 0</p>
</section>
<section class="section-block-markdown-cell">
<p>To train, the labels have to be in a field called <code>labels</code>, while in our dataset it is in a field called <code>label</code>, so we create the new field <code>lables</code> with the same value as <code>label</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>We create a function that does what we want</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">set_labels</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">example</span><span class="p">[</span><span class="s1">'labels'</span><span class="p">]</span> <span class="o">=</span> <span class="n">example</span><span class="p">[</span><span class="s1">'label'</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">example</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We apply the function to the dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">set_labels</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see how the dataset looks like</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'id': 'en_0907914',
 'text': 'Mixed with fir it’s passable\n\nNot the scent I had hoped for . Love the scent of cedar, but this one missed',
 'label': 3,
 'label_text': '3',
 'labels': 3}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>As in the dataset we have the reviews in text, we need to tokenize them in order to put the tokens into the model.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we create a function to tokenize the text. We are going to make it so that all sentences have the same length, so that the tokenizer will truncate when necessary and add padding tokens when necessary. We also tell it to return pytorch tensors.</p>
</section>
<section class="section-block-markdown-cell">
<p>We make the length of each statement 768 tokens because we are using the small GPT2 model, which as we saw in the <a href="https://maximofn.com/gpt2/#Arquitectura">GPT2</a> post has an embedding dimension of 768 tokens.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's try tokenizing a text</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-text-output-error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[11], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> tokens <span style="color: rgb(98,98,98)">=</span> tokenize_function(dataset[<span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">train</span><span style="color: rgb(175,0,0)">'</span>][idx])

Cell <span class="ansi-green-fg">In[10], line 2</span>, in <span class="ansi-cyan-fg">tokenize_function</span><span class="ansi-blue-fg">(examples)</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">tokenize_function</span>(examples):
<span class="ansi-green-fg">----&gt; 2</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> tokenizer(examples[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">text</span><span style="color: rgb(175,0,0)">"</span>], padding<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">max_length</span><span style="color: rgb(175,0,0)">"</span>, truncation<span style="color: rgb(98,98,98)">=</span><span class="ansi-bold" style="color: rgb(0,135,0)">True</span>, max_length<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(98,98,98)">768</span>, return_tensors<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">pt</span><span style="color: rgb(175,0,0)">"</span>)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2883</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.__call__</span><span class="ansi-blue-fg">(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2881</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_in_target_context_manager:
<span class="ansi-green-intense-fg ansi-bold">   2882</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_input_mode()
<span class="ansi-green-fg">-&gt; 2883</span>     encodings <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_call_one(text<span style="color: rgb(98,98,98)">=</span>text, text_pair<span style="color: rgb(98,98,98)">=</span>text_pair, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>all_kwargs)
<span class="ansi-green-intense-fg ansi-bold">   2884</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> text_target <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:
<span class="ansi-green-intense-fg ansi-bold">   2885</span>     <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_switch_to_target_mode()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2989</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._call_one</span><span class="ansi-blue-fg">(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2969</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>batch_encode_plus(
<span class="ansi-green-intense-fg ansi-bold">   2970</span>         batch_text_or_text_pairs<span style="color: rgb(98,98,98)">=</span>batch_text_or_text_pairs,
<span class="ansi-green-intense-fg ansi-bold">   2971</span>         add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   2986</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   2987</span>     )
<span class="ansi-green-intense-fg ansi-bold">   2988</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; 2989</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>encode_plus(
<span class="ansi-green-intense-fg ansi-bold">   2990</span>         text<span style="color: rgb(98,98,98)">=</span>text,
<span class="ansi-green-intense-fg ansi-bold">   2991</span>         text_pair<span style="color: rgb(98,98,98)">=</span>text_pair,
<span class="ansi-green-intense-fg ansi-bold">   2992</span>         add_special_tokens<span style="color: rgb(98,98,98)">=</span>add_special_tokens,
<span class="ansi-green-intense-fg ansi-bold">   2993</span>         padding<span style="color: rgb(98,98,98)">=</span>padding,
<span class="ansi-green-intense-fg ansi-bold">   2994</span>         truncation<span style="color: rgb(98,98,98)">=</span>truncation,
<span class="ansi-green-intense-fg ansi-bold">   2995</span>         max_length<span style="color: rgb(98,98,98)">=</span>max_length,
<span class="ansi-green-intense-fg ansi-bold">   2996</span>         stride<span style="color: rgb(98,98,98)">=</span>stride,
<span class="ansi-green-intense-fg ansi-bold">   2997</span>         is_split_into_words<span style="color: rgb(98,98,98)">=</span>is_split_into_words,
<span class="ansi-green-intense-fg ansi-bold">   2998</span>         pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,
<span class="ansi-green-intense-fg ansi-bold">   2999</span>         return_tensors<span style="color: rgb(98,98,98)">=</span>return_tensors,
<span class="ansi-green-intense-fg ansi-bold">   3000</span>         return_token_type_ids<span style="color: rgb(98,98,98)">=</span>return_token_type_ids,
<span class="ansi-green-intense-fg ansi-bold">   3001</span>         return_attention_mask<span style="color: rgb(98,98,98)">=</span>return_attention_mask,
<span class="ansi-green-intense-fg ansi-bold">   3002</span>         return_overflowing_tokens<span style="color: rgb(98,98,98)">=</span>return_overflowing_tokens,
<span class="ansi-green-intense-fg ansi-bold">   3003</span>         return_special_tokens_mask<span style="color: rgb(98,98,98)">=</span>return_special_tokens_mask,
<span class="ansi-green-intense-fg ansi-bold">   3004</span>         return_offsets_mapping<span style="color: rgb(98,98,98)">=</span>return_offsets_mapping,
<span class="ansi-green-intense-fg ansi-bold">   3005</span>         return_length<span style="color: rgb(98,98,98)">=</span>return_length,
<span class="ansi-green-intense-fg ansi-bold">   3006</span>         verbose<span style="color: rgb(98,98,98)">=</span>verbose,
<span class="ansi-green-intense-fg ansi-bold">   3007</span>         <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   3008</span>     )

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3053</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.encode_plus</span><span class="ansi-blue-fg">(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   3032</span> <span style="color: rgb(175,0,0)">"""</span>
<span class="ansi-green-intense-fg ansi-bold">   3033</span> <span style="color: rgb(175,0,0)">Tokenize and prepare for the model a sequence or a pair of sequences.</span>
<span class="ansi-green-intense-fg ansi-bold">   3034</span> 
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   3049</span> <span style="color: rgb(175,0,0)">        method).</span>
<span class="ansi-green-intense-fg ansi-bold">   3050</span> <span style="color: rgb(175,0,0)">"""</span>
<span class="ansi-green-intense-fg ansi-bold">   3052</span> <span style="color: rgb(95,135,135)"># Backward compatibility for 'truncation_strategy', 'pad_to_max_length'</span>
<span class="ansi-green-fg">-&gt; 3053</span> padding_strategy, truncation_strategy, max_length, kwargs <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_get_padding_truncation_strategies(
<span class="ansi-green-intense-fg ansi-bold">   3054</span>     padding<span style="color: rgb(98,98,98)">=</span>padding,
<span class="ansi-green-intense-fg ansi-bold">   3055</span>     truncation<span style="color: rgb(98,98,98)">=</span>truncation,
<span class="ansi-green-intense-fg ansi-bold">   3056</span>     max_length<span style="color: rgb(98,98,98)">=</span>max_length,
<span class="ansi-green-intense-fg ansi-bold">   3057</span>     pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span>pad_to_multiple_of,
<span class="ansi-green-intense-fg ansi-bold">   3058</span>     verbose<span style="color: rgb(98,98,98)">=</span>verbose,
<span class="ansi-green-intense-fg ansi-bold">   3059</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   3060</span> )
<span class="ansi-green-intense-fg ansi-bold">   3062</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_encode_plus(
<span class="ansi-green-intense-fg ansi-bold">   3063</span>     text<span style="color: rgb(98,98,98)">=</span>text,
<span class="ansi-green-intense-fg ansi-bold">   3064</span>     text_pair<span style="color: rgb(98,98,98)">=</span>text_pair,
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   3080</span>     <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>kwargs,
<span class="ansi-green-intense-fg ansi-bold">   3081</span> )

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2788</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase._get_padding_truncation_strategies</span><span class="ansi-blue-fg">(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   2786</span> <span style="color: rgb(95,135,135)"># Test if we have a padding token</span>
<span class="ansi-green-intense-fg ansi-bold">   2787</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (<span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_token_id <span style="color: rgb(98,98,98)">&lt;</span> <span style="color: rgb(98,98,98)">0</span>):
<span class="ansi-green-fg">-&gt; 2788</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-intense-fg ansi-bold">   2789</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Asking to pad but the tokenizer does not have a padding token. </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2790</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2791</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">or add a new pad token via `tokenizer.add_special_tokens(</span><span style="color: rgb(175,0,0)">{</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">pad_token</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">: </span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">[PAD]</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">})`.</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   2792</span>     )
<span class="ansi-green-intense-fg ansi-bold">   2794</span> <span style="color: rgb(95,135,135)"># Check that we will truncate to a multiple of pad_to_multiple_of if both are provided</span>
<span class="ansi-green-intense-fg ansi-bold">   2795</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> (
<span class="ansi-green-intense-fg ansi-bold">   2796</span>     truncation_strategy <span style="color: rgb(98,98,98)">!=</span> TruncationStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_TRUNCATE
<span class="ansi-green-intense-fg ansi-bold">   2797</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> padding_strategy <span style="color: rgb(98,98,98)">!=</span> PaddingStrategy<span style="color: rgb(98,98,98)">.</span>DO_NOT_PAD
<span class="ansi-green-fg">   (...)</span>
<span class="ansi-green-intense-fg ansi-bold">   2800</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (max_length <span style="color: rgb(98,98,98)">%</span> pad_to_multiple_of <span style="color: rgb(98,98,98)">!=</span> <span style="color: rgb(98,98,98)">0</span>)
<span class="ansi-green-intense-fg ansi-bold">   2801</span> ):

<span class="ansi-red-fg">ValueError</span>: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We get an error because the GPT2 tokenizer does not have a token for padding and asks us to assign one, it also suggests us to do <code>tokenizer.pad_token = tokenizer.eos_token</code>, so we do it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We test again the tokenization function</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">])</span>
<span class="n">tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[100]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(torch.Size([1, 768]), torch.Size([1, 768]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have checked that the function tokenizes well, we apply this function to the dataset, but we also apply it in batches so that it runs faster</p>
<p>We also take advantage of this and eliminate the columns we do not need</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'text'</span><span class="p">,</span> <span class="s1">'label'</span><span class="p">,</span> <span class="s1">'id'</span><span class="p">,</span> <span class="s1">'label_text'</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now let's see how the dataset looks like</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[12]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['id', 'text', 'label', 'label_text', 'labels'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['id', 'text', 'label', 'label_text', 'labels'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['id', 'text', 'label', 'label_text', 'labels'],
        num_rows: 5000
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We can see that we have the fields 'labels', 'input_ids' and 'attention_mask', which is what we are interested in training</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Model">Model<a class="anchor-link" href="#Model">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We instantiate a model for sequence classification and we indicate the number of classes we have</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>It tells us that the weights of the <code>score</code> layer have been initialized randomly and that we have to retrain them, let's see why this happens</p>
</section>
<section class="section-block-markdown-cell">
<p>The GPT2 model would look like this</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">casual_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>While the GPT2 model for generating text is as follows</p>
</section>
<section class="section-block-markdown-cell">
<p>Let's take a look at its architecture</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">casual_model</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>And now the architecture of the model we are going to use to classify the reviews</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[16]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>GPT2ForSequenceClassification(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (score): Linear(in_features=768, out_features=5, bias=False)
)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>There are two things to mention here</p>
<ul>
<li>The first is that in both, the first layer has dimensions of 50257x768, which corresponds to 50257 possible tokens of the GPT2 vocabulary and 768 dimensions of the embedding, so we have done well to tokenize the reviews with a size of 768 tokens.</li>
<li>The second is that the <code>casual</code> model (the text generation model) has at the end a <code>Linear</code> layer that generates 50257 values, that is, it is in charge of predicting the next token and gives a value to each possible token. While the classification model has a <code>Linear</code> layer that only generates 5 values, one for each class, which will give us the probability that the review belongs to each class.</li>
</ul>
<p>That is why we got the message that the weights of the <code>score</code> layer had been initialized randomly, because the transformers library has removed the <code>Linear</code> layer of 768x50257 and added a <code>Linear</code> layer of 768x5, it has initialized it with random values and we have to train it for our particular problem.</p>
</section>
<section class="section-block-markdown-cell">
<p>We delete the casual model, because we are not going to use it.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">del</span> <span class="n">casual_model</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Trainer">Trainer<a class="anchor-link" href="#Trainer">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Let's now configure the training arguments</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-finetuned-amazon-reviews-en-classification"</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>
<span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">28</span>
<span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>
    <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Define a metric for the validation dataloader</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">evaluate</span> <span class="kn">import</span> <span class="n">load</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s2">"accuracy"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">)</span>
    <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We now define the trainer</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'validation'</span><span class="p">],</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We train</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>  0%|          | 0/600000 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-text-output-error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[21], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> trainer<span style="color: rgb(98,98,98)">.</span>train()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:1876</span>, in <span class="ansi-cyan-fg">Trainer.train</span><span class="ansi-blue-fg">(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1873</span> <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:
<span class="ansi-green-intense-fg ansi-bold">   1874</span>     <span style="color: rgb(95,135,135)"># Disable progress bars when uploading models during checkpoints to avoid polluting stdout</span>
<span class="ansi-green-intense-fg ansi-bold">   1875</span>     hf_hub_utils<span style="color: rgb(98,98,98)">.</span>disable_progress_bars()
<span class="ansi-green-fg">-&gt; 1876</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> inner_training_loop(
<span class="ansi-green-intense-fg ansi-bold">   1877</span>         args<span style="color: rgb(98,98,98)">=</span>args,
<span class="ansi-green-intense-fg ansi-bold">   1878</span>         resume_from_checkpoint<span style="color: rgb(98,98,98)">=</span>resume_from_checkpoint,
<span class="ansi-green-intense-fg ansi-bold">   1879</span>         trial<span style="color: rgb(98,98,98)">=</span>trial,
<span class="ansi-green-intense-fg ansi-bold">   1880</span>         ignore_keys_for_eval<span style="color: rgb(98,98,98)">=</span>ignore_keys_for_eval,
<span class="ansi-green-intense-fg ansi-bold">   1881</span>     )
<span class="ansi-green-intense-fg ansi-bold">   1882</span> <span class="ansi-bold" style="color: rgb(0,135,0)">finally</span>:
<span class="ansi-green-intense-fg ansi-bold">   1883</span>     hf_hub_utils<span style="color: rgb(98,98,98)">.</span>enable_progress_bars()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:2178</span>, in <span class="ansi-cyan-fg">Trainer._inner_training_loop</span><span class="ansi-blue-fg">(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)</span>
<span class="ansi-green-intense-fg ansi-bold">   2175</span>     rng_to_sync <span style="color: rgb(98,98,98)">=</span> <span class="ansi-bold" style="color: rgb(0,135,0)">True</span>
<span class="ansi-green-intense-fg ansi-bold">   2177</span> step <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">1</span>
<span class="ansi-green-fg">-&gt; 2178</span> <span class="ansi-bold" style="color: rgb(0,135,0)">for</span> step, inputs <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> <span style="color: rgb(0,135,0)">enumerate</span>(epoch_iterator):
<span class="ansi-green-intense-fg ansi-bold">   2179</span>     total_batched_samples <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">1</span>
<span class="ansi-green-intense-fg ansi-bold">   2181</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>args<span style="color: rgb(98,98,98)">.</span>include_num_input_tokens_seen:

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/accelerate/data_loader.py:454</span>, in <span class="ansi-cyan-fg">DataLoaderShard.__iter__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    452</span> <span style="color: rgb(95,135,135)"># We iterate one batch ahead to check when we are at the end</span>
<span class="ansi-green-intense-fg ansi-bold">    453</span> <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">--&gt; 454</span>     current_batch <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">next</span>(dataloader_iter)
<span class="ansi-green-intense-fg ansi-bold">    455</span> <span class="ansi-bold" style="color: rgb(0,135,0)">except</span> <span class="ansi-bold" style="color: rgb(215,95,95)">StopIteration</span>:
<span class="ansi-green-intense-fg ansi-bold">    456</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">yield</span>

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631</span>, in <span class="ansi-cyan-fg">_BaseDataLoaderIter.__next__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    628</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_sampler_iter <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span>:
<span class="ansi-green-intense-fg ansi-bold">    629</span>     <span style="color: rgb(95,135,135)"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>
<span class="ansi-green-intense-fg ansi-bold">    630</span>     <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_reset()  <span style="color: rgb(95,135,135)"># type: ignore[call-arg]</span>
<span class="ansi-green-fg">--&gt; 631</span> data <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_next_data()
<span class="ansi-green-intense-fg ansi-bold">    632</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_num_yielded <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(98,98,98)">1</span>
<span class="ansi-green-intense-fg ansi-bold">    633</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_dataset_kind <span style="color: rgb(98,98,98)">==</span> _DatasetKind<span style="color: rgb(98,98,98)">.</span>Iterable <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> \
<span class="ansi-green-intense-fg ansi-bold">    634</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_IterableDataset_len_called <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> \
<span class="ansi-green-intense-fg ansi-bold">    635</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_num_yielded <span style="color: rgb(98,98,98)">&gt;</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_IterableDataset_len_called:

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675</span>, in <span class="ansi-cyan-fg">_SingleProcessDataLoaderIter._next_data</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    673</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">_next_data</span>(<span style="color: rgb(0,135,0)">self</span>):
<span class="ansi-green-intense-fg ansi-bold">    674</span>     index <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_next_index()  <span style="color: rgb(95,135,135)"># may raise StopIteration</span>
<span class="ansi-green-fg">--&gt; 675</span>     data <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_dataset_fetcher<span style="color: rgb(98,98,98)">.</span>fetch(index)  <span style="color: rgb(95,135,135)"># may raise StopIteration</span>
<span class="ansi-green-intense-fg ansi-bold">    676</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_pin_memory:
<span class="ansi-green-intense-fg ansi-bold">    677</span>         data <span style="color: rgb(98,98,98)">=</span> _utils<span style="color: rgb(98,98,98)">.</span>pin_memory<span style="color: rgb(98,98,98)">.</span>pin_memory(data, <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_pin_memory_device)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54</span>, in <span class="ansi-cyan-fg">_MapDatasetFetcher.fetch</span><span class="ansi-blue-fg">(self, possibly_batched_index)</span>
<span class="ansi-green-intense-fg ansi-bold">     52</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-intense-fg ansi-bold">     53</span>     data <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>dataset[possibly_batched_index]
<span class="ansi-green-fg">---&gt; 54</span> <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>collate_fn(data)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/data/data_collator.py:271</span>, in <span class="ansi-cyan-fg">DataCollatorWithPadding.__call__</span><span class="ansi-blue-fg">(self, features)</span>
<span class="ansi-green-intense-fg ansi-bold">    270</span> <span class="ansi-bold" style="color: rgb(0,135,0)">def</span> <span style="color: rgb(0,0,255)">__call__</span>(<span style="color: rgb(0,135,0)">self</span>, features: List[Dict[<span style="color: rgb(0,135,0)">str</span>, Any]]) <span style="color: rgb(98,98,98)">-</span><span style="color: rgb(98,98,98)">&gt;</span> Dict[<span style="color: rgb(0,135,0)">str</span>, Any]:
<span class="ansi-green-fg">--&gt; 271</span>     batch <span style="color: rgb(98,98,98)">=</span> pad_without_fast_tokenizer_warning(
<span class="ansi-green-intense-fg ansi-bold">    272</span>         <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>tokenizer,
<span class="ansi-green-intense-fg ansi-bold">    273</span>         features,
<span class="ansi-green-intense-fg ansi-bold">    274</span>         padding<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>padding,
<span class="ansi-green-intense-fg ansi-bold">    275</span>         max_length<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>max_length,
<span class="ansi-green-intense-fg ansi-bold">    276</span>         pad_to_multiple_of<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>pad_to_multiple_of,
<span class="ansi-green-intense-fg ansi-bold">    277</span>         return_tensors<span style="color: rgb(98,98,98)">=</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>return_tensors,
<span class="ansi-green-intense-fg ansi-bold">    278</span>     )
<span class="ansi-green-intense-fg ansi-bold">    279</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">label</span><span style="color: rgb(175,0,0)">"</span> <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> batch:
<span class="ansi-green-intense-fg ansi-bold">    280</span>         batch[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">labels</span><span style="color: rgb(175,0,0)">"</span>] <span style="color: rgb(98,98,98)">=</span> batch[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">label</span><span style="color: rgb(175,0,0)">"</span>]

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/data/data_collator.py:66</span>, in <span class="ansi-cyan-fg">pad_without_fast_tokenizer_warning</span><span class="ansi-blue-fg">(tokenizer, *pad_args, **pad_kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">     63</span> tokenizer<span style="color: rgb(98,98,98)">.</span>deprecation_warnings[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Asking-to-pad-a-fast-tokenizer</span><span style="color: rgb(175,0,0)">"</span>] <span style="color: rgb(98,98,98)">=</span> <span class="ansi-bold" style="color: rgb(0,135,0)">True</span>
<span class="ansi-green-intense-fg ansi-bold">     65</span> <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">---&gt; 66</span>     padded <span style="color: rgb(98,98,98)">=</span> tokenizer<span style="color: rgb(98,98,98)">.</span>pad(<span style="color: rgb(98,98,98)">*</span>pad_args, <span style="color: rgb(98,98,98)">*</span><span style="color: rgb(98,98,98)">*</span>pad_kwargs)
<span class="ansi-green-intense-fg ansi-bold">     67</span> <span class="ansi-bold" style="color: rgb(0,135,0)">finally</span>:
<span class="ansi-green-intense-fg ansi-bold">     68</span>     <span style="color: rgb(95,135,135)"># Restore the state of the warning.</span>
<span class="ansi-green-intense-fg ansi-bold">     69</span>     tokenizer<span style="color: rgb(98,98,98)">.</span>deprecation_warnings[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">Asking-to-pad-a-fast-tokenizer</span><span style="color: rgb(175,0,0)">"</span>] <span style="color: rgb(98,98,98)">=</span> warning_state

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3299</span>, in <span class="ansi-cyan-fg">PreTrainedTokenizerBase.pad</span><span class="ansi-blue-fg">(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)</span>
<span class="ansi-green-intense-fg ansi-bold">   3297</span> <span style="color: rgb(95,135,135)"># The model's main input name, usually `input_ids`, has be passed for padding</span>
<span class="ansi-green-intense-fg ansi-bold">   3298</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>model_input_names[<span style="color: rgb(98,98,98)">0</span>] <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> encoded_inputs:
<span class="ansi-green-fg">-&gt; 3299</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-intense-fg ansi-bold">   3300</span>         <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">You should supply an encoding or a list of encodings to this method </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   3301</span>         <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">that includes </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>model_input_names[<span style="color: rgb(98,98,98)">0</span>]<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">, but you provided </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(0,135,0)">list</span>(encoded_inputs<span style="color: rgb(98,98,98)">.</span>keys())<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   3302</span>     )
<span class="ansi-green-intense-fg ansi-bold">   3304</span> required_input <span style="color: rgb(98,98,98)">=</span> encoded_inputs[<span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>model_input_names[<span style="color: rgb(98,98,98)">0</span>]]
<span class="ansi-green-intense-fg ansi-bold">   3306</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> required_input <span class="ansi-bold" style="color: rgb(175,0,255)">is</span> <span class="ansi-bold" style="color: rgb(0,135,0)">None</span> <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> (<span style="color: rgb(0,135,0)">isinstance</span>(required_input, Sized) <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> <span style="color: rgb(0,135,0)">len</span>(required_input) <span style="color: rgb(98,98,98)">==</span> <span style="color: rgb(98,98,98)">0</span>):

<span class="ansi-red-fg">ValueError</span>: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label', 'labels']</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We get an error again because the model does not have a padding token assigned, so as with the tokenizer we assign it to it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We recreate the trainer arguments with the new model, which now has a padding token, the trainer and train again.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>
    <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">metric_for_best_model</span><span class="o">=</span><span class="n">metric_name</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">logging_dir</span><span class="o">=</span><span class="s2">"./runs"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'validation'</span><span class="p">],</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have seen that all is well, we can train.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="21429" style="width:300px; height:20px; vertical-align: middle;" value="2250"></progress>
      [ 2250/21429 45:38 &lt; 6:29:27, 0.82 it/s, Epoch 0.31/3]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
</tr>
</thead>
<tbody>
</tbody>
</table><p></p></div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="21429" style="width:300px; height:20px; vertical-align: middle;" value="21429"></progress>
      [21429/21429 7:19:25, Epoch 3/3]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.807400</td>
<td>0.820341</td>
<td>0.652000</td>
</tr>
<tr>
<td>2</td>
<td>0.751900</td>
<td>0.802189</td>
<td>0.654600</td>
</tr>
<tr>
<td>3</td>
<td>0.718100</td>
<td>0.810221</td>
<td>0.657800</td>
</tr>
</tbody>
</table><p></p></div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&lt;transformers.trainer_utils.EvalPrediction object at 0x782767ea1450&gt;
&lt;transformers.trainer_utils.EvalPrediction object at 0x782767eeefe0&gt;
&lt;transformers.trainer_utils.EvalPrediction object at 0x782767eecfd0&gt;
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[28]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>TrainOutput(global_step=21429, training_loss=0.7846888848762739, metrics={'train_runtime': 26367.7801, 'train_samples_per_second': 22.755, 'train_steps_per_second': 0.813, 'total_flos': 2.35173445632e+17, 'train_loss': 0.7846888848762739, 'epoch': 3.0})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Evaluation">Evaluation<a class="anchor-link" href="#Evaluation">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Once trained we evaluate on the test dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'test'</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="125" style="width:300px; height:20px; vertical-align: middle;" value="125"></progress>
      [125/125 01:15]
    </div>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>&lt;transformers.trainer_utils.EvalPrediction object at 0x7826ddfded40&gt;
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[29]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'eval_loss': 0.7973636984825134,
 'eval_accuracy': 0.6626,
 'eval_runtime': 76.3016,
 'eval_samples_per_second': 65.529,
 'eval_steps_per_second': 1.638,
 'epoch': 3.0}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Publish-the-model">Publish the model<a class="anchor-link" href="#Publish-the-model">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have our model trained, we can share it with the world, so first we create a model card.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>And now we can publish it. As the first thing we have done is to log in with the huggingface hub, we can upload it to our hub without any problem.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Model-use">Model use<a class="anchor-link" href="#Model-use">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We clean as much as possible</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">gc</span>


<span class="k">def</span> <span class="nf">clear_hardwares</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>


<span class="n">clear_hardwares</span><span class="p">()</span>
<span class="n">clear_hardwares</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we have uploaded the model to our hub we can download it and use it.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>
<span class="n">checkpoints</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>
<span class="n">task</span> <span class="o">=</span> <span class="s2">"text-classification"</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now if we want to return the probability of all classes, we simply use the classifier we just instantiated, with the parameter <code>top_k=None</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">labels</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'label': 'LABEL_4', 'score': 0.8253807425498962},
 {'label': 'LABEL_3', 'score': 0.15411493182182312},
 {'label': 'LABEL_2', 'score': 0.013907806016504765},
 {'label': 'LABEL_0', 'score': 0.003939222544431686},
 {'label': 'LABEL_1', 'score': 0.0026572425849735737}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>If we only want the class with the highest probability we do the same but with the parameter <code>top_k=1</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">label</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">label</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[9]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'label': 'LABEL_4', 'score': 0.8253807425498962}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>And if we want n classes we do the same but with the parameter <code>top_k=n</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">two_labels</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">two_labels</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[10]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[{'label': 'LABEL_4', 'score': 0.8253807425498962},
 {'label': 'LABEL_3', 'score': 0.15411493182182312}]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We can also test the model with Automodel and AutoTokenizer.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-finetuned-amazon-reviews-en-classification"</span>
<span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cuda"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"I love this product"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">logits</span>
<span class="n">lables</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">lables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[26]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>[0.003963470458984375,
 0.0026721954345703125,
 0.01397705078125,
 0.154541015625,
 0.82470703125]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>If you want to test the model further you can see it in <a href="https://huggingface.co/Maximofn/GPT2-small-finetuned-amazon-reviews-en-classification">Maximofn/GPT2-small-finetuned-amazon-reviews-en-classification</a></p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Fine-tuning-for-text-generation-with-Hugging-Face">Fine tuning for text generation with Hugging Face<a class="anchor-link" href="#Fine-tuning-for-text-generation-with-Hugging-Face">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>To make sure I don't have VRAM memory problems, I reboot the notebook.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Login">Login<a class="anchor-link" href="#Login">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>In order to upload the training result to the hub we must first log in, for this we need a token</p>
<p>To create a token, go to the <a href="https://huggingface.co/settings/tokens">setings/tokens</a> page of your account, you will see something like this</p>
<p><img alt="User-Access-Token-dark" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/User-Access-Token-dark.webp"/></p>
<p>Click on <code>New token</code> and a window will appear to create a new token.</p>
<p><img alt="new-token-dark" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/new-token-dark.webp"/></p>
<p>We name the token and create it with the <code>write</code> role, or with the <code>Fine-grained</code> role, which allows us to select exactly what permissions the token will have.</p>
<p>Once created, we copy and paste it below</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Let's use a dataset of <a href="https://huggingface.co/datasets/Maximofn/short-jokes-dataset">English jokes</a></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"Maximofn/short-jokes-dataset"</span><span class="p">)</span>
<span class="n">jokes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[1]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['ID', 'Joke'],
        num_rows: 231657
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's take a look at it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">jokes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['ID', 'Joke'],
        num_rows: 231657
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see that it is a single training set of more than 200 thousand jokes. So later we will have to divide it into training and evaluation.</p>
</section>
<section class="section-block-markdown-cell">
<p>Let's see a sample</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">jokes</span><span class="p">[</span><span class="s1">'train'</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">jokes</span><span class="p">[</span><span class="s1">'train'</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>{'ID': 198387,
 'Joke': 'My hot dislexic co-worker said she had an important massage to give me in her office... When I got there, she told me it can wait until I put on some clothes.'}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see that it has an ID of the joke that does not interest us at all and the joke itself.</p>
</section>
<section class="section-block-markdown-cell">
<p>In case you have low GPU memory I will make a subset of the dataset, choose the percentage of jokes you want to use.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percent_of_train_dataset</span> <span class="o">=</span> <span class="mi">1</span>    <span class="c1"># If you want 50% of the dataset, set this to 0.5</span>

<span class="n">subset_dataset</span> <span class="o">=</span> <span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span> <span class="o">*</span> <span class="n">percent_of_train_dataset</span><span class="p">)))</span>
<span class="n">subset_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[4]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>Dataset({
    features: ['ID', 'Joke'],
    num_rows: 231657
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We now divide the subset into a training set and a validation set.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percent_of_train_dataset</span> <span class="o">=</span> <span class="mf">0.90</span>

<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">subset_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">subset_dataset</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">*</span> <span class="n">percent_of_train_dataset</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">validation_test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>

<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">validation_test_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">validation_test_dataset</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of the train set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">. Size of the validation set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">. Size of the test set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Size of the train set: 208491. Size of the validation set: 11583. Size of the test set: 11583
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Instantiate the tokenizer. We instantiate the padding token of the tokenizer so that it does not give us error as before.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">"right"</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We are going to add two new tokens for joke start and joke end to have more control</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'&lt;SJ&gt;'</span><span class="p">,</span> <span class="s1">'&lt;EJ&gt;'</span><span class="p">]</span>   <span class="c1"># Start and end of joke tokens</span>

<span class="n">num_added_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Added </span><span class="si">{</span><span class="n">num_added_tokens</span><span class="si">}</span><span class="s2"> tokens"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Added 2 tokens
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We create a function to add the new tokens to the sentences</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">joke_column</span> <span class="o">=</span> <span class="s2">"Joke"</span>

<span class="k">def</span> <span class="nf">format_joke</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">example</span><span class="p">[</span><span class="n">joke_column</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'&lt;SJ&gt; '</span> <span class="o">+</span> <span class="n">example</span><span class="p">[</span><span class="s1">'Joke'</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' &lt;EJ&gt;'</span>
    <span class="k">return</span> <span class="n">example</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Select the columns we do not need</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">remove_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">column_names</span> <span class="k">if</span> <span class="n">column</span> <span class="o">!=</span> <span class="n">joke_column</span><span class="p">]</span>
<span class="n">remove_columns</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[9]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>['ID']</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Format the dataset and delete the columns that are not needed</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">test_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[10]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(Dataset({
     features: ['Joke'],
     num_rows: 208491
 }),
 Dataset({
     features: ['Joke'],
     num_rows: 11583
 }),
 Dataset({
     features: ['Joke'],
     num_rows: 11583
 }))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we create a function to tokenize the jokes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="n">joke_column</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We tokenize the dataset and delete the column with the text</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">test_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 208491
 }),
 Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 11583
 }),
 Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 11583
 }))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Model">Model<a class="anchor-link" href="#Model">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now we instantiate the model for text generation and assign the end of string token to the pading token</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see the size of the model vocabulary</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">vocab_size</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>50257</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>It has 50257 tokens, which is the size of the GPT2 vocabulary. But since we said that we were going to create two new tokens with the start of joke and the end of joke, we added them to the model</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>

<span class="n">new_vocab_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Old vocab size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">. New vocab size: </span><span class="si">{</span><span class="n">new_vocab_size</span><span class="si">}</span><span class="s2">. Added </span><span class="si">{</span><span class="n">new_vocab_size</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">vocab_size</span><span class="si">}</span><span class="s2"> tokens"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Old vocab size: 50257. New vocab size: 50259. Added 2 tokens
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Two new tokens have been added</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We configure the training parameters</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">metric_name</span> <span class="o">=</span> <span class="s2">"accuracy"</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">"GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM"</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"./training_results"</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>
<span class="n">BS_TRAIN</span> <span class="o">=</span> <span class="mi">28</span>
<span class="n">BS_EVAL</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">WEIGHT_DECAY</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">WARMUP_STEPS</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">"epoch"</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="n">LR</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">BS_TRAIN</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">BS_EVAL</span><span class="p">,</span>
    <span class="n">warmup_steps</span><span class="o">=</span><span class="n">WARMUP_STEPS</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">WEIGHT_DECAY</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">"cosine"</span><span class="p">,</span>
    <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># metric_for_best_model=metric_name,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we do not use <code>metric_for_best_model</code>, after defining the trainer we explain why</p>
</section>
<section class="section-block-markdown-cell">
<p>We define the trainer</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">validation_dataset</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="c1"># compute_metrics=compute_metrics,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>In this case we do not pass a <code>compute_metrics</code> function, if it is not passed, during the evaluation the <code>loss</code> will be used to evaluate the model. That is why when defining the arguments we do not define <code>metric_for_best_model</code>, because we are not going to use a metric to evaluate the model, but the <code>loss</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>We train</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>  0%|          | 0/625473 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-text-output-error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ValueError</span>                                Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[19], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> trainer<span style="color: rgb(98,98,98)">.</span>train()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:1885</span>, in <span class="ansi-cyan-fg">Trainer.train</span><span class="ansi-blue-fg">(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">   1883</span>         hf_hub_utils<span style="color: rgb(98,98,98)">.</span>enable_progress_bars()
<span class="ansi-green-intense-fg ansi-bold">   1884</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-fg">-&gt; 1885</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> inner_training_loop(
<span class="ansi-green-intense-fg ansi-bold">   1886</span>         args<span style="color: rgb(98,98,98)">=</span>args,
<span class="ansi-green-intense-fg ansi-bold">   1887</span>         resume_from_checkpoint<span style="color: rgb(98,98,98)">=</span>resume_from_checkpoint,
<span class="ansi-green-intense-fg ansi-bold">   1888</span>         trial<span style="color: rgb(98,98,98)">=</span>trial,
<span class="ansi-green-intense-fg ansi-bold">   1889</span>         ignore_keys_for_eval<span style="color: rgb(98,98,98)">=</span>ignore_keys_for_eval,
<span class="ansi-green-intense-fg ansi-bold">   1890</span>     )

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:2216</span>, in <span class="ansi-cyan-fg">Trainer._inner_training_loop</span><span class="ansi-blue-fg">(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)</span>
<span class="ansi-green-intense-fg ansi-bold">   2213</span>     <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>control <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>callback_handler<span style="color: rgb(98,98,98)">.</span>on_step_begin(args, <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>state, <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>control)
<span class="ansi-green-intense-fg ansi-bold">   2215</span> <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>accelerator<span style="color: rgb(98,98,98)">.</span>accumulate(model):
<span class="ansi-green-fg">-&gt; 2216</span>     tr_loss_step <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>training_step(model, inputs)
<span class="ansi-green-intense-fg ansi-bold">   2218</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> (
<span class="ansi-green-intense-fg ansi-bold">   2219</span>     args<span style="color: rgb(98,98,98)">.</span>logging_nan_inf_filter
<span class="ansi-green-intense-fg ansi-bold">   2220</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> is_torch_xla_available()
<span class="ansi-green-intense-fg ansi-bold">   2221</span>     <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> (torch<span style="color: rgb(98,98,98)">.</span>isnan(tr_loss_step) <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> torch<span style="color: rgb(98,98,98)">.</span>isinf(tr_loss_step))
<span class="ansi-green-intense-fg ansi-bold">   2222</span> ):
<span class="ansi-green-intense-fg ansi-bold">   2223</span>     <span style="color: rgb(95,135,135)"># if loss is nan or inf simply add the average of previous logged losses</span>
<span class="ansi-green-intense-fg ansi-bold">   2224</span>     tr_loss <span style="color: rgb(98,98,98)">+</span><span style="color: rgb(98,98,98)">=</span> tr_loss <span style="color: rgb(98,98,98)">/</span> (<span style="color: rgb(98,98,98)">1</span> <span style="color: rgb(98,98,98)">+</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>state<span style="color: rgb(98,98,98)">.</span>global_step <span style="color: rgb(98,98,98)">-</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>_globalstep_last_logged)

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:3238</span>, in <span class="ansi-cyan-fg">Trainer.training_step</span><span class="ansi-blue-fg">(self, model, inputs)</span>
<span class="ansi-green-intense-fg ansi-bold">   3235</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> loss_mb<span style="color: rgb(98,98,98)">.</span>reduce_mean()<span style="color: rgb(98,98,98)">.</span>detach()<span style="color: rgb(98,98,98)">.</span>to(<span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>args<span style="color: rgb(98,98,98)">.</span>device)
<span class="ansi-green-intense-fg ansi-bold">   3237</span> <span class="ansi-bold" style="color: rgb(0,135,0)">with</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>compute_loss_context_manager():
<span class="ansi-green-fg">-&gt; 3238</span>     loss <span style="color: rgb(98,98,98)">=</span> <span style="color: rgb(0,135,0)">self</span><span style="color: rgb(98,98,98)">.</span>compute_loss(model, inputs)
<span class="ansi-green-intense-fg ansi-bold">   3240</span> <span class="ansi-bold" style="color: rgb(0,135,0)">del</span> inputs
<span class="ansi-green-intense-fg ansi-bold">   3241</span> torch<span style="color: rgb(98,98,98)">.</span>cuda<span style="color: rgb(98,98,98)">.</span>empty_cache()

File <span class="ansi-green-fg">~/miniconda3/envs/nlp_/lib/python3.11/site-packages/transformers/trainer.py:3282</span>, in <span class="ansi-cyan-fg">Trainer.compute_loss</span><span class="ansi-blue-fg">(self, model, inputs, return_outputs)</span>
<span class="ansi-green-intense-fg ansi-bold">   3280</span> <span class="ansi-bold" style="color: rgb(0,135,0)">else</span>:
<span class="ansi-green-intense-fg ansi-bold">   3281</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">isinstance</span>(outputs, <span style="color: rgb(0,135,0)">dict</span>) <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">loss</span><span style="color: rgb(175,0,0)">"</span> <span class="ansi-bold" style="color: rgb(175,0,255)">not</span> <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> outputs:
<span class="ansi-green-fg">-&gt; 3282</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">ValueError</span>(
<span class="ansi-green-intense-fg ansi-bold">   3283</span>             <span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">The model did not return a loss from the inputs, only the following keys: </span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   3284</span>             <span style="color: rgb(175,0,0)">f</span><span style="color: rgb(175,0,0)">"</span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">,</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(98,98,98)">.</span>join(outputs<span style="color: rgb(98,98,98)">.</span>keys())<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">. For reference, the inputs it received are </span><span class="ansi-bold" style="color: rgb(175,95,135)">{</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(175,0,0)">,</span><span style="color: rgb(175,0,0)">'</span><span style="color: rgb(98,98,98)">.</span>join(inputs<span style="color: rgb(98,98,98)">.</span>keys())<span class="ansi-bold" style="color: rgb(175,95,135)">}</span><span style="color: rgb(175,0,0)">.</span><span style="color: rgb(175,0,0)">"</span>
<span class="ansi-green-intense-fg ansi-bold">   3285</span>         )
<span class="ansi-green-intense-fg ansi-bold">   3286</span>     <span style="color: rgb(95,135,135)"># We don't use .loss here since the model may return tuples instead of ModelOutput.</span>
<span class="ansi-green-intense-fg ansi-bold">   3287</span>     loss <span style="color: rgb(98,98,98)">=</span> outputs[<span style="color: rgb(175,0,0)">"</span><span style="color: rgb(175,0,0)">loss</span><span style="color: rgb(175,0,0)">"</span>] <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">isinstance</span>(outputs, <span style="color: rgb(0,135,0)">dict</span>) <span class="ansi-bold" style="color: rgb(0,135,0)">else</span> outputs[<span style="color: rgb(98,98,98)">0</span>]

<span class="ansi-red-fg">ValueError</span>: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, it gives us an error, it tells us that the model does not return the value of the loss, which is key to be able to train, let's see why.</p>
</section>
<section class="section-block-markdown-cell">
<p>First, let's see what an example of the dataset looks like.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">sample</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>{'input_ids': [50257,
  4162,
  750,
  262,
  18757,
  6451,
  2245,
  2491,
  30,
  4362,
  340,
  373,
  734,
  10032,
  13,
  220,
  50258,
  50256,
  50256,
  ...,
  50256,
  50256,
  50256],
 'attention_mask': [1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  0,
  0,
  0,
  ...,
  0,
  0,
  0]}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, we have a dictionary with the <code>input_ids</code> and the <code>attention_mask</code>, if we pass it to the model we obtain this</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>None
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see it does not return the value of the loss because it is waiting for a value for <code>labels</code>, which we have not passed it. In the previous example, in which we did fine tuning for text classification, we said that the labels had to be passed to a field of the dataset called <code>labels</code>, but in this case we do not have that field in the dataset.</p>
<p>If we now assign the <code>lables</code> to the <code>input_ids</code> and look again at the <code>input_ids</code> loss</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"attention_mask"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">"input_ids"</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>tensor(102.1873, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we get a <code>loss</code>.</p>
<p>So we have two options, add a <code>labels</code> field to the dataset, with the values of <code>input_ids</code> or use a function of the <code>transformers</code> library called <code>data_collator</code>, in this case we will use <code>DataCollatorForLanguageModeling</code>. Let's take a look at it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForLanguageModeling</span>

<span class="n">my_data_collator</span><span class="o">=</span><span class="n">DataCollatorForLanguageModeling</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mlm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We pass the <code>sample</code> sample through this <code>data_collator</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">collated_sample</span> <span class="o">=</span> <span class="n">my_data_collator</span><span class="p">([</span><span class="n">sample</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see how the output is</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">collated_sample</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>input_ids (torch.Size([1, 768])): tensor([[50257,  4162,   750,   262, 18757,  6451,  2245,  2491,    30,  4362,
           340,   373,   734, 10032,    13,   220, 50258, 50256, ..., 50256, 50256]],
       device='cuda:0')
attention_mask (torch.Size([1, 768])): tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ..., 0, 0]],
       device='cuda:0')
labels (torch.Size([1, 768])): tensor([[50257,  4162,   750,   262, 18757,  6451,  2245,  2491,    30,  4362,
           340,   373,   734, 10032,    13,   220, 50258,  -100,  ...,  -100,  -100]],
       device='cuda:0')
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As you can see, the <code>data_collator</code> has created a <code>labels</code> field and assigned it the values of <code>input_ids</code>. The tokens that are masked have been assigned the value -100. This is because when we define the <code>data_collator</code> we pass the parameter <code>mlm=False</code>, which means that we are not doing <code>Masked Language Modeling</code>, but <code>Language Modeling</code>, that is why it does not mask any original token.</p>
</section>
<section class="section-block-markdown-cell">
<p>Let's see if now we get a <code>loss</code> with this <code>data_collator</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">collated_sample</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">loss</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor(102.7181, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>So we redefine the <code>trainer</code> with the <code>data_collator</code> and we train again</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForLanguageModeling</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">validation_dataset</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">DataCollatorForLanguageModeling</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">mlm</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="22341" style="width:300px; height:20px; vertical-align: middle;" value="22341"></progress>
      [22341/22341 2:33:28, Epoch 3/3]
    </div>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: left;">
<th>Epoch</th>
<th>Training Loss</th>
<th>Validation Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>3.386600</td>
<td>3.258979</td>
</tr>
<tr>
<td>2</td>
<td>3.259900</td>
<td>3.199673</td>
</tr>
<tr>
<td>3</td>
<td>3.212600</td>
<td>3.192009</td>
</tr>
</tbody>
</table><p></p></div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>TrainOutput(global_step=22341, training_loss=3.505178199598342, metrics={'train_runtime': 9209.5353, 'train_samples_per_second': 67.916, 'train_steps_per_second': 2.426, 'total_flos': 2.45146666696704e+17, 'train_loss': 3.505178199598342, 'epoch': 3.0})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Evaluation">Evaluation<a class="anchor-link" href="#Evaluation">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Once the model is trained we evaluate the model on the test dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">eval_dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-html-rendered-html-output-subarea">
<div>
<progress max="362" style="width:300px; height:20px; vertical-align: middle;" value="362"></progress>
      [362/362 01:04]
    </div>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>{'eval_loss': 3.201305866241455,
 'eval_runtime': 65.0033,
 'eval_samples_per_second': 178.191,
 'eval_steps_per_second': 5.569,
 'epoch': 3.0}</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Publish-the-model">Publish the model<a class="anchor-link" href="#Publish-the-model">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We create the model card</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">create_model_card</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We publish it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>events.out.tfevents.1720875425.8de3af1b431d.6946.1:   0%|          | 0.00/364 [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM/commit/d107b3bb0e02076483238f9975697761015ec390', commit_message='End of training', commit_description='', oid='d107b3bb0e02076483238f9975697761015ec390', pr_url=None, pr_revision=None, pr_num=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Model-use">Model use<a class="anchor-link" href="#Model-use">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We clean as much as possible</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">gc</span>


<span class="k">def</span> <span class="nf">clear_hardwares</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">clear_autocast_cache</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">ipc_collect</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>


<span class="n">clear_hardwares</span><span class="p">()</span>
<span class="n">clear_hardwares</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Download the model and tokenizer</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">user</span> <span class="o">=</span> <span class="s2">"maximofn"</span>
<span class="n">checkpoints</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">user</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">"right"</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We check that the tokenizer and the model have the 2 extra tokens we have added.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">tokenizer_vocab</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab</span><span class="p">()</span>
<span class="n">model_vocab</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tokenizer_vocab: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer_vocab</span><span class="p">)</span><span class="si">}</span><span class="s2">. model_vocab: </span><span class="si">{</span><span class="n">model_vocab</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>tokenizer_vocab: 50259. model_vocab: 50259
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see that they have 50259 tokens, i.e., the 50257 tokens of GPT2 plus the 2 we added</p>
</section>
<section class="section-block-markdown-cell">
<p>We create a function to generate jokes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_joke</span><span class="p">(</span><span class="n">prompt_text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"&lt;SJ&gt; </span><span class="si">{</span><span class="n">prompt_text</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">"&lt;EJ&gt;"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We generate a joke</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">generate_joke</span><span class="p">(</span><span class="s2">"Why didn't the frog cross the road?"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:50258 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>"&lt;SJ&gt; Why didn't the frog cross the road? Because he was frog-in-the-face. &lt;EJ&gt;"</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>If you want to test the model further you can see it in <a href="https://huggingface.co/Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM">Maximofn/GPT2-small-finetuned-Maximofn-short-jokes-dataset-casualLM</a></p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Fine-tuning-for-text-classification-with-Pytorch">Fine tuning for text classification with Pytorch<a class="anchor-link" href="#Fine-tuning-for-text-classification-with-Pytorch">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>We repeat the Pytorch training</p>
</section>
<section class="section-block-markdown-cell">
<p>Restart the notebook to make sure</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We downloaded the same dataset as when we did the training with the Hugging Face libraries.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"mteb/amazon_reviews_multi"</span><span class="p">,</span> <span class="s2">"en"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We create a variable with the number of classes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="s1">'label'</span><span class="p">))</span>
<span class="n">num_classes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>5</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Before we processed the whole dataset to create a field called <code>labels</code>, but now it is not necessary because as we are going to program everything ourselves, we adapt to what the dataset looks like.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We create the tokenizer. We assign the padding token so that it does not give us an error as before.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We create a function for tokenizing the dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We tokenize it. We eliminate columns that we don't need, but now we leave the text column</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'id'</span><span class="p">,</span> <span class="s1">'label_text'</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>DatasetDict({
    train: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['text', 'label', 'input_ids', 'attention_mask'],
        num_rows: 5000
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percentage</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">subset_train</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>
<span class="n">percentage</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">subset_validation</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'validation'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'validation'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>
<span class="n">subset_test</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">'test'</span><span class="p">])</span> <span class="o">*</span> <span class="n">percentage</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"len subset_train: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_train</span><span class="p">)</span><span class="si">}</span><span class="s2">, len subset_validation: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_validation</span><span class="p">)</span><span class="si">}</span><span class="s2">, len subset_test: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">subset_test</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>len subset_train: 200000, len subset_validation: 5000, len subset_test: 5000
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Model">Model<a class="anchor-link" href="#Model">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We import the weights and assign the padding token</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at openai-community/gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Device">Device<a class="anchor-link" href="#Device">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Create the device where everything is going to be executed</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">'cpu'</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We pass the model to the device and pass it to FP16 so that it occupies less memory.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pytorch-Dataset">Pytorch Dataset<a class="anchor-link" href="#Pytorch-Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Create a pytorch dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">ReviewsDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">huggingface_dataset</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">huggingface_dataset</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'label'</span><span class="p">]</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'input_ids'</span><span class="p">])</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'attention_mask'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">label</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Instantiate the datasets</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ReviewsDataset</span><span class="p">(</span><span class="n">subset_train</span><span class="p">)</span>
<span class="n">validatation_dataset</span> <span class="o">=</span> <span class="n">ReviewsDataset</span><span class="p">(</span><span class="n">subset_validation</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">ReviewsDataset</span><span class="p">(</span><span class="n">subset_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see a sample</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>(torch.Size([768]), torch.Size([768]), 0)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pytorch-Dataloader">Pytorch Dataloader<a class="anchor-link" href="#Pytorch-Dataloader">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We now create a pytorch dataloader</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">BS</span> <span class="o">=</span> <span class="mi">12</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">validation_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">validatation_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see a sample</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>(torch.Size([12, 768]),
 torch.Size([12, 768]),
 tensor([2, 1, 2, 0, 3, 3, 0, 4, 3, 3, 4, 2]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>To check that everything is OK we pass the sample to the model to see if everything is OK. First we pass the tokens to the device</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">at_mask</span> <span class="o">=</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we pass them to the model</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>odict_keys(['loss', 'logits', 'past_key_values'])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see it gives us the loss and the logits</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor(5.9414, device='cuda:0', dtype=torch.float16,
       grad_fn=&lt;NllLossBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor([[ 6.1953e+00, -1.2275e+00, -2.4824e+00,  5.8867e+00, -1.4734e+01],
        [ 5.4062e+00, -8.4570e-01, -2.3203e+00,  5.1055e+00, -1.1555e+01],
        [ 6.1641e+00, -9.3066e-01, -2.5664e+00,  6.0039e+00, -1.4570e+01],
        [ 5.2266e+00, -4.2358e-01, -2.0801e+00,  4.7461e+00, -1.1570e+01],
        [ 3.8184e+00, -2.3460e-03, -1.7666e+00,  3.4160e+00, -7.7969e+00],
        [ 4.1641e+00, -4.8169e-01, -1.6914e+00,  3.9941e+00, -8.7734e+00],
        [ 4.6758e+00, -3.0298e-01, -2.1641e+00,  4.1055e+00, -9.3359e+00],
        [ 4.1953e+00, -3.2471e-01, -2.1875e+00,  3.9375e+00, -8.3438e+00],
        [-1.1650e+00,  1.3564e+00, -6.2158e-01, -6.8115e-01,  4.8672e+00],
        [ 4.4961e+00, -8.7891e-02, -2.2793e+00,  4.2812e+00, -9.3359e+00],
        [ 4.9336e+00, -2.6627e-03, -2.1543e+00,  4.3711e+00, -1.0742e+01],
        [ 5.9727e+00, -4.3152e-02, -1.4551e+00,  4.3438e+00, -1.2117e+01]],
       device='cuda:0', dtype=torch.float16, grad_fn=&lt;IndexBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Metric">Metric<a class="anchor-link" href="#Metric">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We are going to create a function to obtain the metric, which in this case is going to be the accuracy</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">predicted_labels</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="n">percent</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">percent</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predicted_labels</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">correct</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see how well it calculates</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">compute_accuracy</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>0.1666666716337204</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Optimizer">Optimizer<a class="anchor-link" href="#Optimizer">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>As we are going to need an optimizer, we create one</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span>

<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We create the training loop</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">progresbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">'Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">progresbar</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">at_mask</span> <span class="o">=</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'train_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>
    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'train_loss'</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">})</span>

    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">valid_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">progresbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">'Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">progresbar</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">at_mask</span> <span class="o">=</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
        <span class="n">valid_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">step_accuracy</span> <span class="o">=</span> <span class="n">compute_accuracy</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">+=</span> <span class="n">step_accuracy</span>
        <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'valid_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">step_accuracy</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>

    <span class="n">valid_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">)</span>
    <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'valid_loss'</span><span class="p">:</span> <span class="n">valid_loss</span><span class="p">,</span> <span class="s1">'accuracy'</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">})</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Epoch 1: 100%|██████████| 16667/16667 [44:13&lt;00:00,  6.28it/s, train_loss=nan]
Epoch 1: 100%|██████████| 417/417 [00:32&lt;00:00, 12.72it/s, valid_loss=nan, accuracy=0]
Epoch 2: 100%|██████████| 16667/16667 [44:06&lt;00:00,  6.30it/s, train_loss=nan]
Epoch 2: 100%|██████████| 417/417 [00:32&lt;00:00, 12.77it/s, valid_loss=nan, accuracy=0]
Epoch 3: 100%|██████████| 16667/16667 [44:03&lt;00:00,  6.30it/s, train_loss=nan]
Epoch 3: 100%|██████████| 417/417 [00:32&lt;00:00, 12.86it/s, valid_loss=nan, accuracy=0]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Model-use">Model use<a class="anchor-link" href="#Model-use">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Let's test the model we have trained</p>
</section>
<section class="section-block-markdown-cell">
<p>First we tokenize a text</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">({</span><span class="s2">"text"</span><span class="p">:</span> <span class="s2">"I love this product. It is amazing."</span><span class="p">})</span>
<span class="n">input_tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">input_tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>(torch.Size([1, 768]), torch.Size([1, 768]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we pass it to the model</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">]</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor([[nan, nan, nan, nan, nan]], device='cuda:0', dtype=torch.float16,
       grad_fn=&lt;IndexBackward0&gt;)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see the predictions of these logits</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">predicted</span> <span class="o">=</span> <span class="n">predicted_labels</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">])</span>
<span class="n">predicted</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>tensor([0], device='cuda:0')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Fine-tuning-for-text-generation-with-Pytorch">Fine tuning for text generation with Pytorch<a class="anchor-link" href="#Fine-tuning-for-text-generation-with-Pytorch">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>We repeat the Pytorch training</p>
</section>
<section class="section-block-markdown-cell">
<p>Restart the notebook to make sure</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Dataset">Dataset<a class="anchor-link" href="#Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We download again the dataset of jokes</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">jokes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">"Maximofn/short-jokes-dataset"</span><span class="p">)</span>
<span class="n">jokes</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[1]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>DatasetDict({
    train: Dataset({
        features: ['ID', 'Joke'],
        num_rows: 231657
    })
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Create a subset in case you are short on memory</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percent_of_train_dataset</span> <span class="o">=</span> <span class="mi">1</span>    <span class="c1"># If you want 50% of the dataset, set this to 0.5</span>

<span class="n">subset_dataset</span> <span class="o">=</span> <span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">jokes</span><span class="p">[</span><span class="s2">"train"</span><span class="p">])</span> <span class="o">*</span> <span class="n">percent_of_train_dataset</span><span class="p">)))</span>
<span class="n">subset_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[2]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>Dataset({
    features: ['ID', 'Joke'],
    num_rows: 231657
})</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We divide the dataset into subsets for training, validation and test.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">percent_of_train_dataset</span> <span class="o">=</span> <span class="mf">0.90</span>

<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">subset_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">subset_dataset</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">*</span> <span class="n">percent_of_train_dataset</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">validation_test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>

<span class="n">split_dataset</span> <span class="o">=</span> <span class="n">validation_test_dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">validation_test_dataset</span><span class="o">.</span><span class="n">num_rows</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"train"</span><span class="p">]</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">split_dataset</span><span class="p">[</span><span class="s2">"test"</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of the train set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">. Size of the validation set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">. Size of the test set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Size of the train set: 208491. Size of the validation set: 11583. Size of the test set: 11583
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Tokenizer">Tokenizer<a class="anchor-link" href="#Tokenizer">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We start the tokenizer and assign the end of string token to the padding token</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoints</span> <span class="o">=</span> <span class="s2">"openai-community/gpt2"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">"right"</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We add the special tokens for the beginning and end of the joke.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'&lt;SJ&gt;'</span><span class="p">,</span> <span class="s1">'&lt;EJ&gt;'</span><span class="p">]</span>   <span class="c1"># Start and end of joke tokens</span>

<span class="n">num_added_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Added </span><span class="si">{</span><span class="n">num_added_tokens</span><span class="si">}</span><span class="s2"> tokens"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Added 2 tokens
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We add them to the dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">joke_column</span> <span class="o">=</span> <span class="s2">"Joke"</span>

<span class="k">def</span> <span class="nf">format_joke</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
    <span class="n">example</span><span class="p">[</span><span class="n">joke_column</span><span class="p">]</span> <span class="o">=</span> <span class="s1">'&lt;SJ&gt; '</span> <span class="o">+</span> <span class="n">example</span><span class="p">[</span><span class="s1">'Joke'</span><span class="p">]</span> <span class="o">+</span> <span class="s1">' &lt;EJ&gt;'</span>
    <span class="k">return</span> <span class="n">example</span>

<span class="n">remove_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">column_names</span> <span class="k">if</span> <span class="n">column</span> <span class="o">!=</span> <span class="n">joke_column</span><span class="p">]</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">format_joke</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">)</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">test_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[6]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(Dataset({
     features: ['Joke'],
     num_rows: 208491
 }),
 Dataset({
     features: ['Joke'],
     num_rows: 11583
 }),
 Dataset({
     features: ['Joke'],
     num_rows: 11583
 }))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We tokenize the dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="n">joke_column</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">"max_length"</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">validation_dataset</span> <span class="o">=</span> <span class="n">validation_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="n">joke_column</span><span class="p">])</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">test_dataset</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[7]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 208491
 }),
 Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 11583
 }),
 Dataset({
     features: ['input_ids', 'attention_mask'],
     num_rows: 11583
 }))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Model">Model<a class="anchor-link" href="#Model">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Instantiate the model, assign the padding token and add the new joke start and joke end tokens.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="n">model</span><span class="o">.</span><span class="n">resize_token_embeddings</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>Embedding(50259, 768)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Device">Device<a class="anchor-link" href="#Device">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We create the device and pass the model to the device</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">'cpu'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pytorch-Dataset">Pytorch Dataset<a class="anchor-link" href="#Pytorch-Dataset">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Create a pytorch dataset</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">JokesDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">huggingface_dataset</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">huggingface_dataset</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'input_ids'</span><span class="p">])</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'attention_mask'</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Instantiate training, validation and test datasets</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">train_pytorch_dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="n">validation_pytorch_dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">validation_dataset</span><span class="p">)</span>
<span class="n">test_pytorch_dataset</span> <span class="o">=</span> <span class="n">JokesDataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see a sample</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">train_pytorch_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[12]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(torch.Size([768]), torch.Size([768]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Pytorch-Dataloader">Pytorch Dataloader<a class="anchor-link" href="#Pytorch-Dataloader">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We create the dataloaders</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">BS</span> <span class="o">=</span> <span class="mi">28</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_pytorch_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">validation_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">validation_pytorch_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_pytorch_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see a sample</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(torch.Size([28, 768]), torch.Size([28, 768]))</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We pass it to the model</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[15]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>odict_keys(['logits', 'past_key_values'])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see we have no <code>loss</code> value, as we have seen we have to pass the <code>input_ids</code> and the <code>labels</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="n">output</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[16]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>odict_keys(['loss', 'logits', 'past_key_values'])</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we have <code>loss</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[17]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>80.5625</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Optimizer">Optimizer<a class="anchor-link" href="#Optimizer">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We create an optimizer</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span>

<span class="n">LR</span> <span class="o">=</span> <span class="mf">2e-5</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Training">Training<a class="anchor-link" href="#Training">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We create the training loop</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">progresbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">'Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">at_mask</span> <span class="ow">in</span> <span class="n">progresbar</span><span class="p">:</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">at_mask</span> <span class="o">=</span> <span class="n">at_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">at_mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">'loss'</span><span class="p">]</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'train_loss'</span><span class="p">:</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()})</span>
    <span class="n">train_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="n">progresbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span><span class="s1">'train_loss'</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">})</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Epoch 1: 100%|██████████| 7447/7447 [51:07&lt;00:00,  2.43it/s, train_loss=nan]
Epoch 2: 100%|██████████| 7447/7447 [51:06&lt;00:00,  2.43it/s, train_loss=nan]
Epoch 3: 100%|██████████| 7447/7447 [51:07&lt;00:00,  2.43it/s, train_loss=nan]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Model-use">Model use<a class="anchor-link" href="#Model-use">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We tested the model</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_text</span><span class="p">(</span><span class="n">decoded_joke</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stop_token</span><span class="o">=</span><span class="s1">'&lt;EJ&gt;'</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">({</span><span class="s1">'Joke'</span><span class="p">:</span> <span class="n">decoded_joke</span><span class="p">})</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">nex_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">nex_token_decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">nex_token</span><span class="p">)</span>
    <span class="n">decoded_joke</span> <span class="o">=</span> <span class="n">decoded_joke</span> <span class="o">+</span> <span class="n">nex_token_decoded</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="n">nex_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="s1">'logits'</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">nex_token_decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">nex_token</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">nex_token_decoded</span> <span class="o">==</span> <span class="n">stop_token</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">decoded_joke</span> <span class="o">=</span> <span class="n">decoded_joke</span> <span class="o">+</span> <span class="n">nex_token_decoded</span>
        <span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenize_function</span><span class="p">({</span><span class="s1">'Joke'</span><span class="p">:</span> <span class="n">decoded_joke</span><span class="p">})</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'input_ids'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="p">[</span><span class="s1">'attention_mask'</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">decoded_joke</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">generated_text</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="s2">"&lt;SJ&gt; Why didn't the frog cross the road"</span><span class="p">)</span>
<span class="n">generated_text</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>"&lt;SJ&gt; Why didn't the frog cross the road!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"</pre>
</div>
</div>
</div>
</section>
