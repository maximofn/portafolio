<section class="section-block-markdown-cell">
<h1 id="llm.int8()---8-bit-Matrix-Multiplication-for-Transformers-at-Scale">llm.int8() - 8-bit Matrix Multiplication for Transformers at Scale<a class="anchor-link" href="#llm.int8()---8-bit-Matrix-Multiplication-for-Transformers-at-Scale">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..</p>
</section>
<section class="section-block-markdown-cell">
<p>Na postagem <a href="https://maximofn.com/llms-quantization/">LLMs quantization</a>, explicamos a importância da quantização dos LLMs para economizar memória. Também explicamos que há uma forma de quantização que é a <a href="https://maximofn.com/llms-quantization/#Cuantizaci%C3%B3n-de-punto-cero">quantização de ponto zero</a>, que consiste em transformar os valores dos parâmetros dos pesos linearmente, mas isso tem o problema da degradação dos modelos de linguagem a partir do momento em que eles ultrapassam 2,7 bilhões de parâmetros.</p>
<p><img alt="llm.int8()-degradação" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-degradation.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Quantiza%C3%A7%C3%A3o-de-vetores">Quantização de vetores<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-de-vetores">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Como a quantização de todos os parâmetros dos modelos produz erros nos modelos de idiomas grandes, o que eles propõem no artigo <a href="https://arxiv.org/abs/2208.07339">llm.int8()</a> é realizar a quantização de vetores, ou seja, separar as matrizes dos pesos em vetores, de modo que alguns desses vetores possam ser quantizados em 8 bits, enquanto outros não. Assim, aqueles que podem ser quantizados em 8 bits são quantizados e as multiplicações de matriz são realizadas no formato INT8, enquanto os vetores que não podem ser quantizados são mantidos no formato FP16 e as multiplicações são realizadas no formato FP16.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada em um exemplo</p>
<p>Suponha que tenhamos a matriz</p>
<p><img alt="llm.int8()-A" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-A.webp"/></p>
<p>e queremos multiplicá-lo pela matriz</p>
<p><img alt="llm.int8()-B" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-B.webp"/></p>
<p>Definimos um valor limite e todas as colunas da primeira matriz que têm um valor maior do que esse limite são deixadas no formato FP16; as linhas equivalentes às linhas da primeira matriz na segunda matriz também são deixadas no formato FP16.</p>
<p>Explicando melhor, como a segunda e a quarta colunas da primeira matriz (colunas amarelas) têm valores maiores que um determinado limite, a segunda e a quarta linhas da segunda matriz (linhas amarelas) são deixadas no formato FP16.</p>
<p>No caso de haver valores limiares na segunda matriz, o mesmo seria feito, por exemplo, se na segunda matriz uma linha tivesse um valor maior que um limiar, ela seria deixada no formato FP16, e essa coluna na primeira matriz seria deixada no formato FP16.</p>
<p>As linhas e colunas restantes que não são deixadas no formato FP16 são quantizadas em 8 bits e as multiplicações são realizadas no formato INT8.</p>
<p>Portanto, dividimos a primeira matriz em duas matrizes</p>
<p><img alt="llm.int8()-A_separated" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-A_separated_.webp"/></p>
<p>E a segunda matriz nas duas matrizes</p>
<p><img alt="llm.int8()-B_separated" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-B_separated_.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>Multiplicamos as matrizes em INT8 em um lado</p>
<p><img alt="llm.int8()-AxB-int8" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-AxB-int8_.webp"/></p>
<p>e aqueles no formato FP16, por outro lado</p>
<p><img alt="llm.int8()-AxB-fp16" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-AxB-fp16_.webp"/></p>
<p>Como pode ser visto, a multiplicação das matrizes no formato INT8 nos dá uma matriz de tamanho 3x2, e a multiplicação das matrizes no formato FP16 nos dá outra matriz de tamanho 3x2, de modo que, se as somarmos</p>
<p><img alt="llm.int8()-fp16+int8" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-fp16int8_.webp"/></p>
<p>É interessante notar que ele apresenta o mesmo resultado como se tivéssemos multiplicado as matrizes originais.</p>
<p><img alt="llm.int8()-AxB" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-AxB_.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>Para ver por que isso acontece, se desenvolvermos o produto vetorial das duas matrizes originais</p>
<p><img alt="llm.int8()-AxB-explained" src="https://maximofn.com/wp-content/uploads/2024/07/llm.int8-AxB-explained.webp"/></p>
<p>Vemos que a separação que fizemos não traz problemas.</p>
</section>
<section class="section-block-markdown-cell">
<p>Portanto, podemos concluir que é possível separar linhas e colunas de matrizes para realizar multiplicações de matrizes. Essa separação será feita quando qualquer elemento da linha ou coluna for maior que um valor limite, de modo que as linhas ou colunas que não tiverem um valor maior que esse limite serão codificadas em INT8, ocupando apenas um byte, e as linhas ou colunas que tiverem um elemento maior que esse limite serão passadas para FP16, ocupando 2 bytes. Dessa forma, não teremos problemas de arredondamento, pois os cálculos que fizermos em INT8 serão feitos com valores que não ultrapassem o intervalo de 8 bits.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Valor-limiar-%CE%B1">Valor limiar α<a class="anchor-link" href="#Valor-limiar-%CE%B1">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Como dissemos, vamos separar em linhas e colunas que tenham algum elemento maior que um valor limite, mas qual valor limite devemos escolher? Os autores do artigo fizeram experimentos com vários valores e determinaram que o valor limite deveria ser α=6. Acima desse valor, eles começaram a obter degradações nos modelos de linguagem.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Uso-de-llm.int8()">Uso de llm.int8()<a class="anchor-link" href="#Uso-de-llm.int8()">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver como quantificar um modelo com llm.int8() com a biblioteca de transformadores. Para fazer isso, você precisa ter o <code>bitsandbytes</code> instalado.</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>bitsandbytes
<span class="sb">```</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Carregamos um modelo de parâmetro 1B duas vezes, uma vez da maneira normal e a segunda vez quantificando-o com llm.int8().</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"TinyLlama/TinyLlama-1.1B-Chat-v1.0"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos a quantidade de memória que cada um dos modelos ocupa</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">),</span> <span class="n">model_8bit</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[6]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(4.098002195358276, 1.1466586589813232)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como pode ser visto, o modelo quantizado ocupa muito menos memória.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos agora fazer um teste de geração de texto com os dois modelos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[5]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>tensor([[    1, 15043,   590,  1024,   338,  5918,  4200,   322,   306,   626,
           263,  6189, 29257, 10863,   261]], device='cuda:0')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos o resultado com o modelo normal</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer. I am currently working at [Company Name] as a Machine Learning Engineer. I have a Bachelor's degree in Computer Science from [University Name] and a Master's degree in Computer Science from [University Name]. I
1.7616662979125977
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E agora com o modelo quantizado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_8bit</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer. I am currently working at [Company Name] as a Machine Learning Engineer. I have a Bachelor's degree in Computer Science from [University Name] and a Master's degree in Computer Science from [University Name]. I
9.100712776184082
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos duas coisas: por um lado, obtemos o mesmo texto na saída, portanto, com um modelo muito menor, podemos obter a mesma saída; no entanto, o modelo quantizado leva muito mais tempo para ser executado, portanto, se você precisar usar esse modelo em tempo real, isso não seria aconselhável.</p>
<p>Isso é contraditório, porque poderíamos pensar que um modelo menor teria que ser executado mais rapidamente, mas temos que pensar que, na realidade, os dois modelos, o normal e o quantizado, executam as mesmas operações, apenas um executa todas as operações em FP32 e o outro as executa em INT8 e FP16, mas o modelo quantizado tem que procurar linhas e colunas com valores maiores que o valor limite, separá-las, executar as operações em INT8 e FP16 e, em seguida, juntar os resultados novamente, de modo que o modelo quantizado leva mais tempo para ser executado.</p>
</section>
