<p>
	<!-- Custom stylesheet, it must be in the same directory as the html file -->
	<!-- Loading mathjax macro --><!-- Load mathjax --><!-- MathJax configuration -->
<p>
<style>
	/* Media query para pantallas grandes */
	@media screen and (min-width: 1000px) {
		.container {
			flex-wrap: nowrap;
			display: flex;
			justify-content: space-between;
			height: 100vh; /* altura del viewport */
		}
		
		.indice {
			width: 25%; /* Ancho del índice en pantallas grandes */
			border-right: 1px solid #ccc;
			max-height: calc(100vh - 40px); /* Ajustar según tus necesidades. Aquí estamos restando el doble del padding para mantener todo visible */
			padding: 20px;
			box-sizing: border-box;
			overflow-y: auto; /* desplazamiento vertical cuando sea necesario */
		}
		
		.contenido {
			width: 70%; /* Ancho del contenido en pantallas grandes */
			max-height: calc(100vh - 40px); /* Ajustar según tus necesidades. Aquí estamos restando el doble del padding para mantener todo visible */
			padding: 20px;
			box-sizing: border-box;
			overflow-y: auto; /* desplazamiento vertical cuando sea necesario */
		}
	}
</style>
<!-- Open div notebook -->
<div id="notebook" class="border-box-sizing" tabindex="-1">
	<!-- Open div notebook container -->
	<div id="notebook-container" class="container">
		<!-- Open div indice -->
		<div class="indice">
			<!-- Open div index header -->
			<div class="cell border-box-sizing text_cell rendered">
				<h2 class="prompt input_prompt">
					<span style="
					color: var(--darkreader-text--heading-color, var(--darkreader-text--heading-2-color,
					var(--darkreader-text--headings-color)));
					font-family: var(--fontFamily); font-size: var(--fontSize);
					font-style: var(--fontStyle, inherit);
					font-weight: var(--fontWeight);
					letter-spacing: var(--letterSpacing);
					text-transform: var(--textTransform);
					background-color: transparent;
					--darkreader-inline-color: var(--darkreader-text--darkreader-text--heading-color,
					var(--darkreader-text--darkreader-text--heading-2-color, var(--darkreader-text--darkreader-text--headings-color)));
					--darkreader-inline-bgcolor: transparent;"
					data-darkreader-inline-color="" data-darkreader-inline-bgcolor="">
						Índice
					</span>
					<a class="anchor-link" style="
					font-family: var(--fontFamily);
					font-size: var(--fontSize);
					font-style: var(--fontStyle, inherit);
					font-weight: var(--fontWeight);
					letter-spacing: var(--letterSpacing);
					text-transform: var(--textTransform);
					background-color: transparent;
					--darkreader-inline-bgcolor: transparent;"
					href="#Índice" data-darkreader-inline-bgcolor="">
					</a>
				</h2>
			<!-- Close div index header -->
			</div>
			<!-- Index body -->
			<div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Quantization-by-layers-index">
									<a class="anchor-link" href="#Quantization-by-layers">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Quantization by layers</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Optimal-brain-quantization-(OBQ)-index">
									<a class="anchor-link" href="#Optimal-brain-quantization-(OBQ)">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Optimal brain quantization (OBQ)</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="GPTQ-Algorithm-index">
									<a class="anchor-link" href="#GPTQ-Algorithm">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">GPTQ Algorithm</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Step-1:-Arbitrary-order-information-index">
									<a class="anchor-link" href="#Step-1:-Arbitrary-order-information">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Step 1: Arbitrary order information</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Step-2:-Lazy-batch-updates-index">
									<a class="anchor-link" href="#Step-2:-Lazy-batch-updates">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Step 2: Lazy batch updates</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Step-3:-Cholesky-reformulation-index">
									<a class="anchor-link" href="#Step-3:-Cholesky-reformulation">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Step 3: Cholesky reformulation</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="GPTQ-Results-index">
									<a class="anchor-link" href="#GPTQ-Results">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">GPTQ Results</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Extreme-quantization-index">
									<a class="anchor-link" href="#Extreme-quantization">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Extreme quantization</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Dynamic-discounting-in-inference-index">
									<a class="anchor-link" href="#Dynamic-discounting-in-inference">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Dynamic discounting in inference</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Speed-of-inference-index">
									<a class="anchor-link" href="#Speed-of-inference">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Speed of inference</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Libraries-index">
									<a class="anchor-link" href="#Libraries">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Libraries</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Quantization-of-a-model-index">
									<a class="anchor-link" href="#Quantization-of-a-model">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Quantization of a model</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Unquantized-model-inference-index">
									<a class="anchor-link" href="#Unquantized-model-inference">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Unquantized model inference</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Quantization-of-the-model-to-4-bits-index">
									<a class="anchor-link" href="#Quantization-of-the-model-to-4-bits">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Quantization of the model to 4 bits</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Quantization-of-the-model-to-3-bits-index">
									<a class="anchor-link" href="#Quantization-of-the-model-to-3-bits">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Quantization of the model to 3 bits</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Quantization-of-the-model-to-2-bits-index">
									<a class="anchor-link" href="#Quantization-of-the-model-to-2-bits">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Quantization of the model to 2 bits</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h3 id="Quantization-of-the-model-to-1-bit-index">
									<a class="anchor-link" href="#Quantization-of-the-model-to-1-bit">
										<p style="margin-left: 40px; font-size: 12px; line-height: 1.5;">Quantization of the model to 1 bit</p>
									</a>
								</h3>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Quantization-summary-index">
									<a class="anchor-link" href="#Quantization-summary">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Quantization summary</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Loading-of-the-saved-model-index">
									<a class="anchor-link" href="#Loading-of-the-saved-model">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Loading of the saved model</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
				<!-- Open div header -->
				<div class="cell border-box-sizing text_cell rendered">
					<div class="prompt input_prompt">
						<div class="inner_cell">
							<div class="text_cell_render border-box-sizing rendered_html">
								<h2 id="Loading-of-the-model-uploaded-to-the-hub-index">
									<a class="anchor-link" href="#Loading-of-the-model-uploaded-to-the-hub">
										<p style="margin-left: 0px; font-size: 12px; line-height: 1.5;">Loading of the model uploaded to the hub</p>
									</a>
								</h2>
							</div>
						</div>
					</div>
				</div>
			</div>
		<!-- Close div indice -->
		</div>
 
		<!-- Content -->
		<div class="contenido">
		<div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><a href="https://colab.research.google.com/github/maximofn/portafolio/blob/main/posts/notebooks_translated/2024-07-27-GPTQ_EN.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h1 id="GPTQ:-Accurate-Post-Training-Quantization-for-Generative-Pre-trained-Transformers">
								<a class="anchor-link" href="#GPTQ:-Accurate-Post-Training-Quantization-for-Generative-Pre-trained-Transformers">
									<p style="margin-left: 0px">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</p>
								</a>
							</h1>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In the paper <a href="https://arxiv.org/abs/2210.17323" target="_blank">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> the need to create a post-training quantization method that does not degrade the quality of the model is exposed. In this post we have seen the method <a href="https://maximofn.com/llm-int8/" target="_blank">llm.int8()</a> that quantizes to INT8 some vectors of the weight matrices, as long as none of their values exceeds a threshold value, which is fine, but they do not quantize all the weights of the model. In this paper they propose a method that quantizes all the weights of the model to 4 and 3 bits, without degrading the quality of the model. This saves considerable memory, not only because all the weights are quantized, but also because it is done at 4, 3 bits (and even 1 and 2 bits under certain conditions), instead of 8 bits.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">This notebook has been automatically translated to make it accessible to more people, please let me know if you see any typos.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">## Works on which it is based</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Quantization-by-layers">
								<a class="anchor-link" href="#Quantization-by-layers">
									<p style="margin-left: 20px">Quantization by layers</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">On the one hand, they are based on the works <code><b>Nagel et al., 2020</b></code>; <code><b>Wang et al., 2020</b></code>; <code><b>Hubara et al., 2021</b></code> and <code><b>Frantar et al., 2022</b></code>, which propose to quantize the weights of the layers of a neural network to 4 and 3 bits, without degrading the quality of the model.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Having a data set <code><b>m</b></code> coming from a dataset, to each layer <code><b>l</b></code> you put the data and you get the output of the weights <code><b>W</b></code> of that layer. So what we do is to look for new quantized <code><b>Ŵ</b></code> weights that minimize the quadratic error in relation to the output of the total accuracy layer</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><code><b>argmin_Ŵ||WX- ŴX|||^2</b></code></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The values of <code><b>Ŵ</b></code> are set before performing the quantization process and during the process, each parameter of <code><b>Ŵ</b></code> can change its value independently without depending on the value of the other parameters of <code><b>Ŵ</b></code>.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Optimal-brain-quantization-(OBQ)">
								<a class="anchor-link" href="#Optimal-brain-quantization-(OBQ)">
									<p style="margin-left: 20px">Optimal brain quantization (OBQ)</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In the <code><b>OBQ</b></code> work of <code><b>Frantar et al., 2022</b></code> they optimize the above layered quantization process, making it up to 3 times faster. This helps with large models, as quantizing a large model can take a long time.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The <code><b>OBQ</b></code> method is an approach to solve the layered quantization problem in language models. <code><b>OBQ</b></code> starts from the idea that the quadratic error can be decomposed into the sum of individual errors for each row of the weight matrix. The method then quantizes each weight independently, always updating the unquantized weights to compensate for the error incurred by the quantization.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The method is capable of quantifying medium-sized models in reasonable times, but as it is a cubic complexity algorithm it is extremely expensive to apply to models with billions of parameters.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="GPTQ-Algorithm">
								<a class="anchor-link" href="#GPTQ-Algorithm">
									<p style="margin-left: 10px">GPTQ Algorithm</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Step-1:-Arbitrary-order-information">
								<a class="anchor-link" href="#Step-1:-Arbitrary-order-information">
									<p style="margin-left: 20px">Step 1: Arbitrary order information</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In <code><b>OBQ</b></code> they searched for the row of weights that created the lowest mean square error to quantize, but they realized that doing it randomly did not increase the final mean square error much. So instead of looking for the row that minimizes the mean square error, which created a cubic complexity in the algorithm, it is always done in the same order. Thanks to this, the execution time of the quantization algorithm is greatly reduced.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Step-2:-Lazy-batch-updates">
								<a class="anchor-link" href="#Step-2:-Lazy-batch-updates">
									<p style="margin-left: 20px">Step 2: Lazy batch updates</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As the update of the weights is done row by row, it makes it a slow process and does not take full advantage of the hardware. Therefore, they propose to perform the updates in batches of <code><b>B=128</b></code> rows. This makes better use of the hardware and reduces the execution time of the algorithm.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Step-3:-Cholesky-reformulation">
								<a class="anchor-link" href="#Step-3:-Cholesky-reformulation">
									<p style="margin-left: 20px">Step 3: Cholesky reformulation</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The problem with doing batch updates is that, due to the large scale of the models, numerical errors can occur that affect the accuracy of the algorithm. Specifically, undefined matrices can be obtained, causing the algorithm to update the remaining weights in the wrong directions, resulting in very poor quantization.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">To solve this, the authors of the paper propose to use a Cholesky reformulation, which is a more numerically stable method.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="GPTQ-Results">
								<a class="anchor-link" href="#GPTQ-Results">
									<p style="margin-left: 10px">GPTQ Results</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Below are two graphs with the perplexity measure in the <code><b>WikiText2</b></code> dataset for all sizes of the OPT and BLOOM models. It can be seen that with the RTN quantization technique, the perplexity at some sizes increases a lot, while with GPTQ it remains similar to that obtained with the FP16 model.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="https://maximofn.com/wp-content/uploads/2024/07/GPTQ-figure1.webp" alt="GPTQ-figure1"></p></p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Other plots are shown below, but with the measurement of the accuracy in the <code><b>LAMBADA</b></code> dataset. The same thing happens, while GPTQ remains similar to that obtained with FP16, other quantization methods greatly degrade the quality of the model.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;"><p align="center"><img src="https://maximofn.com/wp-content/uploads/2024/07/GPTQ-figure3.webp" alt="GPTQ-figure3"></p></p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Extreme-quantization">
								<a class="anchor-link" href="#Extreme-quantization">
									<p style="margin-left: 10px">Extreme quantization</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In the previous graphs we have shown the results of quantizing the model to 3 and 4 bits, but we can quantize them to 2 bits, and even to 1 bit.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">By modifying the size of the batches when using the algorithm we can obtain good results quantizing both the model</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<table>
								<tr>
									<th>Modelo </th>
									<th> FP16 </th>
									<th> g128 </th>
									<th> g64 </th>
									<th> g32 </th>
									<th> 3 bits</th>
								</tr>
								<tr>
									<td>OPT-175B </td>
									<td> 8.34 </td>
									<td> 9.58 </td>
									<td> 9.18 </td>
									<td> 8.94 </td>
									<td> 8.68</td>
								</tr>
								<tr>
									<td>BLOOM </td>
									<td> 8.11 </td>
									<td> 9.55 </td>
									<td> 9.17 </td>
									<td> 8.83 </td>
									<td> 8.64</td>
								</tr>
							</table>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In the table above you can see the result of the perplexity in the <code><b>WikiText2</b></code> dataset for the <code><b>OPT-175B</b></code> and <code><b>BLOOM</b></code> models quantized to 3 bits. It can be seen that as smaller batches are used, the perplexity decreases, which means that the quality of the quantized model is better. But it has the problem that the algorithm takes longer to run.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Dynamic-discounting-in-inference">
								<a class="anchor-link" href="#Dynamic-discounting-in-inference">
									<p style="margin-left: 10px">Dynamic discounting in inference</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">During the inference, something called <code><b>dynamic dequantization</b></code> is performed in order to perform the inference. Each layer is dequantized as it is passed through.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">To do this, they developed a kernel that de-quantifies the matrices and performs the matrix products. Although the quantization consumes more computation, the kernel has to access much less memory, which generates significant speedups.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The inference is performed in FP16 by discounting the weights as you go through the layers and the activation function of each layer is also performed in FP16. Although this means that more calculations have to be done, because the weights have to be discounted, these calculations make the overall process faster, because less data has to be fetched from memory. The weights have to be fetched from memory in fewer bits, so in the end, in matrices of many parameters it saves a lot of data. The bottleneck is usually in fetching the data from memory, so even if you have to do more calculations, in the end the inference is faster.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Speed-of-inference">
								<a class="anchor-link" href="#Speed-of-inference">
									<p style="margin-left: 10px">Speed of inference</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The authors of the paper performed a test quantizing the BLOOM-175B model to 3 bits, which occupied about 63 GB of VRAM memory, including the embeddings and the output layer that are kept in FP16. In addition maintaining the 2048 token context window consumes about 9 GB of memory, totaling about 72 GB of VRAM memory. They quantized in 3 bits and not 4 bits to be able to perform this experiment and to be able to fit the model on a single Nvidia A100 GPU with 80 GB of VRAM memory.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">For comparison, normal FP16 inference requires about 350 GB of VRAM memory, which is equivalent to 5 Nvidia A100 GPUs with 80 GB of VRAM memory. And 8-bit quantizing inference using <a href="https://maximofn.com/llm-int8/" target="_blank">llm.int8()</a> requires 3 such GPUs.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Below is a table with the model inference in FP16 and quantized to 3 bits and Nvidia A100 GPUs with 80 GB of VRAM memory and Nvidia A6000 with 48 GB of VRAM memory.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<table>
								<tr>
									<th>GPU (VRAM)</th>
									<th> Average time per token in FP16 (ms) </th>
									<th> Average time per token in 3 bit (ms) </th>
									<th> Acceleration </th>
									<th> Reduction of GPUs required </th>
								</tr>
								<tr>
									<td>A6000 (48GB) </td>
									<td> 589 </td>
									<td> 130 </td>
									<td> ×4.53 </td>
									<td> 8→ 2</td>
								</tr>
								<tr>
									<td>A100 (80GB) </td>
									<td> 230 </td>
									<td> 71 </td>
									<td> 71 ×3.24 </td>
									<td> 5→ 1</td>
								</tr>
							</table>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">For example, using the kernels, the 3-bit OPT-175B model runs on a single A100 (instead of 5) and is approximately 3.25 times faster than the FP16 version in terms of average time per token.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The NVIDIA A6000 GPU has a much lower memory bandwidth, making this strategy even more effective: running the 3-bit OPT-175B model on 2 A6000 GPUs (instead of 8) is approximately 4.53 times faster than the FP16 version.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Libraries">
								<a class="anchor-link" href="#Libraries">
									<p style="margin-left: 10px">Libraries</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The authors of the paper implemented the <a href="https://github.com/IST-DASLab/gptq" target="_blank">GPTQ</a> library. Other libraries were created such as <a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa" target="_blank">GPTQ-for-LLaMa</a>, <a href="https://github.com/turboderp/exllama" target="_blank">exllama</a> and <a href="https://github.com/ggerganov/llama.cpp/" target="_blank">llama.cpp</a>. However these libraries focus only on the llama architecture, so the <a href="https://github.com/AutoGPTQ/AutoGPTQ" target="_blank">AutoGPTQ</a> library gained the most popularity because it has a wider coverage of architectures.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Therefore, this library <a href="https://github.com/AutoGPTQ/AutoGPTQ" target="_blank">AutoGPTQ</a> was integrated by means of an API inside the <a href="https://maximofn.com/hugging-face-transformers/" target="_blank">transformers</a> library. In order to use it, it is necessary to install it as indicated in the <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation" target="_blank">Installation</a> section of its repository and to have the <a href="https://maximofn.com/hugging-face-optimun/" target="_blank">optimun</a> library installed.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In addition to what is indicated in the <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation" target="_blank">Installation</a> section of your repository, it is also advisable to do the following:</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 80px;"><code>git clone https://github.com/PanQiWei/AutoGPTQ<br>cd AutoGPTQ<br>pip install .<br></code></p>
						</div>
					</div>
				</div>
			</div>
	
	
	
	
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">For the GPU quantization kernels developed by the authors of the paper to be installed.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Quantization-of-a-model">
								<a class="anchor-link" href="#Quantization-of-a-model">
									<p style="margin-left: 10px">Quantization of a model</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's see how to quantize a model with the <a href="https://maximofn.com/hugging-face-optimun/" target="_blank">optimun</a> library and the <a href="https://github.com/AutoGPTQ/AutoGPTQ" target="_blank">AutoGPTQ</a> API.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Unquantized-model-inference">
								<a class="anchor-link" href="#Unquantized-model-inference">
									<p style="margin-left: 20px">Unquantized model inference</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We are going to quantize the <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct" target="_blank">meta-flame/Meta-Flame-3-8B-Instruct</a> model which as its name indicates is an 8B parameter model, so in FP16 we would need 16 GB of VRAM memory. First we run the model to see the memory it occupies and the output it generates</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As to use this model we have to ask permission to Meta, we log in to HuggingFace to download the tokenizer and the model.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">huggingface_hub</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">notebook_login</span></p>
<p></p>
<p><span style="color: #6b97e8;">notebook_login</span><span style="color: #e3e11d;">()</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Instantiate the tokenizer and the model</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">AutoTokenizer</span></p>
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">torch</span></p>
<p></p>
<p><span style="color: #6b97e8;">device</span> <span>=</span> <span style="color: #6b97e8;">torch</span><span>.</span><span style="color: #6b97e8;">device</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a34;">&quot;cuda&quot;</span> <span style="color: #a04cc1;">if</span> <span style="color: #6b97e8;">torch</span><span>.</span><span style="color: #6b97e8;">cuda</span><span>.</span><span style="color: #6b97e8;">is_available</span><span style="color: #e3e11d;">()</span> <span style="color: #a04cc1;">else</span> <span style="color: #7e7a34;">&quot;cpu&quot;</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">checkpoint</span> <span>=</span> <span style="color: #7e7a34;">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span></p>
<p></p>
<p><span style="color: #6b97e8;">tokenizer</span> <span>=</span> <span style="color: #6b97e8;">AutoTokenizer</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">model</span> <span>=</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">)</span><span>.</span><span style="color: #6b97e8;">half</span><span style="color: #e3e11d;">()</span><span>.</span><span style="color: #6b97e8;">to</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">device</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's take a look at the memory occupied in FP16</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">model_memory</span> <span>=</span> <span style="color: #6b97e8;">model</span><span>.</span><span style="color: #6b97e8;">get_memory_footprint</span><span style="color: #e3e11d;">()</span><span>/</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a38;">1024</span><span>**</span><span style="color: #7e7a38;">3</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Model memory: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">model_memory</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> GB&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Model memory: 14.96 GB</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see that it occupies almost 15 GB, more or less the 16 GB that we had said it should occupy, but why this difference? Surely this model does not have exactly 8B of parameters, but it has a little less, but when indicating the number of parameters it is rounded to 8B.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We make an inference to see how it does it and how long it takes.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">input_tokens</span> <span>=</span> <span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a34;">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">return_tensors</span><span>=</span><span style="color: #7e7a34;">&quot;pt&quot;</span><span style="color: #e3e11d;">)</span><span>.</span><span style="color: #6b97e8;">to</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">model</span><span>.</span><span style="color: #6b97e8;">device</span><span style="color: #e3e11d;">)</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">max_new_tokens</span> <span>=</span> <span style="color: #7e7a38;">50</span></p>
<p><span style="color: #6b97e8;">outputs</span> <span>=</span> <span style="color: #6b97e8;">model</span><span>.</span><span style="color: #6b97e8;">generate</span><span style="color: #e3e11d;">(</span></p>
<p>    <span style="color: #6b97e8;">input_ids</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">attention_mask</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">attention_mask</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">max_length</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span>.</span><span style="color: #6b97e8;">shape</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">1</span><span style="color: #e3e11d;">]</span> <span>+</span> <span style="color: #6b97e8;">max_new_tokens</span><span style="color: #e3e11d;">,</span></p>
<p><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">decode</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">outputs</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">0</span><span style="color: #e3e11d;">],</span> <span style="color: #6b97e8;">skip_special_tokens</span><span>=</span><span style="color: #7f6e38;">True</span><span style="color: #e3e11d;">))</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Inference time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span><span style="color: #7f6e38;"> </span><span>-</span><span style="color: #7f6e38;"> </span><span style="color: #6b97e8;">t0</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Hello my name is Maximo and I am a Machine Learning Engineer at a startup in the Bay Area. I am passionate about building AI systems that can help humans make better decisions and improve their lives.</p><p>I have a background in computer science and mathematics, and I have been working with machine learning for several years. I</p><p>Inference time: 4.14 s</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Quantization-of-the-model-to-4-bits">
								<a class="anchor-link" href="#Quantization-of-the-model-to-4-bits">
									<p style="margin-left: 20px">Quantization of the model to 4 bits</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's quantize it to 4 bits. I restart the notebook to avoid memory problems, so we log into Hugging Face again.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">huggingface_hub</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">notebook_login</span></p>
<p></p>
<p><span style="color: #6b97e8;">notebook_login</span><span style="color: #e3e11d;">()</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">First I create the tokenizer</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoTokenizer</span></p>
<p></p>
<p><span style="color: #6b97e8;">checkpoint</span> <span>=</span> <span style="color: #7e7a34;">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span></p>
<p><span style="color: #6b97e8;">tokenizer</span> <span>=</span> <span style="color: #6b97e8;">AutoTokenizer</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now we create the quantization configuration. As we have said, this algorithm calculates the error of the quantized weights over the original weights based on inputs from a dataset, so in the configuration we have to pass it with which dataset we want to quantize the model.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The available defaults are <code><b>wikitext2</b></code>,<code><b>c4</b></code>,<code><b>c4-new</b></code>,<code><b>ptb</b></code> and <code><b>ptb-new</b></code>.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We can also create a dataset ourselves from a list of strings</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 80px;"><code>dataset = ["auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."]<br></code></p>
						</div>
					</div>
				</div>
			</div>
	
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We also have to tell it the number of bits that the quantized model has by means of the <code><b>bits</b></code> parameter.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">GPTQConfig</span></p>
<p></p>
<p><span style="color: #6b97e8;">quantization_config</span> <span>=</span> <span style="color: #6b97e8;">GPTQConfig</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">bits</span><span>=</span><span style="color: #7e7a38;">4</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">dataset</span> <span>=</span> <span style="color: #7e7a34;">&quot;c4&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">tokenizer</span><span>=</span><span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We quantify the model</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span></p>
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">model_4bits</span> <span>=</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">device_map</span><span>=</span><span style="color: #7e7a34;">&quot;auto&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">quantization_config</span><span>=</span><span style="color: #6b97e8;">quantization_config</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">t_quantization</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span> <span>-</span> <span style="color: #6b97e8;">t0</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Quantization time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">t_quantization</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s = </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">t_quantization</span><span>/</span><span style="color: #7e7a38;">60</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> min&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Quantization time: 1932.09 s = 32.20 min</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As the quantization process calculates the smallest error between the quantized weights and the original weights by passing inputs through each layer, the quantization process takes time. In this case it took about half an hour</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's see how much memory it occupies now</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">model_4bits_memory</span> <span>=</span> <span style="color: #6b97e8;">model_4bits</span><span>.</span><span style="color: #6b97e8;">get_memory_footprint</span><span style="color: #e3e11d;">()</span><span>/</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a38;">1024</span><span>**</span><span style="color: #7e7a38;">3</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Model memory: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">model_4bits_memory</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> GB&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Model memory: 5.34 GB</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Here we can see a benefit of quantization. While the original model occupied about 15 GB of VRAM, now the quantized model occupies about 5 GB, almost a third of the original size.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We make the inference and see how long it takes</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">input_tokens</span> <span>=</span> <span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a34;">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">return_tensors</span><span>=</span><span style="color: #7e7a34;">&quot;pt&quot;</span><span style="color: #e3e11d;">)</span><span>.</span><span style="color: #6b97e8;">to</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">model_4bits</span><span>.</span><span style="color: #6b97e8;">device</span><span style="color: #e3e11d;">)</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">max_new_tokens</span> <span>=</span> <span style="color: #7e7a38;">50</span></p>
<p><span style="color: #6b97e8;">outputs</span> <span>=</span> <span style="color: #6b97e8;">model_4bits</span><span>.</span><span style="color: #6b97e8;">generate</span><span style="color: #e3e11d;">(</span></p>
<p>    <span style="color: #6b97e8;">input_ids</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">attention_mask</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">attention_mask</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">max_length</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span>.</span><span style="color: #6b97e8;">shape</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">1</span><span style="color: #e3e11d;">]</span> <span>+</span> <span style="color: #6b97e8;">max_new_tokens</span><span style="color: #e3e11d;">,</span></p>
<p><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">decode</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">outputs</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">0</span><span style="color: #e3e11d;">],</span> <span style="color: #6b97e8;">skip_special_tokens</span><span>=</span><span style="color: #7f6e38;">True</span><span style="color: #e3e11d;">))</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Inference time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span><span style="color: #7f6e38;"> </span><span>-</span><span style="color: #7f6e38;"> </span><span style="color: #6b97e8;">t0</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I am passionate about developing innovative solutions that can positively impact society. I am excited to be a part of this community and to learn from and contribute to the discussions here. I am particularly</p><p>Inference time: 2.34 s</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The unquantized model took 4.14 seconds, while now quantized to 4 bits it has taken 2.34 seconds and also generated the text well. We have managed to reduce the inference by almost half.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As the size of the quantized model is almost one third of the model in FP16, we could think that the inference speed should be about three times faster with the quantized model. But we have to remember that in each layer the weights are quantized and the computations are performed in FP16, so we only managed to reduce the inference time by half and not by one third.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now we save the model</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">save_folder</span> <span>=</span> <span style="color: #7e7a34;">&quot;./model_4bits/&quot;</span></p>
<p><span style="color: #6b97e8;">model_4bits</span><span>.</span><span style="color: #6b97e8;">save_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">save_folder</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">save_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">save_folder</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>('./model_4bits/tokenizer_config.json',</p><p> './model_4bits/special_tokens_map.json',</p><p> './model_4bits/tokenizer.json')</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">And we upload it to the hub</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">repo_id</span> <span>=</span> <span style="color: #7e7a34;">&quot;Llama-3-8B-Instruct-GPTQ-4bits&quot;</span></p>
<p><span style="color: #6b97e8;">commit_message</span> <span>=</span> <span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;AutoGPTQ model for </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">bits</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">bits, gr</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">group_size</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">, desc_act=</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">desc_act</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">&quot;</span></p>
<p><span style="color: #6b97e8;">model_4bits</span><span>.</span><span style="color: #6b97e8;">push_to_hub</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">repo_id</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">commit_message</span><span>=</span><span style="color: #6b97e8;">commit_message</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/44cfdcad78db260122943d3f57858c1b840bda17', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False', commit_description='', oid='44cfdcad78db260122943d3f57858c1b840bda17', pr_url=None, pr_revision=None, pr_num=None)</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We also upload the tokenizer. Although we have not changed the tokenizer, we upload it because if a person downloads our model from the hub, he/she does not need to know which tokenizer we have used, so he/she will probably want to download the model and the tokenizer together. We can indicate in the model card which tokenizer we have used to download it, but most likely they will not read the model card, try to download the tokenizer, get an error and not know what to do. So we upload it to save us that problem.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">repo_id</span> <span>=</span> <span style="color: #7e7a34;">&quot;Llama-3-8B-Instruct-GPTQ-4bits&quot;</span></p>
<p><span style="color: #6b97e8;">commit_message</span> <span>=</span> <span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Tokenizers for </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">bits</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">bits, gr</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">group_size</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">, desc_act=</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">desc_act</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">&quot;</span></p>
<p><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">push_to_hub</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">repo_id</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">commit_message</span><span>=</span><span style="color: #6b97e8;">commit_message</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4', commit_message='Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False', commit_description='', oid='75600041ca6e38b5f1fb912ad1803b66656faae4', pr_url=None, pr_revision=None, pr_num=None)</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Quantization-of-the-model-to-3-bits">
								<a class="anchor-link" href="#Quantization-of-the-model-to-3-bits">
									<p style="margin-left: 20px">Quantization of the model to 3 bits</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's quantize it to 3 bits. I restart the notebook to avoid memory problems and log in to Hugging Face again.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">huggingface_hub</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">notebook_login</span></p>
<p></p>
<p><span style="color: #6b97e8;">notebook_login</span><span style="color: #e3e11d;">()</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">First I create the tokenizer</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoTokenizer</span></p>
<p></p>
<p><span style="color: #6b97e8;">checkpoint</span> <span>=</span> <span style="color: #7e7a34;">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span></p>
<p><span style="color: #6b97e8;">tokenizer</span> <span>=</span> <span style="color: #6b97e8;">AutoTokenizer</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We create the quantization configuration, now we indicate that we want to quantize to 3 bits</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">GPTQConfig</span></p>
<p></p>
<p><span style="color: #6b97e8;">quantization_config</span> <span>=</span> <span style="color: #6b97e8;">GPTQConfig</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">bits</span><span>=</span><span style="color: #7e7a38;">3</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">dataset</span> <span>=</span> <span style="color: #7e7a34;">&quot;c4&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">tokenizer</span><span>=</span><span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We quantify the model</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span></p>
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">model_3bits</span> <span>=</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">device_map</span><span>=</span><span style="color: #7e7a34;">&quot;auto&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">quantization_config</span><span>=</span><span style="color: #6b97e8;">quantization_config</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">t_quantization</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span> <span>-</span> <span style="color: #6b97e8;">t0</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Quantization time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">t_quantization</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s = </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">t_quantization</span><span>/</span><span style="color: #7e7a38;">60</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> min&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Quantization time: 1912.69 s = 31.88 min</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">As before, it has taken about half an hour.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's see how much memory it occupies now</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">model_3bits_memory</span> <span>=</span> <span style="color: #6b97e8;">model_3bits</span><span>.</span><span style="color: #6b97e8;">get_memory_footprint</span><span style="color: #e3e11d;">()</span><span>/</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a38;">1024</span><span>**</span><span style="color: #7e7a38;">3</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Model memory: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">model_3bits_memory</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> GB&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Model memory: 4.52 GB</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">The memory occupied by the 3-bit model is also almost 5 GB. The 4-bit model occupied 5.34 GB, while now in 3-bit it occupies 4.52 GB, so we have managed to reduce the size of the model a little more.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We make the inference and see how long it takes</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">input_tokens</span> <span>=</span> <span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a34;">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">return_tensors</span><span>=</span><span style="color: #7e7a34;">&quot;pt&quot;</span><span style="color: #e3e11d;">)</span><span>.</span><span style="color: #6b97e8;">to</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">model_3bits</span><span>.</span><span style="color: #6b97e8;">device</span><span style="color: #e3e11d;">)</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">max_new_tokens</span> <span>=</span> <span style="color: #7e7a38;">50</span></p>
<p><span style="color: #6b97e8;">outputs</span> <span>=</span> <span style="color: #6b97e8;">model_3bits</span><span>.</span><span style="color: #6b97e8;">generate</span><span style="color: #e3e11d;">(</span></p>
<p>    <span style="color: #6b97e8;">input_ids</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">attention_mask</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">attention_mask</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">max_length</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span>.</span><span style="color: #6b97e8;">shape</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">1</span><span style="color: #e3e11d;">]</span> <span>+</span> <span style="color: #6b97e8;">max_new_tokens</span><span style="color: #e3e11d;">,</span></p>
<p><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">decode</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">outputs</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">0</span><span style="color: #e3e11d;">],</span> <span style="color: #6b97e8;">skip_special_tokens</span><span>=</span><span style="color: #7f6e38;">True</span><span style="color: #e3e11d;">))</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Inference time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span><span style="color: #7f6e38;"> </span><span>-</span><span style="color: #7f6e38;"> </span><span style="color: #6b97e8;">t0</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Hello my name is Maximo and I am a Machine Learning Engineer at Google. I am excited to be here today to talk about my work in the field of Machine Learning and to share some of the insights I have gained through my experiences.</p><p>I am a Machine Learning Engineer at Google, and I am excited to be</p><p>Inference time: 2.89 s</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Although the output on 3 bits is good, now the inference time has been 2.89 seconds, while on 4 bits it was 2.34 seconds. More testing should be done to see if it always takes less time on 4 bits, or maybe the difference is so small that sometimes the 3-bit inference is faster and sometimes the 4-bit inference is faster.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">In addition, although the output makes sense, it starts to become repetitive.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Save the model</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">save_folder</span> <span>=</span> <span style="color: #7e7a34;">&quot;./model_3bits/&quot;</span></p>
<p><span style="color: #6b97e8;">model_3bits</span><span>.</span><span style="color: #6b97e8;">save_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">save_folder</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">save_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">save_folder</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>('./model_3bits/tokenizer_config.json',</p><p> './model_3bits/special_tokens_map.json',</p><p> './model_3bits/tokenizer.json')</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">And we upload it to the hub</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">repo_id</span> <span>=</span> <span style="color: #7e7a34;">&quot;Llama-3-8B-Instruct-GPTQ-3bits&quot;</span></p>
<p><span style="color: #6b97e8;">commit_message</span> <span>=</span> <span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;AutoGPTQ model for </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">bits</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">bits, gr</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">group_size</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">, desc_act=</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">desc_act</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">&quot;</span></p>
<p><span style="color: #6b97e8;">model_3bits</span><span>.</span><span style="color: #6b97e8;">push_to_hub</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">repo_id</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">commit_message</span><span>=</span><span style="color: #6b97e8;">commit_message</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-3bits/commit/422fd94a031234c10224ddbe09c0e029a5e9c01f', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 3bits, gr128, desc_act=False', commit_description='', oid='422fd94a031234c10224ddbe09c0e029a5e9c01f', pr_url=None, pr_revision=None, pr_num=None)</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We also upload the tokenizer</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">repo_id</span> <span>=</span> <span style="color: #7e7a34;">&quot;Llama-3-8B-Instruct-GPTQ-3bits&quot;</span></p>
<p><span style="color: #6b97e8;">commit_message</span> <span>=</span> <span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Tokenizers for </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">bits</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">bits, gr</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">group_size</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">, desc_act=</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">desc_act</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">&quot;</span></p>
<p><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">push_to_hub</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">repo_id</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">commit_message</span><span>=</span><span style="color: #6b97e8;">commit_message</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4', commit_message='Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False', commit_description='', oid='75600041ca6e38b5f1fb912ad1803b66656faae4', pr_url=None, pr_revision=None, pr_num=None)</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Quantization-of-the-model-to-2-bits">
								<a class="anchor-link" href="#Quantization-of-the-model-to-2-bits">
									<p style="margin-left: 20px">Quantization of the model to 2 bits</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's quantize it to 2 bits. I restart the notebook to avoid memory problems and log in again in Hugging Face</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">huggingface_hub</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">notebook_login</span></p>
<p></p>
<p><span style="color: #6b97e8;">notebook_login</span><span style="color: #e3e11d;">()</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">First I create the tokenizer</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoTokenizer</span></p>
<p></p>
<p><span style="color: #6b97e8;">checkpoint</span> <span>=</span> <span style="color: #7e7a34;">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span></p>
<p><span style="color: #6b97e8;">tokenizer</span> <span>=</span> <span style="color: #6b97e8;">AutoTokenizer</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We create the quantization configuration. Now we tell it to quantize to 2 bits. In addition we must indicate how many vectors of the weight matrix quantize at a time by means of the parameter <code><b>group_size</b></code>, before by default it had the value 128 and we did not touch it, but now when quantizing to 2 bits, to have less error we put a smaller value. If we leave it at 128, the quantized model would work very badly, in this case I am going to put a value of 16.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">GPTQConfig</span></p>
<p></p>
<p><span style="color: #6b97e8;">quantization_config</span> <span>=</span> <span style="color: #6b97e8;">GPTQConfig</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">bits</span><span>=</span><span style="color: #7e7a38;">2</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">dataset</span> <span>=</span> <span style="color: #7e7a34;">&quot;c4&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">tokenizer</span><span>=</span><span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">group_size</span><span>=</span><span style="color: #7e7a38;">16</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span></p>
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">model_2bits</span> <span>=</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">device_map</span><span>=</span><span style="color: #7e7a34;">&quot;auto&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">quantization_config</span><span>=</span><span style="color: #6b97e8;">quantization_config</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">t_quantization</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span> <span>-</span> <span style="color: #6b97e8;">t0</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Quantization time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">t_quantization</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s = </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">t_quantization</span><span>/</span><span style="color: #7e7a38;">60</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> min&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Quantization time: 1973.12 s = 32.89 min</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see that it has also taken about half an hour.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's see how much memory it occupies now</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">model_2bits_memory</span> <span>=</span> <span style="color: #6b97e8;">model_2bits</span><span>.</span><span style="color: #6b97e8;">get_memory_footprint</span><span style="color: #e3e11d;">()</span><span>/</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a38;">1024</span><span>**</span><span style="color: #7e7a38;">3</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Model memory: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">model_2bits_memory</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> GB&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Model memory: 4.50 GB</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">While quantized at 4 bits it occupied 5.34 GB and at 3 bits it occupied 4.52 GB, now quantized at 2 bits it occupies 4.50 GB, so we have managed to reduce the size of the model a little more.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We make the inference and see how long it takes</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">input_tokens</span> <span>=</span> <span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a34;">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">return_tensors</span><span>=</span><span style="color: #7e7a34;">&quot;pt&quot;</span><span style="color: #e3e11d;">)</span><span>.</span><span style="color: #6b97e8;">to</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">model_2bits</span><span>.</span><span style="color: #6b97e8;">device</span><span style="color: #e3e11d;">)</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">max_new_tokens</span> <span>=</span> <span style="color: #7e7a38;">50</span></p>
<p><span style="color: #6b97e8;">outputs</span> <span>=</span> <span style="color: #6b97e8;">model_2bits</span><span>.</span><span style="color: #6b97e8;">generate</span><span style="color: #e3e11d;">(</span></p>
<p>    <span style="color: #6b97e8;">input_ids</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">attention_mask</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">attention_mask</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">max_length</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span>.</span><span style="color: #6b97e8;">shape</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">1</span><span style="color: #e3e11d;">]</span> <span>+</span> <span style="color: #6b97e8;">max_new_tokens</span><span style="color: #e3e11d;">,</span></p>
<p><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">decode</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">outputs</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">0</span><span style="color: #e3e11d;">],</span> <span style="color: #6b97e8;">skip_special_tokens</span><span>=</span><span style="color: #7f6e38;">True</span><span style="color: #e3e11d;">))</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Inference time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span><span style="color: #7f6e38;"> </span><span>-</span><span style="color: #7f6e38;"> </span><span style="color: #6b97e8;">t0</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Hello my name is Maximo and I am a Machine Learning Engineer.  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #</p><p>Inference time: 2.92 s</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see that the output is already not good, and the inference time is 2.92 seconds, more or less the same as with 3 and 4 bits.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Save the model</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">save_folder</span> <span>=</span> <span style="color: #7e7a34;">&quot;./model_2bits/&quot;</span></p>
<p><span style="color: #6b97e8;">model_2bits</span><span>.</span><span style="color: #6b97e8;">save_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">save_folder</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">save_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">save_folder</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>('./model_2bits/tokenizer_config.json',</p><p> './model_2bits/special_tokens_map.json',</p><p> './model_2bits/tokenizer.json')</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We upload it to the hub</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">repo_id</span> <span>=</span> <span style="color: #7e7a34;">&quot;Llama-3-8B-Instruct-GPTQ-2bits&quot;</span></p>
<p><span style="color: #6b97e8;">commit_message</span> <span>=</span> <span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;AutoGPTQ model for </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">bits</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">bits, gr</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">group_size</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">, desc_act=</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">desc_act</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">&quot;</span></p>
<p><span style="color: #6b97e8;">model_2bits</span><span>.</span><span style="color: #6b97e8;">push_to_hub</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">repo_id</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">commit_message</span><span>=</span><span style="color: #6b97e8;">commit_message</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/13ede006ce0dbbd8aca54212e960eff98ea5ec63', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr16, desc_act=False', commit_description='', oid='13ede006ce0dbbd8aca54212e960eff98ea5ec63', pr_url=None, pr_revision=None, pr_num=None)</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h3 id="Quantization-of-the-model-to-1-bit">
								<a class="anchor-link" href="#Quantization-of-the-model-to-1-bit">
									<p style="margin-left: 20px">Quantization of the model to 1 bit</p>
								</a>
							</h3>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's quantize it to 1 bit. I restart the notebook to avoid memory problems and log in again in Hugging Face</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">huggingface_hub</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">notebook_login</span></p>
<p></p>
<p><span style="color: #6b97e8;">notebook_login</span><span style="color: #e3e11d;">()</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">First I create the tokenizer</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoTokenizer</span></p>
<p></p>
<p><span style="color: #6b97e8;">checkpoint</span> <span>=</span> <span style="color: #7e7a34;">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span></p>
<p><span style="color: #6b97e8;">tokenizer</span> <span>=</span> <span style="color: #6b97e8;">AutoTokenizer</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We create the quantization configuration, now we tell it to quantize to only 1 bit and also to use a <code><b>group_size</b></code> of 8</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">GPTQConfig</span></p>
<p></p>
<p><span style="color: #6b97e8;">quantization_config</span> <span>=</span> <span style="color: #6b97e8;">GPTQConfig</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">bits</span><span>=</span><span style="color: #7e7a38;">2</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">dataset</span> <span>=</span> <span style="color: #7e7a34;">&quot;c4&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">tokenizer</span><span>=</span><span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">group_size</span><span>=</span><span style="color: #7e7a38;">8</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span></p>
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">model_1bits</span> <span>=</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">device_map</span><span>=</span><span style="color: #7e7a34;">&quot;auto&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">quantization_config</span><span>=</span><span style="color: #6b97e8;">quantization_config</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">t_quantization</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span> <span>-</span> <span style="color: #6b97e8;">t0</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Quantization time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">t_quantization</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s = </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">t_quantization</span><span>/</span><span style="color: #7e7a38;">60</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> min&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Quantization time: 2030.38 s = 33.84 min</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see that it also takes about half an hour to quantize</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's see how much memory it occupies now</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">model_1bits_memory</span> <span>=</span> <span style="color: #6b97e8;">model_1bits</span><span>.</span><span style="color: #6b97e8;">get_memory_footprint</span><span style="color: #e3e11d;">()</span><span>/</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a38;">1024</span><span>**</span><span style="color: #7e7a38;">3</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Model memory: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">model_1bits_memory</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> GB&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Model memory: 5.42 GB</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see that in this case it occupies even more than quantized to 2 bits, 4.52 GB.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We make the inference and see how long it takes</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">input_tokens</span> <span>=</span> <span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a34;">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">return_tensors</span><span>=</span><span style="color: #7e7a34;">&quot;pt&quot;</span><span style="color: #e3e11d;">)</span><span>.</span><span style="color: #6b97e8;">to</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">model_1bits</span><span>.</span><span style="color: #6b97e8;">device</span><span style="color: #e3e11d;">)</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">max_new_tokens</span> <span>=</span> <span style="color: #7e7a38;">50</span></p>
<p><span style="color: #6b97e8;">outputs</span> <span>=</span> <span style="color: #6b97e8;">model_1bits</span><span>.</span><span style="color: #6b97e8;">generate</span><span style="color: #e3e11d;">(</span></p>
<p>    <span style="color: #6b97e8;">input_ids</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">attention_mask</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">attention_mask</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">max_length</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span>.</span><span style="color: #6b97e8;">shape</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">1</span><span style="color: #e3e11d;">]</span> <span>+</span> <span style="color: #6b97e8;">max_new_tokens</span><span style="color: #e3e11d;">,</span></p>
<p><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">decode</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">outputs</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">0</span><span style="color: #e3e11d;">],</span> <span style="color: #6b97e8;">skip_special_tokens</span><span>=</span><span style="color: #7f6e38;">True</span><span style="color: #e3e11d;">))</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Inference time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span><span style="color: #7f6e38;"> </span><span>-</span><span style="color: #7f6e38;"> </span><span style="color: #6b97e8;">t0</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Hello my name is Maximo and I am a Machine Learning Engineerimerszuimersimerspinsimersimersingoingoimersurosimersimersimersoleningoimersingopinsimersbirpinsimersimersimersorgeingoimersiringimersimersimersimersimersimersimersンディorge_REFERER ingest羊imersorgeimersimersendetingoШАhandsingo</p><p>Inference time: 3.12 s</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see that the output is very bad and also takes longer than when we have quantized to 2 bits.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Save the model</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">save_folder</span> <span>=</span> <span style="color: #7e7a34;">&quot;./model_1bits/&quot;</span></p>
<p><span style="color: #6b97e8;">model_1bits</span><span>.</span><span style="color: #6b97e8;">save_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">save_folder</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">save_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">save_folder</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>('./model_1bits/tokenizer_config.json',</p><p> './model_1bits/special_tokens_map.json',</p><p> './model_1bits/tokenizer.json')</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We upload it to the hub</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">repo_id</span> <span>=</span> <span style="color: #7e7a34;">&quot;Llama-3-8B-Instruct-GPTQ-1bits&quot;</span></p>
<p><span style="color: #6b97e8;">commit_message</span> <span>=</span> <span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;AutoGPTQ model for </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">bits</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">bits, gr</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">group_size</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">, desc_act=</span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">quantization_config</span><span>.</span><span style="color: #6b97e8;">desc_act</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;">&quot;</span></p>
<p><span style="color: #6b97e8;">model_1bits</span><span>.</span><span style="color: #6b97e8;">push_to_hub</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">repo_id</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">commit_message</span><span>=</span><span style="color: #6b97e8;">commit_message</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/e59ccffc03247e7dcc418f98b482cc02dc7a168d', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr8, desc_act=False', commit_description='', oid='e59ccffc03247e7dcc418f98b482cc02dc7a168d', pr_url=None, pr_revision=None, pr_num=None)</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Quantization-summary">
								<a class="anchor-link" href="#Quantization-summary">
									<p style="margin-left: 10px">Quantization summary</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Let's buy the quantization to 4, 3, 2 and 1 bits</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<table>
								<tr>
									<th>Bits </th>
									<th>Quantization time (min) </th>
									<th>Memory (GB) </th>
									<th>Inference time (s) </th>
									<th>Quality of the output </th>
								</tr>
								<tr>
									<td>FP16 </td>
									<td> 0 </td>
									<td> 14.96 </td>
									<td> 4.14 </td>
									<td> Good </td>
								</tr>
								<tr>
									<td>4 </td>
									<td> 32.20 </td>
									<td> 5.34 </td>
									<td> 2.34 </td>
									<td> Good </td>
								</tr>
								<tr>
									<td>3 </td>
									<td> 31.88 </td>
									<td> 4.52 </td>
									<td> 2.89 </td>
									<td> Good </td>
								</tr>
								<tr>
									<td>2 </td>
									<td> 32.89 </td>
									<td> 4.50 </td>
									<td> 2.92 </td>
									<td> Poor </td>
								</tr>
								<tr>
									<td>1 </td>
									<td> 33.84 </td>
									<td> 5.42 </td>
									<td> 3.12 </td>
									<td> Poor </td>
								</tr>
							</table>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">

						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Looking at this table we see that it does not make sense, in this example, to quantize to less than 4 bits.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Quantizing at 1 and 2 bits clearly does not make sense because the output quality is poor.</p>
						</div>
					</div>
				</div>
			</div>
	
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">But although the output when quantizing at 3 bits is good, it started to be repetitive, so in the long run, it would probably not be a good idea to use that model. Besides neither the savings in quantization time, the savings in VRAM and the savings in inference time is significant compared to quantizing to 4 bits.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Loading-of-the-saved-model">
								<a class="anchor-link" href="#Loading-of-the-saved-model">
									<p style="margin-left: 10px">Loading of the saved model</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now that we have compared the quantization of models, let's see how it would be done to load the 4-bit model that we have saved, since as we have seen, it is the best choice</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">First we load the tokenizer that we have used</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoTokenizer</span></p>
<p></p>
<p><span style="color: #6b97e8;">path</span> <span>=</span> <span style="color: #7e7a34;">&quot;./model_4bits&quot;</span></p>
<p><span style="color: #6b97e8;">tokenizer</span> <span>=</span> <span style="color: #6b97e8;">AutoTokenizer</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">path</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now we load the model we have saved</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span></p>
<p></p>
<p><span style="color: #6b97e8;">load_model_4bits</span> <span>=</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">path</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">device_map</span><span>=</span><span style="color: #7e7a34;">&quot;auto&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Loading checkpoint shards: 100%|██████████| 2/2 [00:00&lt;?, ?it/s]</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see the memory occupied by</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">load_model_4bits_memory</span> <span>=</span> <span style="color: #6b97e8;">load_model_4bits</span><span>.</span><span style="color: #6b97e8;">get_memory_footprint</span><span style="color: #e3e11d;">()</span><span>/</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a38;">1024</span><span>**</span><span style="color: #7e7a38;">3</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Model memory: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">load_model_4bits_memory</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> GB&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Model memory: 5.34 GB</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see that it occupies the same memory as when we quantized it, which is logical.</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We make the inference and see how long it takes</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">input_tokens</span> <span>=</span> <span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a34;">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">return_tensors</span><span>=</span><span style="color: #7e7a34;">&quot;pt&quot;</span><span style="color: #e3e11d;">)</span><span>.</span><span style="color: #6b97e8;">to</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">load_model_4bits</span><span>.</span><span style="color: #6b97e8;">device</span><span style="color: #e3e11d;">)</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">max_new_tokens</span> <span>=</span> <span style="color: #7e7a38;">50</span></p>
<p><span style="color: #6b97e8;">outputs</span> <span>=</span> <span style="color: #6b97e8;">load_model_4bits</span><span>.</span><span style="color: #6b97e8;">generate</span><span style="color: #e3e11d;">(</span></p>
<p>    <span style="color: #6b97e8;">input_ids</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">attention_mask</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">attention_mask</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">max_length</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span>.</span><span style="color: #6b97e8;">shape</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">1</span><span style="color: #e3e11d;">]</span> <span>+</span> <span style="color: #6b97e8;">max_new_tokens</span><span style="color: #e3e11d;">,</span></p>
<p><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">decode</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">outputs</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">0</span><span style="color: #e3e11d;">],</span> <span style="color: #6b97e8;">skip_special_tokens</span><span>=</span><span style="color: #7f6e38;">True</span><span style="color: #e3e11d;">))</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Inference time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span><span style="color: #7f6e38;"> </span><span>-</span><span style="color: #7f6e38;"> </span><span style="color: #6b97e8;">t0</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I have been working with machine learning models for several years. I am excited to be a part of this community and to share my knowledge and experience with others. I am particularly interested in</p><p>Inference time: 3.82 s</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see that the inference is good and it took 3.82 seconds, a little longer than when we quantized it. But as I said before, we would have to do this test many times and take an average.</p>
						</div>
					</div>
				</div>
			</div>
			<!-- Open div header -->
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<h2 id="Loading-of-the-model-uploaded-to-the-hub">
								<a class="anchor-link" href="#Loading-of-the-model-uploaded-to-the-hub">
									<p style="margin-left: 10px">Loading of the model uploaded to the hub</p>
								</a>
							</h2>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now we see how to load the 4-bit model that we have uploaded to the hub</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">First we load the tokenizer that we have uploaded</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoTokenizer</span></p>
<p></p>
<p><span style="color: #6b97e8;">checkpoint</span> <span>=</span> <span style="color: #7e7a34;">&quot;Maximofn/Llama-3-8B-Instruct-GPTQ-4bits&quot;</span></p>
<p><span style="color: #6b97e8;">tokenizer</span> <span>=</span> <span style="color: #6b97e8;">AutoTokenizer</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">Now we load the model we have saved</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">from</span> <span style="color: #4fcd7d;">transformers</span> <span style="color: #833aa0;">import</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span></p>
<p></p>
<p><span style="color: #6b97e8;">load_model_4bits</span> <span>=</span> <span style="color: #6b97e8;">AutoModelForCausalLM</span><span>.</span><span style="color: #6b97e8;">from_pretrained</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">checkpoint</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">device_map</span><span>=</span><span style="color: #7e7a34;">&quot;auto&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see the memory occupied by</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #6b97e8;">load_model_4bits_memory</span> <span>=</span> <span style="color: #6b97e8;">load_model_4bits</span><span>.</span><span style="color: #6b97e8;">get_memory_footprint</span><span style="color: #e3e11d;">()</span><span>/</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a38;">1024</span><span>**</span><span style="color: #7e7a38;">3</span><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Model memory: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">load_model_4bits_memory</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> GB&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Model memory: 5.34 GB</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">It also occupies the same memory</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We make the inference and see how long it takes</p>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing code_cell rendered">
				<div class="input">
					<div class="prompt input_prompt" style="margin-left: 20px;">Code:</div>
					<div class="inner_cell">
						<div class="input_area">
							<div class=" highlight hl-python3">
								<pre class="python" style="margin-left: 40px; line-height: 0%;font-family:monospace;">
<p><span style="color: #833aa0;">import</span> <span style="color: #4fcd7d;">time</span></p>
<p></p>
<p><span style="color: #6b97e8;">input_tokens</span> <span>=</span> <span style="color: #6b97e8;">tokenizer</span><span style="color: #e3e11d;">(</span><span style="color: #7e7a34;">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span style="color: #e3e11d;">,</span> <span style="color: #6b97e8;">return_tensors</span><span>=</span><span style="color: #7e7a34;">&quot;pt&quot;</span><span style="color: #e3e11d;">)</span><span>.</span><span style="color: #6b97e8;">to</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">load_model_4bits</span><span>.</span><span style="color: #6b97e8;">device</span><span style="color: #e3e11d;">)</span></p>
<p></p>
<p><span style="color: #6b97e8;">t0</span> <span>=</span> <span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span></p>
<p><span style="color: #6b97e8;">max_new_tokens</span> <span>=</span> <span style="color: #7e7a38;">50</span></p>
<p><span style="color: #6b97e8;">outputs</span> <span>=</span> <span style="color: #6b97e8;">load_model_4bits</span><span>.</span><span style="color: #6b97e8;">generate</span><span style="color: #e3e11d;">(</span></p>
<p>    <span style="color: #6b97e8;">input_ids</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">attention_mask</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">attention_mask</span><span style="color: #e3e11d;">,</span></p>
<p>    <span style="color: #6b97e8;">max_length</span><span>=</span><span style="color: #6b97e8;">input_tokens</span><span>.</span><span style="color: #6b97e8;">input_ids</span><span>.</span><span style="color: #6b97e8;">shape</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">1</span><span style="color: #e3e11d;">]</span> <span>+</span> <span style="color: #6b97e8;">max_new_tokens</span><span style="color: #e3e11d;">,</span></p>
<p><span style="color: #e3e11d;">)</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">tokenizer</span><span>.</span><span style="color: #6b97e8;">decode</span><span style="color: #e3e11d;">(</span><span style="color: #6b97e8;">outputs</span><span style="color: #e3e11d;">[</span><span style="color: #7e7a38;">0</span><span style="color: #e3e11d;">],</span> <span style="color: #6b97e8;">skip_special_tokens</span><span>=</span><span style="color: #7f6e38;">True</span><span style="color: #e3e11d;">))</span></p>
<p><span style="color: #dfd84a;">print</span><span style="color: #e3e11d;">(</span><span style="color: #1b1477;">f</span><span style="color: #7e7a34;">&quot;Inference time: </span><span style="color: #3b75c2;">{</span><span style="color: #6b97e8;">time</span><span>.</span><span style="color: #6b97e8;">time</span><span style="color: #e3e11d;">()</span><span style="color: #7f6e38;"> </span><span>-</span><span style="color: #7f6e38;"> </span><span style="color: #6b97e8;">t0</span><span style="color: #3b75c2;">:</span><span style="color: #7e7a34;">.2f</span><span style="color: #3b75c2;">}</span><span style="color: #7e7a34;"> s&quot;</span><span style="color: #e3e11d;">)</span></p>
								</pre>
							</div>
						</div>
					</div>
				</div>
				<div class="output_wrapper">
					<div class="output">
						<div class="output_area">
							<div class="prompt" style="margin-left: 20px;">Output:</div>
							<div class="output_subarea output_stream output_stdout output_text">
								<pre style="margin-left: 60px; line-height: 0%;"><p>Hello my name is Maximo and I am a Machine Learning Engineer with a passion for building innovative AI solutions. I have been working in the field of AI for over 5 years, and have gained extensive experience in developing and implementing machine learning models for various industries.</p><p>In my free time, I enjoy reading books on</p><p>Inference time: 3.81 s</p></pre>
							</div>
						</div>
					</div>
				</div>
			</div>
			<div class="cell border-box-sizing text_cell rendered">
				<div class="prompt input_prompt">
					<div class="inner_cell">
						<div class="text_cell_render border-box-sizing rendered_html">
							<p style="margin-left: 0px;">We see that the inference is also good and took 3.81 seconds.</p>
						</div>
					</div>
				</div>
			</div>
		</div>
	<!-- Close class contenido -->
	</div>
<!-- Close div notebook container -->
</div>
<!-- Close div notebook -->
</div>

