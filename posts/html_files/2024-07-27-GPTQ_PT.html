<section class="section-block-markdown-cell">
<h1 id="GPTQ:-Quantiza%C3%A7%C3%A3o-p%C3%B3s-treinamento-precisa-para-transformadores-pr%C3%A9-treinados-generativos">GPTQ: Quantização pós-treinamento precisa para transformadores pré-treinados generativos<a class="anchor-link" href="#GPTQ:-Quantiza%C3%A7%C3%A3o-p%C3%B3s-treinamento-precisa-para-transformadores-pr%C3%A9-treinados-generativos">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<p>No artigo <a href="https://arxiv.org/abs/2210.17323">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a>, é exposta a necessidade de criar um método de quantização pós-treinamento que não prejudique a qualidade do modelo. Nesta postagem, vimos o método <a href="https://maximofn.com/llm-int8/">llm.int8()</a> que quantiza para INT8 alguns vetores das matrizes de peso, desde que nenhum de seus valores exceda um valor limite, o que é bom, mas eles não quantizam todos os pesos do modelo. Neste artigo, eles propõem um método que quantiza todos os pesos do modelo para 4 e 3 bits, sem degradar a qualidade do modelo. Isso economiza bastante memória, não só porque todos os pesos são quantizados, mas também porque isso é feito em 4, 3 bits (e até mesmo 1 e 2 bits sob certas condições), em vez de 8 bits.</p>
</section>
<section class="section-block-markdown-cell">
<p>Este caderno foi traduzido automaticamente para torná-lo acessível a mais pessoas, por favor me avise se você vir algum erro de digitação..</p>
<h2 id="Trabalhos-nos-quais-se-baseia">Trabalhos nos quais se baseia<a class="anchor-link" href="#Trabalhos-nos-quais-se-baseia">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantifica%C3%A7%C3%A3o-em-camadas">Quantificação em camadas<a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-em-camadas">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Por um lado, eles se baseiam nos trabalhos <code>Nagel et al., 2020</code>; <code>Wang et al., 2020</code>; <code>Hubara et al., 2021</code> e <code>Frantar et al., 2022</code>, que propõem quantizar os pesos das camadas de uma rede neural para 4 e 3 bits, sem degradar a qualidade do modelo.</p>
<p>Dado um conjunto de dados <code>m</code> de um conjunto de dados, cada camada <code>l</code> é alimentada com os dados e a saída dos pesos <code>W</code> dessa camada é obtida. Portanto, o que você faz é procurar novos pesos quantizados <code>Ŵ</code> que minimizem o erro quadrático em relação à saída da camada de precisão total.</p>
<p><code>argmin_Ŵ||WX- ŴX||^2</code>.</p>
</section>
<section class="section-block-markdown-cell">
<p>Os valores de <code>Ŵ</code> são definidos antes da execução do processo de quantização e, durante o processo, cada parâmetro de <code>Ŵ</code> pode mudar de valor independentemente, sem depender do valor dos outros parâmetros de <code>Ŵ</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantiza%C3%A7%C3%A3o-ideal-do-c%C3%A9rebro-(OBQ)">Quantização ideal do cérebro (OBQ)<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-ideal-do-c%C3%A9rebro-(OBQ)">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>No trabalho <code>OBQ</code> de <code>Frantar et al., 2022</code>, eles otimizam o processo de quantização em camadas acima, tornando-o até três vezes mais rápido. Isso ajuda com modelos grandes, pois a quantização de um modelo grande pode levar muito tempo.</p>
</section>
<section class="section-block-markdown-cell">
<p>O método <code>OBQ</code> é uma abordagem para resolver o problema de quantização em camadas em modelos de linguagem. O <code>OBQ</code> parte da ideia de que o erro quadrático pode ser decomposto na soma de erros individuais para cada linha da matriz de peso. O método quantifica cada peso de forma independente, sempre atualizando os pesos não quantificados para compensar o erro incorrido pela quantização.</p>
</section>
<section class="section-block-markdown-cell">
<p>O método é capaz de quantificar modelos de tamanho médio em tempos razoáveis, mas, como é um algoritmo de complexidade cúbica, é extremamente caro para ser aplicado a modelos com bilhões de parâmetros.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Algoritmo-GPTQ">Algoritmo GPTQ<a class="anchor-link" href="#Algoritmo-GPTQ">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h3 id="Etapa-1:-informa%C3%A7%C3%B5es-arbitr%C3%A1rias-do-pedido">Etapa 1: informações arbitrárias do pedido<a class="anchor-link" href="#Etapa-1:-informa%C3%A7%C3%B5es-arbitr%C3%A1rias-do-pedido">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>No <code>OBQ</code>, eles procuraram a linha de pesos que criava o menor erro quadrático médio para quantificar, mas perceberam que fazer isso aleatoriamente não aumentava muito o erro quadrático médio final. Assim, em vez de procurar a linha que minimizasse o erro quadrático médio, o que criava uma complexidade cúbica no algoritmo, isso é feito sempre na mesma ordem. Isso reduz bastante o tempo de execução do algoritmo de quantização.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Etapa-2:-atualiza%C3%A7%C3%B5es-em-lote-pregui%C3%A7osas">Etapa 2: atualizações em lote preguiçosas<a class="anchor-link" href="#Etapa-2:-atualiza%C3%A7%C3%B5es-em-lote-pregui%C3%A7osas">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Como a atualização dos pesos é feita linha por linha, isso torna o processo lento e não utiliza totalmente o hardware. Portanto, eles propõem executar as atualizações em lotes de <code>B=128</code> linhas. Isso faz melhor uso do hardware e reduz o tempo de execução do algoritmo.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Etapa-3:-Reformula%C3%A7%C3%A3o-de-Cholesky">Etapa 3: Reformulação de Cholesky<a class="anchor-link" href="#Etapa-3:-Reformula%C3%A7%C3%A3o-de-Cholesky">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>O problema com as atualizações em lote é que, devido à grande escala dos modelos, podem ocorrer erros numéricos que afetam a precisão do algoritmo. Em particular, matrizes indefinidas podem ser obtidas, fazendo com que o algoritmo atualize os pesos restantes nas direções erradas, resultando em uma quantização muito ruim.</p>
<p>Para resolver isso, os autores do artigo propõem o uso de uma reformulação Cholesky, que é um método numericamente mais estável.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Resultados-do-GPTQ">Resultados do GPTQ<a class="anchor-link" href="#Resultados-do-GPTQ">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Abaixo estão dois gráficos com a medida de perplexidade no conjunto de dados <code>WikiText2</code> para todos os tamanhos dos modelos OPT e BLOOM. É possível observar que, com a técnica de quantização RTN, a perplexidade em alguns tamanhos aumenta muito, enquanto com o GPTQ ela permanece semelhante à obtida com o modelo FP16.</p>
</section>
<section class="section-block-markdown-cell">
<p><img alt="GPTQ-figure1" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPTQ-figure1.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>Outros gráficos são mostrados abaixo, mas com a medida de precisão no conjunto de dados <code>LAMBADA</code>. É a mesma coisa, enquanto o GPTQ permanece semelhante ao obtido com o FP16, outros métodos de quantização degradam muito a qualidade do modelo.</p>
</section>
<section class="section-block-markdown-cell">
<p><img alt="GPTQ-figure3" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/GPTQ-figure3.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Quantifica%C3%A7%C3%A3o-extrema">Quantificação extrema<a class="anchor-link" href="#Quantifica%C3%A7%C3%A3o-extrema">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Os gráficos anteriores mostraram os resultados da quantização do modelo em 3 e 4 bits, mas podemos quantizá-los em 2 bits ou até mesmo em 1 bit.</p>
</section>
<section class="section-block-markdown-cell">
<p>Modificando o tamanho dos lotes ao usar o algoritmo, podemos obter bons resultados quantificando tanto o modelo quanto os lotes.</p>
</section>
<section class="section-block-markdown-cell">
<table>
<thead>
<tr>
<th>Modelo</th>
<th>FP16</th>
<th>g128</th>
<th>g64</th>
<th>g32</th>
<th>3 bits</th>
</tr>
</thead>
<tbody>
<tr>
<td>OPT-175B</td>
<td>8,34</td>
<td>9,58</td>
<td>9,18</td>
<td>8,94</td>
<td>8,68</td>
</tr>
<tr>
<td>BLOOM</td>
<td>8,11</td>
<td>9,55</td>
<td>9,17</td>
<td>8,83</td>
<td>8,64</td>
</tr>
</tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Na tabela acima, você pode ver o resultado da perplexidade no conjunto de dados <code>WikiText2</code> para os modelos <code>OPT-175B</code> e <code>BLOOM</code> quantizados em 3 bits. É possível observar que, à medida que lotes menores são usados, a perplexidade diminui, o que significa que a qualidade do modelo quantizado é melhor. Mas o problema é que o algoritmo leva mais tempo para ser executado.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Desconto-din%C3%A2mico-na-infer%C3%AAncia">Desconto dinâmico na inferência<a class="anchor-link" href="#Desconto-din%C3%A2mico-na-infer%C3%AAncia">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Durante a inferência, algo chamado "dequantização dinâmica" é executado para realizar a inferência. Cada camada é dequantificada à medida que passa por ela.</p>
<p>Para fazer isso, eles desenvolveram um kernel que desquantifica as matrizes e executa produtos de matrizes. Embora a descompactação seja mais intensiva em termos de computação, o kernel precisa acessar muito menos memória, o que resulta em um aumento significativo da velocidade.</p>
</section>
<section class="section-block-markdown-cell">
<p>A inferência é realizada no FP16, descontando os pesos à medida que você passa pelas camadas, e a função de ativação de cada camada também é realizada no FP16. Embora isso signifique que mais cálculos tenham de ser feitos, porque os pesos têm de ser descontados, esses cálculos tornam o processo geral mais rápido, porque menos dados têm de ser buscados na memória. Os pesos precisam ser obtidos da memória em menos bits, portanto, no final, em matrizes com muitos parâmetros, isso economiza muitos dados. O gargalo geralmente está na obtenção dos dados da memória, portanto, mesmo que você tenha que fazer mais cálculos, no final a inferência é mais rápida.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Velocidade-de-infer%C3%AAncia">Velocidade de inferência<a class="anchor-link" href="#Velocidade-de-infer%C3%AAncia">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Os autores do artigo testaram a quantização do modelo BLOOM-175B para 3 bits, o que ocupou cerca de 63 GB de memória VRAM, incluindo embeddings e a camada de saída que são mantidos em FP16. Além disso, a manutenção da janela de contexto de 2048 tokens consome cerca de 9 GB de memória, em um total de cerca de 72 GB de memória VRAM. Eles quantizaram em 3 bits e não em 4 bits para poder realizar esse experimento e ajustar o modelo em uma única GPU Nvidia A100 com 80 GB de memória VRAM.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para fins de comparação, a inferência FP16 normal requer cerca de 350 GB de VRAM, o que equivale a 5 GPUs Nvidia A100 com 80 GB de VRAM. E a inferência quantizada de 8 bits usando <a href="https://maximofn.com/llm-int8/">llm.int8()</a> requer 3 dessas GPUs.</p>
</section>
<section class="section-block-markdown-cell">
<p>Abaixo está uma tabela com inferência de modelo em FP16 e 3 bits quantizados em GPUs Nvidia A100 com 80 GB de VRAM e GPUs Nvidia A6000 com 48 GB de VRAM.</p>
<table>
<thead>
<tr>
<th>GPU (VRAM)</th>
<th>Tempo médio por token em FP16 (ms)</th>
<th>Tempo médio por token em 3 bits (ms)</th>
<th>Aceleração</th>
<th>Redução das GPUs necessárias</th>
</tr>
</thead>
<tbody>
<tr>
<td>A6000 (48GB)</td>
<td>589</td>
<td>130</td>
<td>×4.53</td>
<td>8→ 2</td>
</tr>
<tr>
<td>A100 (80GB)</td>
<td>230</td>
<td>71</td>
<td>×3.24</td>
<td>5→ 1</td>
</tr>
</tbody>
</table>
<p>Por exemplo, usando os kernels, o modelo OPT-175B de 3 bits é executado em um único A100 (em vez de 5) e é aproximadamente 3,25 vezes mais rápido do que a versão FP16 em termos de tempo médio por token.</p>
<p>A GPU NVIDIA A6000 tem uma largura de banda de memória muito menor, o que torna essa estratégia ainda mais eficaz: a execução do modelo OPT-175B de 3 bits em 2 GPUs A6000 (em vez de 8) é aproximadamente 4,53 vezes mais rápida do que a versão FP16.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Livrarias">Livrarias<a class="anchor-link" href="#Livrarias">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Os autores do artigo implementaram a biblioteca <a href="https://github.com/IST-DASLab/gptq">GPTQ</a>. Outras bibliotecas foram criadas, como <a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">GPTQ-for-LLaMa</a>, <a href="https://github.com/turboderp/exllama">exllama</a> e <a href="https://github.com/ggerganov/llama.cpp/">llama.cpp</a>. No entanto, essas bibliotecas se concentram apenas na arquitetura llama, de modo que a biblioteca <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a> ganhou mais popularidade porque tem uma cobertura mais ampla de arquiteturas.</p>
<p>Por esse motivo, essa biblioteca <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a> foi integrada por meio de uma API na biblioteca <a href="https://maximofn.com/hugging-face-transformers/">transformers</a>. Para usá-la, é necessário instalá-la conforme indicado na seção <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation">Installation</a> de seu repositório e ter a biblioteca <a href="https://maximofn.com/hugging-face-optimun/">optimun</a> instalada.</p>
</section>
<section class="section-block-markdown-cell">
<p>Além da seção <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation">Installation</a> do seu repositório, você também deve fazer o seguinte:</p>
<div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/PanQiWei/AutoGPTQ
<span class="nb">cd</span><span class="w"> </span>AutoGPTQ
pip<span class="w"> </span>install<span class="w"> </span>.
<span class="sb">```</span>

Para<span class="w"> </span>que<span class="w"> </span>os<span class="w"> </span>kernels<span class="w"> </span>de<span class="w"> </span>quantização<span class="w"> </span>da<span class="w"> </span>GPU<span class="w"> </span>desenvolvidos<span class="w"> </span>pelos<span class="w"> </span>autores<span class="w"> </span><span class="k">do</span><span class="w"> </span>artigo<span class="w"> </span>sejam<span class="w"> </span>instalados.
</pre></div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Quantiza%C3%A7%C3%A3o-de-um-modelo">Quantização de um modelo<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-de-um-modelo">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver como quantificar um modelo com a biblioteca <a href="https://maximofn.com/hugging-face-optimun/">optimun</a> e a API <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a>.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Infer%C3%AAncia-de-modelo-n%C3%A3o-quantificada">Inferência de modelo não quantificada<a class="anchor-link" href="#Infer%C3%AAncia-de-modelo-n%C3%A3o-quantificada">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantificar o modelo <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">meta-call/Meta-Call-3-8B-Instruct</a> que, como o nome indica, é um modelo de 8B parâmetros, portanto, no FP16, precisaríamos de 16 GB de memória VRAM. Primeiro, executamos o modelo para ver quanta memória ele ocupa e a saída que ele gera</p>
</section>
<section class="section-block-markdown-cell">
<p>Como para usar esse modelo temos que pedir permissão ao Meta, entramos no HuggingFace para baixar o tokenizador e o modelo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>

<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Instanciamos o tokenizador e o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver a quantidade de memória que o FP16 ocupa.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 14.96 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que ele ocupa quase 15 GB, mais ou menos os 16 GB que dissemos que deveria ocupar, mas por que essa diferença? Certamente esse modelo não tem exatamente 8B de parâmetros, mas tem um pouco menos, mas ao indicar o número de parâmetros, ele é arredondado para 8B.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos uma inferência para ver como ele faz isso e quanto tempo leva.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer at a startup in the Bay Area. I am passionate about building AI systems that can help humans make better decisions and improve their lives.

I have a background in computer science and mathematics, and I have been working with machine learning for several years. I
Inference time: 4.14 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantiza%C3%A7%C3%A3o-do-modelo-para-4-bits">Quantização do modelo para 4 bits<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-4-bits">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantificar isso em 4 bits. Reinicio o notebook para evitar problemas de memória, então entramos no Hugging Face novamente.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>

<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, crio o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos a configuração de quantização. Como já dissemos, esse algoritmo calcula o erro dos pesos quantizados em relação aos pesos originais com base nas entradas de um conjunto de dados, portanto, na configuração, temos de informá-lo com qual conjunto de dados queremos quantificar o modelo.</p>
<p>Os padrões disponíveis são <code>wikitext2</code>, <code>c4</code>, <code>c4-new</code>, <code>ptb</code> e <code>ptb-new</code>.</p>
<p>Também podemos criar um conjunto de dados a partir de uma lista de cadeias de caracteres.</p>
<div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"auto-gptq é uma biblioteca de quantização de modelos fácil de usar com apis amigáveis, baseada no algoritmo GPTQ."</span><span class="p">]</span><span class="o">.</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Além disso, precisamos informar a ele o número de bits que o modelo quantizado tem por meio do parâmetro <code>bits</code>.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quantificamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Loading checkpoint shards: 100%|██████████| 4/4 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing model.layers blocks : 100%|██████████|32/32 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantization time: 1932.09 s = 32.20 min
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como o processo de quantização calcula o menor erro entre os pesos quantizados e os pesos originais ao passar as entradas por cada camada, o processo de quantização leva tempo. Nesse caso, levou cerca de meia hora</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada na memória que ele ocupa agora</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_4bits_memory</span> <span class="o">=</span> <span class="n">model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 5.34 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Aqui podemos ver um benefício da quantização. Enquanto o modelo original ocupava cerca de 15 GB de VRAM, agora o modelo quantizado ocupa cerca de 5 GB, quase um terço do tamanho original.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e observamos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I am passionate about developing innovative solutions that can positively impact society. I am excited to be a part of this community and to learn from and contribute to the discussions here. I am particularly
Inference time: 2.34 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O modelo não quantizado levou 4,14 segundos, enquanto agora quantizado para 4 bits levou 2,34 segundos e também gerou bem o texto. Conseguimos reduzir a inferência em quase metade.</p>
<p>Como o tamanho do modelo quantizado é quase um terço do modelo FP16, poderíamos pensar que a velocidade de inferência deveria ser cerca de três vezes mais rápida com o modelo quantizado. Mas lembre-se de que, em cada camada, os pesos são quantificados e os cálculos são realizados em FP16, portanto, só conseguimos reduzir o tempo de inferência pela metade e não em um terço.</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora vamos salvar o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">"./model_4bits/"</span>
<span class="n">model_4bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[7]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>('./model_4bits/tokenizer_config.json',
 './model_4bits/special_tokens_map.json',
 './model_4bits/tokenizer.json')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E fazemos o upload para o hub</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-4bits"</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>
<span class="n">model_4bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>README.md: 100%|██████████| 5.17/5.17k [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/44cfdcad78db260122943d3f57858c1b840bda17', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False', commit_description='', oid='44cfdcad78db260122943d3f57858c1b840bda17', pr_url=None, pr_revision=None, pr_num=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Também fizemos o upload do tokenizador. Embora não tenhamos alterado o tokenizador, nós o carregamos porque, se alguém fizer download do nosso modelo a partir do hub, não precisará saber qual tokenizador usamos, portanto, provavelmente desejará fazer o download do modelo e do tokenizador juntos. Podemos indicar no cartão do modelo qual tokenizador usamos para fazer o download, mas é muito provável que a pessoa não leia o cartão do modelo, tente fazer o download do tokenizador, receba um erro e não saiba o que fazer. Por isso, fizemos o upload para evitar esse problema.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-4bits"</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>README.md: 100%|██████████| 0.00/5.17k [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[5]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4', commit_message='Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False', commit_description='', oid='75600041ca6e38b5f1fb912ad1803b66656faae4', pr_url=None, pr_revision=None, pr_num=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantiza%C3%A7%C3%A3o-do-modelo-para-3-bits">Quantização do modelo para 3 bits<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-3-bits">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantificar isso em 3 bits. Reinicio o notebook para evitar problemas de memória e faço login novamente no Hugging Face.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>

<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, crio o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos a configuração de quantização e agora indicamos que queremos quantizar para 3 bits.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quantificamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model_3bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Loading checkpoint shards: 100%|██████████| 4/4 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing model.layers blocks : 100%|██████████|32/32 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantization time: 1912.69 s = 31.88 min
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como antes, demorou cerca de meia hora.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada na memória que ele ocupa agora</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_3bits_memory</span> <span class="o">=</span> <span class="n">model_3bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_3bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 4.52 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O espaço ocupado pela memória do modelo de 3 bits também é de quase 5 GB. O modelo de 4 bits ocupava 5,34 GB, enquanto o modelo de 3 bits agora ocupa 4,52 GB, portanto, conseguimos reduzir um pouco mais o tamanho do modelo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e observamos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_3bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_3bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer at Google. I am excited to be here today to talk about my work in the field of Machine Learning and to share some of the insights I have gained through my experiences.
I am a Machine Learning Engineer at Google, and I am excited to be
Inference time: 2.89 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Embora a saída de 3 bits seja boa, o tempo de inferência passou a ser de 2,89 segundos, enquanto a saída de 4 bits foi de 2,34 segundos. Mais testes devem ser feitos para verificar se sempre leva menos tempo em 4 bits ou se a diferença é tão pequena que às vezes a inferência de 3 bits é mais rápida e às vezes a inferência de 4 bits é mais rápida.</p>
<p>Além disso, embora o resultado faça sentido, ele começa a se tornar repetitivo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Salvamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">"./model_3bits/"</span>
<span class="n">model_3bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>('./model_3bits/tokenizer_config.json',
 './model_3bits/special_tokens_map.json',
 './model_3bits/tokenizer.json')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E fazemos o upload para o hub</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-3bits"</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>
<span class="n">model_3bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>model.safetensors: 100%|██████████| 4.85/4.85G [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[14]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-3bits/commit/422fd94a031234c10224ddbe09c0e029a5e9c01f', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 3bits, gr128, desc_act=False', commit_description='', oid='422fd94a031234c10224ddbe09c0e029a5e9c01f', pr_url=None, pr_revision=None, pr_num=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Também carregamos o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-3bits"</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>README.md: 100%|██████████| 0.00/5.17k [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4', commit_message='Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False', commit_description='', oid='75600041ca6e38b5f1fb912ad1803b66656faae4', pr_url=None, pr_revision=None, pr_num=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantiza%C3%A7%C3%A3o-do-modelo-para-2-bits">Quantização do modelo para 2 bits<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-2-bits">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantificar isso em 2 bits. Reinicio o notebook para evitar problemas de memória e faço login novamente no Hugging Face.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>

<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, crio o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos a configuração de quantização. Agora dizemos a ele para quantizar em 2 bits. Além disso, temos que indicar quantos vetores da matriz de peso ele quantiza ao mesmo tempo por meio do parâmetro <code>group_size</code>, antes, por padrão, ele tinha o valor 128 e não mexemos nele, mas agora, ao quantizar para 2 bits, para ter menos erro, colocamos um valor menor. Se o deixarmos em 128, o modelo quantizado funcionará muito mal. Nesse caso, colocarei um valor de 16.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model_2bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Loading checkpoint shards: 100%|██████████| 4/4 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing model.layers blocks : 100%|██████████|32/32 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantization time: 1973.12 s = 32.89 min
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que isso também levou cerca de meia hora.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada na memória que ele ocupa agora</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_2bits_memory</span> <span class="o">=</span> <span class="n">model_2bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_2bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 4.50 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Enquanto a quantização de 4 bits era de 5,34 GB e a de 3 bits era de 4,52 GB, agora a quantização de 2 bits é de 4,50 GB, portanto, conseguimos reduzir um pouco mais o tamanho do modelo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e observamos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_2bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_2bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer.  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
Inference time: 2.92 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que o resultado já não é bom, além disso, o tempo de inferência é de 2,92 segundos, praticamente o mesmo que com 3 e 4 bits.</p>
</section>
<section class="section-block-markdown-cell">
<p>Salvamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">"./model_2bits/"</span>
<span class="n">model_2bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[7]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>('./model_2bits/tokenizer_config.json',
 './model_2bits/special_tokens_map.json',
 './model_2bits/tokenizer.json')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nós o carregamos no hub</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-2bits"</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>
<span class="n">model_2bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>model.safetensors: 100%|██████████| 4.83/4.83G [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/13ede006ce0dbbd8aca54212e960eff98ea5ec63', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr16, desc_act=False', commit_description='', oid='13ede006ce0dbbd8aca54212e960eff98ea5ec63', pr_url=None, pr_revision=None, pr_num=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantiza%C3%A7%C3%A3o-do-modelo-para-1-bit">Quantização do modelo para 1 bit<a class="anchor-link" href="#Quantiza%C3%A7%C3%A3o-do-modelo-para-1-bit">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantificar isso em 1 bit. Reinicio o notebook para evitar problemas de memória e faço login novamente no Hugging Face.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>

<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, crio o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"meta-llama/Meta-Llama-3-8B-Instruct"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos a configuração de quantização, agora dizemos a ela para quantizar em apenas 1 bit e também para usar um <code>group_size</code> de 8.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPTQConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">"c4"</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model_1bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Loading checkpoint shards: 100%|██████████| 4/4 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing model.layers blocks : 100%|██████████|32/32 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantization time: 2030.38 s = 33.84 min
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que também leva cerca de meia hora para quantificar.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos dar uma olhada na memória que ele ocupa agora</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_1bits_memory</span> <span class="o">=</span> <span class="n">model_1bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">model_1bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 5.42 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que, nesse caso, ele ocupa ainda mais do que quantificado em 2 bits, 4,52 GB.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e observamos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_1bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_1bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineerimerszuimersimerspinsimersimersingoingoimersurosimersimersimersoleningoimersingopinsimersbirpinsimersimersimersorgeingoimersiringimersimersimersimersimersimersimersンディorge_REFERER ingest羊imersorgeimersimersendetingoШАhandsingo
Inference time: 3.12 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que a saída é muito ruim e também demora mais do que quando quantizamos para 2 bits.</p>
</section>
<section class="section-block-markdown-cell">
<p>Salvamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">"./model_1bits/"</span>
<span class="n">model_1bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[7]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>('./model_1bits/tokenizer_config.json',
 './model_1bits/special_tokens_map.json',
 './model_1bits/tokenizer.json')</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Nós o carregamos no hub</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">"Llama-3-8B-Instruct-GPTQ-1bits"</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">"</span>
<span class="n">model_1bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>README.md: 100%|██████████| 0.00/5.17k [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Upload 2 LFS files: 100%|██████████| 0/2 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>model-00002-of-00002.safetensors: 100%|██████████| 0.00/1.05G [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>model-00001-of-00002.safetensors: 100%|██████████| 0.00/4.76G [00:00&lt;?, ?B/s]</pre>
</div>
</div>
<div class="output-area">
<div class="prompt-output-prompt">Out[8]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>CommitInfo(commit_url='https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/e59ccffc03247e7dcc418f98b482cc02dc7a168d', commit_message='AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr8, desc_act=False', commit_description='', oid='e59ccffc03247e7dcc418f98b482cc02dc7a168d', pr_url=None, pr_revision=None, pr_num=None)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Resumo-da-quantifica%C3%A7%C3%A3o">Resumo da quantificação<a class="anchor-link" href="#Resumo-da-quantifica%C3%A7%C3%A3o">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vamos comprar quantização de 4, 3, 2 e 1 bits.</p>
<table>
<thead>
<tr>
<th>Bits</th>
<th>Tempo de quantização (min)</th>
<th>Memória (GB)</th>
<th>Tempo de inferência (s)</th>
<th>Qualidade da saída</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP16</td>
<td>0</td>
<td>14.96</td>
<td>4.14</td>
<td>Bom</td>
</tr>
<tr>
<td>4</td>
<td>32.20</td>
<td>5.34</td>
<td>2.34</td>
<td>Bom</td>
</tr>
<tr>
<td>3</td>
<td>31.88</td>
<td>4.52</td>
<td>2.89</td>
<td>Bom</td>
</tr>
<tr>
<td>2</td>
<td>32.89</td>
<td>4.50</td>
<td>2.92</td>
<td>Ruim</td>
</tr>
<tr>
<td>1</td>
<td>33.84</td>
<td>5.42</td>
<td>3.12</td>
<td>Ruim</td>
</tr>
</tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Observando essa tabela, vemos que não faz sentido, neste exemplo, quantificar com menos de 4 bits.</p>
<p>A quantificação em 1 e 2 bits claramente não faz sentido porque a qualidade da saída é ruim.</p>
<p>Mas, embora a saída quando quantizamos para 3 bits seja boa, ela começou a se tornar repetitiva, portanto, a longo prazo, provavelmente não seria uma boa ideia usar esse modelo. Além disso, nem a economia no tempo de quantização, nem a economia de VRAM, nem a economia no tempo de inferência são significativas em comparação com a quantização para 4 bits.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Carregamento-do-modelo-salvo">Carregamento do modelo salvo<a class="anchor-link" href="#Carregamento-do-modelo-salvo">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Agora que comparamos a quantização dos modelos, vamos ver como seria feito para carregar o modelo de 4 bits que salvamos, já que, como vimos, essa é a melhor opção.</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, carregamos o tokenizador que usamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">path</span> <span class="o">=</span> <span class="s2">"./model_4bits"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, carregamos o modelo que salvamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">load_model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-text-output-subarea">
<pre>Loading checkpoint shards: 100%|██████████| 2/2 [00:00&lt;?, ?it/s]</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos a memória que ele ocupa</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">load_model_4bits_memory</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">load_model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 5.34 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que ele ocupa a mesma memória de quando o quantificamos, o que é lógico.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e observamos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">load_model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I have been working with machine learning models for several years. I am excited to be a part of this community and to share my knowledge and experience with others. I am particularly interested in
Inference time: 3.82 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que a inferência é boa e levou 3,82 segundos, um pouco mais do que quando a quantificamos. Mas, como eu disse antes, teríamos que fazer esse teste várias vezes e tirar uma média.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Carregando-o-modelo-carregado-para-o-hub">Carregando o modelo carregado para o hub<a class="anchor-link" href="#Carregando-o-modelo-carregado-para-o-hub">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Agora veremos como carregar o modelo de 4 bits que carregamos no hub.</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro, carregamos o tokenizador que carregamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">"Maximofn/Llama-3-8B-Instruct-GPTQ-4bits"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora, carregamos o modelo que salvamos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">load_model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">"auto"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos a memória que ele ocupa</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">load_model_4bits_memory</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model memory: </span><span class="si">{</span><span class="n">load_model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 5.34 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ele também ocupa a mesma memória</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e observamos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">"Hello my name is Maximo and I am a Machine Learning Engineer"</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">"pt"</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">load_model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer with a passion for building innovative AI solutions. I have been working in the field of AI for over 5 years, and have gained extensive experience in developing and implementing machine learning models for various industries.

In my free time, I enjoy reading books on
Inference time: 3.81 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que a inferência também é boa e levou 3,81 segundos.</p>
</section>
