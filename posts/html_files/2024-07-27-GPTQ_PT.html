<section class="section-block-markdown-cell">
<h1 id="GPTQ: Quantizacao Precisa Pos-Treinamento para Transformadores Gerativos Pre-Treinados">GPTQ: Quantização Precisa Pós-Treinamento para Transformadores Gerativos Pré-Treinados<a class="anchor-link" href="#GPTQ: Quantizacao Precisa Pos-Treinamento para Transformadores Gerativos Pre-Treinados">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Aviso: Este post foi traduzido para o português usando um modelo de tradução automática. Por favor, me avise se encontrar algum erro.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>No artigo <a href="https://arxiv.org/abs/2210.17323">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> é exposta a necessidade de criar um método de quantização pós-treinamento que não degrade a qualidade do modelo. Neste post, vimos o método <a href="https://maximofn.com/llm-int8/">llm.int8()</a> que quantiza para INT8 alguns vetores das matrizes de pesos, desde que nenhum dos seus valores ultrapasse um valor limite, o que é muito bom, mas não quantiza todos os pesos do modelo. Neste artigo, propõem-se um método que quantiza todos os pesos do modelo para 4 e 3 bits, sem degradar a qualidade do modelo. Isso representa uma economia considerável de memória, não apenas porque todos os pesos são quantizados, mas também porque isso é feito em 4, 3 bits (e até 1 e 2 bits em certas condições), em vez de 8 bits.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Trabalhos nos quais se baseia">Trabalhos nos quais se baseia<a class="anchor-link" href="#Trabalhos nos quais se baseia">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantizacao por camadas">Quantização por camadas<a class="anchor-link" href="#Quantizacao por camadas">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Por um lado, baseiam-se nos trabalhos <code>Nagel et al., 2020</code>; <code>Wang et al., 2020</code>; <code>Hubara et al., 2021</code> e <code>Frantar et al., 2022</code>, que propõem quantizar os pesos das camadas de uma rede neural para 4 e 3 bits, sem degradar a qualidade do modelo.</p>
<p>Dado um conjunto de dados <code>m</code> proveniente de um dataset, a cada camada <code>l</code> são fornecidos os dados e obtém-se a saída dos pesos <code>W</code> dessa camada. Portanto, o que se faz é buscar pesos novos <code>Ŵ</code> quantizados que minimizem o erro quadrático em relação à saída da camada de precisão total.</p>
<p><code>argmin_Ŵ||WX− ŴX||^2</code></p>
</section>
<section class="section-block-markdown-cell">
<p>Os valores de <code>Ŵ</code> são estabelecidos antes de realizar o processo de quantização e durante o processo, cada parâmetro de <code>Ŵ</code> pode mudar de valor independentemente sem depender do valor dos demais parâmetros de <code>Ŵ</code>.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantizacao otima do cerebro (OBC)">Quantização ótima do cérebro (OBC)<a class="anchor-link" href="#Quantizacao otima do cerebro (OBC)">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>No trabalho de <code>OBQ</code> de <code>Frantar et al., 2022</code> otimizam o processo de quantização por camadas anterior, tornando-o até 3 vezes mais rápido. Isso ajuda com os modelos grandes, pois quantizar um modelo grande pode levar muito tempo.</p>
</section>
<section class="section-block-markdown-cell">
<p>O método <code>OBQ</code> é uma abordagem para resolver o problema de quantização em camadas em modelos de linguagem. <code>OBQ</code> parte da ideia de que o erro quadrático pode ser decomposto na soma de erros individuais para cada linha da matriz de pesos. Em seguida, o método quantiza cada peso de forma independente, atualizando sempre os pesos não quantizados para compensar o erro incorrido pela quantização.</p>
</section>
<section class="section-block-markdown-cell">
<p>O método é capaz de quantificar modelos de tamanho médio em tempos razoáveis, mas como é um algoritmo de complexidade cúbica, torna-se extremamente custoso aplicá-lo a modelos com bilhões de parâmetros.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Algoritmo de GPTQ">Algoritmo de GPTQ<a class="anchor-link" href="#Algoritmo de GPTQ">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<h3 id="Passo 1: Informacao em ordem arbitraria">Passo 1: Informação em ordem arbitrária<a class="anchor-link" href="#Passo 1: Informacao em ordem arbitraria">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Em <code>OBQ</code> buscava-se a linha de pesos que criasse o menor erro quadrático médio para quantizar, mas percebeu-se que ao fazê-lo de maneira aleatória não aumentava muito o erro quadrático médio final. Por isso, em vez de buscar a linha que minimiza o erro quadrático médio, o que criava uma complexidade cúbica no algoritmo, sempre se faz na mesma ordem. Graças a isso, reduz-se muito o tempo de execução do algoritmo de quantização.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Passo 2: Atualizacoes em lote preguicosas">Passo 2: Atualizações em lote preguiçosas<a class="anchor-link" href="#Passo 2: Atualizacoes em lote preguicosas">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Ao fazer a atualização dos pesos linha a linha, isso causa um processo lento e não aproveita totalmente o hardware.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Passo 3: Reformulacao de Cholesky">Passo 3: Reformulação de Cholesky<a class="anchor-link" href="#Passo 3: Reformulacao de Cholesky">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>O problema de fazer as atualizações em lotes é que, devido à grande escala dos modelos, podem ocorrer erros numéricos que afetam a precisão do algoritmo. Especificamente, podem ser obtidas matrizes indefinidas, o que faz com que o algoritmo atualize os pesos restantes em direções incorretas, resultando em uma quantização muito ruim.</p>
<p>Para resolver isso, os autores do artigo propõem usar uma reformulação de Cholesky, que é um método numericamente mais estável.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Resultados do GPTQ">Resultados do GPTQ<a class="anchor-link" href="#Resultados do GPTQ">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>A seguir estão duas gráficas com a medida da perplexidade (perplexity) no dataset <code>WikiText2</code> para todos os tamanhos dos modelos OPT e BLOOM. Pode-se ver que com a técnica de quantização RTN, a perplexidade em alguns tamanhos aumenta muito, enquanto com GPTQ mantém-se similar à obtida com o modelo em FP16.</p>
</section>
<section class="section-block-markdown-cell">
<img src="https://images.maximofn.com/GPTQ-figure1.webp" alt="GPTQ-figure1">
</section>
<section class="section-block-markdown-cell">
<p>A seguir estão mostradas outras gráficas, mas com a medida do accuracy no dataset <code>LAMBADA</code>. Ocorre o mesmo, enquanto GPTQ mantém-se semelhante ao obtido com FP16, outros métodos de quantização degradam muito a qualidade do modelo</p>
</section>
<section class="section-block-markdown-cell">
<img src="https://images.maximofn.com/GPTQ-figure3.webp" alt="GPTQ-figure3">
</section>
<section class="section-block-markdown-cell">
<h2 id="Quantizacao extrema">Quantização extrema<a class="anchor-link" href="#Quantizacao extrema">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Nos gráficos anteriores foram mostrados os resultados da quantização do modelo para 3 e 4 bits, mas podemos quantizá-los para 2 bits, e até mesmo para apenas 1 bit.</p>
</section>
<section class="section-block-markdown-cell">
<p>Modificando o tamanho dos batches ao utilizar o algoritmo, podemos obter bons resultados quantizando tanto o modelo</p>
</section>
<section class="section-block-markdown-cell">
<table>
  <thead>
    <tr>
      <th>Modelo</th>
      <th>FP16</th>
      <th>g128</th>
      <th>g64</th>
      <th>g32</th>
      <th>3 bits</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>OPT-175B</td>
      <td>8,34</td>
      <td>9,58</td>
      <td>9,18</td>
      <td>8,94</td>
      <td>8,68</td>
    </tr>
    <tr>
      <td>BLOOM</td>
      <td>8,11</td>
      <td>9,55</td>
      <td>9,17</td>
      <td>8,83</td>
      <td>8,64</td>
    </tr>
  </tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Na tabela anterior, pode-se ver o resultado da perplexidade no conjunto de dados <code>WikiText2</code> para os modelos <code>OPT-175B</code> e <code>BLOOM</code> quantizados a 3 bits. Pode-se observar que à medida que se usam lotes menores, a perplexidade diminui, o que significa que a qualidade do modelo quantizado é melhor. No entanto, isso tem o problema de que o algoritmo leva mais tempo para ser executado.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Desquantizacao dinamica na inferencia">Desquantização dinâmica na inferência<a class="anchor-link" href="#Desquantizacao dinamica na inferencia">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Durante a inferência, algo chamado <code>descuantificação dinâmica</code> (<code>dynamic dequantization</code>) é realizada para permitir a inferência. Cada camada é descuantificada à medida que passa por elas.</p>
<p>Para isso, eles desenvolveram um kernel que desquantiza as matrizes e realiza os produtos matriciais. Embora a desquantização consuma mais cálculos, o kernel precisa acessar muito menos memória, o que gera acelerações significativas.</p>
</section>
<section class="section-block-markdown-cell">
<p>A inferência é realizada em FP16, desquantizando os pesos à medida que se passa pelas camadas e a função de ativação de cada camada também é realizada em FP16. Embora isso faça com que seja necessário realizar mais cálculos, pois é preciso desquantizar, esses cálculos fazem com que o processo total seja mais rápido, porque menos dados precisam ser trazidos da memória. É necessário trazer da memória os pesos em menos bits, de forma que, no final, em matrizes com muitos parâmetros, isso resulta em uma economia significativa de dados. O gargalo normalmente está em trazer os dados da memória, portanto, mesmo que seja necessário realizar mais cálculos, a inferência acaba sendo mais rápida.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Velocidade de inferencia">Velocidade de inferência<a class="anchor-link" href="#Velocidade de inferencia">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Os autores do paper realizaram um teste quantizando o modelo BLOOM-175B para 3 bits, o que ocupava cerca de 63 GB de memória VRAM, incluindo os embeddings e a camada de saída que permanecem em FP16. Além disso, manter a janela de contexto de 2048 tokens consome cerca de 9 GB de memória, o que totaliza aproximadamente 72 GB de memória VRAM. Eles quantizaram para 3 bits e não para 4 para poder realizar este experimento e fazer o modelo caber em uma única GPU Nvidia A100 com 80 GB de memória VRAM.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para comparação, a inferência normal em FP16 requer cerca de 350 GB de memória VRAM, o que equivale a 5 GPUs Nvidia A100 com 80 GB de memória VRAM. E a inferência quantizando para 8 bits usando <a href="https://maximofn.com/llm-int8/">llm.int8()</a> requer 3 dessas GPUs.</p>
</section>
<section class="section-block-markdown-cell">
<p>A seguir, está apresentada uma tabela com a inferência do modelo em FP16 e quantizado para 3 bits em GPUs Nvidia A100 com 80 GB de memória VRAM e Nvidia A6000 com 48 GB de memória VRAM.</p>
<table>
  <thead>
    <tr>
      <th>GPU (VRAM)</th>
      <th>tempo médio por token em FP16 (ms)</th>
      <th>tempo médio por token em 3 bits (ms)</th>
      <th>Aceleração</th>
      <th>Redução de GPUs necessárias</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>A6000 (48GB)</td>
      <td>589</td>
      <td>130</td>
      <td>×4,53</td>
      <td>8→ 2</td>
    </tr>
    <tr>
      <td>A100 (80GB)</td>
      <td>230</td>
      <td>71</td>
      <td>×3,24</td>
      <td>5→ 1</td>
    </tr>
  </tbody>
</table>
<p>Por exemplo, utilizando os kernels, o modelo OPT-175B de 3 bits é executado em uma única A100 (em vez de 5) e é aproximadamente 3,25 vezes mais rápido que a versão FP16 em termos de tempo médio por token.</p>
<p>A GPU NVIDIA A6000 tem uma largura de banda de memória muito menor, portanto, esta estratégia é ainda mais eficaz: executar o modelo OPT-175B de 3 bits em 2 GPUs A6000 (em vez de 8) é aproximadamente 4,53 vezes mais rápido que a versão FP16.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Bibliotecas">Bibliotecas<a class="anchor-link" href="#Bibliotecas">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Os autores do paper implementaram a biblioteca <a href="https://github.com/IST-DASLab/gptq">GPTQ</a>. Outras bibliotecas foram criadas, como <a href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">GPTQ-for-LLaMa</a>, <a href="https://github.com/turboderp/exllama">exllama</a> e <a href="https://github.com/ggerganov/llama.cpp/">llama.cpp</a>. No entanto, essas bibliotecas se concentram apenas na arquitetura llama, por isso a biblioteca <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a> foi a que ganhou mais popularidade porque possui uma cobertura mais ampla de arquiteturas.</p>
<p>Por isso, a biblioteca <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a> foi integrada por meio de uma API dentro da biblioteca <a href="https://maximofn.com/hugging-face-transformers/">transformers</a>. Para poder usá-la, é necessário instalá-la conforme indicado na seção <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation">Installation</a> do seu repositório e ter a biblioteca <a href="https://maximofn.com/hugging-face-optimun/">optimun</a> instalada.</p>
</section>
<section class="section-block-markdown-cell">
<p>Além de fazer o que indicam na seção <a href="https://github.com/AutoGPTQ/AutoGPTQ#installation">Installation</a> do seu repositório, também é recomendável fazer o seguinte:</p>
<section class="section-block-markdown-cell">
      <div class='highlight'><pre><code class="language-bash">git clone https://github.com/PanQiWei/AutoGPTQ<br>cd AutoGPTQ<br>pip install .</code></pre></div>
      </section>
<p>Para que se instalem os kernels de quantização na GPU que os autores do paper desenvolveram.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Quantizacao de um modelo">Quantização de um modelo<a class="anchor-link" href="#Quantizacao de um modelo">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver como quantizar um modelo com a biblioteca <a href="https://maximofn.com/hugging-face-optimun/">optimun</a> e a API de <a href="https://github.com/AutoGPTQ/AutoGPTQ">AutoGPTQ</a>.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Inferencia do modelo nao quantizado">Inferência do modelo não quantizado<a class="anchor-link" href="#Inferencia do modelo nao quantizado">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantizar o modelo <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">meta-llama/Meta-Llama-3-8B-Instruct</a> que, como seu nome indica, é um modelo de 8B de parâmetros, portanto em FP16 precisaríamos de 16 GB de memória VRAM. Primeiro executamos o modelo para ver a memória que ele ocupa e a saída que gera</p>
</section>
<section class="section-block-markdown-cell">
<p>Como precisamos pedir permissão à Meta para usar esse modelo, fazemos login no Hugging Face para poder baixar o tokenizador e o modelo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>
<span class="w"> </span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Instanciamos o tokenizador e o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="w"> </span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>
<span class="w"> </span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver a memória que ocupa em FP16</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 14.96 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que ocupa quase 15 GB, mais ou menos os 16 GB que havíamos dito que deveria ocupar, mas por que essa diferença? Provavelmente esse modelo não tem exatamente 8B de parâmetros, mas sim um pouco menos, mas na hora de indicar o número de parâmetros arredonda-se para 8B.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos uma inferência para ver como ele faz e o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer at a startup in the Bay Area. I am passionate about building AI systems that can help humans make better decisions and improve their lives.

I have a background in computer science and mathematics, and I have been working with machine learning for several years. I
Inference time: 4.14 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantizacao do modelo para 4 bits">Quantização do modelo para 4 bits<a class="anchor-link" href="#Quantizacao do modelo para 4 bits">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantizá-lo para 4 bits. Reinicio o notebook para não ter problemas de memória, então precisamos fazer login novamente no Hugging Face.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>
<span class="w"> </span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro crio o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos a configuração de quantização. Como dissemos, este algoritmo calcula o erro dos pesos quantizados em relação aos originais com base nas entradas de um conjunto de dados, portanto, na configuração, precisamos especificar com qual conjunto de dados queremos quantizar o modelo.</p>
<p>Os disponíveis por padrão são <code>wikitext2</code>, <code>c4</code>, <code>c4-new</code>, <code>ptb</code> e <code>ptb-new</code>.</p>
<p>Também podemos criar nós um dataset a partir de uma lista de strings</p>
<div class='highlight'><pre><code class="language-python">dataset = ["o auto-gptq é uma biblioteca de quantização de modelos fácil de usar com APIs amigáveis ao usuário, baseada no algoritmo GPTQ."]
</code></pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Além disso, temos que informar o número de bits do modelo quantizado por meio do parâmetro <code>bits</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTQConfig</span>
<span class="w"> </span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quantizamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Loading checkpoint shards: 100%|██████████| 4/4 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing model.layers blocks : 100%|██████████|32/32 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
&#x20;&#x20;warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantization time: 1932.09 s = 32.20 min
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como o processo de quantização calcula o menor erro entre os pesos quantizados e os originais ao passar entradas por cada camada, o processo de quantização demora. Neste caso, levou em média uma hora.</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver a memória que ocupa agora</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_4bits_memory</span> <span class="o">=</span> <span class="n">model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 5.34 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Aqui podemos ver um benefício da quantização. Enquanto o modelo original ocupava cerca de 15 GB de VRAM, agora o modelo quantizado ocupa cerca de 5 GB, quase um terço do tamanho original.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e vemos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I am passionate about developing innovative solutions that can positively impact society. I am excited to be a part of this community and to learn from and contribute to the discussions here. I am particularly
Inference time: 2.34 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O modelo não quantizado levou 4,14 segundos, enquanto agora quantizado para 4 bits levou 2,34 segundos e ainda gerou o texto corretamente. Conseguimos reduzir a inferência quase pela metade.</p>
<p>Como o tamanho do modelo quantizado é quase um terço do modelo em FP16, poderíamos pensar que a velocidade de inferência deveria ser cerca de três vezes mais rápida com o modelo quantizado. Mas é preciso lembrar que em cada camada os pesos são desquantizados e os cálculos são realizados em FP16, por isso conseguimos reduzir o tempo de inferência pela metade e não por um terço.</p>
</section>
<section class="section-block-markdown-cell">
<p>Agora salvamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">&quot;./model_4bits/&quot;</span>
<span class="n">model_4bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;./model_4bits/tokenizer_config.json&#x27;,
 &#x27;./model_4bits/special_tokens_map.json&#x27;,
 &#x27;./model_4bits/tokenizer.json&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E o enviamos para o hub</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-4bits&quot;</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">model_4bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>README.md: 100%|██████████| 5.17/5.17k [00:00&amp;lt;?, ?B/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/44cfdcad78db260122943d3f57858c1b840bda17&#x27;, commit_message=&#x27;AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;44cfdcad78db260122943d3f57858c1b840bda17&#x27;, pr_url=None, pr_revision=None, pr_num=None)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Também enviamos o tokenizador. Embora não tenhamos alterado o tokenizador, o enviamos porque, se uma pessoa baixar nosso modelo do hub, ela pode não saber qual tokenizador usamos, então provavelmente querrá baixar o modelo e o tokenizador juntos. Podemos indicar na model card qual tokenizador usamos para que ela possa baixá-lo, mas é mais provável que a pessoa não leia a model card, tente baixar o tokenizador, obtenha um erro e não saiba o que fazer. Então enviamos para evitar esse problema.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-4bits&quot;</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>README.md: 100%|██████████| 0.00/5.17k [00:00&amp;lt;?, ?B/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4&#x27;, commit_message=&#x27;Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;75600041ca6e38b5f1fb912ad1803b66656faae4&#x27;, pr_url=None, pr_revision=None, pr_num=None)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantizacao do modelo para 3 bits">Quantização do modelo para 3 bits<a class="anchor-link" href="#Quantizacao do modelo para 3 bits">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantizá-lo para 3 bits. Reinicio o notebook para não ter problemas de memória e volto a fazer login no Hugging Face</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>
<span class="w"> </span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro crio o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos a configuração de quantização, agora indicamos que queremos quantizar para 3 bits</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTQConfig</span>
<span class="w"> </span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quantizamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model_3bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Loading checkpoint shards: 100%|██████████| 4/4 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing model.layers blocks : 100%|██████████|32/32 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
&#x20;&#x20;warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantization time: 1912.69 s = 31.88 min
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Assim como antes, levou uma média de meia hora</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver a memória que ocupa agora</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_3bits_memory</span> <span class="o">=</span> <span class="n">model_3bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_3bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 4.52 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>A memória que o modelo ocupa em 3 bits também é quase de 5 GB. O modelo em 4 bits ocupava 5,34 GB, enquanto agora em 3 bits ocupa 4,52 GB, portanto conseguimos reduzir um pouco mais o tamanho do modelo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e vemos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_3bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_3bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer at Google. I am excited to be here today to talk about my work in the field of Machine Learning and to share some of the insights I have gained through my experiences.
I am a Machine Learning Engineer at Google, and I am excited to be
Inference time: 2.89 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Embora a saída de 3 bits seja boa, agora o tempo de inferência foi de 2,89 segundos, enquanto em 4 bits foi de 2,34 segundos. Seriam necessárias mais testes para ver se sempre é mais rápido em 4 bits, ou pode ser que a diferença seja tão pequena que às vezes a inferência em 3 bits seja mais rápida e outras vezes a inferência em 4 bits.</p>
<p>Além disso, embora a saída faça sentido, começa a se tornar repetitiva.</p>
</section>
<section class="section-block-markdown-cell">
<p>Guardamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">&quot;./model_3bits/&quot;</span>
<span class="n">model_3bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;./model_3bits/tokenizer_config.json&#x27;,
 &#x27;./model_3bits/special_tokens_map.json&#x27;,
 &#x27;./model_3bits/tokenizer.json&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>E o enviamos para o Hub</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-3bits&quot;</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">model_3bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>model.safetensors: 100%|██████████| 4.85/4.85G [00:00&amp;lt;?, ?B/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-3bits/commit/422fd94a031234c10224ddbe09c0e029a5e9c01f&#x27;, commit_message=&#x27;AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 3bits, gr128, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;422fd94a031234c10224ddbe09c0e029a5e9c01f&#x27;, pr_url=None, pr_revision=None, pr_num=None)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Também subimos o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-3bits&quot;</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Tokenizers for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>README.md: 100%|██████████| 0.00/5.17k [00:00&amp;lt;?, ?B/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-4bits/commit/75600041ca6e38b5f1fb912ad1803b66656faae4&#x27;, commit_message=&#x27;Tokenizers for meta-llama/Meta-Llama-3-8B-Instruct: 4bits, gr128, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;75600041ca6e38b5f1fb912ad1803b66656faae4&#x27;, pr_url=None, pr_revision=None, pr_num=None)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantizacao do modelo para 2 bits">Quantização do modelo para 2 bits<a class="anchor-link" href="#Quantizacao do modelo para 2 bits">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantizá-lo para 2 bits. Reinicio o notebook para não ter problemas de memória e faço login novamente no Hugging Face</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>
<span class="w"> </span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro crio o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos a configuração de quantização. Agora dizemos que queremos quantizar para 2 bits. Além disso, é necessário indicar quantos vetores da matriz de pesos são quantizados de uma vez através do parâmetro <code>group_size</code>, antes por padrão tinha o valor 128 e não o alteramos, mas agora ao quantizar para 2 bits, para ter menos erro, colocamos um valor menor. Se deixarmos em 128, o modelo quantizado funcionaria muito mal, nesse caso vou colocar um valor de 16.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTQConfig</span>
<span class="w"> </span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model_2bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Loading checkpoint shards: 100%|██████████| 4/4 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing model.layers blocks : 100%|██████████|32/32 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
&#x20;&#x20;warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantization time: 1973.12 s = 32.89 min
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que também levou cerca de meia hora</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver a memória que ocupa agora</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_2bits_memory</span> <span class="o">=</span> <span class="n">model_2bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_2bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 4.50 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Enquanto quantizado em 4 bits ocupava 5,34 GB e em 3 bits ocupava 4,52 GB, agora quantizado em 2 bits ocupa 4,50 GB, conseguindo assim reduzir ainda mais o tamanho do modelo.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e vemos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_2bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_2bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer.  # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
Inference time: 2.92 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que já a saída não é boa, além disso, o tempo de inferência é de 2,92 segundos, mais ou menos o mesmo que com 3 e 4 bits</p>
</section>
<section class="section-block-markdown-cell">
<p>Guardamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">&quot;./model_2bits/&quot;</span>
<span class="n">model_2bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;./model_2bits/tokenizer_config.json&#x27;,
 &#x27;./model_2bits/special_tokens_map.json&#x27;,
 &#x27;./model_2bits/tokenizer.json&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O subimos para o hub</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-2bits&quot;</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">model_2bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>model.safetensors: 100%|██████████| 4.83/4.83G [00:00&amp;lt;?, ?B/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/13ede006ce0dbbd8aca54212e960eff98ea5ec63&#x27;, commit_message=&#x27;AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr16, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;13ede006ce0dbbd8aca54212e960eff98ea5ec63&#x27;, pr_url=None, pr_revision=None, pr_num=None)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Quantizacao do modelo para 1 bit">Quantização do modelo para 1 bit<a class="anchor-link" href="#Quantizacao do modelo para 1 bit">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Vamos quantizá-lo para 1 bit. Reinicio o notebook para não ter problemas de memória e faço login novamente no Hugging Face</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">notebook_login</span>
<span class="w"> </span>
<span class="n">notebook_login</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro crio o tokenizador</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Criamos a configuração de quantização, agora dizemos que quantize apenas para 1 bit e além disso use um <code>group_size</code> de 8</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPTQConfig</span>
<span class="w"> </span>
<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">group_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">model_1bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
<span class="n">t_quantization</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantization time: </span><span class="si">{</span><span class="n">t_quantization</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s = </span><span class="si">{</span><span class="n">t_quantization</span><span class="o">/</span><span class="mi">60</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> min&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Loading checkpoint shards: 100%|██████████| 4/4 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing model.layers blocks : 100%|██████████|32/32 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantizing layers inside the block: 100%|██████████| 7/7 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
&#x20;&#x20;warnings.warn(
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Quantization time: 2030.38 s = 33.84 min
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que também leva uma média de trinta minutos para quantizar</p>
</section>
<section class="section-block-markdown-cell">
<p>Vamos ver a memória que ocupa agora</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">model_1bits_memory</span> <span class="o">=</span> <span class="n">model_1bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">model_1bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 5.42 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que neste caso ocupa até mesmo mais que quantizado a 2 bits, 4,52 GB.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e vemos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model_1bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_1bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineerimerszuimersimerspinsimersimersingoingoimersurosimersimersimersoleningoimersingopinsimersbirpinsimersimersimersorgeingoimersiringimersimersimersimersimersimersimersンディorge_REFERER ingest羊imersorgeimersimersendetingoШАhandsingo
Inference time: 3.12 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que a saída é muito ruim e além disso demora mais do que quando quantizamos para 2 bits</p>
</section>
<section class="section-block-markdown-cell">
<p>Salvamos o modelo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">save_folder</span> <span class="o">=</span> <span class="s2">&quot;./model_1bits/&quot;</span>
<span class="n">model_1bits</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_folder</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>(&#x27;./model_1bits/tokenizer_config.json&#x27;,
 &#x27;./model_1bits/special_tokens_map.json&#x27;,
 &#x27;./model_1bits/tokenizer.json&#x27;)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>O subimos para o hub</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">repo_id</span> <span class="o">=</span> <span class="s2">&quot;Llama-3-8B-Instruct-GPTQ-1bits&quot;</span>
<span class="n">commit_message</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;AutoGPTQ model for </span><span class="si">{</span><span class="n">checkpoint</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">bits</span><span class="si">}</span><span class="s2">bits, gr</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">group_size</span><span class="si">}</span><span class="s2">, desc_act=</span><span class="si">{</span><span class="n">quantization_config</span><span class="o">.</span><span class="n">desc_act</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">model_1bits</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>README.md: 100%|██████████| 0.00/5.17k [00:00&amp;lt;?, ?B/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Upload 2 LFS files: 100%|██████████| 0/2 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>model-00002-of-00002.safetensors: 100%|██████████| 0.00/1.05G [00:00&amp;lt;?, ?B/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>model-00001-of-00002.safetensors: 100%|██████████| 0.00/4.76G [00:00&amp;lt;?, ?B/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>CommitInfo(commit_url=&#x27;https://huggingface.co/Maximofn/Llama-3-8B-Instruct-GPTQ-2bits/commit/e59ccffc03247e7dcc418f98b482cc02dc7a168d&#x27;, commit_message=&#x27;AutoGPTQ model for meta-llama/Meta-Llama-3-8B-Instruct: 2bits, gr8, desc_act=False&#x27;, commit_description=&#x27;&#x27;, oid=&#x27;e59ccffc03247e7dcc418f98b482cc02dc7a168d&#x27;, pr_url=None, pr_revision=None, pr_num=None)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Resumo da quantizacao">Resumo da quantização<a class="anchor-link" href="#Resumo da quantizacao">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a comparar a quantização de 4, 3, 2 e 1 bit</p>
<table>
  <thead>
    <tr>
      <th>Bits</th>
      <th>Tempo de quantização (min)</th>
      <th>Memória (GB)</th>
      <th>Tempo de inferência (s)</th>
      <th>Qualidade da saída</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>FP16</td>
      <td>0</td>
      <td>14,96</td>
      <td>4,14</td>
      <td>Boa</td>
    </tr>
    <tr>
      <td>4</td>
      <td>32,20</td>
      <td>5,34</td>
      <td>2,34</td>
      <td>Boa</td>
    </tr>
    <tr>
      <td>3</td>
      <td>31,88</td>
      <td>4,52</td>
      <td>2,89</td>
      <td>Boa</td>
    </tr>
    <tr>
      <td>2</td>
      <td>32,89</td>
      <td>4,50</td>
      <td>2,92</td>
      <td>Ruim</td>
    </tr>
    <tr>
      <td>1</td>
      <td>33,84</td>
      <td>5,42</td>
      <td>3,12</td>
      <td>Ruim</td>
    </tr>
  </tbody>
</table>
</section>
<section class="section-block-markdown-cell">
<p>Vendo esta tabela vemos que não faz sentido, neste exemplo, quantizar a menos de 4 bits.</p>
<p>Quantizar para 1 e 2 bits claramente não faz sentido porque a qualidade da saída é ruim.</p>
<p>Mas embora a saída ao quantizar para 3 bits seja boa, começa a ser repetitiva, pelo que a longo prazo, provavelmente não seria uma boa ideia usar esse modelo. Além disso, nem o ganho de tempo de quantização, o ganho de VRAM nem o ganho de tempo de inferência é significativo em comparação com a quantização para 4 bits.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Carregamento do modelo salvo">Carregamento do modelo salvo<a class="anchor-link" href="#Carregamento do modelo salvo">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Agora que comparamos a quantização de modelos, vamos ver como seria para carregar o modelo de 4 bits que salvamos, já que, como vimos, é a melhor opção.</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro carregamos o tokenizador que temos usado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="w"> </span>
<span class="n">path</span> <span class="o">=</span> <span class="s2">&quot;./model_4bits&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora carregamos o modelo que salvamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="w"> </span>
<span class="n">load_model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Loading checkpoint shards: 100%|██████████| 2/2 [00:00&amp;lt;?, ?it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos a memória que ocupa</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">load_model_4bits_memory</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">load_model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 5.34 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que ocupa a mesma memória que quando o quantizamos, o que é lógico.</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e vemos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">load_model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer. I have a strong background in computer science and mathematics, and I have been working with machine learning models for several years. I am excited to be a part of this community and to share my knowledge and experience with others. I am particularly interested in
Inference time: 3.82 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que a inferência é boa e levou 3,82 segundos, um pouco mais do que quando a quantizamos. Mas como já disse anteriormente, seria necessário fazer este teste muitas vezes e tirar uma média.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Carregamento do modelo enviado para o hub">Carregamento do modelo enviado para o hub<a class="anchor-link" href="#Carregamento do modelo enviado para o hub">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Agora vamos ver como carregar o modelo de 4 bits que subimos ao Hub.</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro carregamos o tokenizador que we subimos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="w"> </span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;Maximofn/Llama-3-8B-Instruct-GPTQ-4bits&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora carregamos o modelo que salvamos.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="w"> </span>
<span class="n">load_model_4bits</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos a memória que ocupa</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">load_model_4bits_memory</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model memory: </span><span class="si">{</span><span class="n">load_model_4bits_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Model memory: 5.34 GB
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Ocupa a mesma memória</p>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos a inferência e vemos o tempo que leva</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="w"> </span>
<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello my name is Maximo and I am a Machine Learning Engineer&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">load_model_4bits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="w"> </span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">load_model_4bits</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
<span class="w">    </span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span>
<span class="w">    </span><span class="n">attention_mask</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
<span class="w">    </span><span class="n">max_length</span><span class="o">=</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inference time: </span><span class="si">{</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Hello my name is Maximo and I am a Machine Learning Engineer with a passion for building innovative AI solutions. I have been working in the field of AI for over 5 years, and have gained extensive experience in developing and implementing machine learning models for various industries.

In my free time, I enjoy reading books on
Inference time: 3.81 s
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que a inferência também é boa e levou 3,81 segundos.</p>
</section>