<section class="section-block-markdown-cell">
<h1 id="RAG:-Fundamentals-and-Advanced-Techniques">RAG: Fundamentals and Advanced Techniques<a class="anchor-link" href="#RAG:-Fundamentals-and-Advanced-Techniques">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>In this post we are going to see what the <code>RAG</code> (<code>Retrieval Augmented Generation</code>) technique consists of and how it can be implemented in a language model.</p>
</section>
<section class="section-block-markdown-cell">
<p>To make it free, instead of using an OpenAI account (as you'll see in most tutorials) we're going to use the <code>API inference</code> of Hugging Face, which has a free tier of 1000 requests per day, which is more than enough to make this post.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Setting-up-Hugging-Face's-API-Inference">Setting up Hugging Face's <code>API Inference</code><a class="anchor-link" href="#Setting-up-Hugging-Face's-API-Inference">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>To be able to use the <code>API Inference</code> of HuggingFace, the first thing you need is to have an account on HuggingFace, once you have it you have to go to <a href="https://huggingface.co/settings/keys">Access tokens</a> in your profile settings and generate a new token.</p>
<p>We need to give it a name, in my case I'm going to name it <code>rag-fundamentals</code> and enable the permission <code>Make calls to serverless Inference API</code>. A token will be created for us that we need to copy</p>
</section>
<section class="section-block-markdown-cell">
<p>To manage the token we are going to create a file in the same path where we are working called <code>.env</code> and we are going to put the token that we have copied in the file in the following way:</p>
<div class="highlight"><pre><span></span><span class="nv">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="o">=</span><span class="s2">"hf_...."</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Now to be able to obtain the token we need to have <code>dotenv</code> installed, which we install via</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>python-dotenv
</pre></div>
<p>And we run the following</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">dotenv</span>

<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>

<span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have a token, we create a client, for which we need to have the <code>huggingface_hub</code> library installed, which we do using conda or pip</p>
<div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>huggingface_hub
</pre></div>
<p>or</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>huggingface_hub
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Now we have to choose which model we are going to use. You can see the available models on the <a href="https://huggingface.co/docs/api-inference/supported-models">Supported models</a> page of the <code>API Inference</code> documentation of Hugging Face.</p>
<p>As at the time of writing the post, the best available is <code>Qwen2.5-72B-Instruct</code>, so we will use that model.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we can create the client</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">MODEL</span><span class="p">)</span>
<span class="n">client</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>&lt;InferenceClient(model='Qwen/Qwen2.5-72B-Instruct', timeout=None)&gt;</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We make a test to see if it works</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span>
	<span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, qué tal?"</span> <span class="p">}</span>
<span class="p">]</span>

<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
	<span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> 
	<span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
	<span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
	<span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
	<span class="n">stream</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>¡Hola! Estoy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="What-is-RAG?">What is <code>RAG</code>?<a class="anchor-link" href="#What-is-RAG?">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p><code>RAG</code> stands for <code>Retrieval Augmented Generation</code>, it's a technique created to obtain information from documents. Although LLMs can be very powerful and have a lot of knowledge, they will never be able to answer you about private documents, such as your company's reports, internal documentation, etc. That's why <code>RAG</code> was created, to be able to use these LLMs on that private documentation.</p>
<p><img alt="¿Qué es RAG?" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>The idea is that a user asks a question about that private documentation, the system is able to get the part of the documentation where the answer to that question is, it is passed to an LLM the question and the part of the documentation and the LLM generates the answer for the user</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="How-is-information-stored?">How is information stored?<a class="anchor-link" href="#How-is-information-stored?">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>It is known, and if you didn't know, I'll tell you now, that LLMs have a limit of information that can be passed to them, this is called the context window. This is due to the internal architectures of LLMs that are not relevant now. But the important thing is that you can't pass a document and a question without more, because it's likely that the LLM won't be able to process all that information.</p>
<p>In cases where more information is usually passed than its context window allows, what usually happens is that the LLM does not pay attention to the end of the input. Imagine you ask the LLM something about your document, that information is at the end of the document and the LLM does not read it.</p>
<p>Therefore, what is done is to divide the documentation into blocks called <code>chunk</code>s. So that the documentation is stored in a bunch of <code>chunk</code>s, which are pieces of that documentation. So when the user asks a question, the <code>chunk</code> where the answer to that question is, is passed to the LLM.</p>
<p>In addition to dividing the documentation into <code>chunks</code>, these are converted to embeddings, which are numerical representations of the <code>chunks</code>. This is done because LLMs actually don't understand text, but numbers, and the <code>chunks</code> are converted to numbers so that the LLM can understand them. If you want to learn more about embeddings, you can read my post about <a href="https://www.maximofn.com/transformers">transformers</a> in which I explain how transformers work, which is the architecture behind LLMs. You can also read my post about <a href="https://www.maximofn.com/chromadb">ChromaDB</a> where I explain how embeddings are stored in a vector database. And it would also be interesting for you to read my post about the <a href="https://www.maximofn.com/hugging-face-tokenizers">HuggingFace Tokenizers</a> library, which explains how text is tokenized, which is the step prior to generating embeddings.</p>
<p><img alt="RAG - embeddings" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG-embeddings.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<h3 id="How-to-get-the-correct-chunk?">How to get the correct <code>chunk</code>?<a class="anchor-link" href="#How-to-get-the-correct-chunk?">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We've said that the documentation is divided into <code>chunks</code> and the <code>chunk</code> containing the answer to the user's question is passed to the LLM. But, how do we know which <code>chunk</code> contains the answer? To do this, what is done is to convert the user's question into an embedding, and the similarity between the question's embedding and the embeddings of the <code>chunks</code> is calculated. So, the <code>chunk</code> with the highest similarity is the one that is passed to the LLM.</p>
<p><img alt="RAG - embeddings similarity" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-chunk_retreival.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Let's-review-what-RAG-is">Let's review what <code>RAG</code> is<a class="anchor-link" href="#Let's-review-what-RAG-is">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>On the one hand, we have the <code>retrieval</code>, which is obtaining the correct <code>chunk</code> of documentation, on the other hand, we have the <code>augmented</code>, which is passing the user's question and the <code>chunk</code> to the LLM, and finally, we have the <code>generation</code>, which is obtaining the response generated by the LLM.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Vector-Database">Vector Database<a class="anchor-link" href="#Vector-Database">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>We have seen that documentation is divided into <code>chunks</code> and stored in a vector database, so we need to use one. For this post, I'm going to use <a href="https://www.trychroma.com/">ChromaDB</a>, which is a widely used vector database and I also have a <a href="https://www.maximofn.com/chromadb">post</a> where I explain how it works.</p>
</section>
<section class="section-block-markdown-cell">
<p>To get started, we first need to install the ChromaDB library, to do this we install it with Conda or with pip</p>
<div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::chromadb
</pre></div>
<p>or</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>chromadb
</pre></div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Embedding-Function">Embedding Function<a class="anchor-link" href="#Embedding-Function">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>As we've said, everything will be based on embeddings, so the first thing we do is create a function to get embeddings from a text. We're going to use the <code>sentence-transformers/all-MiniLM-L6-v2</code> model</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">chromadb.utils.embedding_functions</span> <span class="k">as</span> <span class="nn">embedding_functions</span>

<span class="n">EMBEDDING_MODEL</span> <span class="o">=</span> <span class="s2">"sentence-transformers/all-MiniLM-L6-v2"</span>
      
<span class="n">huggingface_ef</span> <span class="o">=</span> <span class="n">embedding_functions</span><span class="o">.</span><span class="n">HuggingFaceEmbeddingFunction</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">EMBEDDING_MODEL</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We test the embedding function</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">huggingface_ef</span><span class="p">([</span><span class="s2">"Hello, how are you?"</span><span class="p">,])</span>
<span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[6]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(384,)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We obtain a 384-dimensional embedding. Although the mission of this post is not to explain embeddings, in summary, our embedding function has categorized the phrase <code>Hello, how are you?</code> in a 384-dimensional space.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Cliente-de-ChromaDB">Cliente de ChromaDB<a class="anchor-link" href="#Cliente-de-ChromaDB">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have our embedding function we can create a ChromaDB client</p>
</section>
<section class="section-block-markdown-cell">
<p>First, we create a folder where the vector database will be stored</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
      
<span class="n">chroma_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"chromadb_persisten_storage"</span><span class="p">)</span>
<span class="n">chroma_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we create the client</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">chromadb</span> <span class="kn">import</span> <span class="n">PersistentClient</span>

<span class="n">chroma_client</span> <span class="o">=</span> <span class="n">PersistentClient</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">chroma_path</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Collection">Collection<a class="anchor-link" href="#Collection">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>When we have the ChromaDB client, the next thing we need to do is create a collection. A collection is a set of vectors, in our case the <code>chunks</code> of the documentation.</p>
</section>
<section class="section-block-markdown-cell">
<p>We create it by specifying the embedding function we are going to use</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">collection_name</span> <span class="o">=</span> <span class="s2">"document_qa_collection"</span>
<span class="n">collection</span> <span class="o">=</span> <span class="n">chroma_client</span><span class="o">.</span><span class="n">get_or_create_collection</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">huggingface_ef</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Document-Upload">Document Upload<a class="anchor-link" href="#Document-Upload">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have created the vector database, we need to split the documentation into <code>chunk</code>s and store them in the vector database.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Document-Loading-Function">Document Loading Function<a class="anchor-link" href="#Document-Loading-Function">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>First, we create a function to load all <code>.txt</code> documents from a directory</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">),</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"id"</span><span class="p">:</span> <span class="n">file</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()}</span>

<span class="k">def</span> <span class="nf">load_documents_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">".txt"</span><span class="p">):</span>
            <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">documents</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Function-to-divide-documentation-into-chunks">Function to divide documentation into <code>chunk</code>s<a class="anchor-link" href="#Function-to-divide-documentation-into-chunks">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Once we have the documents, we divide them into <code>chunk</code>s</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">chunk_size</span>
        <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">chunk_overlap</span>
    <span class="k">return</span> <span class="n">chunks</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Function-to-generate-embeddings-from-a-chunk">Function to generate embeddings from a <code>chunk</code><a class="anchor-link" href="#Function-to-generate-embeddings-from-a-chunk">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have the <code>chunk</code>s, we generate the <code>embedding</code>s for each of them</p>
</section>
<section class="section-block-markdown-cell">
<p>Later we will see why, but to generate the embeddings we are going to do it locally and not through the Hugging Face API. To do this, we need to have <a href="https://pytorch.org">PyTorch</a> and <code>sentence-transformers</code> installed, so we do</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>sentence-transformers
</pre></div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">EMBEDDING_MODEL</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embedding</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's try this embedding function locally</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello, how are you?"</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">embedding</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(384,)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see that we get an embedding of the same dimension as when we did it with the Hugging Face API</p>
</section>
<section class="section-block-markdown-cell">
<p>The <code>sentence-transformers/all-MiniLM-L6-v2</code> model has only 22M parameters, so you will be able to run it on any GPU. Even if you don't have a GPU, you will be able to run it on a CPU.</p>
</section>
<section class="section-block-markdown-cell">
<p>The LLM we are going to use to generate responses, which is <code>Qwen2.5-72B-Instruct</code>, as its name suggests, is a 72B parameter model, so this model cannot be run on just any GPU and on a CPU it is unthinkable how slow it would be. That's why we will use this LLM via the API, but when generating the <code>embedding</code>s we can do it locally without any issues</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Documents-we-are-going-to-test-with">Documents we are going to test with<a class="anchor-link" href="#Documents-we-are-going-to-test-with">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>To perform all these tests, I have downloaded the dataset <a href="https://www.kaggle.com/datasets/harshsinghal/aws-case-studies-and-blogs">aws-case-studies-and-blogs</a> and left it in the <code>rag-txt_dataset</code> folder, with the following commands I tell you how to download and unzip it</p>
</section>
<section class="section-block-markdown-cell">
<p>We create the folder where we are going to download the documents</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>mkdir<span class="w"> </span>rag_txt_dataset
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We download the <code>.zip</code> with the documents</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>curl<span class="w"> </span>-L<span class="w"> </span>-o<span class="w"> </span>./rag_txt_dataset/archive.zip<span class="w"> </span>https://www.kaggle.com/api/v1/datasets/download/harshsinghal/aws-case-studies-and-blogs
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 1430k  100 1430k    0     0  1082k      0  0:00:01  0:00:01 --:--:-- 2440k
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We unzip the <code>.zip</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>unzip<span class="w"> </span>rag_txt_dataset/archive.zip<span class="w"> </span>-d<span class="w"> </span>rag_txt_dataset
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Archive:  rag_txt_dataset/archive.zip
  inflating: rag_txt_dataset/23andMe Case Study _ Life Sciences _ AWS.txt  
  inflating: rag_txt_dataset/36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt  
  inflating: rag_txt_dataset/54gene _ Case Study _ AWS.txt  
  inflating: rag_txt_dataset/6sense Case Study.txt  
  inflating: rag_txt_dataset/ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt  
  inflating: rag_txt_dataset/AEON Case Study.txt  
  inflating: rag_txt_dataset/ALTBalaji _ Amazon Web Services.txt  
  inflating: rag_txt_dataset/AWS Case Study - Ineos Team UK.txt  
  inflating: rag_txt_dataset/AWS Case Study - StreamAMG.txt  
  inflating: rag_txt_dataset/AWS Case Study_ Creditsafe.txt  
  inflating: rag_txt_dataset/AWS Case Study_ Immowelt.txt  
  inflating: rag_txt_dataset/AWS Customer Case Study _ Kepler Provides Effective Monitoring of Elderly Care Home Residents Using AWS _ AWS.txt  
  inflating: rag_txt_dataset/AWS announces 21 startups selected for the AWS generative AI accelerator _ AWS Startups Blog.txt  
  inflating: rag_txt_dataset/AWS releases smart meter data analytics _ AWS for Industries.txt  
  inflating: rag_txt_dataset/Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt  
  inflating: rag_txt_dataset/Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt  
  ...
  inflating: rag_txt_dataset/Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt  
  inflating: rag_txt_dataset/Zoox Case Study _ Automotive _ AWS.txt  
  inflating: rag_txt_dataset/e-banner Streamlines Its Contact Center Operations and Facilitates a Fully Remote Workforce with Amazon Connect _ e-banner Case Study _ AWS.txt  
  inflating: rag_txt_dataset/iptiQ Case Study.txt  
  inflating: rag_txt_dataset/mod.io Provides Low Latency Gamer Experience Globally on AWS _ Case Study _ AWS.txt  
  inflating: rag_txt_dataset/myposter Case Study.txt  
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's delete the <code>.zip</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>rm<span class="w"> </span>rag_txt_dataset/archive.zip
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see what's left</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls<span class="w"> </span>rag_txt_dataset
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>'23andMe Case Study _ Life Sciences _ AWS.txt'
'36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt'
'54gene _ Case Study _ AWS.txt'
'6sense Case Study.txt'
'Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt'
'Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt'
'Accelerating customer onboarding using Amazon Connect _ NCS Case Study _ AWS.txt'
'Accelerating Migration at Scale Using AWS Application Migration Service with 3M Company _ Case Study _ AWS.txt'
'Accelerating Time to Market Using AWS and AWS Partner AccelByte _ Omeda Studios Case Study _ AWS.txt'
'Achieving Burstable Scalability and Consistent Uptime Using AWS Lambda with TiVo _ Case Study _ AWS.txt'
'Acrobits Uses Amazon Chime SDK to Easily Create Video Conferencing Application Boosting Collaboration for Global Users _ Acrobits Case Study _ AWS.txt'
'Actuate AI Case study.txt'
'ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt'
'Adzuna doubles its email open rates using Amazon SES _ Adzuna Case Study _ AWS.txt'
'AEON Case Study.txt'
'ALTBalaji _ Amazon Web Services.txt'
'Amanotes Stays on Beat by Delivering Simple Music Games to Millions Worldwide on AWS.txt'
'Amazon OpenSearch Services vector database capabilities explained _ AWS Big Data Blog.txt'
'Anghami Case Study.txt'
'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt'
...
'What Will Generative AI Mean for Your Business_ _ AWS Cloud Enterprise Strategy Blog.txt'
'Which Recurring Business Processes Can Small and Medium Businesses Automate_ _ AWS Smart Business Blog.txt'
 Windsor.txt
'Wireless Car Case Study _ AWS IoT Core _ AWS.txt'
'Yamato Logistics (HK) case study.txt'
'Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt'
'Zoox Case Study _ Automotive _ AWS.txt'
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Creating-the-chunks!">Creating the <code>chunk</code>s!<a class="anchor-link" href="#Creating-the-chunks!">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We list the documents with the function we had created</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset_path</span> <span class="o">=</span> <span class="s2">"rag_txt_dataset"</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">load_documents_from_directory</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We check that we have done it well</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"id"</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Run Jobs at Scale While Optimizing for Cost Using Amazon EC2 Spot Instances with ActionIQ _ ActionIQ Case Study _ AWS.txt
Recommend and dynamically filter items based on user context in Amazon Personalize _ AWS Machine Learning Blog.txt
Windsor.txt
Bank of Montreal Case Study _ AWS.txt
The Mill Adventure Case Study.txt
Optimize software development with Amazon CodeWhisperer _ AWS DevOps Blog.txt
Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt
THREAD _ Life Sciences _ AWS.txt
Deep Pool Optimizes Software Quality Control Using Amazon QuickSight _ Deep Pool Case Study _ AWS.txt
Upstox Saves 1 Million Annually Using Amazon S3 Storage Lens _ Upstox Case Study _ AWS.txt
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Now we create the <code>chunk</code>s.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">chunked_documents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">split_text</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
        <span class="n">chunked_documents</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"id"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">document</span><span class="p">[</span><span class="s1">'id'</span><span class="p">]</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">chunk</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[17]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>3611</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As we can see, there are 3611 <code>chunk</code>s. Since the daily limit of the Hugging Face API is 1000 calls on the free account, if we want to create embeddings of all the <code>chunk</code>s, we would run out of available calls and also wouldn't be able to create embeddings of all the <code>chunk</code>s</p>
</section>
<section class="section-block-markdown-cell">
<p>We recall again, this embedding model is very small, only 22M parameters, so it can be run on almost any computer, faster or slower, but it can be run.</p>
</section>
<section class="section-block-markdown-cell">
<p>As we are only going to create the embeddings of the <code>chunk</code>s once, even if we don't have a very powerful computer and it takes a long time, it will only be executed once. Then, when we want to ask questions about the documentation, that's when we will generate the embeddings of the prompt with the Hugging Face API and use the LLM with the API. So, we will only have to go through the process of generating the embeddings of the <code>chunk</code>s once.</p>
</section>
<section class="section-block-markdown-cell">
<p>We generate the embeddings of the <code>chunk</code>s</p>
</section>
<section class="section-block-markdown-cell">
<p>Last library we are going to have to install. Since the process of generating the embeddings of the <code>chunk</code>s is going to be slow, we are going to install <code>tqdm</code> so that it shows us a progress bar. We install it with conda or with pip, as you prefer</p>
<div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::tqdm
</pre></div>
<p>or</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>tqdm
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>We generate the embeddings of the <code>chunk</code>s</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tqdm</span>

<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error with document </span><span class="si">{</span><span class="n">chunk</span><span class="p">[</span><span class="s1">'id'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>100%|██████████| 3611/3611 [00:16&lt;00:00, 220.75it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We see an example</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Chunk id: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'id'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\n\n</span><span class="s2">text: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'text'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\n\n</span><span class="s2">embedding shape: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'embedding'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Chunk id: BNS Group Case Study _ Amazon Web Services.txt_0,

text: Reducing Virtual Machines from 40 to 12
The founders of BNS had been contemplating a migration from the company’s on-premises data center to the public cloud and observed a growing demand for cloud-based operations among current and potential BNS customers.
Français
Configures security according to cloud best practices
Clive Pereira, R&amp;D director at BNS Group, explains, “The database that records Praisal’s SMS traffic resides in Praisal’s AWS environment. Praisal can now run complete analytics across its data and gain insights into what’s happening with its SMS traffic, which is a real game-changer for the organization.”  
Español
 AWS ISV Accelerate Program
 Receiving Strategic, Foundational Support from ISV Specialists
 Learn More
The value that AWS places on the ISV stream sealed the deal in our choice of cloud provider.” 
日本語
  Contact Sales 
BNS is an Australian software provider focused on secure enterprise SMS and fax messaging. Its software runs on the Windows platform and is l,

embedding shape: (384,)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Loading-the-chunks-into-the-vector-database">Loading the <code>chunk</code>s into the vector database<a class="anchor-link" href="#Loading-the-chunks-into-the-vector-database">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Once we have all the chunks generated, we load them into the vector database. We use <code>tqdm</code> again to show us a progress bar, because this is also going to be slow</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tqdm</span>

<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
    <span class="n">collection</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span>
        <span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]],</span>
        <span class="n">documents</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span>
        <span class="n">embeddings</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>100%|██████████| 3611/3611 [00:59&lt;00:00, 60.77it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Questions">Questions<a class="anchor-link" href="#Questions">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have the vector database, we can ask questions to the documentation. To do this, we need a function that returns the correct <code>chunk</code></p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Getting-the-correct-chunk">Getting the correct <code>chunk</code><a class="anchor-link" href="#Getting-the-correct-chunk">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now we need a function that returns the correct <code>chunk</code>, let's create it</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">collection</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query_texts</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">n_results</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Finally, we create a <code>query</code>.</p>
<p>To generate the query, I randomly selected the document <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code>, passed it to an LLM, and asked it to generate a question about the document. The question it generated is</p>
<pre><code>¿Cómo utilizó Neeva Karpenter y Amazon EC2 Spot Instances para mejorar la gestión de su infraestructura y la optimización de costos?
</code></pre>
<p>So we get the most relevant <code>chunk</code>s for that question</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">"How did Neeva use Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization?"</span>
<span class="n">top_chunks</span> <span class="o">=</span> <span class="n">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Let's see what <code>chunk</code>s it has returned</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_chunks</span><span class="p">[</span><span class="s2">"ids"</span><span class="p">][</span><span class="mi">0</span><span class="p">])):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Rank </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">'ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, distance: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">'distances'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Rank 1: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_0, distance: 0.29233667254447937
Rank 2: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_5, distance: 0.4007825255393982
Rank 3: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_1, distance: 0.4317566752433777
Rank 4: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_6, distance: 0.43832334876060486
Rank 5: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_4, distance: 0.44625571370124817
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>As I said, the document I had chosen at random was <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code> and as can be seen the <code>chunk</code>s it has returned are from that document. That is, out of more than 3000 <code>chunk</code>s that were in the database, it has been able to return the most relevant <code>chunk</code>s to that question, it seems that this works!</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Generate-the-response">Generate the response<a class="anchor-link" href="#Generate-the-response">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now that we have the most relevant <code>chunk</code>s, we pass them to the LLM, along with the question, so that it generates a response</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">relevant_chunks</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">chunk</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">relevant_chunks</span><span class="p">])</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"You are an assistant for question-answering. You have to answer the following question:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Answer the question with the following information:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">message</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">prompt</span> <span class="p">}</span>
    <span class="p">]</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> 
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
    <span class="k">return</span> <span class="n">response</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>We test the function</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">top_chunks</span><span class="p">[</span><span class="s2">"documents"</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Here’s how:

### Early Collaboration with Karpenter
In late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.

### Combining Spot Instances and On-Demand Instances
Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.

### Flexibility and Instance Diversification
According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter's adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.

### Improved Scalability and Agility
By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:
- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.
- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.

### Enhanced Development Cycles
The integration of Karpenter and Spot Instances has also accelerated Neeva's development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.

### Cost Savings and Budget Control
Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.

### Future Plans
Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, "The bulk of our compute is or will be managed using Karpenter going forward."

### Conclusion
By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>When I asked the LLM to generate a question about the document, I also asked it to generate the correct answer. This is the answer the LLM gave me</p>
<div class="highlight"><pre><span></span>Neeva utilizó Karpenter y Amazon EC2 Spot Instances para mejorar su gestión de infraestructura y optimización de costos de varias maneras:

Administración de Instancias Simplificada:

Karpenter: Al adoptar Karpenter, Neeva simplificó el proceso de aprovisionamiento y administración de recursos de cómputo para sus clústeres Amazon EKS. Karpenter aprovisiona y desaprovisiona instancias automáticamente en función de la carga de trabajo, eliminando la necesidad de configuraciones manuales y reduciendo la complejidad de comprender diferentes instancias de cómputo.
Instancias Spot: Neeva aprovechó las instancias Spot de Amazon EC2, que son capacidad de EC2 no utilizada disponible con un descuento significativo (hasta un 90% de ahorro de costos). Esto permitió a la empresa controlar los costos mientras cumplía con sus requisitos de rendimiento.
Escalabilidad mejorada:

Karpenter: La capacidad de Karpenter para escalar dinámicamente los recursos permitió a Neeva iniciar nuevas instancias rápidamente, lo que permitió a la empresa iterar a una velocidad mayor y realizar más experimentos en menos tiempo.
Instancias Spot: El uso de instancias Spot proporcionó flexibilidad y diversificación de instancias, lo que facilitó a Neeva escalar sus recursos de cómputo de manera eficiente.
Productividad mejorada:

Karpenter: Al democratizar los cambios de infraestructura, Karpenter permitió que cualquier ingeniero modificara configuraciones de Kubernetes, reduciendo la dependencia de la experiencia especializada. Esto ahorró al equipo de Neeva hasta 100 horas a la semana de tiempo de espera en administración de sistemas.
Instancias Spot: La capacidad de aprovisionar y desaprovisionar rápidamente instancias Spot redujo los retrasos en la canalización de desarrollo, asegurando que los trabajos no se bloquearan debido a la falta de recursos disponibles.
Eficiencia de Costos:

Karpenter: Las mejores prácticas de Karpenter para Spot Instances, incluyendo flexibilidad y diversificación de instancias, ayudaron a Neeva a utilizar estas instancias de manera más efectiva, manteniéndose dentro del presupuesto.
Instancias Spot: El ahorro de costos al utilizar instancias Spot permitió a Neeva ejecutar trabajos a gran escala, como la indexación, por casi el mismo costo pero en una fracción del tiempo. Por ejemplo, Neeva redujo sus trabajos de indexación de 18 horas a solo 3 horas.
Mejora en la Utilización de Recursos:

Karpenter: Karpenter proporcionó una mejor visibilidad en el uso de los recursos de cómputo, lo que permitió a Neeva rastrear y optimizar su consumo de recursos de manera más cercana.
Instancias Spot: La combinación de Karpenter y Instancias Spot permitió a Neeva ejecutar modelos de lenguaje grandes de manera más eficiente, mejorando la experiencia de búsqueda para sus usuarios.
En resumen, la adopción de Karpenter y Amazon EC2 Spot Instances por parte de Neeva mejoró significativamente la gestión de su infraestructura, la optimización de costos y la eficiencia general de desarrollo, lo que permitió a la empresa ofrecer mejores experiencias de búsqueda sin anuncios a sus usuarios.
</pre></div>
<p>And this has been the response generated by our <code>RAG</code></p>
<div class="highlight"><pre><span></span>Neeva, un motor de búsqueda nativo en la nube y sin anuncios fundado en 2019, ha aprovechado Karpenter y Amazon EC2 Spot Instances para mejorar significativamente la gestión de su infraestructura y la optimización de costos. Aquí está cómo:

### Colaboración Temprana con Karpenter
A fines de 2021, Neeva comenzó a trabajar en estrecha colaboración con el equipo de Karpenter, experimentando y contribuyendo con correcciones a una versión temprana de Karpenter. Esta colaboración permitió a Neeva integrar Karpenter con su panel de Kubernetes, lo que permitió a la empresa recopilar valiosas métricas sobre el uso y el rendimiento.

### Combinando Instancias Spot y Instancias a Petición
Neeva ejecuta sus trabajos a gran escala, lo que puede generar costos significativos. Para gestionar estos costos de manera efectiva, la empresa adoptó una combinación de instancias Spot de Amazon EC2 y instancias bajo demanda. Las instancias Spot permiten a Neeva ofrecer precios por la capacidad EC2 no utilizada, a menudo a una fracción del precio bajo demanda, mientras que las instancias bajo demanda proporcionan la confiabilidad necesaria para tuberías críticas.

### Flexibilidad y Diversificación de Instancias
Según Mohit Agarwal, líder de ingeniería de infraestructura en Neeva, la adopción de Karpenter de las mejores prácticas para Spot Instances, incluyendo la flexibilidad y la diversificación de instancias, ha sido crucial. Este enfoque garantiza que Neeva pueda ajustar dinámicamente sus recursos de cómputo para satisfacer las cargas de trabajo variables mientras minimiza los costos.

### Mejora de la escalabilidad y la agilidad
Al utilizar Karpenter para aprovisionar recursos de infraestructura para sus clústeres de Amazon EKS, Neeva ha logrado varios beneficios clave:
- **Escalabilidad**: Neeva puede escalar sus recursos de cómputo hacia arriba o hacia abajo según sea necesario, asegurando que siempre tenga la capacidad necesaria para manejar sus cargas de trabajo.
- **Agilidad**: La empresa puede iterar rápidamente y democratizar los cambios de infraestructura, reduciendo el tiempo dedicado a la administración de sistemas hasta 100 horas a la semana.

### Ciclos de Desarrollo Mejorados
La integración de Karpenter y Spot Instances también ha acelerado los ciclos de desarrollo de Neeva. La empresa ahora puede lanzar nuevas características y mejoras de manera más rápida, lo que es esencial para mantener una ventaja competitiva en el mercado de motores de búsqueda.

### Ahorros de Costos y Control de Presupuesto
Al utilizar instancias Spot, Neeva ha podido mantenerse dentro de su presupuesto mientras cumple con sus requisitos de rendimiento. Esta optimización de costos es fundamental para una empresa que prioriza las experiencias centradas en el usuario y no tiene incentivos competitivos de publicidad.

### Planes Futuros
Neeva se compromete a continuar su innovación y expansión. La empresa planea lanzarse en nuevas regiones y mejorar aún más su motor de búsqueda, todo mientras mantiene la eficiencia de costos. Como nota Mohit Agarwal, "La mayor parte de nuestro cómputo es o será administrado utilizando Karpenter en el futuro".

### Conclusión
Al aprovechar Karpenter y Amazon EC2 Spot Instances, Neeva no solo ha optimizado sus costos de infraestructura, sino que también ha mejorado su escalabilidad, agilidad y velocidad de desarrollo. Este enfoque estratégico ha posicionado a Neeva para ofrecer experiencias de búsqueda de alta calidad y sin anuncios a sus usuarios, mientras mantiene un fuerte enfoque en el control de costos y la innovación.
</pre></div>
<p>So we can conclude that <code>RAG</code> has worked correctly!!!</p>
</section>
