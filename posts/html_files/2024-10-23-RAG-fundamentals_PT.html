<section class="section-block-markdown-cell">
<h1 id="RAG:-Fundamentos-e-t%C3%A9cnicas-avan%C3%A7adas">RAG: Fundamentos e técnicas avançadas<a class="anchor-link" href="#RAG:-Fundamentos-e-t%C3%A9cnicas-avan%C3%A7adas">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Aviso: Este post foi traduzido para o português usando um modelo de tradução automática. Por favor, me avise se encontrar algum erro.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>Neste post vamos a ver em que consiste a técnica de <code>RAG</code> (<code>Retrieval Augmented Generation</code>) e como se pode implementar em um modelo de linguagem.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para que saia de graça, em vez de usar uma conta do OpenAI (como você verá na maioria dos tutoriais) vamos usar o <code>API inference</code> do Hugging Face, que tem um plano gratuito de 1000 requisições por dia, que para fazer este post é mais do que suficiente.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Configura%C3%A7%C3%A3o-da-API-Inference-da-Hugging-Face">Configuração da <code>API Inference</code> da Hugging Face<a class="anchor-link" href="#Configura%C3%A7%C3%A3o-da-API-Inference-da-Hugging-Face">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Para poder usar a <code>API Inference</code> da HuggingFace, o primeiro que você precisa é ter uma conta na HuggingFace, uma vez que a tenha, é necessário ir para <a href="https://huggingface.co/settings/keys">Access tokens</a> nas configurações do seu perfil e gerar um novo token.</p>
<p>Temos que colocar um nome, no meu caso vou colocá-lo como <code>rag-fundamentals</code> e habilitar a permissão <code>Make calls to serverless Inference API</code>. Será criado um token que temos que copiar</p>
</section>
<section class="section-block-markdown-cell">
<p>Para gerenciar o token vamos a criar um arquivo na mesma rota em que estivermos trabalhando chamado <code>.env</code> e vamos a colocar o token que copiamos no arquivo da seguinte maneira:</p>
<div class="highlight"><pre><span></span><span class="nv">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="o">=</span><span class="s2">"hf_...."</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Agora para poder obter o token precisamos ter instalado <code>dotenv</code>, que o instalamos mediante</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>python-dotenv
</pre></div>
<p>E executamos o seguinte</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">dotenv</span>

<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>

<span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora que temos um token, criamos um cliente, para isso precisamos ter instalada a biblioteca <code>huggingface_hub</code>, que fazemos mediante conda ou pip</p>
<div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>huggingface_hub
</pre></div>
<p>o</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>huggingface_hub
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Agora temos que escolher qual modelo vamos usar. Você pode ver os modelos disponíveis na página de <a href="https://huggingface.co/docs/api-inference/supported-models">Supported models</a> da documentação da <code>API Inference</code> da Hugging Face.</p>
<p>Como na hora de escrever o post, o melhor disponível é <code>Qwen2.5-72B-Instruct</code>, vamos usar esse modelo.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">MODEL</span> <span class="o">=</span> <span class="s2">"Qwen/Qwen2.5-72B-Instruct"</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora podemos criar o cliente</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">MODEL</span><span class="p">)</span>
<span class="n">client</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[3]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>&lt;InferenceClient(model='Qwen/Qwen2.5-72B-Instruct', timeout=None)&gt;</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Fazemos uma prova para ver se funciona</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span>
	<span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="s2">"Hola, qué tal?"</span> <span class="p">}</span>
<span class="p">]</span>

<span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
	<span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> 
	<span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
	<span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
	<span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
	<span class="n">stream</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>¡Hola! Estoy bien, gracias por preguntar. ¿Cómo estás tú? ¿En qué puedo ayudarte hoy?
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="O-que-%C3%A9-RAG?">O que é <code>RAG</code>?<a class="anchor-link" href="#O-que-%C3%A9-RAG?">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p><code>RAG</code> são as siglas de <code>Retrieval Augmented Generation</code>, é uma técnica criada para obter informações de documentos. Embora os LLMs possam chegar a ser muito poderosos e ter muito conhecimento, nunca vão ser capazes de responder sobre alguns documentos privados, como relatórios da sua empresa, documentação interna, etc. Por isso foi criado o <code>RAG</code>, para poder usar esses LLMs nessa documentação privada.</p>
<p><img alt="O que é RAG?" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>A ideia consiste em que um usuário faz uma pergunta sobre essa documentação privada, o sistema é capaz de obter a parte da documentação na qual está a resposta para essa pergunta, passa-se a pergunta e a parte da documentação para um LLM e o LLM gera a resposta para o usuário</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Como-a-informa%C3%A7%C3%A3o-%C3%A9-armazenada?">Como a informação é armazenada?<a class="anchor-link" href="#Como-a-informa%C3%A7%C3%A3o-%C3%A9-armazenada?">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>É sabido, e se não sabia, agora estou te contando, que os LLMs têm um limite de informação que pode ser passada para eles, isso é chamado de janela de contexto. Isso ocorre devido às arquiteturas internas dos LLMs, que agora não vêm ao caso. Mas o importante é que não se pode passar um documento e uma pergunta sem mais, porque é provável que o LLM não seja capaz de processar toda essa informação.</p>
<p>Nos casos em que se costuma passar mais informações do que a janela de contexto permite, o que geralmente acontece é que o LLM não presta atenção ao final da entrada. Imagine que você pergunte ao LLM algo sobre o seu documento, que essa informação esteja no final do documento e o LLM não a leia.</p>
<p>Por isso, o que se faz é dividir a documentação em blocos chamados <code>chunk</code>s. De modo que a documentação é armazenada em um monte de <code>chunk</code>s, que são pedaços daquela documentação. Assim, quando o usuário faz uma pergunta, passa-se ao LLM o <code>chunk</code> em que está a resposta para aquela pergunta.</p>
<p>Além de dividir a documentação em <code>chunk</code>s, esses são convertidos em embeddings, que são representações numéricas dos <code>chunk</code>s. Isso é feito porque os LLMs na verdade não entendem texto, mas números, e os <code>chunk</code>s são convertidos em números para que o LLM possa entendê-los. Se você quiser entender mais sobre os embeddings, pode ler meu post sobre <a href="https://www.maximofn.com/transformers">transformers</a> no qual explico como funcionam os transformers, que é a arquitetura por trás dos LLMs. Você também pode ler meu post sobre <a href="https://www.maximofn.com/chromadb">ChromaDB</a> onde explico como os embeddings são armazenados em um banco de dados vetorial. E além disso, seria interessante que lesse meu post sobre a biblioteca <a href="https://www.maximofn.com/hugging-face-tokenizers">HuggingFace Tokenizers</a> na qual se explica como o texto é tokenizado, que é o passo anterior à geração dos embeddings</p>
<p><img alt="RAG - embeddings" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/RAG-embeddings.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Como-se-obt%C3%A9m-o-chunk-correto?">Como se obtém o <code>chunk</code> correto?<a class="anchor-link" href="#Como-se-obt%C3%A9m-o-chunk-correto?">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Dissemos que a documentação se divide em <code>chunk</code>s e se passa ao LLM o <code>chunk</code> no qual está a resposta à pergunta do usuário. Mas, como se sabe em qual <code>chunk</code> está a resposta? Para isso, o que se faz é converter a pergunta do usuário em um embedding, e se calcula a similaridade entre o embedding da pergunta e os embeddings dos <code>chunk</code>s. De modo que o <code>chunk</code> com maior similaridade é o que se passa ao LLM.</p>
<p><img alt="RAG - semelhança de embeddings" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/rag-chunk_retreival.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Vamos-rever-o-que-%C3%A9-RAG">Vamos rever o que é <code>RAG</code><a class="anchor-link" href="#Vamos-rever-o-que-%C3%A9-RAG">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Por um lado, temos o <code>retrieval</code>, que é obter o <code>chunk</code> correto da documentação, por outro lado, temos o <code>augmented</code>, que é passar ao LLM a pergunta do usuário e o <code>chunk</code> e, por último, temos o <code>generation</code>, que é obter a resposta gerada pelo LLM.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Base-de-dados-vetorial">Base de dados vetorial<a class="anchor-link" href="#Base-de-dados-vetorial">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Vimos que a documentação se divide em <code>chunk</code>s e é salva em um banco de dados vetorial, por isso precisamos usar um. Para este post vou usar <a href="https://www.trychroma.com/">ChromaDB</a>, que é um banco de dados vetorial bastante usado e sobre o qual também tenho um <a href="https://www.maximofn.com/chromadb">post</a> em que explico como funciona.</p>
</section>
<section class="section-block-markdown-cell">
<p>Para isso, primeiro precisamos instalar a biblioteca do ChromaDB, para isso a instalamos com Conda ou com pip</p>
<div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::chromadb
</pre></div>
<p>o</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>chromadb
</pre></div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Fun%C3%A7%C3%A3o-de-embedding">Função de embedding<a class="anchor-link" href="#Fun%C3%A7%C3%A3o-de-embedding">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Como dissemos, tudo se baseará em embeddings, por isso o primeiro que fazemos é criar uma função para obter embeddings de um texto. Vamos usar o modelo <code>sentence-transformers/all-MiniLM-L6-v2</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">chromadb.utils.embedding_functions</span> <span class="k">as</span> <span class="nn">embedding_functions</span>

<span class="n">EMBEDDING_MODEL</span> <span class="o">=</span> <span class="s2">"sentence-transformers/all-MiniLM-L6-v2"</span>
      
<span class="n">huggingface_ef</span> <span class="o">=</span> <span class="n">embedding_functions</span><span class="o">.</span><span class="n">HuggingFaceEmbeddingFunction</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">RAG_FUNDAMENTALS_ADVANCE_TECHNIQUES_TOKEN</span><span class="p">,</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">EMBEDDING_MODEL</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Testamos a função de embedding</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">embedding</span> <span class="o">=</span> <span class="n">huggingface_ef</span><span class="p">([</span><span class="s2">"Hello, how are you?"</span><span class="p">,])</span>
<span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[6]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(384,)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Obtemos um embedding de dimensão 384. Embora a missão deste post não seja explicar os embeddings, em resumo, nossa função de embedding categorizou a frase <code>Hello, how are you?</code> em um espaço de 384 dimensões.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Cliente-ChromaDB">Cliente ChromaDB<a class="anchor-link" href="#Cliente-ChromaDB">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Agora que temos nossa função de embedding podemos criar um cliente de ChromaDB</p>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro criamos uma pasta onde será guardado o banco de dados vetorial</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
      
<span class="n">chroma_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"chromadb_persisten_storage"</span><span class="p">)</span>
<span class="n">chroma_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos o cliente</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">chromadb</span> <span class="kn">import</span> <span class="n">PersistentClient</span>

<span class="n">chroma_client</span> <span class="o">=</span> <span class="n">PersistentClient</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">chroma_path</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Cole%C3%A7%C3%A3o">Coleção<a class="anchor-link" href="#Cole%C3%A7%C3%A3o">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Quando temos o cliente do ChromaDB, o seguinte que precisamos é criar uma coleção. Uma coleção é um conjunto de vetores, no nosso caso os <code>chunks</code> da documentação.</p>
</section>
<section class="section-block-markdown-cell">
<p>Criamos indicando-lhe a função de embedding que vamos a usar</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">collection_name</span> <span class="o">=</span> <span class="s2">"document_qa_collection"</span>
<span class="n">collection</span> <span class="o">=</span> <span class="n">chroma_client</span><span class="o">.</span><span class="n">get_or_create_collection</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span> <span class="n">embedding_function</span><span class="o">=</span><span class="n">huggingface_ef</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Carga-de-documentos">Carga de documentos<a class="anchor-link" href="#Carga-de-documentos">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Agora que criamos a base de dados vetorial, temos que dividir a documentação em <code>chunk</code>s e salvá-los na base de dados vetorial.</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Fun%C3%A7%C3%A3o-de-carregamento-de-documentos">Função de carregamento de documentos<a class="anchor-link" href="#Fun%C3%A7%C3%A3o-de-carregamento-de-documentos">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Primeiro criamos uma função para carregar todos os documentos <code>.txt</code> de um diretório</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">):</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">),</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">"id"</span><span class="p">:</span> <span class="n">file</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()}</span>

<span class="k">def</span> <span class="nf">load_documents_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">".txt"</span><span class="p">):</span>
            <span class="n">documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_one_document_from_directory</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="n">file</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">documents</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Fun%C3%A7%C3%A3o-para-dividir-a-documenta%C3%A7%C3%A3o-em-chunks">Função para dividir a documentação em <code>chunk</code>s<a class="anchor-link" href="#Fun%C3%A7%C3%A3o-para-dividir-a-documenta%C3%A7%C3%A3o-em-chunks">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Uma vez que temos os documentos, os dividimos em <code>chunk</code>s</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">chunk_size</span>
        <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">chunk_overlap</span>
    <span class="k">return</span> <span class="n">chunks</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Fun%C3%A7%C3%A3o-para-gerar-embeddings-de-um-chunk">Função para gerar embeddings de um <code>chunk</code><a class="anchor-link" href="#Fun%C3%A7%C3%A3o-para-gerar-embeddings-de-um-chunk">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Agora que temos os <code>chunk</code>s, geramos os <code>embedding</code>s de cada um deles</p>
</section>
<section class="section-block-markdown-cell">
<p>Depois veremos por quê, mas para gerar os embeddings vamos a fazê-lo de maneira local e não por meio da API da Hugging Face. Para isso, precisamos ter instalado o <a href="https://pytorch.org">PyTorch</a> e <code>sentence-transformers</code>, para isso fazemos</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>sentence-transformers
</pre></div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>

<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">EMBEDDING_MODEL</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embedding</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a testar agora esta função de embeddings localmente</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s2">"Hello, how are you?"</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">embedding</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[13]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>(384,)</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos que obtemos um embedding da mesma dimensão que quando o fazíamos com a API da Hugging Face</p>
</section>
<section class="section-block-markdown-cell">
<p>O modelo <code>sentence-transformers/all-MiniLM-L6-v2</code> tem apenas 22M de parâmetros, por isso você poderá executá-lo em qualquer GPU. Mesmo que você não tenha uma GPU, poderá executá-lo em uma CPU.</p>
</section>
<section class="section-block-markdown-cell">
<p>O LLM que vamos a usar para gerar as respostas, que é o <code>Qwen2.5-72B-Instruct</code>, como o seu nome indica, é um modelo de 72B de parâmetros, por isso este modelo não pode ser executado em qualquer GPU e em uma CPU é impensável devido à lentidão. Por isso, este LLM sim o usaremos por meio da API, mas na hora de gerar os <code>embedding</code>s podemos fazer localmente sem problemas</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Documentos-com-os-quais-vamos-testar">Documentos com os quais vamos testar<a class="anchor-link" href="#Documentos-com-os-quais-vamos-testar">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Para fazer todas estas provas, eu baixei o conjunto de dados <a href="https://www.kaggle.com/datasets/harshsinghal/aws-case-studies-and-blogs">aws-case-studies-and-blogs</a> e o coloquei na pasta <code>rag-txt_dataset</code>, com os seguintes comandos eu digo como baixá-lo e descomprimi-lo</p>
</section>
<section class="section-block-markdown-cell">
<p>Criamos a pasta onde vamos baixar os documentos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>mkdir<span class="w"> </span>rag_txt_dataset
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Descarregamos o <code>.zip</code> com os documentos</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>curl<span class="w"> </span>-L<span class="w"> </span>-o<span class="w"> </span>./rag_txt_dataset/archive.zip<span class="w"> </span>https://www.kaggle.com/api/v1/datasets/download/harshsinghal/aws-case-studies-and-blogs
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 1430k  100 1430k    0     0  1082k      0  0:00:01  0:00:01 --:--:-- 2440k
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Descomprimimos o <code>.zip</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>unzip<span class="w"> </span>rag_txt_dataset/archive.zip<span class="w"> </span>-d<span class="w"> </span>rag_txt_dataset
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Archive:  rag_txt_dataset/archive.zip
  inflating: rag_txt_dataset/23andMe Case Study _ Life Sciences _ AWS.txt  
  inflating: rag_txt_dataset/36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt  
  inflating: rag_txt_dataset/54gene _ Case Study _ AWS.txt  
  inflating: rag_txt_dataset/6sense Case Study.txt  
  inflating: rag_txt_dataset/ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt  
  inflating: rag_txt_dataset/AEON Case Study.txt  
  inflating: rag_txt_dataset/ALTBalaji _ Amazon Web Services.txt  
  inflating: rag_txt_dataset/AWS Case Study - Ineos Team UK.txt  
  inflating: rag_txt_dataset/AWS Case Study - StreamAMG.txt  
  inflating: rag_txt_dataset/AWS Case Study_ Creditsafe.txt  
  inflating: rag_txt_dataset/AWS Case Study_ Immowelt.txt  
  inflating: rag_txt_dataset/AWS Customer Case Study _ Kepler Provides Effective Monitoring of Elderly Care Home Residents Using AWS _ AWS.txt  
  inflating: rag_txt_dataset/AWS announces 21 startups selected for the AWS generative AI accelerator _ AWS Startups Blog.txt  
  inflating: rag_txt_dataset/AWS releases smart meter data analytics _ AWS for Industries.txt  
  inflating: rag_txt_dataset/Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt  
  inflating: rag_txt_dataset/Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt  
  ...
  inflating: rag_txt_dataset/Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt  
  inflating: rag_txt_dataset/Zoox Case Study _ Automotive _ AWS.txt  
  inflating: rag_txt_dataset/e-banner Streamlines Its Contact Center Operations and Facilitates a Fully Remote Workforce with Amazon Connect _ e-banner Case Study _ AWS.txt  
  inflating: rag_txt_dataset/iptiQ Case Study.txt  
  inflating: rag_txt_dataset/mod.io Provides Low Latency Gamer Experience Globally on AWS _ Case Study _ AWS.txt  
  inflating: rag_txt_dataset/myposter Case Study.txt  
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Removamos o <code>.zip</code></p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>rm<span class="w"> </span>rag_txt_dataset/archive.zip
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vemos o que nos quedou</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls<span class="w"> </span>rag_txt_dataset
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>'23andMe Case Study _ Life Sciences _ AWS.txt'
'36 new or updated datasets on the Registry of Open Data_ AI analysis-ready datasets and more _ AWS Public Sector Blog.txt'
'54gene _ Case Study _ AWS.txt'
'6sense Case Study.txt'
'Accelerate Time to Business Value Using Amazon SageMaker at Scale with NatWest Group _ Case Study _ AWS.txt'
'Accelerate Your Analytics Journey on AWS with DXC Analytics and AI Platform _ AWS Partner Network (APN) Blog.txt'
'Accelerating customer onboarding using Amazon Connect _ NCS Case Study _ AWS.txt'
'Accelerating Migration at Scale Using AWS Application Migration Service with 3M Company _ Case Study _ AWS.txt'
'Accelerating Time to Market Using AWS and AWS Partner AccelByte _ Omeda Studios Case Study _ AWS.txt'
'Achieving Burstable Scalability and Consistent Uptime Using AWS Lambda with TiVo _ Case Study _ AWS.txt'
'Acrobits Uses Amazon Chime SDK to Easily Create Video Conferencing Application Boosting Collaboration for Global Users _ Acrobits Case Study _ AWS.txt'
'Actuate AI Case study.txt'
'ADP Developed an Innovative and Secure Digital Wallet in a Few Months Using AWS Services _ Case Study _ AWS.txt'
'Adzuna doubles its email open rates using Amazon SES _ Adzuna Case Study _ AWS.txt'
'AEON Case Study.txt'
'ALTBalaji _ Amazon Web Services.txt'
'Amanotes Stays on Beat by Delivering Simple Music Games to Millions Worldwide on AWS.txt'
'Amazon OpenSearch Services vector database capabilities explained _ AWS Big Data Blog.txt'
'Anghami Case Study.txt'
'Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt'
...
'What Will Generative AI Mean for Your Business_ _ AWS Cloud Enterprise Strategy Blog.txt'
'Which Recurring Business Processes Can Small and Medium Businesses Automate_ _ AWS Smart Business Blog.txt'
 Windsor.txt
'Wireless Car Case Study _ AWS IoT Core _ AWS.txt'
'Yamato Logistics (HK) case study.txt'
'Zomato Saves Big by Using AWS Graviton2 to Power Data-Driven Business Insights.txt'
'Zoox Case Study _ Automotive _ AWS.txt'
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="A-criar-os-chunks!">A criar os <code>chunk</code>s!<a class="anchor-link" href="#A-criar-os-chunks!">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Listamos os documentos com a função que havíamos criado</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">dataset_path</span> <span class="o">=</span> <span class="s2">"rag_txt_dataset"</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">load_documents_from_directory</span><span class="p">(</span><span class="n">dataset_path</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Verificamos que o fizemos bem</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"id"</span><span class="p">])</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Run Jobs at Scale While Optimizing for Cost Using Amazon EC2 Spot Instances with ActionIQ _ ActionIQ Case Study _ AWS.txt
Recommend and dynamically filter items based on user context in Amazon Personalize _ AWS Machine Learning Blog.txt
Windsor.txt
Bank of Montreal Case Study _ AWS.txt
The Mill Adventure Case Study.txt
Optimize software development with Amazon CodeWhisperer _ AWS DevOps Blog.txt
Announcing enhanced table extractions with Amazon Textract _ AWS Machine Learning Blog.txt
THREAD _ Life Sciences _ AWS.txt
Deep Pool Optimizes Software Quality Control Using Amazon QuickSight _ Deep Pool Case Study _ AWS.txt
Upstox Saves 1 Million Annually Using Amazon S3 Storage Lens _ Upstox Case Study _ AWS.txt
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Agora criamos os <code>chunk</code>s.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">chunked_documents</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">split_text</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
        <span class="n">chunked_documents</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">"id"</span><span class="p">:</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">document</span><span class="p">[</span><span class="s1">'id'</span><span class="p">]</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"text"</span><span class="p">:</span> <span class="n">chunk</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt-output-prompt">Out[17]:</div>
<div class="output-text-output-subareaoutput_execute_result">
<pre>3611</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como vemos, há 3611 <code>chunk</code>s. Como o limite diário da API da Hugging Face são 1000 chamadas na conta gratuita, se quisermos criar embeddings de todos os <code>chunk</code>s, acabaríamos as chamadas disponíveis e, além disso, não poderíamos criar embeddings de todos os <code>chunk</code>s</p>
</section>
<section class="section-block-markdown-cell">
<p>Lembramos que este modelo de embeddings é muito pequeno, somente 22M de parâmetros, por isso quase em qualquer computador pode ser executado, mais rápido ou mais lento, mas é possível.</p>
</section>
<section class="section-block-markdown-cell">
<p>Como só vamos a criar os embeddings dos <code>chunk</code>s uma vez, embora não tenhamos um computador muito potente e leve muito tempo, só será executado uma vez. Depois, quando quisermos fazer perguntas sobre a documentação, aí sim geraremos os embeddings do prompt com a API da Hugging Face e usaremos o LLM com a API. Portanto, só vamos ter que passar pelo processo de gerar os embeddings dos <code>chunk</code>s uma vez</p>
</section>
<section class="section-block-markdown-cell">
<p>Geramos os embeddings dos <code>chunk</code>s</p>
</section>
<section class="section-block-markdown-cell">
<p>Última biblioteca que vamos a ter que instalar. Como o processo de gerar os embeddings dos <code>chunk</code>s vai ser lento, vamos a instalar <code>tqdm</code> para que nos mostre uma barra de progresso. A instalamos com conda ou com pip, como preferir.</p>
<div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>conda-forge::tqdm
</pre></div>
<p>o</p>
<div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>tqdm
</pre></div>
</section>
<section class="section-block-markdown-cell">
<p>Geramos os embeddings dos <code>chunk</code>s</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tqdm</span>

<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Error with document </span><span class="si">{</span><span class="n">chunk</span><span class="p">[</span><span class="s1">'id'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>100%|██████████| 3611/3611 [00:16&lt;00:00, 220.75it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vejamos um exemplo</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>

<span class="n">idx</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Chunk id: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'id'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\n\n</span><span class="s2">text: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'text'</span><span class="p">]</span><span class="si">}</span><span class="s2">,</span><span class="se">\n\n</span><span class="s2">embedding shape: </span><span class="si">{</span><span class="n">chunked_documents</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="s1">'embedding'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Chunk id: BNS Group Case Study _ Amazon Web Services.txt_0,

text: Reducing Virtual Machines from 40 to 12
The founders of BNS had been contemplating a migration from the company’s on-premises data center to the public cloud and observed a growing demand for cloud-based operations among current and potential BNS customers.
Français
Configures security according to cloud best practices
Clive Pereira, R&amp;D director at BNS Group, explains, “The database that records Praisal’s SMS traffic resides in Praisal’s AWS environment. Praisal can now run complete analytics across its data and gain insights into what’s happening with its SMS traffic, which is a real game-changer for the organization.”  
Español
 AWS ISV Accelerate Program
 Receiving Strategic, Foundational Support from ISV Specialists
 Learn More
The value that AWS places on the ISV stream sealed the deal in our choice of cloud provider.” 
日本語
  Contact Sales 
BNS is an Australian software provider focused on secure enterprise SMS and fax messaging. Its software runs on the Windows platform and is l,

embedding shape: (384,)
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Carregar-os-chunks-no-banco-de-dados-vetorial">Carregar os <code>chunk</code>s no banco de dados vetorial<a class="anchor-link" href="#Carregar-os-chunks-no-banco-de-dados-vetorial">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Uma vez que temos todos os chunks gerados, os carregamos na base de dados vetorial. Voltamos a usar <code>tqdm</code> para que nos mostre uma barra de progresso, porque isso também vai ser lento</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tqdm</span>

<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="n">chunked_documents</span><span class="p">)</span>

<span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
    <span class="n">collection</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span>
        <span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"id"</span><span class="p">]],</span>
        <span class="n">documents</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"text"</span><span class="p">],</span>
        <span class="n">embeddings</span><span class="o">=</span><span class="n">chunk</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stderr-output-text">
<pre>100%|██████████| 3611/3611 [00:59&lt;00:00, 60.77it/s]
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Perguntas">Perguntas<a class="anchor-link" href="#Perguntas">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>Agora que temos a base de dados vetorial, podemos fazer perguntas à documentação. Para isso, precisamos de uma função que nos devolva o <code>chunk</code> correto</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Obter-o-chunk-correto">Obter o <code>chunk</code> correto<a class="anchor-link" href="#Obter-o-chunk-correto">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Agora precisamos de uma função que nos devolva o <code>chunk</code> correto, vamos criá-la</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">collection</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query_texts</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">n_results</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Por último, criamos uma <code>query</code>.</p>
<p>Para gerar a consulta, peguei aleatoriamente o documento <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code>, passei-o para um LLM e pedi que gerasse uma pergunta sobre o documento. A pergunta que gerou é</p>
<pre><code>Como a Neeva utilizou o Karpenter e as Instâncias Spot da Amazon EC2 para melhorar a gestão de infraestrutura e a otimização de custos?
</code></pre>
<p>Então obtemos os <code>chunk</code>s mais relevantes para essa pergunta</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">"How did Neeva use Karpenter and Amazon EC2 Spot Instances to improve its infrastructure management and cost optimization?"</span>
<span class="n">top_chunks</span> <span class="o">=</span> <span class="n">get_top_k_documents</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Vamos a ver qué <code>chunk</code>s nos devolveu</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">top_chunks</span><span class="p">[</span><span class="s2">"ids"</span><span class="p">][</span><span class="mi">0</span><span class="p">])):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Rank </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">'ids'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, distance: </span><span class="si">{</span><span class="n">top_chunks</span><span class="p">[</span><span class="s1">'distances'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Rank 1: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_0, distance: 0.29233667254447937
Rank 2: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_5, distance: 0.4007825255393982
Rank 3: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_1, distance: 0.4317566752433777
Rank 4: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_6, distance: 0.43832334876060486
Rank 5: Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt_4, distance: 0.44625571370124817
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Como eu havia dito, o documento que eu havia escolhido ao acaso era <code>Using Amazon EC2 Spot Instances and Karpenter to Simplify and Optimize Kubernetes Infrastructure _ Neeva Case Study _ AWS.txt</code> e como se pode ver os <code>chunk</code>s que nos devolveu são desse documento. Ou seja, de mais de 3000 <code>chunk</code>s que havia no banco de dados, foi capaz de devolver os <code>chunk</code>s mais relevantes diante disso, parece que isso funciona!</p>
</section>
<section class="section-block-markdown-cell">
<h3 id="Gerar-a-resposta">Gerar a resposta<a class="anchor-link" href="#Gerar-a-resposta">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Como já temos os <code>chunk</code>s mais relevantes, passamos para o LLM, juntamente com a pergunta, para que este gere uma resposta</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">relevant_chunks</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\n\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">chunk</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">relevant_chunks</span><span class="p">])</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"You are an assistant for question-answering. You have to answer the following question:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Answer the question with the following information:</span><span class="se">\n\n</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s2">"</span>
    <span class="n">message</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span> <span class="s2">"role"</span><span class="p">:</span> <span class="s2">"user"</span><span class="p">,</span> <span class="s2">"content"</span><span class="p">:</span> <span class="n">prompt</span> <span class="p">}</span>
    <span class="p">]</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">message</span><span class="p">,</span> 
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">stream</span><span class="o">=</span><span class="n">stream</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
    <span class="k">return</span> <span class="n">response</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Testamos a função</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">top_chunks</span><span class="p">[</span><span class="s2">"documents"</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Neeva, a cloud-native, ad-free search engine founded in 2019, has leveraged Karpenter and Amazon EC2 Spot Instances to significantly improve its infrastructure management and cost optimization. Here’s how:

### Early Collaboration with Karpenter
In late 2021, Neeva began working closely with the Karpenter team, experimenting with and contributing fixes to an early version of Karpenter. This collaboration allowed Neeva to integrate Karpenter with its Kubernetes dashboard, enabling the company to gather valuable metrics on usage and performance.

### Combining Spot Instances and On-Demand Instances
Neeva runs its jobs on a large scale, which can lead to significant costs. To manage these costs effectively, the company adopted a combination of Amazon EC2 Spot Instances and On-Demand Instances. Spot Instances allow Neeva to bid on unused EC2 capacity, often at a fraction of the On-Demand price, while On-Demand Instances provide the necessary reliability for critical pipelines.

### Flexibility and Instance Diversification
According to Mohit Agarwal, infrastructure engineering lead at Neeva, Karpenter's adoption of best practices for Spot Instances, including flexibility and instance diversification, has been crucial. This approach ensures that Neeva can dynamically adjust its compute resources to meet varying workloads while minimizing costs.

### Improved Scalability and Agility
By using Karpenter to provision infrastructure resources for its Amazon EKS clusters, Neeva has achieved several key benefits:
- **Scalability**: Neeva can scale its compute resources up or down as needed, ensuring that it always has the necessary capacity to handle its workloads.
- **Agility**: The company can iterate quickly and democratize infrastructure changes, reducing the time spent on systems administration by up to 100 hours per week.

### Enhanced Development Cycles
The integration of Karpenter and Spot Instances has also accelerated Neeva's development cycles. The company can now launch new features and improvements more rapidly, which is essential for maintaining a competitive edge in the search engine market.

### Cost Savings and Budget Control
Using Spot Instances, Neeva has been able to stay within its budget while meeting its performance requirements. This cost optimization is critical for a company that prioritizes user-first experiences and has no competing incentives from advertising.

### Future Plans
Neeva is committed to continuing its innovation and expansion. The company plans to launch in new regions and further improve its search engine, all while maintaining cost efficiency. As Mohit Agarwal notes, "The bulk of our compute is or will be managed using Karpenter going forward."

### Conclusion
By leveraging Karpenter and Amazon EC2 Spot Instances, Neeva has not only optimized its infrastructure costs but also enhanced its scalability, agility, and development speed. This strategic approach has positioned Neeva to deliver high-quality, ad-free search experiences to its users while maintaining a strong focus on cost control and innovation.
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<p>Quando pedi ao LLM que me gerasse uma pergunta sobre o documento, também pedi que me gerasse a resposta correta. Esta é a resposta que o LLM me deu</p>
<div class="highlight"><pre><span></span>A Neeva utilizou o Karpenter e as Instâncias Spot da Amazon EC2 para melhorar o gerenciamento de infraestrutura e a otimização de custos de várias maneiras:

Gerenciamento de Instâncias Simplificado:

Karpenter: Ao adotar o Karpenter, a Neeva simplificou o processo de provisionamento e gerenciamento de recursos de computação para seus clusters Amazon EKS. O Karpenter provisiona e desprovisiona instâncias automaticamente com base na carga de trabalho, eliminando a necessidade de configurações manuais e reduzindo a complexidade de entender diferentes instâncias de computação.
Instâncias Spot: A Neeva aproveitou as Instâncias Spot da Amazon EC2, que são capacidades EC2 não utilizadas disponíveis com um desconto significativo (até 90% de economia de custos). Isso permitiu que a empresa controlasse os custos ao mesmo tempo em que atendia aos seus requisitos de desempenho.
Escalabilidade Aprimorada:

Karpenter: A capacidade do Karpenter de dimensionar dinamicamente os recursos permitiu que a Neeva iniciasse novas instâncias rapidamente, permitindo que a empresa iterasse com uma velocidade maior e executasse mais experimentos em menos tempo.
Instâncias Spot: O uso de Instâncias Spot forneceu flexibilidade e diversificação de instâncias, tornando mais fácil para a Neeva dimensionar seus recursos de computação de forma eficiente.
Melhoria da Produtividade:

Karpenter: Ao democratizar as alterações de infraestrutura, o Karpenter permitiu que qualquer engenheiro modificasse as configurações do Kubernetes, reduzindo a dependência de especialidades. Isso economizou para a equipe da Neeva até 100 horas por semana de tempo de espera na administração de sistemas.
Instâncias Spot: A capacidade de provisionar e desprovisionar rapidamente instâncias Spot reduziu os atrasos no pipeline de desenvolvimento, garantindo que os trabalhos não ficassem presos devido à falta de recursos disponíveis.
Eficiência de Custo:

Karpenter: As melhores práticas do Karpenter para Instâncias Spot, incluindo flexibilidade e diversificação de instâncias, ajudaram a Neeva a usar essas instâncias de forma mais eficaz, permanecendo dentro do orçamento.
Instâncias Spot: As economias de custo decorrentes do uso de Instâncias Spot permitiram que a Neeva executasse trabalhos em larga escala, como indexação, por quase o mesmo custo, mas em uma fração do tempo. Por exemplo, a Neeva reduziu seus trabalhos de indexação de 18 horas para apenas 3 horas.
Melhor Utilização de Recursos:

Karpenter: O Karpenter forneceu uma visibilidade melhor sobre o uso de recursos de computação, permitindo que a Neeva acompanhasse e otimizasse seu consumo de recursos de forma mais próxima.
Instâncias Spot: A combinação de Karpenter e Instâncias Spot permitiu que a Neeva executasse grandes modelos de linguagem de forma mais eficiente, aprimorando a experiência de pesquisa para seus usuários.
Em resumo, a adoção do Karpenter e das Instâncias Spot da Amazon EC2 pela Neeva melhorou significativamente a gestão de infraestrutura, a otimização de custos e a eficiência geral de desenvolvimento, permitindo que a empresa forneça melhores experiências de busca sem anúncios aos seus usuários.
</pre></div>
<p>E esta foi a resposta gerada pelo nosso <code>RAG</code></p>
<div class="highlight"><pre><span></span>Neeva, um mecanismo de busca nativo em nuvem e sem anúncios, fundado em 2019, aproveitou o Karpenter e as Instâncias Spot da Amazon EC2 para melhorar significativamente a gestão de sua infraestrutura e a otimização de custos. Veja como:

### Colaboração Inicial com Karpenter
No es necesario traducir a portugues, el texto dice que se debe traducir al portugues texto markdown, pero el texto proporcionado ya está en español, sin embargo, si se refiere a traducir al portugues, aquí está la traducción:

Fim de 2021, a Neeva começou a trabalhar em estreita colaboração com a equipe do Karpenter, experimentando e contribuindo com correções para uma versão inicial do Karpenter. Essa colaboração permitiu que a Neeva integrasse o Karpenter com o painel do Kubernetes, habilitando a empresa a coletar métricas valiosas sobre uso e desempenho.

### Combinando Instâncias Spot e Instâncias On-Demand
A Neeva executa seus trabalhos em larga escala, o que pode levar a custos significativos. Para gerenciar esses custos de forma eficaz, a empresa adotou uma combinação de Instâncias Spot da Amazon EC2 e Instâncias On-Demand. As Instâncias Spot permitem que a Neeva faça ofertas sobre a capacidade não utilizada da EC2, frequentemente por uma fração do preço On-Demand, enquanto as Instâncias On-Demand fornecem a confiabilidade necessária para pipelines críticos.

### Flexibilidade e Diversificação de Instâncias
De acordo com Mohit Agarwal, líder de engenharia de infraestrutura da Neeva, a adoção de Karpenter das melhores práticas para Instâncias Spot, incluindo flexibilidade e diversificação de instâncias, foi crucial. Essa abordagem garante que a Neeva possa ajustar dinamicamente seus recursos de computação para atender a cargas de trabalho variadas, minimizando os custos.

### Melhorada Escalabilidade e Agilidade
Ao utilizar o Karpenter para provisionar recursos de infraestrutura para seus clusters Amazon EKS, a Neeva alcançou vários benefícios importantes:
- **Escalabilidade**: Neeva pode dimensionar seus recursos de computação para cima ou para baixo conforme necessário, garantindo que sempre tenha a capacidade necessária para lidar com suas cargas de trabalho.
- **Agilidade**: A empresa pode iterar rapidamente e democratizar as alterações de infraestrutura, reduzindo o tempo gasto em administração de sistemas em até 100 horas por semana.

### Ciclos de Desenvolvimento Aprimorados
A integração do Karpenter e das Instâncias Spot também acelerou os ciclos de desenvolvimento da Neeva. A empresa agora pode lançar novos recursos e melhorias mais rapidamente, o que é essencial para manter uma vantagem competitiva no mercado de mecanismos de busca.

### Economia de Custo e Controle Orçamentário
Usando Instâncias Spot, a Neeva conseguiu permanecer dentro do seu orçamento ao mesmo tempo em que atende aos seus requisitos de desempenho. Essa otimização de custos é fundamental para uma empresa que prioriza experiências centradas no usuário e não tem incentivos concorrentes de publicidade.

### Planos Futuros
A Neeva está comprometida em continuar sua inovação e expansão. A empresa planeja lançar em novas regiões e melhorar ainda mais seu mecanismo de busca, mantendo ao mesmo tempo a eficiência de custos. Como Mohit Agarwal observa, "A maior parte de nosso processamento é ou será gerenciada usando Karpenter em frente."

### Conclusão
Aproveitando Karpenter e Instâncias Spot da Amazon EC2, a Neeva não apenas otimizou seus custos de infraestrutura, mas também melhorou sua escalabilidade, agilidade e velocidade de desenvolvimento. Essa abordagem estratégica posicionou a Neeva para oferecer experiências de busca de alta qualidade e sem anúncios aos seus usuários, mantendo um forte foco no controle de custos e inovação.
</pre></div>
<p>Por lo que podemos concluir que el <code>RAG</code> ha funcionado correctamente!!!</p>
<p>Debería ser:</p>
<p>Por lo que podemos concluir que o <code>RAG</code> funcionou corretamente!!!</p>
</section>
