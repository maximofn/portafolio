<section class="section-block-markdown-cell">
<h1 id="Hugging-Face-Inference-Providers">Hugging Face Inference Providers<a class="anchor-link" href="#Hugging-Face-Inference-Providers">¶</a></h1>
</section>
<section class="section-block-markdown-cell">
<blockquote>
<p>Disclaimer: This post has been translated to English using a machine translation model. Please, let me know if you find any mistakes.</p>
</blockquote>
</section>
<section class="section-block-markdown-cell">
<p>It is clear that the largest hub of AI models is Hugging Face. And now they are offering the possibility to perform inference on some of their models using serverless GPU providers.</p>
</section>
<section class="section-block-markdown-cell">
<p>One of those models is <a href="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B">Wan-AI/Wan2.1-T2V-14B</a>, which as of writing this post is the best open-source video generation model, as can be seen in the <a href="https://artificialanalysis.ai/text-to-video/arena?tab=Leaderboard">Artificial Analysis Video Generation Arena Leaderboard</a>
<img alt="video generation arena leaderboard" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Video-arena-leaderboard-wan21.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<p>If we look at its <a href="https://huggingface.co/Wan-AI/Wan2.1-T2V-14B">modelcard</a> we can see on the right a button that says <code>Replicate</code>.
<img alt="Wan2.1-T2V-14B modelcard" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Wan2.1-T2V-14B.webp"/></p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Inference-providers">Inference providers<a class="anchor-link" href="#Inference-providers">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>If we go to the <a href="https://huggingface.co/settings/inference-providers">Inference providers</a> settings page, we will see something like this
<img alt="Inference Providers" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/Inference%20Providers.webp"/>
Where we can press the button with a key to enter the API KEY of the provider we want to use, or leave selected the path with two dots. If we choose the first option, it will be the provider who charges us for the inference, while in the second option, it will be Hugging Face who charges us for the inference. So do what suits you best.</p>
</section>
<section class="section-block-markdown-cell">
<h2 id="Inference-with-Replicate">Inference with Replicate<a class="anchor-link" href="#Inference-with-Replicate">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>In my case, I obtained an API KEY from Replicate and added it to a file called <code>.env</code>, which is where I will store the API KEYS and which you should not upload to GitHub, GitLab, or your project repository.
The <code>.env</code> must have this format</p>
<div class="highlight"><pre><span></span><span class="n">HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS</span><span class="o">=</span><span class="s2">"hf_aL...AY"</span><span class="n">REPLICATE_API_KEY</span><span class="o">=</span><span class="s2">"r8_Sh...UD"</span><span class="err">```</span>

<span class="n">Where</span> <span class="err">`</span><span class="n">HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS</span><span class="err">`</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">token</span> <span class="n">you</span> <span class="n">need</span> <span class="n">to</span> <span class="n">obtain</span> <span class="kn">from</span> <span class="p">[</span><span class="n">Hugging</span> <span class="n">Face</span><span class="p">](</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">huggingface</span><span class="o">.</span><span class="n">co</span><span class="o">/</span><span class="n">settings</span><span class="o">/</span><span class="n">tokens</span><span class="p">)</span> <span class="ow">and</span> <span class="err">`</span><span class="n">REPLICATE_API_KEY</span><span class="err">`</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">API</span> <span class="n">KEY</span> <span class="n">of</span> <span class="n">Replicate</span> <span class="n">which</span> <span class="n">you</span> <span class="n">can</span> <span class="n">obtain</span> <span class="kn">from</span> <span class="p">[</span><span class="n">Replicate</span><span class="p">](</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">replicate</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">account</span><span class="o">/</span><span class="n">api</span><span class="o">-</span><span class="n">tokens</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Reading-the-API-Keys">Reading the API Keys<a class="anchor-link" href="#Reading-the-API-Keys">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>The first thing we need to do is read the API KEYS from the <code>.env</code> file</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">dotenv</span>
<span class="n">dotenv</span><span class="o">.</span><span class="n">load_dotenv</span><span class="p">()</span>

<span class="n">REPLICATE_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"REPLICATE_API_KEY"</span><span class="p">)</span>
<span class="n">HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">"HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS"</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Logging-in-the-Hugging-Face-hub">Logging in the Hugging Face hub<a class="anchor-link" href="#Logging-in-the-Hugging-Face-hub">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>To be able to use the Wan-AI/Wan2.1-T2V-14B model, as it is on the Hugging Face hub, we need to log in.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
<span class="n">login</span><span class="p">(</span><span class="n">HUGGINGFACE_TOKEN_INFERENCE_PROVIDERS</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Inference-Client">Inference Client<a class="anchor-link" href="#Inference-Client">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Now we create an inference client, we have to specify the provider, the API KEY and in this case, additionally, we are going to set a <code>timeout</code> of 1000 seconds, because by default it is 60 seconds and the model takes quite a while to generate the video.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">InferenceClient</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">InferenceClient</span><span class="p">(</span>
	<span class="n">provider</span><span class="o">=</span><span class="s2">"replicate"</span><span class="p">,</span>
	<span class="n">api_key</span><span class="o">=</span><span class="n">REPLICATE_API_KEY</span><span class="p">,</span>
	<span class="n">timeout</span><span class="o">=</span><span class="mi">1000</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Video-Generation">Video Generation<a class="anchor-link" href="#Video-Generation">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>We already have everything to generate our video. We use the <code>text_to_video</code> method of the client, pass it the prompt, and tell it which model from the hub we want to use; if not, it will use the default one.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">video</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">text_to_video</span><span class="p">(</span>
	<span class="s2">"Funky dancer, dancing in a rehearsal room. She wears long hair that moves to the rhythm of her dance."</span><span class="p">,</span>
	<span class="n">model</span><span class="o">=</span><span class="s2">"Wan-AI/Wan2.1-T2V-14B"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section class="section-block-markdown-cell">
<h3 id="Saving-the-video">Saving the video<a class="anchor-link" href="#Saving-the-video">¶</a></h3>
</section>
<section class="section-block-markdown-cell">
<p>Finally, we save the video, which is of type <code>bytes</code>, to a file on our disk.</p>
</section>
<section class="section-block-code-cell-">
<div class="input-code">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">output_path</span> <span class="o">=</span> <span class="s2">"output_video.mp4"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Video saved to: </span><span class="si">{</span><span class="n">output_path</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
<div class="output-wrapper">
<div class="output-area">
<div class="prompt"></div>
<div class="output-subarea-output-stream-output-stdout-output-text">
<pre>Video saved to: output_video.mp4
</pre>
</div>
</div>
</div>
</section>
<section class="section-block-markdown-cell">
<h2 id="Generated-video">Generated video<a class="anchor-link" href="#Generated-video">¶</a></h2>
</section>
<section class="section-block-markdown-cell">
<p>This is the video generated by the model
<video controls="" src="https://pub-fb664c455eca46a2ba762a065ac900f7.r2.dev/wan2_1_video.webm"></video></p>
</section>
